{"id": "keras-32", "project": "keras", "bug_id": "32", "buggy_code": "epsilon: threshold for measuring the new optimum,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n        self.epsilon = epsilon\n            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\n                self.wait += 1", "fixed_code": "min_delta: threshold for measuring the new optimum,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n        self.min_delta = min_delta\n            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n                self.wait += 1", "description": ""}
{"id": "keras-35", "project": "keras", "bug_id": "35", "buggy_code": "if self.preprocessing_function:\n            x = self.preprocessing_function(x)\n                           target_size=self.target_size,", "fixed_code": "if self.image_data_generator.preprocessing_function:\n                x = self.image_data_generator.preprocessing_function(x)\n                           target_size=None,\n            if self.image_data_generator.preprocessing_function:\n                img = self.image_data_generator.preprocessing_function(img)\n            if self.target_size is not None:\n                width_height_tuple = (self.target_size[1], self.target_size[0])\n                if img.size != width_height_tuple:\n                    if self.interpolation not in _PIL_INTERPOLATION_METHODS:\n                        raise ValueError(\n                            'Invalid interpolation method {} specified. Supported '\n                            'methods are {}'.format(\n                                self.interpolation,\n                                \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n                    resample = _PIL_INTERPOLATION_METHODS[self.interpolation]\n                    img = img.resize(width_height_tuple, resample)", "description": ""}
{"id": "keras-34", "project": "keras", "bug_id": "34", "buggy_code": "validation_generator = validation_data\n                output_generator = generator\n                output_generator = generator\n                output_generator = generator", "fixed_code": "if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n    def __iter__(self):\n        \"\"\"Create an infinite generator that iterate over the Sequence.\"\"\"\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item", "description": ""}
{"id": "keras-33", "project": "keras", "bug_id": "33", "buggy_code": "if sys.version_info < (3,) and isinstance(text, unicode):\n        translate_map = dict((ord(c), unicode(split)) for c in filters)\n        translate_map = maketrans(filters, split * len(filters))\n    text = text.translate(translate_map)", "fixed_code": "if sys.version_info < (3,):\n        if isinstance(text, unicode):\n            translate_map = dict((ord(c), unicode(split)) for c in filters)\n            text = text.translate(translate_map)\n        elif len(split) == 1:\n            translate_map = maketrans(filters, split * len(filters))\n            text = text.translate(translate_map)\n        else:\n            for c in filters:\n                text = text.replace(c, split)\n        translate_dict = dict((c, split) for c in filters)\n        translate_map = maketrans(translate_dict)\n        text = text.translate(translate_map)", "description": ""}
{"id": "keras-20", "project": "keras", "bug_id": "20", "buggy_code": "padding='valid', data_format=None):\n        output_shape=output_shape)\ndef _preprocess_conv2d_input(x, data_format):\n        if not _has_nchw_support():\n                     padding='valid', data_format=None):\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n    x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n                               padding=padding,\n                               data_format=tf_data_format)\n                     padding='valid', data_format=None):\n        in inputs/kernels/outputs.\n                                                        filter_flip=not flip_filters)\n                                              out_pad_h)\n                                             out_pad_w)\n            data_format=self.data_format)\n                                                        out_pad_h)\n                                                        out_pad_w)\n        config.pop('dilation_rate')\ndef deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):", "fixed_code": "padding='valid', data_format=None, dilation_rate=(1, 1)):\n        output_shape=output_shape,\n        dilation=dilation_rate)\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n        if not _has_nchw_support() or force_transpose:\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n        dilation_rate: tuple of 2 integers.\n    # tf.nn.atrous_conv2d_transpose input only supports NHWC format\n    if data_format == 'channels_first' and dilation_rate != (1, 1):\n        force_transpose = True\n    else:\n        force_transpose = False\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)\n    if dilation_rate == (1, 1):\n        x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n                                   padding=padding,\n                                   data_format=tf_data_format)\n    else:\n        assert dilation_rate[0] == dilation_rate[1]\n        x = tf.nn.atrous_conv2d_transpose(\n            x, kernel, output_shape, dilation_rate[0], padding)\n\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n            in inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n                 dilation_rate=(1, 1),\n            dilation_rate=dilation_rate,\n                                              out_pad_h,\n                                              self.dilation_rate[0])\n                                             out_pad_w,\n                                             self.dilation_rate[1])\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate)\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\ndef deconv_length(dim_size, stride_size, kernel_size, padding,\n                  output_padding, dilation=1):\n        dilation: dilation rate, integer.\n    # Get the dilated kernel size\n    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)", "description": ""}
{"id": "keras-18", "project": "keras", "bug_id": "18", "buggy_code": "self.session_kwargs = session_kwargs\n        fetched = self._callable_fn(*array_vals)", "fixed_code": "# self.session_kwargs is used for _legacy_call\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n        # Handle run_options.\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n        if self.run_metadata:\n            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n        else:\n            fetched = self._callable_fn(*array_vals)\n            # callable generated by Session._make_callable_from_options accepts\n            # `run_metadata` keyword argument since TF 1.10\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)", "description": ""}
{"id": "keras-9", "project": "keras", "bug_id": "9", "buggy_code": "block = docstring[starting_point:(None if ending_point == -1 else\n                                      ending_point - 1)]", "fixed_code": "block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else\n                                      section_end)]", "description": ""}
{"id": "keras-11", "project": "keras", "bug_id": "11", "buggy_code": "is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        if is_sequence:\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n                if isinstance(val_data, Sequence):\n                if isinstance(val_data, Sequence):\n            if is_sequence:\n            if is_sequence:\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        if is_sequence:\n            if is_sequence:\n            if is_sequence:\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n        if is_sequence:\n            if is_sequence:\n            if is_sequence:\n    K.backend() == 'tensorflow',\n    K.backend() == 'tensorflow',", "fixed_code": "from .training_utils import is_sequence\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n        if use_sequence_api:\n    val_use_sequence_api = is_sequence(validation_data)\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n                if is_sequence(val_data):\n                if is_sequence(val_data):\n            if use_sequence_api:\n            if use_sequence_api:\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n        if use_sequence_api:\n            if use_sequence_api:\n            if use_sequence_api:\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n        if use_sequence_api:\n            if use_sequence_api:\n            if use_sequence_api:\nfrom ..utils import Sequence\n\n\ndef is_sequence(seq):\n    \"\"\"Determine if an object follows the Sequence API.\n\n    # Arguments\n        seq: a possible Sequence object\n\n    # Returns\n        boolean, whether the object follows the Sequence API.\n    \"\"\"\n    # TODO Dref360: Decide which pattern to follow. First needs a new TF Version.\n    return (getattr(seq, 'use_sequence_api', False)\n            or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))\n    use_sequence_api = True\n\n    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,\n    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,", "description": ""}
{"id": "keras-7", "project": "keras", "bug_id": "7", "buggy_code": "return np.squeeze(self.model.predict(x, **kwargs))", "fixed_code": "return np.squeeze(self.model.predict(x, **kwargs), axis=-1)", "description": ""}
{"id": "keras-29", "project": "keras", "bug_id": "29", "buggy_code": "for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n            for i, m in enumerate(self.metrics):\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()", "fixed_code": "self.stateful_metric_functions = []\n                            self.stateful_metric_functions.append(metric_fn)\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n            for m in self.stateful_metric_functions:\n                m.reset_states()", "description": ""}
{"id": "keras-16", "project": "keras", "bug_id": "16", "buggy_code": "if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n        config = []\n            config.append({\n        return copy.deepcopy(config)\n        model = cls()\n        for conf in config:", "fixed_code": "self._build_input_shape = None\n            self._build_input_shape = input_shape\n        layer_configs = []\n            layer_configs.append({\n        config = {\n            'name': self.name,\n            'layers': copy.deepcopy(layer_configs)\n        }\n        if self._build_input_shape:\n            config['build_input_shape'] = self._build_input_shape\n        return config\n        if 'name' in config:\n            name = config['name']\n            build_input_shape = config.get('build_input_shape')\n            layer_configs = config['layers']\n        model = cls(name=name)\n        for conf in layer_configs:\n        if not model.inputs and build_input_shape:\n            model.build(build_input_shape)", "description": ""}
{"id": "keras-42", "project": "keras", "bug_id": "42", "buggy_code": "steps_per_epoch,\n                divided by the batch size. Not used if using `Sequence`.\n        if val_gen and not validation_steps:\n            raise ValueError('When using a generator for validation data, '\n                             'you must specify a value for '\n                             '`validation_steps`.')\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if is_sequence:\n            steps_per_epoch = len(generator)\n    def evaluate_generator(self, generator, steps,\n                Not used if using Sequence.\n        if is_sequence:\n            steps = len(generator)\n    def predict_generator(self, generator, steps,\n                Not used if using Sequence.\n        if is_sequence:\n            steps = len(generator)\n                      steps_per_epoch,\n    def evaluate_generator(self, generator, steps,\n    def predict_generator(self, generator, steps,", "fixed_code": "steps_per_epoch=None,\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps_per_epoch is None:\n            if is_sequence:\n                steps_per_epoch = len(generator)\n            else:\n                raise ValueError('`steps_per_epoch=None` is only valid for a'\n                                 ' generator based on the `keras.utils.Sequence`'\n                                 ' class. Please specify `steps_per_epoch` or use'\n                                 ' the `keras.utils.Sequence` class.')\n\n        if (val_gen and not isinstance(validation_data, Sequence) and\n                not validation_steps):\n            raise ValueError('`validation_steps=None` is only valid for a'\n                             ' generator based on the `keras.utils.Sequence`'\n                             ' class. Please specify `validation_steps` or use'\n                             ' the `keras.utils.Sequence` class.')\n    def evaluate_generator(self, generator, steps=None,\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n    def predict_generator(self, generator, steps=None,\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n                      steps_per_epoch=None,\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n    def evaluate_generator(self, generator, steps=None,\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n    def predict_generator(self, generator, steps=None,\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.", "description": ""}
{"id": "keras-45", "project": "keras", "bug_id": "45", "buggy_code": "x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o", "fixed_code": "x_i = K.dot(inputs_i, self.kernel_i)\n            x_f = K.dot(inputs_f, self.kernel_f)\n            x_c = K.dot(inputs_c, self.kernel_c)\n            x_o = K.dot(inputs_o, self.kernel_o)\n            if self.use_bias:\n                x_i = K.bias_add(x_i, self.bias_i)\n                x_f = K.bias_add(x_f, self.bias_f)\n                x_c = K.bias_add(x_c, self.bias_c)\n                x_o = K.bias_add(x_o, self.bias_o)", "description": ""}
{"id": "keras-6", "project": "keras", "bug_id": "6", "buggy_code": "score_array /= K.mean(mask)", "fixed_code": "score_array /= K.mean(mask) + K.epsilon()", "description": ""}
{"id": "keras-28", "project": "keras", "bug_id": "28", "buggy_code": "(self.end_index - self.start_index) /\n                self.start_index, self.end_index, size=self.batch_size)\n                                    self.stride, self.end_index), self.stride)", "fixed_code": "if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n\n            (self.end_index - self.start_index + 1) /\n                self.start_index, self.end_index + 1, size=self.batch_size)\n                                    self.stride, self.end_index + 1), self.stride)", "description": ""}
{"id": "keras-17", "project": "keras", "bug_id": "17", "buggy_code": "return K.cast(K.equal(K.max(y_true, axis=-1),", "fixed_code": "# flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\n    return K.cast(K.equal(K.flatten(y_true),", "description": ""}
{"id": "keras-1", "project": "keras", "bug_id": "1", "buggy_code": "return tf_state_ops.assign(x, new_x)\n    return tf_state_ops.assign_add(x, increment)\n    return tf_state_ops.assign_sub(x, decrement)\n    # TODO\n    return tf.Print(x, [x], message)\n    \"\"\"Print the message and the tensor when evaluated and return the same\n    tensor.\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        return K.random_uniform(shape, self.minval, self.maxval,\n                                dtype=dtype, seed=self.seed)\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n    def test_print_tensor(self):\n        check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n        check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)\n        x = np.random.randn(3, 4)\n        increment = np.random.randn(3, 4)\n        x += increment\n        K.eval(K.update_add(x_var, increment))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n        x = np.random.randn(3, 4)\n        decrement = np.random.randn(3, 4)\n        x -= decrement\n        K.eval(K.update_sub(x_var, decrement))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n        # test standard normal as well as a normal with a different set of parameters\n            rand = K.eval(K.random_normal((300, 200),\n                                          mean=mean, stddev=std, seed=1337))\n            assert rand.shape == (300, 200)\n            # test that random_normal also generates different values when used\n            # within a function\n            r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n            samples = np.array([K.eval(r) for _ in range(200)])\n            assert np.abs(np.mean(samples) - mean) < std * 0.015\n            assert np.abs(np.std(samples) - std) < std * 0.015\n\n        rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n        assert rand.shape == (200, 100)\n        r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples)) < 0.015\n        assert max_val - 0.015 < np.max(samples) <= max_val\n        assert min_val + 0.015 > np.min(samples) >= min_val\n\n        rand = K.eval(K.random_binomial((200, 100), p))\n        assert rand.shape == (200, 100)\n        r = K.random_binomial((10, 10), p)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - p) < 0.015\n        assert np.max(samples) == 1\n        assert np.min(samples) == 0\n\n        rand = K.eval(K.truncated_normal((300, 200),\n                                         mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads", "fixed_code": "import sys\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n    op = tf_state_ops.assign_add(x, increment)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n@symbolic\n    op = tf.print(message, x, output_stream=sys.stdout)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n    \"\"\"Print the message & the tensor when evaluated & return the same tensor.\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n        x = K.random_uniform(shape, self.minval, self.maxval,\n                             dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n            self.seed += 1\n    def test_print_tensor(self, capsys):\n        for k in [KTH, KTF]:\n            x = k.placeholder((1, 1))\n            y = k.print_tensor(x, 'msg')\n            fn = k.function([x], [y])\n            _ = fn([np.ones((1, 1))])\n            out, err = capsys.readouterr()\n            # Theano inserts \"__str__ = \" for no good reason\n            assert out.replace('__str__ = ', '') == 'msg [[1.]]\\n'\n\n    @pytest.mark.skipif(K.backend() == 'theano',\n                        reason='theano returns tuples for update ops')\n    def test_update(self):\n        x = np.ones((3, 4))\n        x_var = K.variable(x)\n        new_x = np.random.random((3, 4))\n\n        op = K.update(x_var, new_x)\n        K.eval(op)\n\n        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n\n        x = np.ones((3, 4))\n        increment = np.random.random((3, 4))\n        op = K.update_add(x_var, increment)\n        K.eval(op)\n        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n        x = np.ones((3, 4))\n        decrement = np.random.random((3, 4))\n        op = K.update_sub(x_var, decrement)\n        K.eval(op)\n        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n        # TODO: make this a parameterized test\n            rand = K.eval(K.random_normal((200, 200),\n                                          mean=mean,\n                                          stddev=std))\n            assert rand.shape == (200, 200)\n        rand = K.eval(K.random_uniform((200, 200), min_val, max_val))\n        assert rand.shape == (200, 200)\n        rand = K.eval(K.random_binomial((200, 200), p))\n        assert rand.shape == (200, 200)\n        rand = K.eval(K.truncated_normal((200, 200),\n                                         mean=mean,\n                                         stddev=std))\n        assert rand.shape == (200, 200)", "description": ""}
{"id": "keras-10", "project": "keras", "bug_id": "10", "buggy_code": "weight array.\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n\n        return sample_weight\n    elif isinstance(class_weight, dict):\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n        return weights\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())", "fixed_code": "weight array. If both `sample_weights` and `class_weights` are provided,\n    the weights are multiplied together.\n\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n\n    # Everything has weight 1 by default.\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())", "description": ""}
{"id": "keras-19", "project": "keras", "bug_id": "19", "buggy_code": "new_output = n_s[0]\n        # States are a flat list\n        # in reverse order of the cell stack.\n        # This allows to preserve the requirement\n        # `stack.state_size[0] == output_dim`.\n        # e.g. states of a 2-layer LSTM would be\n        # `[h2, c2, h1, c1]`\n        for cell in self.cells[::-1]:\n        for cell in self.cells[::-1]:\n        nested_states = nested_states[::-1]\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n            if hasattr(cell.state_size, '__len__'):\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n        output_dim = state_size[0]", "fixed_code": "new_output = n_s[-1]\n        # reverse_state_order determines whether the state size will be in a\n        # reverse order of the cells' state. User might want to set this to True\n        # to keep the existing behavior. This is only useful when use\n        # `RNN(return_state=True)` since the state will be returned as the same\n        # order of state_size.\n        self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n        if self.reverse_state_order:\n            warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '\n                          'will soon be deprecated. Please update the code to '\n                          'work with the natural order of states if you '\n                          'reply on the RNN states, '\n                          'eg `RNN(return_state=True)`.')\n        # States are a flat list of the individual cell state size.\n        # e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.\n        # In the case of reverse_state_order=True, the state_size will be\n        # `[h2, c2, h1, c1]`.\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n    @property\n    def output_size(self):\n        if getattr(self.cells[-1], 'output_size', None) is not None:\n            return self.cells[-1].output_size\n        if hasattr(self.cells[-1].state_size, '__len__'):\n            return self.cells[-1].state_size[0]\n        else:\n            return self.cells[-1].state_size\n\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n        if self.reverse_state_order:\n            nested_states = nested_states[::-1]\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n        self.output_size = self.units\n        self.output_size = self.units\n        self.output_size = self.units", "description": ""}
{"id": "keras-26", "project": "keras", "bug_id": "26", "buggy_code": "new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]", "fixed_code": "new_states = [\n                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n                             new_states[i], states[i]) for i in range(len(states))\n                ]", "description": ""}
{"id": "keras-8", "project": "keras", "bug_id": "8", "buggy_code": "add_unprocessed_node(layer, node_data)\n                    return\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)", "fixed_code": "\"\"\"Add node to layer list\n\n            Args:\n                layer: layer object\n                node_data: Node data specifying layer call\n            \"\"\"\n            \"\"\"Reconstruct node by linking to inbound layers\n\n            Args:\n                layer: Layer to process\n                node_data: List of layer configs\n\n            Raises:\n                ValueError: For incorrect layer config\n                LookupError: If layer required is not found\n            \"\"\"\n                # Raise an error if the corresponding layer node\n                # has not yet been created\n                    raise LookupError\n\n\n\n                # Process all nodes in layer, if not yet processed\n                    node_data_list = unprocessed_nodes[layer]\n\n                    # Process nodes in order\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n\n                        # If the node does not have all inbound layers\n                        # available, stop processing and continue later\n                        except LookupError:\n                            break\n\n                        node_index += 1\n\n                    # If not all nodes processed then store unprocessed nodes\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    # If all nodes processed remove the layer\n                    else:\n                        del unprocessed_nodes[layer]\n        # Create lits of input and output tensors and return new class", "description": ""}
{"id": "keras-21", "project": "keras", "bug_id": "21", "buggy_code": "baseline=None):", "fixed_code": "restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n                 baseline=None,\n                 restore_best_weights=False):\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)", "description": ""}
{"id": "keras-38", "project": "keras", "bug_id": "38", "buggy_code": "input_shape = (input_shape[0], input_shape[1], output_dim)", "fixed_code": "input_shape = (input_shape[0], output_dim)", "description": ""}
{"id": "keras-36", "project": "keras", "bug_id": "36", "buggy_code": "strides = (1, 1) + strides + (1,)\n        strides = (1, 1, 1) + strides", "fixed_code": "strides = (1,) + strides * 2 + (1,)\n        strides = (1, 1) + strides * 2", "description": ""}
{"id": "keras-31", "project": "keras", "bug_id": "31", "buggy_code": "label_length = tf.to_int32(tf.squeeze(label_length))\n    input_length = tf.to_int32(tf.squeeze(input_length))", "fixed_code": "label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))", "description": ""}
{"id": "keras-30", "project": "keras", "bug_id": "30", "buggy_code": "if isinstance(x, list):\n                if isinstance(x, list):", "fixed_code": "if x is None or len(x) == 0:\n                        # Handle data tensors support when no input given\n                        # step-size = 1 for data tensors\n                        batch_size = 1\n                    elif isinstance(x, list):\n                if x is None or len(x) == 0:\n                    # Handle data tensors support when no input given\n                    # step-size = 1 for data tensors\n                    batch_size = 1\n                elif isinstance(x, list):", "description": ""}
{"id": "keras-37", "project": "keras", "bug_id": "37", "buggy_code": "is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                                 ' Keras tensors and non-Keras tensors')\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))", "fixed_code": "is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n        self.input_spec = layer.input_spec\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        if isinstance(inputs, list):\n            if len(inputs) > 1:\n                initial_state = inputs[1:]\n            inputs = inputs[0]\n\n        if initial_state is None:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)\n\n        # Standardize `initial_state` into list\n        if isinstance(initial_state, tuple):\n            initial_state = list(initial_state)\n        elif not isinstance(initial_state, list):\n            initial_state = [initial_state]\n\n        # Check if `initial_state` can be splitted into half\n        num_states = len(initial_state)\n        if num_states % 2 > 0:\n            raise ValueError(\n                'When passing `initial_state` to a Bidirectional RNN, the state '\n                'should be a list containing the states of the underlying RNNs. '\n                'Found: ' + str(initial_state))\n\n        # Applies the same workaround as in `RNN.__call__`, without handling constants\n        kwargs['initial_state'] = initial_state\n        additional_inputs = initial_state\n        additional_specs = [InputSpec(shape=K.int_shape(state))\n                            for state in initial_state]\n        self.forward_layer.state_spec = additional_specs[:num_states // 2]\n        self.backward_layer.state_spec = additional_specs[num_states // 2:]\n\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                raise ValueError('The initial state of a Bidirectional'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state\n            full_input = [inputs] + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n\n            # Perform the call with temporarily replaced input_spec\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(Bidirectional, self).__call__(full_input, **kwargs)\n            self.input_spec = original_input_spec\n            return output\n        else:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)", "description": ""}
{"id": "keras-39", "project": "keras", "bug_id": "39", "buggy_code": "current < self.target):", "fixed_code": "(self.target is not None and current < self.target)):", "description": ""}
{"id": "keras-41", "project": "keras", "bug_id": "41", "buggy_code": "raise StopIteration(e)\n                        self.queue.put(generator_output)\n                except Exception:\n                    raise\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n                   np.random.randint(batch_size, 2, 50))\n    with pytest.raises(StopIteration):\n    with pytest.raises(StopIteration):\n                   np.random.randint(batch_size, 2, 50))\n    with pytest.raises(StopIteration):\n            custom_generator(), good_batches + 1, 1,\n            workers=4, use_multiprocessing=True,\n    with pytest.raises(StopIteration):\n    with pytest.raises(StopIteration):\n    with pytest.raises(StopIteration):", "fixed_code": "import traceback\n            six.raise_from(StopIteration(e), e)\n        self._manager = None\n                        self.queue.put((True, generator_output))\n                except Exception as e:\n                    # Can't pick tracebacks.\n                    # As a compromise, print the traceback and pickle None instead.\n                    if self._use_multiprocessing:\n                        traceback.print_exc()\n                        setattr(e, '__traceback__', None)\n                    elif not hasattr(e, '__traceback__'):\n                        setattr(e, '__traceback__', sys.exc_info()[2])\n                    self.queue.put((False, e))\n                    break\n                self._manager = multiprocessing.Manager()\n                self.queue = self._manager.Queue(maxsize=max_queue_size)\n        if self._manager:\n            self._manager.shutdown()\n                success, value = self.queue.get()\n                # Rethrow any exceptions found in the queue\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                # Yield regular values\n                if value is not None:\n                    yield value\n\n        # Make sure to rethrow the first exception in the queue, if any\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)\n                   np.random.randint(batch_size, 12, 50))\n    with pytest.raises(RuntimeError):\n    with pytest.raises(RuntimeError):\n    workers = 4\n                   np.random.randint(batch_size, 12, 50))\n    with pytest.raises(RuntimeError):\n            custom_generator(), good_batches * workers + 1, 1,\n            workers=workers, use_multiprocessing=True,\n    with pytest.raises(RuntimeError):\n    with pytest.raises(RuntimeError):\n    with pytest.raises(RuntimeError):", "description": ""}
{"id": "keras-24", "project": "keras", "bug_id": "24", "buggy_code": "tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)", "fixed_code": "if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)", "description": ""}
{"id": "keras-4", "project": "keras", "bug_id": "4", "buggy_code": "grads = self.optimizer.compute_gradients(loss, params)", "fixed_code": "grads = self.optimizer.compute_gradients(loss, var_list=params)", "description": ""}
{"id": "keras-15", "project": "keras", "bug_id": "15", "buggy_code": "self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n            self.csv_file = open(self.filename, 'w' + self.file_flags)\n\n                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)", "fixed_code": "import io\nimport sys\n        if six.PY2:\n            self.file_flags = 'b'\n            self._open_args = {}\n        else:\n            self.file_flags = ''\n            self._open_args = {'newline': '\\n'}\n            mode = 'a'\n            mode = 'w'\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n            fieldnames = ['epoch'] + self.keys\n            if six.PY2:\n                fieldnames = [unicode(x) for x in fieldnames]\n                                         fieldnames=fieldnames,\n                                         dialect=CustomDialect)", "description": ""}
{"id": "keras-3", "project": "keras", "bug_id": "3", "buggy_code": "output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))", "fixed_code": "if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)", "description": ""}
{"id": "keras-40", "project": "keras", "bug_id": "40", "buggy_code": "output_dim = self.cell.state_size[0]\n            output_dim = self.cell.state_size\n            state_shape = [(input_shape[0], output_dim) for _ in self.states]", "fixed_code": "state_size = self.cell.state_size\n            state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n            state_shape = [(input_shape[0], dim) for dim in state_size]", "description": ""}
{"id": "keras-13", "project": "keras", "bug_id": "13", "buggy_code": "val_enqueuer_gen = iter_sequence_infinite(generator)", "fixed_code": "val_enqueuer_gen = iter_sequence_infinite(val_data)\n                    validation_steps = validation_steps or len(val_data)", "description": ""}
{"id": "keras-5", "project": "keras", "bug_id": "5", "buggy_code": "cache_dir = os.path.join(os.path.expanduser('~'), '.keras')", "fixed_code": "if 'KERAS_HOME' in os.environ:\n            cache_dir = os.environ.get('KERAS_HOME')\n        else:\n            cache_dir = os.path.join(os.path.expanduser('~'), '.keras')", "description": ""}
{"id": "keras-14", "project": "keras", "bug_id": "14", "buggy_code": "return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),", "fixed_code": "# If the shape of y_true is (num_samples, 1), flatten to (num_samples,)\n    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),", "description": ""}
{"id": "thefuck-32", "project": "thefuck", "bug_id": "32", "buggy_code": "return 'ls' in command.script and not ('ls -' in command.script)", "fixed_code": "return (command.script == 'ls'\n            or command.script.startswith('ls ')\n            and not ('ls -' in command.script))", "description": ""}
{"id": "thefuck-20", "project": "thefuck", "bug_id": "20", "buggy_code": "for c in command.script.split()[1:]:\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])", "fixed_code": "from thefuck.shells import quote\n    for c in command.split_script[1:]:\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))", "description": ""}
{"id": "thefuck-27", "project": "thefuck", "bug_id": "27", "buggy_code": "return 'open http://' + command.script[5:]", "fixed_code": "return command.script.replace('open ', 'open http://')", "description": ""}
{"id": "thefuck-9", "project": "thefuck", "bug_id": "9", "buggy_code": "command.script_parts.pop(upstream_option_index)", "fixed_code": "try:\n            command.script_parts.pop(upstream_option_index)\n        except IndexError:\n            # This happens for `git push -u`\n            pass", "description": ""}
{"id": "thefuck-11", "project": "thefuck", "bug_id": "11", "buggy_code": "return replace_argument(command.script, 'push', push_upstream)", "fixed_code": "# If --set-upstream or -u are passed, remove it and its argument. This is\n    # because the remaining arguments are concatenated onto the command suggested\n    # by git, which includes --set-upstream and its argument\n    upstream_option_index = -1\n    try:\n        upstream_option_index = command.script_parts.index('--set-upstream')\n    except ValueError:\n        pass\n    try:\n        upstream_option_index = command.script_parts.index('-u')\n    except ValueError:\n        pass\n    if upstream_option_index is not -1:\n        command.script_parts.pop(upstream_option_index)\n        command.script_parts.pop(upstream_option_index)\n\n    return replace_argument(\" \".join(command.script_parts), 'push', push_upstream)", "description": ""}
{"id": "thefuck-7", "project": "thefuck", "bug_id": "7", "buggy_code": "return \"php -s\" in command.script\n\n\nrequires_output = False", "fixed_code": "return \" -s \" in command.script", "description": ""}
{"id": "thefuck-29", "project": "thefuck", "bug_id": "29", "buggy_code": "\"\"\"Returns new settings with new values from `kwargs`.\"\"\"\n        conf = dict(self)\n        conf.update(kwargs)", "fixed_code": "\"\"\"\n        Returns new settings with values from `kwargs` for unset settings.\n        \"\"\"\n        conf = dict(kwargs)\n        conf.update(self)", "description": ""}
{"id": "thefuck-16", "project": "thefuck", "bug_id": "16", "buggy_code": "alias = \"TF_ALIAS={0}\" \\\n                \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n        alias = \"alias {0}='TF_ALIAS={0}\" \\\n                ' TF_SHELL_ALIASES=$(alias)' \\\n                \" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n            os.environ.get('PYTHONIOENCODING', '>-not-set-<')))", "fixed_code": "# It is VERY important to have the variables declared WITHIN the alias\n        alias = \"alias {0}='TF_CMD=$(TF_ALIAS={0}\" \\\n                \" PYTHONIOENCODING=utf-8\" \\\n                \" TF_SHELL_ALIASES=$(alias)\" \\\n                \" thefuck $(fc -ln -1)) &&\" \\\n        # It is VERY important to have the variables declared WITHIN the alias\n        # It is VERY important to have the variables declared WITHIN the alias\n        alias = \"alias {0}='TF_CMD=$(TF_ALIAS={0}\" \\\n                \" TF_SHELL_ALIASES=$(alias)\" \\\n                \" thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n            os.environ.get('PYTHONIOENCODING', '!!not-set!!')))", "description": ""}
{"id": "thefuck-6", "project": "thefuck", "bug_id": "6", "buggy_code": "and \" already exists.\" in command.output)\n        r\"fatal: A branch named '([^']*)' already exists.\", command.output)[0]", "fixed_code": "and \"' already exists.\" in command.output)\n        r\"fatal: A branch named '(.+)' already exists.\", command.output)[0]\n    branch_name = branch_name.replace(\"'\", r\"\\'\")", "description": ""}
{"id": "thefuck-28", "project": "thefuck", "bug_id": "28", "buggy_code": "from thefuck.utils import memoize\n        '^{file} \\(line {line}\\):',\n        # ghc, make, ruby, zsh:\n        '^{file}:{line}:',\n    # ignored for now\n    editor_call = '{} {} +{}'.format(os.environ['EDITOR'],\n                                     m.group('file'),\n                                     m.group('line'))", "fixed_code": "from thefuck.utils import memoize, wrap_settings\n# order is important: only the first match is considered\n        '^{file} \\\\(line {line}\\\\):',\n        # ghc, make, ruby, zsh:\n        '^{file}:{line}:',\n@wrap_settings({'fixlinecmd': '{editor} {file} +{line}',\n                'fixcolcmd': None})\n    # ignored by default\n    if settings.fixcolcmd and 'col' in m.groupdict():\n        editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],\n                                                file=m.group('file'),\n                                                line=m.group('line'),\n                                                col=m.group('col'))\n    else:\n        editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],\n                                                 file=m.group('file'),\n                                                 line=m.group('line'))", "description": ""}
{"id": "thefuck-17", "project": "thefuck", "bug_id": "17", "buggy_code": "from subprocess import Popen, PIPE\nfrom ..utils import DEVNULL, memoize, cache\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n    @cache('.bashrc', '.bash_profile')\n        proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n        return dict(\n                self._parse_alias(alias)\n                for alias in proc.stdout.read().decode('utf-8').split('\\n')\n                if alias and '=' in alias)\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')", "fixed_code": "from ..utils import memoize\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n        return dict(self._parse_alias(alias)\n                    for alias in raw_aliases if alias and '=' in alias)\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')", "description": ""}
{"id": "thefuck-1", "project": "thefuck", "bug_id": "1", "buggy_code": "broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]", "fixed_code": "broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]", "description": ""}
{"id": "thefuck-10", "project": "thefuck", "bug_id": "10", "buggy_code": "last_arg = command.script_parts[-1]\n\n        last_arg + ' --help',", "fixed_code": "last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n\n    # If there are no man pages for last_arg, suggest `last_arg --help` instead.\n    # Otherwise, suggest `--help` after suggesting other man page sections.\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n\n        help_command,", "description": ""}
{"id": "thefuck-19", "project": "thefuck", "bug_id": "19", "buggy_code": "return replace_argument(command.script, 'push', 'push --force')", "fixed_code": "return replace_argument(command.script, 'push', 'push --force-with-lease')", "description": ""}
{"id": "thefuck-26", "project": "thefuck", "bug_id": "26", "buggy_code": "machine = \"\"\n    return shells.and_(\"vagrant up \" +  machine, command.script)", "fixed_code": "machine = None\n\n    startAllInstances = shells.and_(\"vagrant up\", command.script)\n    if machine is None: \n        return startAllInstances\n    else:\n        return [ shells.and_(\"vagrant up \" +  machine, command.script), startAllInstances]", "description": ""}
{"id": "thefuck-8", "project": "thefuck", "bug_id": "8", "buggy_code": "# The regex has to be a bytes-style regex since reading from a file\n    # like stdin returns a bytes-style object and a string-style regex\n    # wouldn't work.\n    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)\n    lines = proc.stdout.read()", "fixed_code": "operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)\n    lines = proc.stdout.read().decode(\"utf-8\")", "description": ""}
{"id": "thefuck-21", "project": "thefuck", "bug_id": "21", "buggy_code": "return (command.script.split()[1] == 'stash'\n            and 'usage:' in command.stderr)", "fixed_code": "splited_script = command.script.split()\n    if len(splited_script) > 1:\n        return (splited_script[1] == 'stash'\n                and 'usage:' in command.stderr)\n    else:\n        return False", "description": ""}
{"id": "thefuck-31", "project": "thefuck", "bug_id": "31", "buggy_code": "return '{} --staged'.format(command.script)", "fixed_code": "return command.script.replace(' diff', ' diff --staged')", "description": ""}
{"id": "thefuck-30", "project": "thefuck", "bug_id": "30", "buggy_code": "return 'EDITOR' in os.environ and _search(command.stderr)", "fixed_code": "if 'EDITOR' not in os.environ:\n        return False\n\n    m = _search(command.stderr)\n\n    return m and os.path.isfile(m.group('file'))", "description": ""}
{"id": "thefuck-24", "project": "thefuck", "bug_id": "24", "buggy_code": "CorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))\n\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n        commands = {(command.script, command.side_effect): command\n                    if command.script != self._cached[0].script\n                    or command.side_effect != self._cached[0].side_effect}\n        return commands.values()", "fixed_code": "class CorrectedCommand(object):\n    def __init__(self, script, side_effect, priority):\n        self.script = script\n        self.side_effect = side_effect\n        self.priority = priority\n\n    def __eq__(self, other):\n        \"\"\"Ignores `priority` field.\"\"\"\n        if isinstance(other, CorrectedCommand):\n            return (other.script, other.side_effect) ==\\\n                   (self.script, self.side_effect)\n        else:\n            return False\n\n    def __hash__(self):\n        return (self.script, self.side_effect).__hash__()\n\n    def __repr__(self):\n        return 'CorrectedCommand(script={}, side_effect={}, priority={})'.format(\n            self.script, self.side_effect, self.priority)\n\n            if command != first:\n        commands = {command\n                    if command.script != self._cached[0]}\n        return commands", "description": ""}
{"id": "thefuck-23", "project": "thefuck", "bug_id": "23", "buggy_code": "with shelve.open(cache_path) as db:", "fixed_code": "from contextlib import closing\n        # A bit obscure, but simplest way to generate unique key for\n        # functions and methods in python 2 and 3:\n        with closing(shelve.open(cache_path)) as db:", "description": ""}
{"id": "thefuck-4", "project": "thefuck", "bug_id": "4", "buggy_code": "alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)", "fixed_code": "alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue", "description": ""}
{"id": "thefuck-15", "project": "thefuck", "bug_id": "15", "buggy_code": "return ('did not match any file(s) known to git.' in command.stderr\n            and \"Did you forget to 'git add'?\" in command.stderr)\n            r\"error: pathspec '([^']*)' \"\n            r\"did not match any file\\(s\\) known to git.\", command.stderr)[0]", "fixed_code": "return 'did not match any file(s) known to git.' in command.stderr\n        r\"error: pathspec '([^']*)' \"\n        r'did not match any file\\(s\\) known to git.', command.stderr)[0]", "description": ""}
{"id": "thefuck-3", "project": "thefuck", "bug_id": "3", "buggy_code": "proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],\n        version = proc.stdout.read().decode('utf-8').strip()", "fixed_code": "proc = Popen(['fish', '--version'],\n        version = proc.stdout.read().decode('utf-8').split()[-1]", "description": ""}
{"id": "thefuck-12", "project": "thefuck", "bug_id": "12", "buggy_code": "get_valid_history_without_current, get_closest\n    return (command.script_parts", "fixed_code": "get_valid_history_without_current, get_closest, which\n    return (not which(command.script_parts[0])", "description": ""}
{"id": "thefuck-2", "project": "thefuck", "bug_id": "2", "buggy_code": "for path in os.environ.get('PATH', '').split(':')", "fixed_code": "for path in os.environ.get('PATH', '').split(os.pathsep)", "description": ""}
{"id": "thefuck-13", "project": "thefuck", "bug_id": "13", "buggy_code": "return ('branch' in command.script\n            and \"fatal: A branch named '\" in command.stderr", "fixed_code": "return (\"fatal: A branch named '\" in command.stderr\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],", "description": ""}
{"id": "thefuck-5", "project": "thefuck", "bug_id": "5", "buggy_code": "and 'set-upstream' in command.output)", "fixed_code": "and 'git push --set-upstream' in command.output)", "description": ""}
{"id": "thefuck-14", "project": "thefuck", "bug_id": "14", "buggy_code": "overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()\n        if overridden_aliases:\n            return [alias.strip() for alias in overridden_aliases.split(',')]\n        else:\n            return ['cd', 'grep', 'ls', 'man', 'open']", "fixed_code": "default = {'cd', 'grep', 'ls', 'man', 'open'}\n        for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):\n            default.add(alias.strip())\n        return default", "description": ""}
{"id": "thefuck-22", "project": "thefuck", "bug_id": "22", "buggy_code": "commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)", "fixed_code": "if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)", "description": ""}
{"id": "thefuck-25", "project": "thefuck", "bug_id": "25", "buggy_code": "return re.sub('^mkdir (.*)', 'mkdir -p \\\\1', command.script)", "fixed_code": "return re.sub('\\\\bmkdir (.*)', 'mkdir -p \\\\1', command.script)", "description": ""}
{"id": "PySnooper-1", "project": "PySnooper", "bug_id": "1", "buggy_code": "encoding = 'ascii'\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))", "fixed_code": "PY2 = not PY3\nif pycompat.PY2:\n    from io import open\n        encoding = 'utf-8'\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))", "description": ""}
{"id": "PySnooper-3", "project": "PySnooper", "bug_id": "3", "buggy_code": "with open(output_path, 'a') as output_file:", "fixed_code": "with open(output, 'a') as output_file:", "description": ""}
{"id": "PySnooper-2", "project": "PySnooper", "bug_id": "2", "buggy_code": "def get_local_reprs(frame, watch=()):\n    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]\n        encoding = 'ascii'\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n\n        stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])\n\n                                       get_local_reprs(frame, watch=self.watch)\n                return_value_repr = utils.get_shortish_repr(arg)", "fixed_code": "import os\nif pycompat.PY2:\n    from io import open\ndef get_local_reprs(frame, watch=(), custom_repr=()):\n    result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]\n        encoding = 'utf-8'\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n    Customize how values are represented as strings::\n\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n\n            custom_repr=(),\n        if len(custom_repr) == 2 and not all(isinstance(x,\n                      pycompat.collections_abc.Iterable) for x in custom_repr):\n            custom_repr = (custom_repr,)\n        self.custom_repr = custom_repr\n        if DISABLED:\n            return function\n        if DISABLED:\n            return\n        stack = self.thread_local.__dict__.setdefault(\n            'original_trace_functions', []\n        )\n        if DISABLED:\n            return\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)", "description": ""}
{"id": "luigi-33", "project": "luigi", "bug_id": "33", "buggy_code": "positional_params = [(n, p) for n, p in params if p.significant]", "fixed_code": "positional_params = [(n, p) for n, p in params if not p.is_global]", "description": ""}
{"id": "luigi-20", "project": "luigi", "bug_id": "20", "buggy_code": "if params[param_name].significant:\n                params_str[param_name] = params[param_name].serialize(param_value)", "fixed_code": "params_str[param_name] = params[param_name].serialize(param_value)", "description": ""}
{"id": "luigi-18", "project": "luigi", "bug_id": "18", "buggy_code": "elif task.scheduler_disable_time is not None:", "fixed_code": "elif task.scheduler_disable_time is not None and new_status != DISABLED:", "description": ""}
{"id": "luigi-27", "project": "luigi", "bug_id": "27", "buggy_code": "def parse_from_input(self, param_name, x):\n            if self.has_value:\n                return self.value\n        if self.has_value:\n            description.append(\" [default: %s]\" % (self.value,))\n            params[param_name] = self.parse_from_input(param_name, value)\n                self.set_global(self.parse_from_input(param_name, value))", "fixed_code": "def parse_from_input(self, param_name, x, task_name=None):\n            if self.has_task_value(param_name=param_name, task_name=task_name):\n                return self.task_value(param_name=param_name, task_name=task_name)\n        if self.has_task_value(param_name=param_name, task_name=task_name):\n            value = self.task_value(param_name=param_name, task_name=task_name)\n            description.append(\" [default: %s]\" % (value,))\n            params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)\n                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))", "description": ""}
{"id": "luigi-9", "project": "luigi", "bug_id": "9", "buggy_code": "set_tasks[\"failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n                                      if status == 'PENDING' and task not in set_tasks[\"failed\"] and task not in set_tasks[\"completed\"] and not ext}\n                                          if status == 'PENDING' and task not in set_tasks[\"failed\"] and task not in set_tasks[\"completed\"] and ext}\n            if task in set_tasks[\"failed\"] or task in set_tasks[\"upstream_failure\"]:\n    if set_tasks[\"failed\"]:\n        smiley = \":(\"\n        reason = \"there were failed tasks\"\n        if set_tasks[\"scheduling_error\"]:\n            reason += \" and tasks whose scheduling failed\"", "fixed_code": "set_tasks[\"ever_failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"failed\"] = set_tasks[\"ever_failed\"] - set_tasks[\"completed\"]\n                                      if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and not ext}\n                                          if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and ext}\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n    \"ever_failed\",\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            smiley = \":)\"\n            reason = \"there were failed tasks but they all suceeded in a retry\"\n        else:\n            smiley = \":(\"\n            reason = \"there were failed tasks\"\n            if set_tasks[\"scheduling_error\"]:\n                reason += \" and tasks whose scheduling failed\"\n\n    \"\"\"\n    Test that a task once crashing and then succeeding should be counted as no failure.\n    \"\"\"\n    def test_retry_sucess_task(self):\n        class Foo(luigi.Task):\n            run_count = 0\n\n            def run(self):\n                self.run_count += 1\n                if self.run_count == 1:\n                    raise ValueError()\n\n            def complete(self):\n                return self.run_count > 0\n\n        self.run_and_expect('Foo --scheduler-retry-delay=0', 0)\n        self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)\n        self.run_with_config(dict(task_failed='3'), 'Foo', 0)", "description": ""}
{"id": "luigi-11", "project": "luigi", "bug_id": "11", "buggy_code": "task.params.get(name) == value for name, value in unbatched_params.items())):", "fixed_code": "task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):", "description": ""}
{"id": "luigi-7", "project": "luigi", "bug_id": "7", "buggy_code": "if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed first", "fixed_code": "if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it", "description": ""}
{"id": "luigi-16", "project": "luigi", "bug_id": "16", "buggy_code": "if task.id not in necessary_tasks and self._state.prune(task, self._config):", "fixed_code": "removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:", "description": ""}
{"id": "luigi-6", "project": "luigi", "bug_id": "6", "buggy_code": "elif isinstance(value, list):\n\n    class _DictParamEncoder(JSONEncoder):\n        \"\"\"\n        JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n        \"\"\"\n        def default(self, obj):\n            if isinstance(obj, _FrozenOrderedDict):\n                return obj.get_wrapped()\n            return json.JSONEncoder.default(self, obj)\n\n        return json.dumps(x, cls=DictParameter._DictParamEncoder)\n        Ensure that list parameter is converted to a tuple so it can be hashed.\n        return list(json.loads(x))\n        return json.dumps(x)\nclass TupleParameter(Parameter):\n\n            return tuple(tuple(x) for x in json.loads(x))  # loop required to parse tuple of tuples\n    def serialize(self, x):\n        \"\"\"\n        Opposite of :py:meth:`parse`.\n\n        Converts the value ``x`` to a string.\n\n        :param x: the value to serialize.\n        \"\"\"\n        return json.dumps(x)", "fixed_code": "elif isinstance(value, list) or isinstance(value, tuple):\nclass _DictParamEncoder(JSONEncoder):\n    \"\"\"\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n    \"\"\"\n    def default(self, obj):\n        if isinstance(obj, _FrozenOrderedDict):\n            return obj.get_wrapped()\n        return json.JSONEncoder.default(self, obj)\n\n\n        return json.dumps(x, cls=_DictParamEncoder)\n        Ensure that struct is recursively converted to a tuple so it can be hashed.\n        return list(json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        return json.dumps(x, cls=_DictParamEncoder)\nclass TupleParameter(ListParameter):\n            # loop required to parse tuple of tuples\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))", "description": ""}
{"id": "luigi-28", "project": "luigi", "bug_id": "28", "buggy_code": "return stdout and table in stdout", "fixed_code": "return stdout and table.lower() in stdout", "description": ""}
{"id": "luigi-17", "project": "luigi", "bug_id": "17", "buggy_code": "return scheduler.CentralPlannerScheduler(prune_on_get_work=True)", "fixed_code": "return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)", "description": ""}
{"id": "luigi-1", "project": "luigi", "bug_id": "1", "buggy_code": "metrics = self._scheduler._state._metrics_collector.generate_latest()\n            metrics.configure_http_handler(self)", "fixed_code": "metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n            metrics_collector.configure_http_handler(self)", "description": ""}
{"id": "luigi-10", "project": "luigi", "bug_id": "10", "buggy_code": "return state.get_pending_tasks()", "fixed_code": "return six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())", "description": ""}
{"id": "luigi-19", "project": "luigi", "bug_id": "19", "buggy_code": "if new_status == FAILED and task.can_disable():", "fixed_code": "if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "description": ""}
{"id": "luigi-26", "project": "luigi", "bug_id": "26", "buggy_code": "if not job.jar() or not os.path.exists(job.jar()):", "fixed_code": "if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):", "description": ""}
{"id": "luigi-8", "project": "luigi", "bug_id": "8", "buggy_code": "\"where table_schema = %s and table_name = %s limit 1\")\n                     \"where tablename = %s limit 1\")", "fixed_code": "\"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n                     \"where tablename = lower(%s) limit 1\")", "description": ""}
{"id": "luigi-31", "project": "luigi", "bug_id": "31", "buggy_code": "in_workers = assistant or worker in task.workers", "fixed_code": "in_workers = (assistant and task.workers) or worker in task.workers", "description": ""}
{"id": "luigi-30", "project": "luigi", "bug_id": "30", "buggy_code": "try:\n                new_deps = self._run_get_new_deps()\n                if new_deps is None:\n                    status = RUNNING\n                else:\n                    status = SUSPENDED\n                    logger.info(\n                        '[pid %s] Worker %s new requirements      %s',\n                        os.getpid(), self.worker_id, self.task.task_id)\n                    return\n            finally:\n                if status != SUSPENDED:\n                    self.task.trigger_event(\n                        Event.PROCESSING_TIME, self.task, time.time() - t0)\n                    error_message = json.dumps(self.task.on_success())\n                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                                self.worker_id, self.task.task_id)\n                    self.task.trigger_event(Event.SUCCESS, self.task)\n                    status = DONE", "fixed_code": "new_deps = self._run_get_new_deps()\n\n            if new_deps is None:\n                status = DONE\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                error_message = json.dumps(self.task.on_success())\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task.task_id)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n\n            else:\n                status = SUSPENDED\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task.task_id)", "description": ""}
{"id": "luigi-24", "project": "luigi", "bug_id": "24", "buggy_code": "command += [name, '\"{0}={1}\"'.format(prop, value)]", "fixed_code": "command += [name, '{0}={1}'.format(prop, value)]", "description": ""}
{"id": "luigi-23", "project": "luigi", "bug_id": "23", "buggy_code": "return scheduler.CentralPlannerScheduler()\n        self.last_active = last_active  # seconds since epoch", "fixed_code": "return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n    prune_on_get_work = parameter.BoolParameter(default=False)\n\n        self.last_active = last_active or time.time()  # seconds since epoch\n        if self._config.prune_on_get_work:\n            self.prune()", "description": ""}
{"id": "luigi-4", "project": "luigi", "bug_id": "4", "buggy_code": "if len(self.columns) > 0:", "fixed_code": "if self.columns and len(self.columns) > 0:", "description": ""}
{"id": "luigi-15", "project": "luigi", "bug_id": "15", "buggy_code": "if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:", "fixed_code": "if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:", "description": ""}
{"id": "luigi-3", "project": "luigi", "bug_id": "3", "buggy_code": "except ValueError:\n            return literal_eval(x)  # if this causes an error, let that error be raised.", "fixed_code": "except (ValueError, TypeError):\n            return tuple(literal_eval(x))  # if this causes an error, let that error be raised.", "description": ""}
{"id": "luigi-12", "project": "luigi", "bug_id": "12", "buggy_code": "import logging\ndef get_autoconfig_client():\n    configured_client = hdfs_config.get_configured_hdfs_client()\n    if configured_client == \"webhdfs\":\n        return hdfs_webhdfs_client.WebHdfsClient()\n    if configured_client == \"snakebite\":\n        return hdfs_snakebite_client.SnakebiteHdfsClient()\n    if configured_client == \"snakebite_with_hadoopcli_fallback\":\n        return luigi.contrib.target.CascadingClient([hdfs_snakebite_client.SnakebiteHdfsClient(),\n                                                     hdfs_hadoopcli_clients.create_hadoopcli_client()])\n    if configured_client == \"hadoopcli\":\n        return hdfs_hadoopcli_clients.create_hadoopcli_client()\n    raise Exception(\"Unknown hdfs client \" + configured_client)", "fixed_code": "import logging\nimport threading\n_AUTOCONFIG_CLIENT = threading.local()\n\ndef get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):\n    try:\n        return client_cache.client\n    except AttributeError:\n        configured_client = hdfs_config.get_configured_hdfs_client()\n        if configured_client == \"webhdfs\":\n            client_cache.client = hdfs_webhdfs_client.WebHdfsClient()\n        elif configured_client == \"snakebite\":\n            client_cache.client = hdfs_snakebite_client.SnakebiteHdfsClient()\n        elif configured_client == \"snakebite_with_hadoopcli_fallback\":\n            client_cache.client = luigi.contrib.target.CascadingClient([\n                hdfs_snakebite_client.SnakebiteHdfsClient(),\n                hdfs_hadoopcli_clients.create_hadoopcli_client(),\n            ])\n        elif configured_client == \"hadoopcli\":\n            client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()\n        else:\n            raise Exception(\"Unknown hdfs client \" + configured_client)\n        return client_cache.client", "description": ""}
{"id": "luigi-2", "project": "luigi", "bug_id": "2", "buggy_code": "\"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id)\n            raise ValueError(\"Target not supported\")", "fixed_code": "super(BeamDataflowJobTask, self).__init__()\n        \"\"\"\n            Given a luigi Target, determine a stringly typed path to pass as a\n            Dataflow job argument.\n        \"\"\"\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n            raise ValueError(\"Target %s not supported\" % target)", "description": ""}
{"id": "luigi-13", "project": "luigi", "bug_id": "13", "buggy_code": "self.fs.mkdir(d)", "fixed_code": "self.mkdir(d)", "description": ""}
{"id": "luigi-5", "project": "luigi", "bug_id": "5", "buggy_code": "# Modify task_that_inherits by subclassing it and adding methods\n        @task._task_wraps(task_that_inherits)\n        class Wrapped(task_that_inherits):\n            def clone_parent(_self, **args):\n                return _self.clone(cls=self.task_to_inherit, **args)\n\n        return Wrapped\n        # Modify task_that_requres by subclassing it and adding methods\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n\n        return Wrapped", "fixed_code": "# Get all parameter objects from the underlying task\n            # Check if the parameter exists in the inheriting task\n                # If not, add it to the inheriting task\n        # Modify task_that_inherits by adding methods\n        def clone_parent(_self, **args):\n            return _self.clone(cls=self.task_to_inherit, **args)\n        task_that_inherits.clone_parent = clone_parent\n        return task_that_inherits\n        # Modify task_that_requres by adding methods\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires", "description": ""}
{"id": "luigi-14", "project": "luigi", "bug_id": "14", "buggy_code": "disable_failures = parameter.IntParameter(default=None,\n    disable_hard_timeout = parameter.IntParameter(default=None,\n        if (self.failures.first_failure_time is not None and\n                self.disable_hard_timeout):\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed_code": "disable_failures = parameter.IntParameter(default=999999999,\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n        if self.failures.first_failure_time is not None:\n        if new_status == FAILED and task.status != DISABLED:", "description": ""}
{"id": "luigi-22", "project": "luigi", "bug_id": "22", "buggy_code": "def __init__(self, worker_id, last_active=None):", "fixed_code": "def __init__(self, worker_id, last_active=time.time()):", "description": ""}
{"id": "luigi-25", "project": "luigi", "bug_id": "25", "buggy_code": "path = self.s3_load_path()", "fixed_code": "path = self.s3_load_path", "description": ""}
{"id": "cookiecutter-1", "project": "cookiecutter", "bug_id": "1", "buggy_code": "with open(context_file) as file_handle:", "fixed_code": "with open(context_file, encoding='utf-8') as file_handle:", "description": ""}
{"id": "cookiecutter-4", "project": "cookiecutter", "bug_id": "4", "buggy_code": "from .hooks import run_hook, EXIT_SUCCESS\n        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\n    return proc.wait()\n    return run_script(temp.name, cwd)\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed_code": "class FailedHookException(CookiecutterException):\n    \"\"\"\n    Raised when a hook script fails\n    \"\"\"\n    FailedHookException,\nfrom .hooks import run_hook\n        try:\n            run_hook('pre_gen_project', project_dir, context)\n        except FailedHookException:\n            shutil.rmtree(project_dir, ignore_errors=True)\nfrom .exceptions import FailedHookException\n    exit_status = proc.wait()\n    if exit_status != EXIT_SUCCESS:\n        raise FailedHookException(\n            \"Hook script failed (exit status: %d)\" % exit_status)\n    run_script(temp.name, cwd)\n        return\n    run_script_with_context(script, project_dir, context)", "description": ""}
{"id": "cookiecutter-3", "project": "cookiecutter", "bug_id": "3", "buggy_code": "prompt, type=click.Choice(choices), default=default", "fixed_code": "prompt, type=click.Choice(choices), default=default, show_choices=False", "description": ""}
{"id": "cookiecutter-2", "project": "cookiecutter", "bug_id": "2", "buggy_code": "return os.path.abspath(os.path.join(hooks_dir, hook_file))\n    return None\n    script = find_hook(hook_name)\n    if script is None:\n    run_script_with_context(script, project_dir, context)", "fixed_code": "scripts = []\n            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\n    if len(scripts) == 0:\n        return None\n    return scripts\n    scripts = find_hook(hook_name)\n    if not scripts:\n    for script in scripts:\n        run_script_with_context(script, project_dir, context)", "description": ""}
{"id": "scrapy-32", "project": "scrapy", "bug_id": "32", "buggy_code": "configure_logging(settings)\n        log_scrapy_info(settings)", "fixed_code": "configure_logging(self.settings)\n        log_scrapy_info(self.settings)", "description": ""}
{"id": "scrapy-35", "project": "scrapy", "bug_id": "35", "buggy_code": "cls_path = settings.get('SPIDER_LOADER_CLASS',\n                            settings.get('SPIDER_MANAGER_CLASS'))", "fixed_code": "cls_path = settings.get('SPIDER_MANAGER_CLASS',\n                            settings.get('SPIDER_LOADER_CLASS'))", "description": ""}
{"id": "scrapy-34", "project": "scrapy", "bug_id": "34", "buggy_code": "fields = {}", "fixed_code": "fields = getattr(_class, 'fields', {})", "description": ""}
{"id": "scrapy-33", "project": "scrapy", "bug_id": "33", "buggy_code": "from scrapy.utils.log import logformatter_adapter\n                                           extra={'spider': spider, 'failure': f}))\n                                           extra={'spider': spider, 'failure': f}))\n                                           extra={'spider': spider, 'failure': f}))\n                                            extra={'spider': spider, 'failure': f}))\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\nfrom scrapy.utils.log import logformatter_adapter\n                                   extra={'spider': spider, 'failure': f}))\n            extra={'spider': spider, 'failure': _failure}\n                             extra={'spider': spider, 'failure': download_failure})\n                             extra={'spider': spider, 'failure': output})\n                         extra={'spider': spider, 'failure': failure})\n                                            extra={'spider': spider, 'failure': f}))\n\"\"\"\nThis module is kept to provide a helpful warning about its removal.\n\"\"\"\nwarnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n              \"the builtin Python library for logging. Read the updated \"\n              \"logging entry in the documentation to learn more.\",\n              ScrapyDeprecationWarning, stacklevel=2)\n# Imports kept for backwards-compatibility\nDEBUG = logging.DEBUG\nINFO = logging.INFO\nWARNING = logging.WARNING\nERROR = logging.ERROR\nCRITICAL = logging.CRITICAL\nSILENT = CRITICAL + 1\ndef msg(message=None, _level=logging.INFO, **kw):\n    warnings.warn('log.msg has been deprecated, create a python logger and '\n                  'log through it instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n    level = kw.pop('level', _level)\n    message = kw.pop('format', message)\n    # NOTE: logger.log doesn't handle well passing empty dictionaries with format\n    # arguments because of some weird use-case:\n    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269\n    logger.log(level, message, *[kw] if kw else [])\ndef err(_stuff=None, _why=None, **kw):\n    warnings.warn('log.err has been deprecated, create a python logger and '\n                  'use its error method instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n    level = kw.pop('level', logging.ERROR)\n    failure = kw.pop('failure', _stuff) or Failure()\n    message = kw.pop('why', _why) or failure.value\n    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})\n                         extra={'spider': info.spider, 'failure': f})\n            f.value, extra={'spider': info.spider, 'failure': f})\n                        extra={'spider': info.spider, 'failure': value}\n                         extra={'spider': spider, 'failure': failure})", "fixed_code": "from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n                             exc_info=failure_to_exc_info(output),\n                             extra={'spider': spider})\nfrom scrapy.utils.log import failure_to_exc_info\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\nfrom scrapy.utils.log import failure_to_exc_info\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n# -*- coding: utf-8 -*-\nimport sys\nfrom logging.config import dictConfig\nfrom twisted.python import log as twisted_log\nimport scrapy\nfrom scrapy.settings import overridden_settings, Settings\ndef failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.tb)\nclass TopLevelFormatter(logging.Filter):\n    \"\"\"Keep only top level loggers's name (direct children from root) from\n    records.\n    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n    the old Scrapy log behaviour and helps shortening long names.\n    Since it can't be set for just one logger (it won't propagate for its\n    children), it's going to be set in the root handler, with a parametrized\n    `loggers` list where it should act.\n    \"\"\"\n    def __init__(self, loggers=None):\n        self.loggers = loggers or []\n    def filter(self, record):\n        if any(record.name.startswith(l + '.') for l in self.loggers):\n            record.name = record.name.split('.', 1)[0]\n        return True\nDEFAULT_LOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'loggers': {\n        'scrapy': {\n            'level': 'DEBUG',\n        },\n        'twisted': {\n            'level': 'ERROR',\n        },\n    }\n}\n\n\ndef configure_logging(settings=None):\n    \"\"\"Initialize and configure default loggers\n\n    This function does:\n      - Route warnings and twisted logging through Python standard logging\n      - Set FailureFormatter filter on Scrapy logger\n      - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n      - Create a handler for the root logger according to given settings\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver('twisted')\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict):\n        settings = Settings(settings)\n\n    if settings:\n        logging.root.setLevel(logging.NOTSET)\n\n        if settings.getbool('LOG_STDOUT'):\n            sys.stdout = StreamLogger(logging.getLogger('stdout'))\n\n        # Set up the default log handler\n        filename = settings.get('LOG_FILE')\n        if filename:\n            encoding = settings.get('LOG_ENCODING')\n            handler = logging.FileHandler(filename, encoding=encoding)\n        elif settings.getbool('LOG_ENABLED'):\n            handler = logging.StreamHandler()\n        else:\n            handler = logging.NullHandler()\n\n        formatter = logging.Formatter(\n            fmt=settings.get('LOG_FORMAT'),\n            datefmt=settings.get('LOG_DATEFORMAT')\n        )\n        handler.setFormatter(formatter)\n        handler.setLevel(settings.get('LOG_LEVEL'))\n        handler.addFilter(TopLevelFormatter(['scrapy']))\n        logging.root.addHandler(handler)\n\n\ndef log_scrapy_info(settings):\n    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n\n    logger.info(\"Optional features available: %(features)s\",\n                {'features': \", \".join(scrapy.optional_features)})\n\n    d = dict(overridden_settings(settings))\n    logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n\n\nclass StreamLogger(object):\n    \"\"\"Fake file-like stream object that redirects writes to a logger instance\n\n    Taken from:\n        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n    \"\"\"\n    def __init__(self, logger, log_level=logging.INFO):\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = ''\n\n    def write(self, buf):\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())\n\n\nclass LogCounterHandler(logging.Handler):\n    \"\"\"Record log levels count into a crawler stats\"\"\"\n\n    def __init__(self, crawler, *args, **kwargs):\n        super(LogCounterHandler, self).__init__(*args, **kwargs)\n        self.crawler = crawler\n\n    def emit(self, record):\n        sname = 'log_count/{}'.format(record.levelname)\n        self.crawler.stats.inc_value(sname)\n\n\ndef logformatter_adapter(logkws):\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {'level', 'msg', 'args'} <= set(logkws):\n        warnings.warn('Missing keys in LogFormatter method',\n                      ScrapyDeprecationWarning)\n\n    if 'format' in logkws:\n        warnings.warn('`format` key in LogFormatter methods has been '\n                      'deprecated, use `msg` instead',\n                      ScrapyDeprecationWarning)\n\n    level = logkws.get('level', logging.INFO)\n    message = logkws.get('format', logkws.get('msg'))\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = logkws if not logkws.get('args') else logkws['args']\n\n    return (level, message, args)\nfrom scrapy.utils.log import failure_to_exc_info\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\nfrom scrapy.utils.log import failure_to_exc_info\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\nfrom scrapy.utils.log import failure_to_exc_info\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})", "description": ""}
{"id": "scrapy-20", "project": "scrapy", "bug_id": "20", "buggy_code": "for url in sitemap_urls_from_robots(response.body):", "fixed_code": "for url in sitemap_urls_from_robots(response.text):", "description": ""}
{"id": "scrapy-18", "project": "scrapy", "bug_id": "18", "buggy_code": "filename = to_native_str(content_disposition).split(';')[1].split('=')[1]", "fixed_code": "filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]", "description": ""}
{"id": "scrapy-27", "project": "scrapy", "bug_id": "27", "buggy_code": "response.status in getattr(spider, 'handle_httpstatus_list', [])):", "fixed_code": "response.status in getattr(spider, 'handle_httpstatus_list', []) or\n               response.status in request.meta.get('handle_httpstatus_list', []) or\n               request.meta.get('handle_httpstatus_all', False)):", "description": ""}
{"id": "scrapy-11", "project": "scrapy", "bug_id": "11", "buggy_code": "output += f.extrabuf", "fixed_code": "output += f.extrabuf[-f.extrasize:]", "description": ""}
{"id": "scrapy-7", "project": "scrapy", "bug_id": "7", "buggy_code": "import six\n        return urljoin(form.base_url, form.action)", "fixed_code": "import six\n\nfrom w3lib.html import strip_html5_whitespace\n\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))", "description": ""}
{"id": "scrapy-29", "project": "scrapy", "bug_id": "29", "buggy_code": "s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"", "fixed_code": "s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"", "description": ""}
{"id": "scrapy-16", "project": "scrapy", "bug_id": "16", "buggy_code": "unquote)\nfrom scrapy.utils.python import to_native_str\n    - percent encode paths and query arguments. non-ASCII characters are\n      percent-encoded using UTF-8 (RFC-3986)\n    - remove query arguments with blank values (unless keep_blank_values is True)\n    - remove fragments (unless keep_fragments is True)\n    The url passed can be a str or unicode, while the url returned is always a\n    str.\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n    # XXX: copied from w3lib.url.safe_url_string to add encoding argument\n    # path = to_native_str(path, encoding)\n    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'\n    path = safe_url_string(_unquotepath(path)) or '/'\n    return unquote(path)\n    return urlparse(to_native_str(url, encoding))", "fixed_code": "import six\n                                    quote, unquote)\nif six.PY3:\n    from urllib.parse import unquote_to_bytes\nfrom scrapy.utils.python import to_bytes, to_native_str, to_unicode\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n\n        # default encoding for path component SHOULD be UTF-8\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n\n        # encoding of query and fragment follows page encoding\n        # or form-charset (if known and passed)\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n\n\n    - percent encode paths ; non-ASCII characters are percent-encoded\n      using UTF-8 (RFC-3986)\n    - percent encode query arguments ; non-ASCII characters are percent-encoded\n      using passed `encoding` (UTF-8 by default)\n    - remove query arguments with blank values (unless `keep_blank_values` is True)\n    - remove fragments (unless `keep_fragments` is True)\n    The url passed can be bytes or unicode, while the url returned is\n    always a native str (bytes in Python 2, unicode in Python 3).\n    # If supplied `encoding` is not compatible with all characters in `url`,\n    # fallback to UTF-8 as safety net.\n    # UTF-8 can handle all Unicode characters,\n    # so we should be covered regarding URL normalization,\n    # if not for proper URL expected by remote website.\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    # 1. decode query-string as UTF-8 (or keep raw bytes),\n    #    sort values,\n    #    and percent-encode them back\n    if not six.PY2:\n        # Python3's urllib.parse.parse_qsl does not work as wanted\n        # for percent-encoded characters that do not match passed encoding,\n        # they get lost.\n        #\n        # e.g., 'q=b%a3' becomes [('q', 'b\\ufffd')]\n        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),\n        #      instead of \\xa3 that you get with Python2's parse_qsl)\n        #\n        # what we want here is to keep raw bytes, and percent encode them\n        # so as to preserve whatever encoding what originally used.\n        #\n        # See https://tools.ietf.org/html/rfc3987#section-6.4:\n        #\n        # For example, it is possible to have a URI reference of\n        # \"http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9\", where the\n        # document name is encoded in iso-8859-1 based on server settings, but\n        # where the fragment identifier is encoded in UTF-8 according to\n        # [XPointer]. The IRI corresponding to the above URI would be (in XML\n        # notation)\n        # \"http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;\".\n        # Similar considerations apply to query parts.  The functionality of\n        # IRIs (namely, to be able to include non-ASCII characters) can only be\n        # used if the query part is encoded in UTF-8.\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)\n    #    and percent-encode path again (this normalizes to upper-case %XX)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n\n    # every part should be safe already\n\n    if six.PY3:\n        # standard lib's unquote() does not work in Python 3\n        # for non-UTF-8 percent-escaped characters, they get lost.\n        # e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)\n        #\n        # unquote_to_bytes() returns raw bytes instead\n        return unquote_to_bytes(path)\n    else:\n        # in Python 2, '%a3' becomes '\\xa3', which is what we want\n        return unquote(path)\n    return urlparse(to_unicode(url, encoding))\n\n\nif six.PY3:\n    from urllib.parse import _coerce_args, unquote_to_bytes\n\n    def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):\n        \"\"\"Parse a query given as a string argument.\n\n        Data are returned as a list of name, value pairs as bytes.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n\n        \"\"\"\n        # This code is the same as Python3's parse_qsl()\n        # (at https://hg.python.org/cpython/rev/c38ac7ab8d9a)\n        # except for the unquote(s, encoding, errors) calls replaced\n        # with unquote_to_bytes(s)\n        qs, _coerce_result = _coerce_args(qs)\n        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n        r = []\n        for name_value in pairs:\n            if not name_value and not strict_parsing:\n                continue\n            nv = name_value.split('=', 1)\n            if len(nv) != 2:\n                if strict_parsing:\n                    raise ValueError(\"bad query field: %r\" % (name_value,))\n                # Handle case of a control-name with no equal sign\n                if keep_blank_values:\n                    nv.append('')\n                else:\n                    continue\n            if len(nv[1]) or keep_blank_values:\n                name = nv[0].replace('+', ' ')\n                name = unquote_to_bytes(name)\n                name = _coerce_result(name)\n                value = nv[1].replace('+', ' ')\n                value = unquote_to_bytes(value)\n                value = _coerce_result(value)\n                r.append((name, value))\n        return r", "description": ""}
{"id": "scrapy-17", "project": "scrapy", "bug_id": "17", "buggy_code": ">>> response_status_message(200)\n    '200 OK'\n\n    >>> response_status_message(404)\n    '404 Not Found'\n    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))", "fixed_code": "return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))", "description": ""}
{"id": "scrapy-1", "project": "scrapy", "bug_id": "1", "buggy_code": "if url_pattern.match(domain):\n        domains = [re.escape(d) for d in allowed_domains if d is not None]", "fixed_code": "domains = []\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n            else:\n                domains.append(re.escape(domain))", "description": ""}
{"id": "scrapy-10", "project": "scrapy", "bug_id": "10", "buggy_code": "from scrapy.utils.python import to_native_str\n        # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8\n        location = to_native_str(response.headers['location'].decode('latin1'))", "fixed_code": "from w3lib.url import safe_url_string\n\n        location = safe_url_string(response.headers['location'])", "description": ""}
{"id": "scrapy-19", "project": "scrapy", "bug_id": "19", "buggy_code": "# python3 uses request.unverifiable\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname", "fixed_code": "def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n\n    # python3 uses attributes instead of methods\n    @property\n    def full_url(self):\n        return self.get_full_url()\n\n    @property\n    def host(self):\n        return self.get_host()\n\n    @property\n    def type(self):\n        return self.get_type()\n\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()", "description": ""}
{"id": "scrapy-26", "project": "scrapy", "bug_id": "26", "buggy_code": "``False`` and ``None`` return ``False``. \n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n        else:\n            return self[name]", "fixed_code": "``False`` and ``None`` return ``False``.\n            # When users defined a _BASE setting, they explicitly don't want to\n            # use any of Scrapy's defaults. Therefore, we only use these entries\n            # from self[name] (where the defaults now live) that have a priority\n            # higher than 'default'\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n        return self[name]", "description": ""}
{"id": "scrapy-21", "project": "scrapy", "bug_id": "21", "buggy_code": "self._parsers.pop(netloc).callback(None)", "fixed_code": "rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)", "description": ""}
{"id": "scrapy-38", "project": "scrapy", "bug_id": "38", "buggy_code": "'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',", "fixed_code": "'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',", "description": ""}
{"id": "scrapy-36", "project": "scrapy", "bug_id": "36", "buggy_code": "return objcls.from_crawler(crawler, *args, **kwargs)\n        return objcls.from_settings(settings, *args, **kwargs)\n        return objcls(*args, **kwargs)", "fixed_code": "Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n    extension has not been implemented correctly).\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = 'from_crawler'\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = 'from_settings'\n        instance = objcls(*args, **kwargs)\n        method_name = '__new__'\n    if instance is None:\n        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n    return instance", "description": ""}
{"id": "scrapy-31", "project": "scrapy", "bug_id": "31", "buggy_code": "return to_native_str(self.request.headers.get(name, default))\n            (to_native_str(k), [to_native_str(x) for x in v])\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]", "fixed_code": "return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]", "description": ""}
{"id": "scrapy-30", "project": "scrapy", "bug_id": "30", "buggy_code": "for obj in vars(module).itervalues():\n               issubclass(obj, ScrapyCommand) and \\\n               obj.__module__ == module.__name__:\n        self.out = ''\n        self.err = ''\n        comm = proc.communicate()\n        return comm[0].strip()", "fixed_code": "for obj in vars(module).values():\n                    issubclass(obj, ScrapyCommand) and \\\n                    obj.__module__ == module.__name__:\n        self.out = b''\n        self.err = b''\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)", "description": ""}
{"id": "scrapy-37", "project": "scrapy", "bug_id": "37", "buggy_code": "if ':' not in self._url:", "fixed_code": "if ('://' not in self._url) and (not self._url.startswith('data:')):", "description": ""}
{"id": "scrapy-39", "project": "scrapy", "bug_id": "39", "buggy_code": "if self.make_requests_from_url is not Spider.make_requests_from_url:\n                \"Spider.make_requests_from_url method is deprecated; \"\n                \"it won't be called in future Scrapy releases. \"\n                \"Please override start_requests method instead.\"", "fixed_code": "cls = self.__class__\n        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n                \"Spider.make_requests_from_url method is deprecated; it \"\n                \"won't be called in future Scrapy releases. Please \"\n                \"override Spider.start_requests method instead (see %s.%s).\" % (\n                    cls.__module__, cls.__name__\n                ),", "description": ""}
{"id": "scrapy-24", "project": "scrapy", "bug_id": "24", "buggy_code": "_responseMatcher = re.compile('HTTP/1\\.. 200')\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'", "fixed_code": "_responseMatcher = re.compile(b'HTTP/1\\.. 200')\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'", "description": ""}
{"id": "scrapy-23", "project": "scrapy", "bug_id": "23", "buggy_code": "user_pass = '%s:%s' % (unquote(user), unquote(password))\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)", "fixed_code": "from scrapy.utils.python import to_bytes\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)", "description": ""}
{"id": "scrapy-4", "project": "scrapy", "bug_id": "4", "buggy_code": "exc_info = failure.value, failure.type, failure.getTracebackObject()", "fixed_code": "exc_info = failure.type, failure.value, failure.getTracebackObject()", "description": ""}
{"id": "scrapy-15", "project": "scrapy", "bug_id": "15", "buggy_code": "to_native_str(parts.netloc.encode('idna')),", "fixed_code": "# IDNA encoding can fail for too long labels (>63 characters)\n    # or missing labels (e.g. http://.example.com)\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n\n        to_native_str(netloc),", "description": ""}
{"id": "scrapy-3", "project": "scrapy", "bug_id": "3", "buggy_code": "from six.moves.urllib.parse import urljoin\n        location = safe_url_string(response.headers['location'])", "fixed_code": "from six.moves.urllib.parse import urljoin, urlparse\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')", "description": ""}
{"id": "scrapy-40", "project": "scrapy", "bug_id": "40", "buggy_code": "if self.binary:\n            return to_bytes(value, encoding=self.encoding)\n        else:\n            return to_unicode(value, encoding=self.encoding)", "fixed_code": "encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value", "description": ""}
{"id": "scrapy-2", "project": "scrapy", "bug_id": "2", "buggy_code": "while len(self) >= self.limit:\n            self.popitem(last=False)", "fixed_code": "if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)", "description": ""}
{"id": "scrapy-13", "project": "scrapy", "bug_id": "13", "buggy_code": "EXPIRES = 0", "fixed_code": "EXPIRES = 90", "description": ""}
{"id": "scrapy-14", "project": "scrapy", "bug_id": "14", "buggy_code": "return ctype in (b'application/x-gzip', b'application/gzip')", "fixed_code": "import re\n_is_gzipped_re = re.compile(br'^application/(x-)?gzip\\b', re.I)\n    return _is_gzipped_re.search(ctype) is not None", "description": ""}
{"id": "scrapy-22", "project": "scrapy", "bug_id": "22", "buggy_code": "else:", "fixed_code": "elif isinstance(serialized_value, six.text_type):\n        else:\n            self._xg_characters(str(serialized_value))", "description": ""}
{"id": "scrapy-25", "project": "scrapy", "bug_id": "25", "buggy_code": "return form.action or form.base_url\n    root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)", "fixed_code": "from scrapy.utils.response import get_base_url\n        return urljoin(form.base_url, form.action)\n    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))", "description": ""}
{"id": "ansible-18", "project": "ansible", "bug_id": "18", "buggy_code": "desc=\"Perform various Role related operations.\",\n            description='your description',\n        display.display(\"- %s was created successfully\" % obj_name)", "fixed_code": "desc=\"Perform various Role and Collection related operations.\",\n            description='your {0} description'.format(galaxy_type),\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))", "description": ""}
{"id": "ansible-9", "project": "ansible", "bug_id": "9", "buggy_code": "args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)", "fixed_code": "args = [SUBMAN_CMD, 'attach', '--pool', pool_id]\n                if quantity is not None:\n                    args.extend(['--quantity', to_native(quantity)])\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity", "description": ""}
{"id": "ansible-11", "project": "ansible", "bug_id": "11", "buggy_code": "from ansible.module_utils.connection import exec_command\nfrom ansible.module_utils.network.ios.ios import load_config\nimport re\n            banner_cmd += want['text'].strip()\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n    if text:\n        text = str(text).strip()", "fixed_code": "from ansible.module_utils.network.ios.ios import get_config, load_config\nfrom re import search, M\n            banner_cmd += want['text'].strip('\\n')\n    \"\"\"\n    This function gets the banner config without stripping any whitespaces,\n    and then fetches the required banner from it.\n    :param module:\n    :return: banner config dict object.\n    \"\"\"\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n    else:\n        output = None", "description": ""}
{"id": "ansible-7", "project": "ansible", "bug_id": "7", "buggy_code": "for key in to_remove:\n        commands.append(\"no {0}\".format(key))", "fixed_code": "for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))", "description": ""}
{"id": "ansible-16", "project": "ansible", "bug_id": "16", "buggy_code": "# Always use 'processor' count for ARM systems\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):\n            'processor_count': 16,\n            'processor_vcpus': 16\n            'processor_count': 48,\n            'processor_vcpus': 48", "fixed_code": "# The fields for Power CPUs include 'processor' and 'cpu'.\n        # Always use 'processor' count for ARM and Power systems\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n            'processor_count': 8,\n            'processor_vcpus': 8\n            'processor_count': 24,\n            'processor_vcpus': 24", "description": ""}
{"id": "ansible-6", "project": "ansible", "bug_id": "6", "buggy_code": "# In the case we are checking a new requirement on a base requirement (parent != None) we can't accept\n                # version as '*' (unknown version) unless the requirement is also '*'.\n                if parent and version == '*' and requirement != '*':\n                    break\n                elif requirement == '*' or version == '*':\n                    continue\n            version = manifest['version']\n        existing[0].add_requirement(to_text(collection_info), requirement)", "fixed_code": "# In the case we are checking a new requirement on a base requirement (parent != None) we can't accept\n            # version as '*' (unknown version) unless the requirement is also '*'.\n            if parent and version == '*' and requirement != '*':\n                display.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\n                                \"install does not have a version set, the collection may not work.\"\n                                % (to_text(self), req, parent))\n                continue\n            elif requirement == '*' or version == '*':\n                continue\n            version = to_text(manifest['version'], errors='surrogate_or_strict')\n\n            if not hasattr(LooseVersion(version), 'version'):\n                display.warning(\"Collection at '%s' does not have a valid version set, falling back to '*'. Found \"\n                                \"version: '%s'\" % (to_text(b_path), version))\n                version = '*'\n\n        existing[0].add_requirement(parent, requirement)", "description": ""}
{"id": "ansible-10", "project": "ansible", "bug_id": "10", "buggy_code": "current_line.next.prev = current_line.prev", "fixed_code": "self.prev = None\n        self.next = None\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev", "description": ""}
{"id": "ansible-8", "project": "ansible", "bug_id": "8", "buggy_code": "parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path", "fixed_code": "import ntpath\n        # use normpath() to remove doubled slashed and convert forward to backslashes\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n\n        # Becuase ntpath.join treats any component that begins with a backslash as an absolute path,\n        # we have to strip slashes from at least the beginning, otherwise join will ignore all previous\n        # path components except for the drive.\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])", "description": ""}
{"id": "ansible-4", "project": "ansible", "bug_id": "4", "buggy_code": "_collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)", "fixed_code": "from ansible.template import is_template, Environment\nfrom ansible.utils.display import Display\n\ndisplay = Display()\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n        # This duplicates static attr checking logic from post_validate()\n        # because if the user attempts to template a collection name, it will\n        # error before it ever gets to the post_validate() warning.\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))", "description": ""}
{"id": "ansible-15", "project": "ansible", "bug_id": "15", "buggy_code": "if needs_update('state') and not needs_update('vrf'):", "fixed_code": "if needs_update('state'):", "description": ""}
{"id": "ansible-3", "project": "ansible", "bug_id": "3", "buggy_code": "elif path == '/etc/lsb-release' and 'Kali' in data:", "fixed_code": "elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n            # Kali does not provide /etc/lsb-release anymore", "description": ""}
{"id": "ansible-12", "project": "ansible", "bug_id": "12", "buggy_code": "import os\n\n            ret.append(os.getenv(var, ''))", "fixed_code": "from ansible.utils import py3compat\n            ret.append(py3compat.environ.get(var, ''))", "description": ""}
{"id": "ansible-2", "project": "ansible", "bug_id": "2", "buggy_code": "def __gt__(self, other):\n        return not self.__lt__(other)\n\n        return self.__gt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__lt__(other)\n\n        return self.__gt__(other) or self.__eq__(other)", "fixed_code": "def __gt__(self, other):\n        return not self.__le__(other)\n\n        return not self.__lt__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n\n        return not self.__lt__(other)", "description": ""}
{"id": "ansible-13", "project": "ansible", "bug_id": "13", "buggy_code": "name, dummy, requirement = collection_input.partition(':')\n    elif urlparse(collection).scheme:\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)", "fixed_code": "from ansible.module_utils import six\nurlparse = six.moves.urllib.parse.urlparse\n                    requirement = None\n                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                            urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                        # Arg is a file path or URL to a collection\n                        name = collection_input\n                    else:\n                        name, dummy, requirement = collection_input.partition(':')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))", "description": ""}
{"id": "ansible-5", "project": "ansible", "bug_id": "5", "buggy_code": "msg = \"missing required arguments: %s\" % \", \".join(missing)\n    expected = \"TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)\"\n        assert e.value == expected\n        assert \"TypeError: 'NoneType' object is not iterable\" in to_native(te.error)", "fixed_code": "msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\n    expected = \"parameters are mutually exclusive: string1|string2, box|fox|socks\"\n\n    assert to_native(e.value) == expected\n    assert \"'NoneType' object is not iterable\" in to_native(te.value)", "description": ""}
{"id": "ansible-14", "project": "ansible", "bug_id": "14", "buggy_code": "url = _urljoin(self.api_server, data['next_link'])\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))", "fixed_code": "try:\n    from urllib.parse import urlparse\nexcept ImportError:\n    # Python 2\n    from urlparse import urlparse\n\n\n            # https://github.com/ansible/ansible/issues/64355\n            # api_server contains part of the API path but next_link includes the the /api part so strip it out.\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n\n                url = _urljoin(base_url, data['next_link'])\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))", "description": ""}
{"id": "tornado-11", "project": "tornado", "bug_id": "11", "buggy_code": "if headers.get(\"Transfer-Encoding\") == \"chunked\":", "fixed_code": "if headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":", "description": ""}
{"id": "tornado-7", "project": "tornado", "bug_id": "7", "buggy_code": "from tornado.concurrent import TracebackFuture, is_future\n\n        return executor.submit(func, *args)", "fixed_code": "from tornado.concurrent import TracebackFuture, is_future, chain_future\n        c_future = executor.submit(func, *args)\n        # Concurrent Futures are not usable with await. Wrap this in a\n        # Tornado Future instead, using self.add_future for thread-safety.\n        t_future = TracebackFuture()\n        self.add_future(c_future, lambda f: chain_future(f, t_future))\n        return t_future", "description": ""}
{"id": "tornado-6", "project": "tornado", "bug_id": "6", "buggy_code": "import weakref\n    _ioloop_for_asyncio = weakref.WeakKeyDictionary()", "fixed_code": "_ioloop_for_asyncio = dict()\n        # If an asyncio loop was closed through an asyncio interface\n        # instead of IOLoop.close(), we'd never hear about it and may\n        # have left a dangling reference in our map. In case an\n        # application (or, more likely, a test suite) creates and\n        # destroys a lot of event loops in this way, check here to\n        # ensure that we don't have a lot of dead loops building up in\n        # the map.\n        #\n        # TODO(bdarnell): consider making self.asyncio_loop a weakref\n        # for AsyncIOMainLoop and make _ioloop_for_asyncio a\n        # WeakKeyDictionary.\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n        del IOLoop._ioloop_for_asyncio[self.asyncio_loop]", "description": ""}
{"id": "tornado-1", "project": "tornado", "bug_id": "1", "buggy_code": "assert self.stream is not None\n        self.stream.set_nodelay(value)", "fixed_code": "assert self.ws_connection is not None\n        self.ws_connection.set_nodelay(value)\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)", "description": ""}
{"id": "tornado-4", "project": "tornado", "bug_id": "4", "buggy_code": "if (start is not None and start >= size) or end == 0:\n                # content, or when a suffix with length 0 is specified\n            if start is not None and start < 0:\n                start += size", "fixed_code": "if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\n                # content, or when a suffix with length 0 is specified.\n                # https://tools.ietf.org/html/rfc7233#section-2.1\n                # A byte-range-spec is invalid if the last-byte-pos value is present\n                # and less than the first-byte-pos.", "description": ""}
{"id": "tornado-15", "project": "tornado", "bug_id": "15", "buggy_code": "root = os.path.abspath(root)\n        # os.path.abspath strips a trailing /\n        # it needs to be temporarily added back for requests to root/", "fixed_code": "\"static_foo.txt\",\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root) + os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.", "description": ""}
{"id": "tornado-3", "project": "tornado", "bug_id": "3", "buggy_code": "if self._instance_cache.get(self.io_loop) is not self:\n            del self._instance_cache[self.io_loop]", "fixed_code": "cached_val = self._instance_cache.pop(self.io_loop, None)\n            # If there's an object other than self in the instance\n            # cache for our IOLoop, something has gotten mixed up. A\n            # value of None appears to be possible when this is called\n            # from a destructor (HTTPClient.__del__) as the weakref\n            # gets cleared before the destructor runs.\n            if cached_val is not None and cached_val is not self:", "description": ""}
{"id": "tornado-12", "project": "tornado", "bug_id": "12", "buggy_code": "from tornado.concurrent import TracebackFuture, return_future\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)", "fixed_code": "from tornado.concurrent import TracebackFuture, return_future, chain_future\n        args = urlparse.parse_qs(escape.native_str(response.body))\n        # Thanks to the _auth_return_future decorator, our \"callback\"\n        # argument is a Future, which we cannot pass as a callback to\n        # oauth2_request. Instead, have oauth2_request return a\n        # future and chain them together.\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)", "description": ""}
{"id": "tornado-2", "project": "tornado", "bug_id": "2", "buggy_code": "and \"Transfer-Encoding\" not in headers", "fixed_code": "and (\n                    \"Transfer-Encoding\" not in headers\n                    or headers[\"Transfer-Encoding\"] == \"chunked\"\n                )", "description": ""}
{"id": "tornado-13", "project": "tornado", "bug_id": "13", "buggy_code": "or start_line.method in (\"HEAD\", \"GET\")):", "fixed_code": "or getattr(start_line, 'method', None) in (\"HEAD\", \"GET\")):\n            # start_line may be a request or reponse start line; only\n            # the former has a method attribute.\n    'tornado.test.http1connection_test',", "description": ""}
{"id": "tornado-5", "project": "tornado", "bug_id": "5", "buggy_code": "callback_time_sec = self.callback_time / 1000.0", "fixed_code": "callback_time_sec = self.callback_time / 1000.0\n            # The period should be measured from the start of one call\n            # to the start of the next. If one call takes too long,\n            # skip cycles to get back to a multiple of the original\n            # schedule.\n        else:\n            # If the clock moved backwards, ensure we advance the next\n            # timeout instead of recomputing the same value again.\n            # This may result in long gaps between callbacks if the\n            # clock jumps backwards by a lot, but the far more common\n            # scenario is a small NTP adjustment that should just be\n            # ignored.\n            #\n            # Note that on some systems if time.time() runs slower\n            # than time.monotonic() (most common on windows), we\n            # effectively experience a small backwards time jump on\n            # every iteration because PeriodicCallback uses\n            # time.time() while asyncio schedules callbacks using\n            # time.monotonic().\n            # https://github.com/tornadoweb/tornado/issues/2333\n            self._next_timeout += callback_time_sec", "description": ""}
{"id": "tornado-14", "project": "tornado", "bug_id": "14", "buggy_code": "if IOLoop.current(instance=False) is None:", "fixed_code": "if IOLoop.current(instance=False) is not None:", "description": ""}
{"id": "tqdm-9", "project": "tqdm", "bug_id": "9", "buggy_code": "if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n        if gui: # pragma: no cover\n        if gui: # pragma: no cover\n        return len(self.iterable)\n            if gui: # pragma: no cover\n                        if gui: # pragma: no cover", "fixed_code": "if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n        if gui:  # pragma: no cover\n        if gui:  # pragma: no cover\n        return len(self.iterable) if self.iterable else self.total\n            if gui:  # pragma: no cover\n                        if gui:  # pragma: no cover", "description": ""}
{"id": "tqdm-7", "project": "tqdm", "bug_id": "7", "buggy_code": "RE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')", "fixed_code": "RE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')", "description": ""}
{"id": "tqdm-6", "project": "tqdm", "bug_id": "6", "buggy_code": "else self.total)", "fixed_code": "else getattr(self, \"total\", None))", "description": ""}
{"id": "tqdm-1", "project": "tqdm", "bug_id": "1", "buggy_code": "return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))", "fixed_code": "return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)", "description": ""}
{"id": "tqdm-8", "project": "tqdm", "bug_id": "8", "buggy_code": "l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)", "fixed_code": "l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)", "description": ""}
{"id": "tqdm-4", "project": "tqdm", "bug_id": "4", "buggy_code": "total *= unit_scale", "fixed_code": "if total:\n                total *= unit_scale", "description": ""}
{"id": "tqdm-2", "project": "tqdm", "bug_id": "2", "buggy_code": "if ncols:\n                return disp_trim(res, ncols)\n            if ncols:\n                return disp_trim(res, ncols)\n    if RE_ANSI.search(data):  # assume ANSI reset is required\n        return data + \"\\033[0m\"", "fixed_code": "return disp_trim(res, ncols) if ncols else res\n            return disp_trim(res, ncols) if ncols else res\n    ansi_present = bool(RE_ANSI.search(data))\n    if ansi_present and bool(RE_ANSI.search(data)):\n        # assume ANSI reset is required\n        return data if data.endswith(\"\\033[0m\") else data + \"\\033[0m\"", "description": ""}
{"id": "tqdm-5", "project": "tqdm", "bug_id": "5", "buggy_code": "if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None", "fixed_code": "if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n\n            self.total = total", "description": ""}
{"id": "fastapi-9", "project": "fastapi", "bug_id": "9", "buggy_code": "schema=BodySchema(None),", "fixed_code": "BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n\n        schema=BodySchema(**BodySchema_kwargs),", "description": ""}
{"id": "fastapi-11", "project": "fastapi", "bug_id": "11", "buggy_code": "return (\n    )", "fixed_code": "if not (\n    ):\n        return False\n    if field.sub_fields:\n        if not all(is_scalar_field(f) for f in field.sub_fields):\n            return False\n    return True", "description": ""}
{"id": "fastapi-7", "project": "fastapi", "bug_id": "7", "buggy_code": "status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={\"detail\": exc.errors()}", "fixed_code": "from fastapi.encoders import jsonable_encoder\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"detail\": jsonable_encoder(exc.errors())},", "description": ""}
{"id": "fastapi-16", "project": "fastapi", "bug_id": "16", "buggy_code": "if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )", "fixed_code": "encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )", "description": ""}
{"id": "fastapi-6", "project": "fastapi", "bug_id": "6", "buggy_code": "if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):", "fixed_code": "if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):", "description": ""}
{"id": "fastapi-1", "project": "fastapi", "bug_id": "1", "buggy_code": "include_none: bool = True,\n            include_none=include_none,\n                and (value is not None or include_none)\n                    include_none=include_none,\n                    include_none=include_none,\n                    include_none=include_none,\n        include_none=include_none,\n            include_none=False,\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)\n    res: Any, *, by_alias: bool = True, exclude_unset: bool\n            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)\n                by_alias=by_alias, skip_defaults=exclude_unset\n            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res\n            k: _prepare_response_content(v, exclude_unset=exclude_unset)\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n    return ModelSubclass(sub={}, y=1)", "fixed_code": "response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n                and (value is not None or not exclude_none)\n                    exclude_none=exclude_none,\n                    exclude_none=exclude_none,\n                    exclude_defaults=exclude_defaults,\n                    exclude_none=exclude_none,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n            exclude_none=True,\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)\n    res: Any,\n    *,\n    by_alias: bool = True,\n    exclude_unset: bool,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n            return res.dict(\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n                by_alias=by_alias, skip_defaults=exclude_unset,\n            _prepare_response_content(\n                item,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for item in res\n            k: _prepare_response_content(\n                v,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                    response_model_exclude_defaults=route.response_model_exclude_defaults,\n                    response_model_exclude_none=route.response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n    z: int = 0\n    w: int = None\n\n\nclass ModelDefaults(BaseModel):\n    w: Optional[str] = None\n    x: Optional[str] = None\n    y: str = \"y\"\n    z: str = \"z\"\n    return ModelSubclass(sub={}, y=1, z=0)\n\n\n@app.get(\n    \"/exclude_unset\", response_model=ModelDefaults, response_model_exclude_unset=True\n)\ndef get() -> ModelDefaults:\n    return ModelDefaults(x=None, y=\"y\")\n\n\n@app.get(\n    \"/exclude_defaults\",\n    response_model=ModelDefaults,\n    response_model_exclude_defaults=True,\n)\ndef get() -> ModelDefaults:\n    return ModelDefaults(x=None, y=\"y\")\n\n\n@app.get(\n    \"/exclude_none\", response_model=ModelDefaults, response_model_exclude_none=True\n)\ndef get() -> ModelDefaults:\n    return ModelDefaults(x=None, y=\"y\")\n\n\n@app.get(\n    \"/exclude_unset_none\",\n    response_model=ModelDefaults,\n    response_model_exclude_unset=True,\n    response_model_exclude_none=True,\n)\ndef get() -> ModelDefaults:\n    return ModelDefaults(x=None, y=\"y\")\n\n\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\n\n\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\n\n\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\n\n\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}", "description": ""}
{"id": "fastapi-8", "project": "fastapi", "bug_id": "8", "buggy_code": "route = self.route_class(", "fixed_code": "route_class_override: Optional[Type[APIRoute]] = None,\n        route_class = route_class_override or self.route_class\n        route = route_class(\n                    route_class_override=type(route),", "description": ""}
{"id": "fastapi-4", "project": "fastapi", "bug_id": "4", "buggy_code": "operation[\"parameters\"] = parameters", "fixed_code": "operation[\"parameters\"] = list(\n                    {param[\"name\"]: param for param in parameters}.values()\n                )", "description": ""}
{"id": "fastapi-3", "project": "fastapi", "bug_id": "3", "buggy_code": "if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )  # pragma: nocover", "fixed_code": "def _prepare_response_content(\n    res: Any, *, by_alias: bool = True, exclude_unset: bool\n) -> Any:\n    if isinstance(res, BaseModel):\n        if PYDANTIC_1:\n            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)\n        else:\n            return res.dict(\n                by_alias=by_alias, skip_defaults=exclude_unset\n            )  # pragma: nocover\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(v, exclude_unset=exclude_unset)\n            for k, v in res.items()\n        }\n    return res\n\n\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )", "description": ""}
{"id": "fastapi-12", "project": "fastapi", "bug_id": "12", "buggy_code": "raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN,\n                detail=\"Invalid authentication credentials\",\n            )", "fixed_code": "if self.auto_error:\n                raise HTTPException(\n                    status_code=HTTP_403_FORBIDDEN,\n                    detail=\"Invalid authentication credentials\",\n                )\n            else:\n                return None", "description": ""}
{"id": "fastapi-2", "project": "fastapi", "bug_id": "2", "buggy_code": "route = APIWebSocketRoute(path, endpoint=endpoint, name=name)", "fixed_code": "route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )", "description": ""}
{"id": "fastapi-13", "project": "fastapi", "bug_id": "13", "buggy_code": "if responses is None:\n                    responses = {}\n                responses = {**responses, **route.responses}\n                    responses=responses,", "fixed_code": "if responses is None:\n            responses = {}\n                combined_responses = {**responses, **route.responses}\n                    responses=combined_responses,", "description": ""}
{"id": "fastapi-5", "project": "fastapi", "bug_id": "5", "buggy_code": "use_type.__fields__[f.name] = f", "fixed_code": "use_type.__fields__[f.name] = create_cloned_field(f)", "description": ""}
{"id": "fastapi-14", "project": "fastapi", "bug_id": "14", "buggy_code": "additionalProperties: Optional[Union[bool, Any]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]", "fixed_code": "additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n    responses: Union[Responses, Dict[str, Response]]", "description": ""}
{"id": "sanic-1", "project": "sanic", "bug_id": "1", "buggy_code": "self.named_response_middleware[_rn].append(middleware)", "fixed_code": "self.named_response_middleware[_rn].appendleft(middleware)", "description": ""}
{"id": "sanic-4", "project": "sanic", "bug_id": "4", "buggy_code": "if \"//\" in self.app.config.SERVER_NAME:\n            return self.app.url_for(view_name, _external=True, **kwargs)", "fixed_code": "try:\n            if \"//\" in self.app.config.SERVER_NAME:\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass", "description": ""}
{"id": "sanic-3", "project": "sanic", "bug_id": "3", "buggy_code": "netloc = self.config.get(\"SERVER_NAME\", \"\")", "fixed_code": "# If the route has host defined, split that off\n        # TODO: Retain netloc and path separately in Route objects\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n\n            netloc = host or self.config.get(\"SERVER_NAME\", \"\")", "description": ""}
{"id": "sanic-5", "project": "sanic", "bug_id": "5", "buggy_code": "\"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},", "fixed_code": "\"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},", "description": ""}
{"id": "matplotlib-20", "project": "matplotlib", "bug_id": "20", "buggy_code": "ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n        Check if a point is in an axes.\n        axes: topmost axes containing the point, or None if no axes.\n\n                     if a.patch.contains_point(xy)]", "fixed_code": "ax.set_visible(False)\n        Return the topmost visible `~.axes.Axes` containing the point *xy*.\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n                     if a.patch.contains_point(xy) and a.get_visible()]", "description": ""}
{"id": "matplotlib-27", "project": "matplotlib", "bug_id": "27", "buggy_code": "self._label = str(label)", "fixed_code": "self._label = label", "description": ""}
{"id": "matplotlib-11", "project": "matplotlib", "bug_id": "11", "buggy_code": "if dpi is not None:\n            dpi_orig = self.figure.dpi\n            self.figure.dpi = dpi\n            tx, ty = self._get_xy_display()\n            return Bbox.from_bounds(tx, ty, 0, 0)\n        bbox, info, descent = self._get_layout(self._renderer)\n        x, y = self.get_unitless_position()\n        x, y = self.get_transform().transform((x, y))\n        bbox = bbox.translated(x, y)\n        if dpi is not None:\n            self.figure.dpi = dpi_orig\n        return bbox", "fixed_code": "if dpi is None:\n            dpi = self.figure.dpi\n            with cbook._setattr_cm(self.figure, dpi=dpi):\n                tx, ty = self._get_xy_display()\n                return Bbox.from_bounds(tx, ty, 0, 0)\n        with cbook._setattr_cm(self.figure, dpi=dpi):\n            bbox, info, descent = self._get_layout(self._renderer)\n            x, y = self.get_unitless_position()\n            x, y = self.get_transform().transform((x, y))\n            bbox = bbox.translated(x, y)\n            return bbox", "description": ""}
{"id": "matplotlib-7", "project": "matplotlib", "bug_id": "7", "buggy_code": "if hasattr(intensity, 'mask'):", "fixed_code": "if np.ma.is_masked(intensity):", "description": ""}
{"id": "matplotlib-29", "project": "matplotlib", "bug_id": "29", "buggy_code": "a, b = self.get_view_interval()\n        if inverted:\n            self.set_view_interval(max(a, b), min(a, b), ignore=True)\n        else:\n            self.set_view_interval(min(a, b), max(a, b), ignore=True)", "fixed_code": "# Currently, must be implemented in subclasses using set_xlim/set_ylim\n        # rather than generically using set_view_interval, so that shared\n        # axes get updated as well.\n        raise NotImplementedError('Derived must override')\n    def set_inverted(self, inverted):\n        # docstring inherited\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n\n    def set_inverted(self, inverted):\n        # docstring inherited\n        a, b = self.get_view_interval()\n        self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)", "description": ""}
{"id": "matplotlib-6", "project": "matplotlib", "bug_id": "6", "buggy_code": "if c.size == xsize:", "fixed_code": "# handle the documented special case of a 2D array with 1\n                # row which as RGB(A) to broadcast.\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:", "description": ""}
{"id": "matplotlib-28", "project": "matplotlib", "bug_id": "28", "buggy_code": "if self.get_xscale() == 'log':", "fixed_code": "if self.get_xscale() == 'log' and (left <= 0 or right <= 0):\n            # Axes init calls set_xlim(0, 1) before get_xlim() can be called,\n            # so only grab the limits if we really need them.\n            old_left, old_right = self.get_xlim()", "description": ""}
{"id": "matplotlib-1", "project": "matplotlib", "bug_id": "1", "buggy_code": "def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n    If *draw_disabled* is True, additionally replace drawing methods on\n    *renderer* by no-ops.  This is used by the tight-bbox-saving renderer,\n    which needs to walk through the artist tree to compute the tight-bbox, but\n    for which the output file may be closed early.\n    if draw_disabled:\n        for meth_name in dir(RendererBase):\n            if (meth_name.startswith(\"draw_\")\n                    or meth_name in [\"open_group\", \"close_group\"]):\n                setattr(renderer, meth_name, lambda *args, **kwargs: None)\n\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n            return backend_bases._get_renderer(fig, draw_disabled=True)", "fixed_code": "from matplotlib.cbook import _setattr_cm\ndef _get_renderer(figure, print_method=None):\n    If you need a renderer without any active draw methods use\n    cbook._setattr_cm to temporary patch them out at your call site.\n\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n\n        from .cbook import _setattr_cm\n        from .backend_bases import RendererBase\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n            return backend_bases._get_renderer(fig)", "description": ""}
{"id": "matplotlib-10", "project": "matplotlib", "bug_id": "10", "buggy_code": "# special-case label color to also apply to the offset text", "fixed_code": "# labelOn and labelcolor also apply to the offset text.\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))", "description": ""}
{"id": "matplotlib-26", "project": "matplotlib", "bug_id": "26", "buggy_code": "setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),", "fixed_code": "setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),", "description": ""}
{"id": "matplotlib-21", "project": "matplotlib", "bug_id": "21", "buggy_code": "def line_props_with_rcdefaults(subkey, explicit, zdelta=0):\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n            'whiskerprops', whiskerprops)\n            'capprops', capprops)\n            'medianprops', medianprops, zdelta)", "fixed_code": "def line_props_with_rcdefaults(subkey, explicit, zdelta=0,\n                                       use_marker=True):\n            if not use_marker:\n                d['marker'] = ''\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n            'whiskerprops', whiskerprops, use_marker=False)\n            'capprops', capprops, use_marker=False)\n            'medianprops', medianprops, zdelta, use_marker=False)", "description": ""}
{"id": "matplotlib-30", "project": "matplotlib", "bug_id": "30", "buggy_code": "x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])", "fixed_code": "if N == 1:\n        # convention: use the y = f(x=1) value for a 1-element lookup table\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])", "description": ""}
{"id": "matplotlib-24", "project": "matplotlib", "bug_id": "24", "buggy_code": "setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),", "fixed_code": "setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),", "description": ""}
{"id": "matplotlib-23", "project": "matplotlib", "bug_id": "23", "buggy_code": "x0, x1 = map(x_trf.inverted().transform, dL.intervalx)\n        y0, y1 = map(y_trf.inverted().transform, dL.intervaly)", "fixed_code": "x0, x1 = map(x_trf.transform, dL.intervalx)\n        y0, y1 = map(y_trf.transform, dL.intervaly)", "description": ""}
{"id": "matplotlib-4", "project": "matplotlib", "bug_id": "4", "buggy_code": "def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n        colors : list of colors, default: 'k'\n    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\n        colors : list of colors, default: 'k'\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,", "fixed_code": "def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n        colors : list of colors, default: :rc:`lines.color`\n    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',\n        colors : list of colors, default: :rc:`lines.color`\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,", "description": ""}
{"id": "matplotlib-15", "project": "matplotlib", "bug_id": "15", "buggy_code": "vmin=-1.0, vmax=1.0),\n                                              vmin=-1.0, vmax=1.0),\n    def __init__(self, linthresh, linscale=1.0,\n                 vmin=None, vmax=None, clip=False):\n            number of decades to use for each half of the linear range. For\n            example, when *linscale* == 1.0 (the default), the space used for\n            the positive and negative halves of the linear range will be equal\n            to one decade in the logarithmic range.\n        self._linscale_adj = (linscale / (1.0 - np.e ** -1))\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n                                              vmin=-1.0, vmax=1.0),", "fixed_code": "vmin=-1.0, vmax=1.0, base=10),\n                                              vmin=-1.0, vmax=1.0, base=10),\n    def __init__(self, linthresh, linscale=1.0, vmin=None, vmax=None,\n                 clip=False, base=None):\n            number of powers of *base* (decades for base 10) to use for each\n            half of the linear range. For example, when *linscale* == 1.0\n            (the default), the space used for the positive and negative halves\n            of the linear range will be equal to a decade in the logarithmic\n            range if ``base=10``.\n        base : float, default: None\n            For v3.2 the default is the old value of ``np.e``, but that is\n            deprecated for v3.3 when base will default to 10.  During the\n            transition, specify the *base* kwarg to avoid a deprecation\n            warning.\n        if base is None:\n            self._base = np.e\n            cbook.warn_deprecated(\"3.3\", message=\"default base will change \"\n                \"from np.e to 10.  To suppress this warning specify the base \"\n                \"kwarg.\")\n        else:\n            self._base = base\n        self._log_base = np.log(self._base)\n\n        self._linscale_adj = (linscale / (1.0 - self._base ** -1))\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n                                              vmin=-1.0, vmax=1.0, base=10),", "description": ""}
{"id": "matplotlib-3", "project": "matplotlib", "bug_id": "3", "buggy_code": "self._filled = True", "fixed_code": "# Initial guess: Assume the marker is filled unless the fillstyle is\n        # set to 'none'. The marker function will override this for unfilled\n        # markers.\n        self._filled = self._fillstyle != 'none'", "description": ""}
{"id": "matplotlib-12", "project": "matplotlib", "bug_id": "12", "buggy_code": "y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\n\n        ymin = np.resize(ymin, x.shape)\n        ymax = np.resize(ymax, x.shape)\n        verts = [((thisx, thisymin), (thisx, thisymax))\n                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\n        lines = mcoll.LineCollection(verts, colors=colors,", "fixed_code": "# Create and combine masked_arrays from input\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n        # Create and combine masked_arrays from input\n        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\n        ymin = np.ravel(ymin)\n        ymax = np.ravel(ymax)\n\n        masked_verts = np.ma.empty((len(x), 2, 2))\n        masked_verts[:, 0, 0] = x\n        masked_verts[:, 0, 1] = ymin\n        masked_verts[:, 1, 0] = x\n        masked_verts[:, 1, 1] = ymax\n        lines = mcoll.LineCollection(masked_verts, colors=colors,", "description": ""}
{"id": "matplotlib-2", "project": "matplotlib", "bug_id": "2", "buggy_code": "For non-filled markers, the *edgecolors* kwarg is ignored and\n            forced to 'face' internally.\n            edgecolors = 'face'\n                facecolors=colors,\n                edgecolors=edgecolors,", "fixed_code": "For non-filled markers, *edgecolors* is ignored. Instead, the color\n            is determined like with 'face', i.e. from *c*, *colors*, or\n            *facecolors*.\n                facecolors=colors if marker_obj.is_filled() else 'none',\n                edgecolors=edgecolors if marker_obj.is_filled() else colors,", "description": ""}
{"id": "matplotlib-5", "project": "matplotlib", "bug_id": "5", "buggy_code": "linewidths = rcParams['lines.linewidth']", "fixed_code": "if linewidths is None:\n                linewidths = rcParams['lines.linewidth']\n            elif np.iterable(linewidths):\n                linewidths = [\n                    lw if lw is not None else rcParams['lines.linewidth']\n                    for lw in linewidths]", "description": ""}
{"id": "matplotlib-14", "project": "matplotlib", "bug_id": "14", "buggy_code": "# Update bbox last, as it depends on font properties.", "fixed_code": "# Update fontproperties first, as it has lowest priority.\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n        # Update bbox last, as it depends on font properties.", "description": ""}
{"id": "matplotlib-25", "project": "matplotlib", "bug_id": "25", "buggy_code": "if positions is None or len(positions) == 0:\n        elif hasattr(positions, 'ndim') and positions.ndim > 1:", "fixed_code": "if positions is None:\n            raise ValueError('positions must be an array-like object')\n        # Force a copy of positions\n        positions = np.array(positions, copy=True)\n        if positions.size == 0:\n        elif positions.ndim > 1:", "description": ""}
{"id": "youtube-dl-32", "project": "youtube-dl", "bug_id": "32", "buggy_code": "transform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n//.*$', r'\\1', j)\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)", "fixed_code": "strip_jsonp,\n            transform_source=strip_jsonp,\n    return re.sub(\n        r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?://[^\\n]*)*$', r'\\1', code)", "description": ""}
{"id": "youtube-dl-35", "project": "youtube-dl", "bug_id": "35", "buggy_code": "'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),", "fixed_code": "upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n\n            'upload_date': unified_strdate(upload_date_str),\n        '%d/%m/%Y %H:%M:%S',", "description": ""}
{"id": "youtube-dl-34", "project": "youtube-dl", "bug_id": "34", "buggy_code": "([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\\[|\\{)", "fixed_code": "if cause:\n            msg += u' (caused by %r)' % cause\n            ([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\n                (?=\\[|\\{)\n            )", "description": ""}
{"id": "youtube-dl-33", "project": "youtube-dl", "bug_id": "33", "buggy_code": "import re\n\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n    date_format =  '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)", "fixed_code": "video_id = self._match_id(url)\n        timestamp = parse_iso8601(data['CreatedTime'])\n        r'(\\.[0-9]+)?(?:Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n    date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)", "description": ""}
{"id": "youtube-dl-20", "project": "youtube-dl", "bug_id": "20", "buggy_code": "(?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?", "fixed_code": "(?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?", "description": ""}
{"id": "youtube-dl-18", "project": "youtube-dl", "bug_id": "18", "buggy_code": "for f in ('_type', 'url', 'ie_key'):", "fixed_code": "for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):", "description": ""}
{"id": "youtube-dl-27", "project": "youtube-dl", "bug_id": "27", "buggy_code": "mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:\\.\\d+)?)$', time_expr)\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))", "fixed_code": "mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))", "description": ""}
{"id": "youtube-dl-9", "project": "youtube-dl", "bug_id": "9", "buggy_code": "def _parse_format_selection(tokens, endwith=[]):\n                    if string in endwith:\n                    elif string == ')':\n                        # ')' will be handled by the parentheses group\n                    if string == ',':\n                        second_choice = _parse_format_selection(tokens, [','])\n                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])\n                        audio_selector = _parse_format_selection(tokens, [','])\n                        current_selector = None\n                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))", "fixed_code": "def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n                    if string == ')':\n                        if not inside_group:\n                            # ')' will be handled by the parentheses group\n                            tokens.restore_last_token()\n                    elif inside_merge and string in ['/', ',']:\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])", "description": ""}
{"id": "youtube-dl-11", "project": "youtube-dl", "bug_id": "11", "buggy_code": "if int_str is None:\n        return None", "fixed_code": "if not isinstance(int_str, compat_str):\n        return int_str", "description": ""}
{"id": "youtube-dl-7", "project": "youtube-dl", "bug_id": "7", "buggy_code": "return v\n        if v.startswith(\"'\"):", "fixed_code": "v = re.sub(r\"\\\\'\", \"'\", v[1:-1])\n        elif v.startswith(\"'\"):", "description": ""}
{"id": "youtube-dl-29", "project": "youtube-dl", "bug_id": "29", "buggy_code": "return compat_str(upload_date)", "fixed_code": "if upload_date is not None:\n        return compat_str(upload_date)", "description": ""}
{"id": "youtube-dl-16", "project": "youtube-dl", "bug_id": "16", "buggy_code": "with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n        ('http://www.w3.org/ns/ttml', [\n            'http://www.w3.org/2004/11/ttaf1',\n            'http://www.w3.org/2006/04/ttaf1',\n            'http://www.w3.org/2006/10/ttaf1',\n        ('http://www.w3.org/ns/ttml#styling', [\n            'http://www.w3.org/ns/ttml#style',\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))", "fixed_code": "with open(dfxp_file, 'rb') as f:\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n    dfxp = compat_etree_fromstring(dfxp_data)", "description": ""}
{"id": "youtube-dl-42", "project": "youtube-dl", "bug_id": "42", "buggy_code": "fix_xml_all_ampersand,\n            transform_source=fix_xml_all_ampersand) \n    fix_xml_all_ampersand,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n        def fix_ampersand(s):\n            \"\"\" Fix unencoded ampersand in XML \"\"\"\n            return s.replace(u'& ', '&amp; ')\n            u'Downloading info', transform_source=fix_ampersand)\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')", "fixed_code": "fix_xml_ampersands\n            transform_source=fix_xml_ampersands)\n    fix_xml_ampersands,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n    fix_xml_ampersands,\n            u'Downloading info', transform_source=fix_xml_ampersands)\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        u'&amp;',\n        xml_str)", "description": ""}
{"id": "youtube-dl-6", "project": "youtube-dl", "bug_id": "6", "buggy_code": "return 0.0\n        begin_time = parse_dfxp_time_expr(para.attrib['begin'])\n            end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])", "fixed_code": "return\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n            if not dur:\n                continue\n            end_time = begin_time + dur", "description": ""}
{"id": "youtube-dl-28", "project": "youtube-dl", "bug_id": "28", "buggy_code": "return compat_chr(int(numstr, base))", "fixed_code": "# See https://github.com/rg3/youtube-dl/issues/7518\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass", "description": ""}
{"id": "youtube-dl-1", "project": "youtube-dl", "bug_id": "1", "buggy_code": "'': lambda v: v is not None,\n        '!': lambda v: v is None,", "fixed_code": "'': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),", "description": ""}
{"id": "youtube-dl-10", "project": "youtube-dl", "bug_id": "10", "buggy_code": "\"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\\")?)*\"|\n        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\')?)*'|", "fixed_code": "\"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nu]))*[^\"\\\\]*\"|\n        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nu]))*[^'\\\\]*'|", "description": ""}
{"id": "youtube-dl-19", "project": "youtube-dl", "bug_id": "19", "buggy_code": "filename = expand_path(outtmpl % template_dict)", "fixed_code": "import string\n            # expand_path translates '%%' into '%' and '$$' into '$'\n            # correspondingly that is not what we want since we need to keep\n            # '%%' intact for template dict substitution step. Working around\n            # with boundary-alike separator hack.\n            sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])\n            outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))\n\n            # outtmpl should be expand_path'ed before template dict substitution\n            # because meta fields may contain env variables we don't want to\n            # be expanded. For example, for outtmpl \"%(title)s.%(ext)s\" and\n            # title \"Hello $PATH\", we don't want `$PATH` to be expanded.\n            filename = expand_path(outtmpl).replace(sep, '') % template_dict", "description": ""}
{"id": "youtube-dl-26", "project": "youtube-dl", "bug_id": "26", "buggy_code": "(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|", "fixed_code": "\\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|", "description": ""}
{"id": "youtube-dl-8", "project": "youtube-dl", "bug_id": "8", "buggy_code": "current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))", "fixed_code": "current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])", "description": ""}
{"id": "youtube-dl-21", "project": "youtube-dl", "bug_id": "21", "buggy_code": "if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):", "fixed_code": "if isinstance(path, bytes):\n        path = path.decode('utf-8')\n    if isinstance(base, bytes):\n        base = base.decode('utf-8')\n    if not isinstance(base, compat_str) or not re.match(\n            r'^(?:https?:)?//', base):", "description": ""}
{"id": "youtube-dl-43", "project": "youtube-dl", "bug_id": "43", "buggy_code": "m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)", "fixed_code": "m = re.match(r'(?:https?:|)//[^/]+/(?:[^?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)", "description": ""}
{"id": "youtube-dl-38", "project": "youtube-dl", "bug_id": "38", "buggy_code": "self.report_login()\n        login_page = self._download_webpage(login_page_req, None, note=False,\n        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))\n            login_results = compat_urllib_request.urlopen(request).read()\n                'fb_dtsg': self._search_regex(r'\"fb_dtsg\":\"(.*?)\"', login_results, 'fb_dtsg'),\n                'submit[Continue]': self._search_regex(r'<input value=\"(.*?)\" name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))\n            check_response = compat_urllib_request.urlopen(check_req).read()", "fixed_code": "urlencode_postdata,\n        login_page = self._download_webpage(login_page_req, None,\n            note='Downloading login page',\n        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))\n            login_results = self._download_webpage(request, None,\n                note='Logging in', errnote='unable to fetch login page')\n                'fb_dtsg': self._search_regex(r'name=\"fb_dtsg\" value=\"(.+?)\"', login_results, 'fb_dtsg'),\n                'submit[Continue]': self._search_regex(r'<button[^>]+value=\"(.*?)\"[^>]+name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))\n            check_response = self._download_webpage(check_req, None,\n                note='Confirming login')\n\n\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')", "description": ""}
{"id": "youtube-dl-36", "project": "youtube-dl", "bug_id": "36", "buggy_code": "(?:[^#?]*\\#!/)?", "fixed_code": "(?:[^#]*?\\#!/)?", "description": ""}
{"id": "youtube-dl-31", "project": "youtube-dl", "bug_id": "31", "buggy_code": "duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?$''', s)\n    res = int(m.group('secs'))\n        if m.group('hours'):\n            res += int(m.group('hours')) * 60 * 60", "fixed_code": "parse_duration,\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n        (?:\n            (?P<only_mins>[0-9.]+)\\s*(?:mins?|minutes?)\\s*|\n            (?P<only_hours>[0-9.]+)\\s*(?:hours?)|\n\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?\n        )$''', s)\n    res = 0\n    if m.group('only_mins'):\n        return float_or_none(m.group('only_mins'), invscale=60)\n    if m.group('only_hours'):\n        return float_or_none(m.group('only_hours'), invscale=60 * 60)\n    if m.group('secs'):\n        res += int(m.group('secs'))\n    if m.group('hours'):\n        res += int(m.group('hours')) * 60 * 60", "description": ""}
{"id": "youtube-dl-37", "project": "youtube-dl", "bug_id": "37", "buggy_code": "lambda m: m.group(0).decode('unicode-escape'), s)", "fixed_code": "import codecs\n    unicode_escape = codecs.getdecoder('unicode_escape')\n        lambda m: unicode_escape(m.group(0))[0],\n        s)", "description": ""}
{"id": "youtube-dl-39", "project": "youtube-dl", "bug_id": "39", "buggy_code": "if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'", "fixed_code": "limit_length,\n    }, {\n        'note': 'Video without discernible title',\n        'url': 'https://www.facebook.com/video.php?v=274175099429670',\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video #274175099429670',\n        }\n            video_title = limit_length(video_title, 80)\n\n\ndef limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s", "description": ""}
{"id": "youtube-dl-41", "project": "youtube-dl", "bug_id": "41", "buggy_code": "date_str = date_str.replace(',',' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9:]*$', '', date_str)", "fixed_code": "date_str = date_str.replace(',', ' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)", "description": ""}
{"id": "youtube-dl-24", "project": "youtube-dl", "bug_id": "24", "buggy_code": "if m.group('strval') is not None:\n            comparison_value = m.group('strval')\n        actual_value = dct.get(m.group('key'))", "fixed_code": "actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/rg3/youtube-dl/issues/11082).\n            actual_value is not None and m.group('intval') is not None and\n                isinstance(actual_value, compat_str)):\n            comparison_value = m.group('strval') or m.group('intval')", "description": ""}
{"id": "youtube-dl-23", "project": "youtube-dl", "bug_id": "23", "buggy_code": "elif v.startswith('/*') or v == ',':\n        /\\*.*?\\*/|,(?=\\s*[\\]}])|", "fixed_code": "elif v.startswith('/*') or v.startswith('//') or v == ',':\n        /\\*.*?\\*/|//[^\\n]*|,(?=\\s*[\\]}])|", "description": ""}
{"id": "youtube-dl-4", "project": "youtube-dl", "bug_id": "4", "buggy_code": "r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]+)\\)$' % _NAME_RE, expr)\n                for v in m.group('args').split(',')])", "fixed_code": "r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()", "description": ""}
{"id": "youtube-dl-15", "project": "youtube-dl", "bug_id": "15", "buggy_code": "[a-zA-Z_][.a-zA-Z_0-9]*|", "fixed_code": "(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|", "description": ""}
{"id": "youtube-dl-3", "project": "youtube-dl", "bug_id": "3", "buggy_code": "r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)", "fixed_code": "r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)", "description": ""}
{"id": "youtube-dl-12", "project": "youtube-dl", "bug_id": "12", "buggy_code": "op = lambda attr, value: not str_op", "fixed_code": "op = lambda attr, value: not str_op(attr, value)", "description": ""}
{"id": "youtube-dl-40", "project": "youtube-dl", "bug_id": "40", "buggy_code": "from struct import unpack, pack\n        return unpack('!Q', self.read(8))[0]\n        return unpack('!I', self.read(4))[0]\n        return unpack('!B', self.read(1))[0]\n    stream.write(pack('!L', len(metadata))[1:])", "fixed_code": "struct_pack,\n    struct_unpack,\n        return struct_unpack('!Q', self.read(8))[0]\n        return struct_unpack('!I', self.read(4))[0]\n        return struct_unpack('!B', self.read(1))[0]\n    stream.write(struct_pack('!L', len(metadata))[1:])\nimport struct\n\ntry:\n    struct.pack(u'!I', 0)\nexcept TypeError:\n    # In Python 2.6 (and some 2.7 versions), struct requires a bytes argument\n    def struct_pack(spec, *args):\n        if isinstance(spec, compat_str):\n            spec = spec.encode('ascii')\n        return struct.pack(spec, *args)\n\n    def struct_unpack(spec, *args):\n        if isinstance(spec, compat_str):\n            spec = spec.encode('ascii')\n        return struct.unpack(spec, *args)\nelse:\n    struct_pack = struct.pack\n    struct_unpack = struct.unpack", "description": ""}
{"id": "youtube-dl-2", "project": "youtube-dl", "bug_id": "2", "buggy_code": "try:\n                            existing_format = next(\n                                fo for fo in formats\n                                if fo['format_id'] == representation_id)\n                        except StopIteration:\n                            full_info = formats_dict.get(representation_id, {}).copy()\n                            full_info.update(f)\n                            formats.append(full_info)\n                        else:\n                            existing_format.update(f)", "fixed_code": "# According to [1, 5.3.5.2, Table 7, page 35] @id of Representation\n                        # is not necessarily unique within a Period thus formats with\n                        # the same `format_id` are quite possible. There are numerous examples\n                        # of such manifests (see https://github.com/rg3/youtube-dl/issues/15111,\n                        # https://github.com/rg3/youtube-dl/issues/13919)\n                        full_info = formats_dict.get(representation_id, {}).copy()\n                        full_info.update(f)\n                        formats.append(full_info)", "description": ""}
{"id": "youtube-dl-13", "project": "youtube-dl", "bug_id": "13", "buggy_code": "if re.match(r'^(?:https?:)?//', path):", "fixed_code": "if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):", "description": ""}
{"id": "youtube-dl-5", "project": "youtube-dl", "bug_id": "5", "buggy_code": "pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta\n        return calendar.timegm(timetuple.timetuple())", "fixed_code": "pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n        return calendar.timegm(timetuple) + pm_delta * 3600", "description": ""}
{"id": "youtube-dl-14", "project": "youtube-dl", "bug_id": "14", "buggy_code": "def _extract_chapters(description, duration):\n        chapters = self._extract_chapters(description_original, video_duration)", "fixed_code": "def _extract_chapters_from_json(self, webpage, video_id, duration):\n        if not webpage:\n            return\n        player = self._parse_json(\n            self._search_regex(\n                r'RELATED_PLAYER_ARGS[\"\\']\\s*:\\s*({.+})\\s*,?\\s*\\n', webpage,\n                'player args', default='{}'),\n            video_id, fatal=False)\n        if not player or not isinstance(player, dict):\n            return\n        watch_next_response = player.get('watch_next_response')\n        if not isinstance(watch_next_response, compat_str):\n            return\n        response = self._parse_json(watch_next_response, video_id, fatal=False)\n        if not response or not isinstance(response, dict):\n            return\n        chapters_list = try_get(\n            response,\n            lambda x: x['playerOverlays']\n                       ['playerOverlayRenderer']\n                       ['decoratedPlayerBarRenderer']\n                       ['decoratedPlayerBarRenderer']\n                       ['playerBar']\n                       ['chapteredPlayerBarRenderer']\n                       ['chapters'],\n            list)\n        if not chapters_list:\n            return\n\n        def chapter_time(chapter):\n            return float_or_none(\n                try_get(\n                    chapter,\n                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n                    int),\n                scale=1000)\n        chapters = []\n        for next_num, chapter in enumerate(chapters_list, start=1):\n            start_time = chapter_time(chapter)\n            if start_time is None:\n                continue\n            end_time = (chapter_time(chapters_list[next_num])\n                        if next_num < len(chapters_list) else duration)\n            if end_time is None:\n                continue\n            title = try_get(\n                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n                compat_str)\n            chapters.append({\n                'start_time': start_time,\n                'end_time': end_time,\n                'title': title,\n            })\n        return chapters\n\n    def _extract_chapters_from_description(description, duration):\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)", "description": ""}
{"id": "youtube-dl-22", "project": "youtube-dl", "bug_id": "22", "buggy_code": "if (m.group('strval') is not None or\n            comparison_value = m.group('strval') or m.group('intval')", "fixed_code": "(?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n        if (m.group('quotedstrval') is not None or\n            m.group('strval') is not None or\n            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')\n            quote = m.group('quote')\n            if quote is not None:\n                comparison_value = comparison_value.replace(r'\\%s' % quote, quote)", "description": ""}
{"id": "youtube-dl-25", "project": "youtube-dl", "bug_id": "25", "buggy_code": "(r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n                i = int(im.group(0), base)", "fixed_code": "(r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n                i = int(im.group(1), base)", "description": ""}
{"id": "httpie-1", "project": "httpie", "bug_id": "1", "buggy_code": "if not exists(filename + suffix):\n            return filename + suffix", "fixed_code": "import errno\ndef trim_filename(filename, max_len):\n    if len(filename) > max_len:\n        trim_by = len(filename) - max_len\n        name, ext = os.path.splitext(filename)\n        if trim_by >= len(name):\n            filename = filename[:-trim_by]\n        else:\n            filename = name[:-trim_by] + ext\n    return filename\n\n\ndef get_filename_max_length(directory):\n    try:\n        max_len = os.pathconf(directory, 'PC_NAME_MAX')\n    except OSError as e:\n        if e.errno == errno.EINVAL:\n            max_len = 255\n        else:\n            raise\n    return max_len\n\n\ndef trim_filename_if_needed(filename, directory='.', extra=0):\n    max_len = get_filename_max_length(directory) - extra\n    if len(filename) > max_len:\n        filename = trim_filename(filename, max_len)\n    return filename\n\n\n        try_filename = trim_filename_if_needed(filename, extra=len(suffix))\n        try_filename += suffix\n        if not exists(try_filename):\n            return try_filename", "description": ""}
{"id": "httpie-4", "project": "httpie", "bug_id": "4", "buggy_code": "if 'Host' not in headers:", "fixed_code": "if 'Host' not in self._orig.headers:", "description": ""}
{"id": "httpie-2", "project": "httpie", "bug_id": "2", "buggy_code": "#       Network errors vs. bugs, etc.", "fixed_code": "requests_session.max_redirects = args.max_redirects", "description": ""}
{"id": "httpie-5", "project": "httpie", "bug_id": "5", "buggy_code": "regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep", "fixed_code": "self.escapes = ['\\\\\\\\' + sep for sep in separators]\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep", "description": ""}
{"id": "spacy-7", "project": "spacy", "bug_id": "7", "buggy_code": "Last tested with: v2.1.0\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n            seen_tokens.update(range(span.start, span.end))\n    get_sort_key = lambda span: (span.end - span.start, span.start)", "fixed_code": "Last tested with: v2.2.1\n    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n        # Check for end - 1 here because boundaries are inclusive\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n    get_sort_key = lambda span: (span.end - span.start, -span.start)", "description": ""}
{"id": "spacy-6", "project": "spacy", "bug_id": "6", "buggy_code": "return self.pipeline.pop(self.pipe_names.index(name))", "fixed_code": "removed = self.pipeline.pop(self.pipe_names.index(name))\n        return removed", "description": ""}
{"id": "spacy-1", "project": "spacy", "bug_id": "1", "buggy_code": "msg = getattr(err_cls, code)\n            return \"[{code}] {msg}\".format(code=code, msg=msg)", "fixed_code": "if not code.startswith('__'):\n                msg = getattr(err_cls, code)\n                return \"[{code}] {msg}\".format(code=code, msg=msg)\n            else:\n                return super().__getattribute__(code)", "description": ""}
{"id": "spacy-4", "project": "spacy", "bug_id": "4", "buggy_code": "head = (int(head) - 1) if head != \"0\" else id_", "fixed_code": "head = (int(head) - 1) if head not in [\"0\", \"_\"] else id_", "description": ""}
{"id": "spacy-3", "project": "spacy", "bug_id": "3", "buggy_code": "text_regex = re.compile(r\"(?<=<text xml:space=\\\"preserve\\\">).*(?=</text)\")\n    text_search = text_regex.search(article_text)", "fixed_code": "text_tag_regex = re.compile(r\"(?<=<text).*?(?=>)\")\ntext_regex = re.compile(r\"(?<=<text>).*(?=</text)\")\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)", "description": ""}
{"id": "spacy-5", "project": "spacy", "bug_id": "5", "buggy_code": "docs = _pipe(pipe, docs, kwargs)", "fixed_code": "docs = _pipe(docs, pipe, kwargs)", "description": ""}
{"id": "black-20", "project": "black", "bug_id": "20", "buggy_code": "src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"", "fixed_code": "src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"", "description": ""}
{"id": "black-18", "project": "black", "bug_id": "18", "buggy_code": "with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n        with open(src, \"w\", encoding=src_buffer.encoding) as f:\n            sys.stdout.write(diff_contents)\n    src = sys.stdin.read()\n            sys.stdout.write(dst)\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n        nl = \"\\r\\n\" if \"\\r\\n\" in src_txt[:1024] else \"\\n\"\n        src_txt += nl", "fixed_code": "import io\n\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff_contents)\n            f.detach()\n    newline, encoding, src = prepare_input(sys.stdin.buffer.read())\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    \"\"\"Analyze `src` and return a tuple of (newline, encoding, decoded_contents)\n\n    Where `newline` is either CRLF or LF, and `decoded_contents` is decoded with\n    universal newlines (i.e. only LF).\n    \"\"\"\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n\n\n        src_txt += \"\\n\"", "description": ""}
{"id": "black-9", "project": "black", "bug_id": "9", "buggy_code": "# Python 2-compatible code, so don't try Python 3 grammar.\n        return [pygram.python_grammar]", "fixed_code": "# Python 3-compatible code, so don't try Python 2 grammar\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]", "description": ""}
{"id": "black-11", "project": "black", "bug_id": "11", "buggy_code": "if not line.should_explode and is_line_short_enough(\n        line, line_length=line_length, line_str=line_str", "fixed_code": "# we don't want to split special comments like type annotations\n    # https://github.com/python/typing/issues/186\n    has_special_comment = False\n    for leaf in line.leaves:\n        for comment in line.comments_after(leaf):\n            if leaf.type == token.COMMA and is_special_comment(comment):\n                has_special_comment = True\n\n    if (\n        not has_special_comment\n        and not line.should_explode\n        and is_line_short_enough(line, line_length=line_length, line_str=line_str)\ndef is_special_comment(leaf: Leaf) -> bool:\n    \"\"\"Return True if the given leaf is a special comment.\n    Only returns true for type comments for now.\"\"\"\n    t = leaf.type\n    v = leaf.value\n    return bool(\n        (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(\"# type:\"))\n    )", "description": ""}
{"id": "black-7", "project": "black", "bug_id": "7", "buggy_code": "node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))", "fixed_code": "# Add parentheses around long tuple unpacking in assignments.\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)", "description": ""}
{"id": "black-16", "project": "black", "bug_id": "16", "buggy_code": "normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()", "fixed_code": "Symbolic links pointing outside of the root directory are ignored.\n\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n\n            raise", "description": ""}
{"id": "black-6", "project": "black", "bug_id": "6", "buggy_code": "TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\ndef get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n            pygram.python_grammar_no_print_statement_no_exec_statement,\n            pygram.python_grammar_no_print_statement,\n            pygram.python_grammar,\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n        return [pygram.python_grammar_no_print_statement_no_exec_statement]\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n    def __init__(self, grammar, convert=None, logger=None):\n        tokens = tokenize.generate_tokens(stream.readline)\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\ndef generate_tokens(readline):\n                        if async_def:", "fixed_code": "from blib2to3.pgen2.tokenize import TokenizerConfig\n    # The following two feature-flags are mutually exclusive, and exactly one should be\n    # set for every version of python.\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n@dataclass(frozen=True)\nclass ParserConfig:\n    grammar: Grammar\n    tokenizer_config: TokenizerConfig = TokenizerConfig()\n\n\ndef get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:\n            # Python 3.7+\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=True),\n            ),\n            # Python 3.0-3.6\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=False),\n            ),\n            # Python 2.7 with future print_function import\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            # Python 2.7\n            ParserConfig(pygram.python_grammar),\n        return [\n            # Python 2.7 with future print_function import\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            # Python 2.7\n            ParserConfig(pygram.python_grammar),\n        ]\n        configs = []\n        # If we have to parse both, try to parse async as a keyword first\n        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):\n            # Python 3.7+\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=True),\n                )\n            )\n        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):\n            # Python 3.0-3.6\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=False),\n                )\n            )\n        # At least one of the above branches must have been taken, because every Python\n        # version has exactly one of the two 'ASYNC_IS_*' flags\n        return configs\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n    def __init__(\n        self,\n        grammar,\n        convert=None,\n        logger=None,\n        tokenizer_config=tokenize.TokenizerConfig(),\n    ):\n        self.tokenizer_config = tokenizer_config\n        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\nfrom attr import dataclass\n@dataclass(frozen=True)\nclass TokenizerConfig:\n    async_is_reserved_keyword: bool = False\n\ndef generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n                        if async_is_reserved_keyword or async_def:\n\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\n\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))\n\n\n\n\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\n\n\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))", "description": ""}
{"id": "black-17", "project": "black", "bug_id": "17", "buggy_code": "if src_txt[-1] != \"\\n\":", "fixed_code": "if not lines:\n        return \"\", encoding, \"\\n\"\n\n    if src_txt[-1:] != \"\\n\":", "description": ""}
{"id": "black-1", "project": "black", "bug_id": "1", "buggy_code": "executor = ProcessPoolExecutor(max_workers=worker_count)\n        executor.shutdown()\n    executor: Executor,", "fixed_code": "try:\n        executor = ProcessPoolExecutor(max_workers=worker_count)\n    except OSError:\n        # we arrive here if the underlying system does not support multi-processing\n        # like in AWS Lambda, in which case we gracefully fallback to the default\n        # mono-process Executor by using None\n        executor = None\n\n        if executor is not None:\n            executor.shutdown()\n    executor: Optional[Executor],", "description": ""}
{"id": "black-10", "project": "black", "bug_id": "10", "buggy_code": "elif char == ' ':\n            elif char == '\\t':\n                current_column += 4", "fixed_code": "elif char in ' \\t':", "description": ""}
{"id": "black-8", "project": "black", "bug_id": "8", "buggy_code": "# Ensure a trailing comma when expected.\n                if leaves[-1].type != token.COMMA:\n                    leaves.append(Leaf(token.COMMA, \",\"))", "fixed_code": "# Ensure a trailing comma for imports, but be careful not to add one after\n            # any comments.\n                for i in range(len(leaves) - 1, -1, -1):\n                    if leaves[i].type == STANDALONE_COMMENT:\n                        continue\n                    elif leaves[i].type == token.COMMA:\n                        break\n                    else:\n                        leaves.insert(i + 1, Leaf(token.COMMA, \",\"))\n                        break", "description": ""}
{"id": "black-21", "project": "black", "bug_id": "21", "buggy_code": "mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False", "fixed_code": "mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False, encoding=\"utf8\"", "description": ""}
{"id": "black-23", "project": "black", "bug_id": "23", "buggy_code": "drv = driver.Driver(grammar, pytree.convert)\n    try:\n        result = drv.parse_string(src_txt, True)\n    except ParseError as pe:\n        lineno, column = pe.context[1]\n        lines = src_txt.splitlines()\n            faulty_line = lines[lineno - 1]\n        except IndexError:\n            faulty_line = \"<line number missing in source>\"\n        raise ValueError(f\"Cannot parse: {lineno}:{column}: {faulty_line}\") from None\n        raise AssertionError(f\"cannot parse source: {exc}\") from None\nasync def coroutine(arg):\nasync def coroutine(arg):", "fixed_code": "GRAMMARS = [\n    pygram.python_grammar_no_print_statement_no_exec_statement,\n    pygram.python_grammar_no_print_statement,\n    pygram.python_grammar_no_exec_statement,\n    pygram.python_grammar,\n]\n\n\n    for grammar in GRAMMARS:\n        drv = driver.Driver(grammar, pytree.convert)\n            result = drv.parse_string(src_txt, True)\n            break\n\n        except ParseError as pe:\n            lineno, column = pe.context[1]\n            lines = src_txt.splitlines()\n            try:\n                faulty_line = lines[lineno - 1]\n            except IndexError:\n                faulty_line = \"<line number missing in source>\"\n            exc = ValueError(f\"Cannot parse: {lineno}:{column}: {faulty_line}\")\n    else:\n        raise exc from None\n        elif (\n            prevp.type == token.RIGHTSHIFT\n            and prevp.parent\n            and prevp.parent.type == syms.shift_expr\n            and prevp.prev_sibling\n            and prevp.prev_sibling.type == token.NAME\n            and prevp.prev_sibling.value == 'print'\n        ):\n            # Python 2 print chevron\n            return NO\n\n        major, minor = sys.version_info[:2]\n        raise AssertionError(\n            f\"cannot use --safe with this file; failed to parse source file \"\n            f\"with Python {major}.{minor}'s builtin AST. Re-run with --fast \"\n            f\"or stop using deprecated Python 2 syntax. AST error message: {exc}\"\n        )\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\n\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n\n  exec(\"new-style exec\", {}, {})\nasync def coroutine(arg, exec=False):\n    exec(\"new-style exec\", {}, {})\nasync def coroutine(arg, exec=False):", "description": ""}
{"id": "black-4", "project": "black", "bug_id": "4", "buggy_code": "before -= self.previous_after", "fixed_code": "before = (\n            # Black should not insert empty lines at the beginning\n            # of the file\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )", "description": ""}
{"id": "black-15", "project": "black", "bug_id": "15", "buggy_code": "Type,\nclass FormatError(Exception):\n    \"\"\"Base exception for `# fmt: on` and `# fmt: off` handling.\n\n    It holds the number of bytes of the prefix consumed before the format\n    control comment appeared.\n    \"\"\"\n\n    def __init__(self, consumed: int) -> None:\n        super().__init__(consumed)\n        self.consumed = consumed\n\n    def trim_prefix(self, leaf: Leaf) -> None:\n        leaf.prefix = leaf.prefix[self.consumed :]\n\n    def leaf_from_consumed(self, leaf: Leaf) -> Leaf:\n        \"\"\"Returns a new Leaf from the consumed part of the prefix.\"\"\"\n        unformatted_prefix = leaf.prefix[: self.consumed]\n        return Leaf(token.NEWLINE, unformatted_prefix)\n\n\nclass FormatOn(FormatError):\n    \"\"\"Found a comment like `# fmt: on` in the file.\"\"\"\n\n\nclass FormatOff(FormatError):\n    \"\"\"Found a comment like `# fmt: off` in the file.\"\"\"\n\n\n    def show(cls, code: str) -> None:\n        list(v.visit(lib2to3_parse(code)))\nclass UnformattedLines(Line):\n    \"\"\"Just like :class:`Line` but stores lines which aren't reformatted.\"\"\"\n\n    def append(self, leaf: Leaf, preformatted: bool = True) -> None:\n        \"\"\"Just add a new `leaf` to the end of the lines.\n\n        The `preformatted` argument is ignored.\n\n        Keeps track of indentation `depth`, which is useful when the user\n        says `# fmt: on`. Otherwise, doesn't do anything with the `leaf`.\n        \"\"\"\n        try:\n            list(generate_comments(leaf))\n        except FormatOn as f_on:\n            self.leaves.append(f_on.leaf_from_consumed(leaf))\n            raise\n\n        self.leaves.append(leaf)\n        if leaf.type == token.INDENT:\n            self.depth += 1\n        elif leaf.type == token.DEDENT:\n            self.depth -= 1\n\n    def __str__(self) -> str:\n        \"\"\"Render unformatted lines from leaves which were added with `append()`.\n\n        `depth` is not used for indentation in this case.\n        \"\"\"\n        if not self:\n            return \"\\n\"\n\n        res = \"\"\n        for leaf in self.leaves:\n            res += str(leaf)\n        return res\n\n    def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Not implemented in this class. Raises `NotImplementedError`.\"\"\"\n        raise NotImplementedError(\"Unformatted lines don't store comments separately.\")\n\n    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n        \"\"\"Does nothing and returns False.\"\"\"\n        return False\n\n    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"Does nothing and returns False.\"\"\"\n        return False\n\n\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n            if self.current_line.__class__ == type:\n                self.current_line.depth += indent\n            else:\n                self.current_line = type(depth=self.current_line.depth + indent)\n        self.current_line = type(depth=complete_line.depth + indent)\n    def visit(self, node: LN) -> Iterator[Line]:\n        \"\"\"Main method to visit `node` and its children.\n\n        Yields :class:`Line` objects.\n        \"\"\"\n        if isinstance(self.current_line, UnformattedLines):\n            # File contained `# fmt: off`\n            yield from self.visit_unformatted(node)\n\n        else:\n            yield from super().visit(node)\n\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        # any comment within brackets is subject to splitting\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        # regular trailing comment\n                        self.current_line.append(comment)\n                        yield from self.line()\n\n                    else:\n                        # regular standalone comment\n                        yield from self.line()\n\n                        self.current_line.append(comment)\n                        yield from self.line()\n\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n\n            except FormatOn as f_on:\n                # This only happens here if somebody says \"fmt: on\" multiple\n                # times in a row.\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n    def visit_unformatted(self, node: LN) -> Iterator[Line]:\n        \"\"\"Used when file contained a `# fmt: off`.\"\"\"\n        if isinstance(node, Node):\n            for child in node.children:\n                yield from self.visit(child)\n\n        else:\n            try:\n                self.current_line.append(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.line()\n                yield from self.visit(node)\n\n            if node.type == token.ENDMARKER:\n                # somebody decided not to put a final `# fmt: on`\n                yield from self.line()\n        if pc.value in FMT_ON:\n            raise FormatOn(pc.consumed)\n\n        if pc.value in FMT_OFF:\n            if pc.type == STANDALONE_COMMENT:\n                raise FormatOff(pc.consumed)\n\n            prev = preceding_leaf(leaf)\n            if not prev or prev.type in WHITESPACE:  # standalone comment in disguise\n                raise FormatOff(pc.consumed)\n    if isinstance(line, UnformattedLines) or line.is_comment:\n    \"\"\"Allow `# fmt: off`/`# fmt: on` within bracket pairs.\n\n    Ignores `# fmt: off` and `# fmt: on` outside of brackets.\n\n    Raises :exc:`SyntaxError` if no matching `# fmt: on` is found for a `# fmt: off`\n    given inside brackets.\n    \"\"\"\n        try_again = hide_fmt_off(node)\ndef hide_fmt_off(node: Node) -> bool:\n    bt = BracketTracker()\n    for leaf in node.leaves():\n        bt.mark(leaf)\n        if bt.depth == 0:\n            continue\n    while container is not None:", "fixed_code": "def show(cls, code: Union[str, Leaf, Node]) -> None:\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n    def line(self, indent: int = 0) -> Iterator[Line]:\n            self.current_line.depth += indent\n        self.current_line = Line(depth=complete_line.depth + indent)\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    # any comment within brackets is subject to splitting\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    # regular trailing comment\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    # regular standalone comment\n                    yield from self.line()\n\n                    self.current_line.append(comment)\n                    yield from self.line()\n\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n    def visit_STANDALONE_COMMENT(self, leaf: Leaf) -> Iterator[Line]:\n        if not self.current_line.bracket_tracker.any_open_brackets():\n            yield from self.line()\n        yield from self.visit_default(leaf)\n        if parent.type == syms.file_input:\n            break\n\n    if line.is_comment:\n    \"\"\"Convert content between `# fmt: off`/`# fmt: on` into standalone comments.\"\"\"\n        try_again = convert_one_fmt_off_pair(node)\ndef convert_one_fmt_off_pair(node: Node) -> bool:\n    \"\"\"Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.\n    Returns True if a pair was converted.\n    \"\"\"\n    for leaf in node.leaves():\n                # We only want standalone comments. If there's no previous leaf or\n                # the previous leaf is indentation, it's a standalone comment in\n                # disguise.\n                if comment.type != STANDALONE_COMMENT:\n                    prev = preceding_leaf(leaf)\n                    if prev and prev.type not in WHITESPACE:\n                        continue\n\n                if hidden_value.endswith(\"\\n\"):\n                    # That happens when one of the `ignored_nodes` ended with a NEWLINE\n                    # leaf (possibly followed by a DEDENT).\n                    hidden_value = hidden_value[:-1]\n    \"\"\"Starting from the container of `leaf`, generate all leaves until `# fmt: on`.\n\n    Stops at the end of the block.\n    \"\"\"\n    while container is not None and container.type != token.ENDMARKER:", "description": ""}
{"id": "black-3", "project": "black", "bug_id": "3", "buggy_code": "exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False", "fixed_code": "exists=True, file_okay=True, dir_okay=False, readable=True, allow_dash=False", "description": ""}
{"id": "black-12", "project": "black", "bug_id": "12", "buggy_code": "_for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n            self._for_loop_variable += 1\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n            self._for_loop_variable -= 1\n            self._lambda_arguments += 1\n        if self._lambda_arguments and leaf.type == token.COLON:\n            self._lambda_arguments -= 1", "fixed_code": "_for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n            self._for_loop_depths.append(self.depth)\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n            self._for_loop_depths.pop()\n            self._lambda_argument_depths.append(self.depth)\n        if (\n            self._lambda_argument_depths\n            and self._lambda_argument_depths[-1] == self.depth\n            and leaf.type == token.COLON\n        ):\n            self._lambda_argument_depths.pop()", "description": ""}
{"id": "black-2", "project": "black", "bug_id": "2", "buggy_code": "is_fmt_on = False\n        for comment in list_comments(container.prefix, is_endmarker=False):\n            if comment.value in FMT_ON:\n                is_fmt_on = True\n            elif comment.value in FMT_OFF:\n                is_fmt_on = False\n        if is_fmt_on:\n        yield container\n        container = container.next_sibling", "fixed_code": "if fmt_on(container):\n        # fix for fmt: on in children\n        if contains_fmt_on_at_column(container, leaf.column):\n            for child in container.children:\n                if contains_fmt_on_at_column(child, leaf.column):\n                    return\n                yield child\n        else:\n            yield container\n            container = container.next_sibling\n\ndef fmt_on(container: LN) -> bool:\n    is_fmt_on = False\n    for comment in list_comments(container.prefix, is_endmarker=False):\n        if comment.value in FMT_ON:\n            is_fmt_on = True\n        elif comment.value in FMT_OFF:\n            is_fmt_on = False\n    return is_fmt_on\n\n\ndef contains_fmt_on_at_column(container: LN, column: int) -> bool:\n    for child in container.children:\n        if (\n            isinstance(child, Node)\n            and first_leaf_column(child) == column\n            or isinstance(child, Leaf)\n            and child.column == column\n        ):\n            if fmt_on(child):\n                return True\n\n    return False\n\n\ndef first_leaf_column(node: Node) -> Optional[int]:\n    for child in node.children:\n        if isinstance(child, Leaf):\n            return child.column\n    return None", "description": ""}
{"id": "black-13", "project": "black", "bug_id": "13", "buggy_code": "if token == 'def':\n                            async_def = True\n                            async_def_indent = indents[-1]", "fixed_code": "if token in ('def', 'for'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]", "description": ""}
{"id": "black-5", "project": "black", "bug_id": "5", "buggy_code": "if leaf.parent and leaf.parent.type == syms.arglist:\n            # Ensure a trailing comma for imports, but be careful not to add one after\n            # any comments.\n            if original.is_import:", "fixed_code": "if leaf.parent and leaf.parent.type in {\n                    syms.arglist,\n                    syms.typedargslist,\n                }:\n            # Ensure a trailing comma for imports and standalone function arguments, but\n            # be careful not to add one after any comments.\n            no_commas = original.is_def and not any(\n                l.type == token.COMMA for l in leaves\n            )\n\n            if original.is_import or no_commas:", "description": ""}
{"id": "black-14", "project": "black", "bug_id": "14", "buggy_code": "imports = set()\n            for import_from_child in first_child.children[3:]:\n                if isinstance(import_from_child, Leaf):\n                    if import_from_child.type == token.NAME:\n                        imports.add(import_from_child.value)\n                else:\n                    assert import_from_child.type == syms.import_as_names\n                    for leaf in import_from_child.children:\n                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n                            imports.add(leaf.value)\nfrom __future__ import unicode_literals\nfrom __future__ import unicode_literals", "fixed_code": "Generator,\n    imports: Set[str] = set()\n\n    def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:\n        for child in children:\n            if isinstance(child, Leaf):\n                if child.type == token.NAME:\n                    yield child.value\n            elif child.type == syms.import_as_name:\n                orig_name = child.children[0]\n                assert isinstance(orig_name, Leaf), \"Invalid syntax parsing imports\"\n                assert orig_name.type == token.NAME, \"Invalid syntax parsing imports\"\n                yield orig_name.value\n            elif child.type == syms.import_as_names:\n                yield from get_imports_from_children(child.children)\n            else:\n                assert False, \"Invalid syntax parsing imports\"\n\n            imports |= set(get_imports_from_children(first_child.children[3:]))\nfrom __future__ import unicode_literals as _unicode_literals\nfrom __future__ import absolute_import\nfrom __future__ import print_function as lol, with_function\nfrom __future__ import unicode_literals as _unicode_literals\nfrom __future__ import absolute_import\nfrom __future__ import print_function as lol, with_function", "description": ""}
{"id": "black-22", "project": "black", "bug_id": "22", "buggy_code": "from functools import partial\n    Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union\n    comments: Dict[LeafID, Leaf] = Factory(dict)\n            if self.maybe_adapt_standalone_comment(leaf):\n                return\n        return bool(self) and self.leaves[0].type == STANDALONE_COMMENT\n            self.leaves.pop()\n                self.leaves.pop()\n            self.leaves.pop()\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        \"\"\"Hack a standalone comment to act as a trailing comment for line splitting.\n\n        If this line has brackets and a standalone `comment`, we need to adapt\n        it to be able to still reformat the line.\n\n        This is not perfect, the line to which the standalone comment gets\n        appended will appear \"too long\" when splitting.\n        \"\"\"\n        if not (\n        comment.type = token.COMMENT\n        comment.prefix = '\\n' + '    ' * (self.depth + 1)\n        return self.append_comment(comment)\n\n    def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Add an inline comment to the line.\"\"\"\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n    def last_non_delimiter(self) -> Leaf:\n        \"\"\"Return the last non-delimiter on the line. Raise LookupError otherwise.\"\"\"\n        for i in range(len(self.leaves)):\n            last = self.leaves[-i - 1]\n            if not is_delimiter(last):\n                return last\n        raise LookupError(\"No non-delimiters found\")\n        for comment in self.comments.values():\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        \"\"\"Does nothing and returns False.\"\"\"\n        return False\n\n    if isinstance(line, UnformattedLines):\n    if len(line_str) <= line_length and '\\n' not in line_str:\n        split_funcs = [delimiter_split]\n        if '\\n' not in line_str:\n            # Only attempt RHS if we don't have multiline strings or comments\n            # on this line.\n            split_funcs.append(right_hand_split)\n            for l in split_func(line, py36=py36):\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n    This kind of split doesn't increase indentation.\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)", "fixed_code": "from functools import partial, wraps\n    Callable,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\nIndex = int\nSplitFunc = Callable[['Line', bool], Iterator['Line']]\n    comments: List[Tuple[Index, Leaf]] = Factory(list)\n    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:\n        \"\"\"Like :func:`append()` but disallow invalid standalone comment structure.\n\n        Raises ValueError when any `leaf` is appended after a standalone comment\n        or when a standalone comment is not the first leaf on the line.\n        \"\"\"\n        if self.bracket_tracker.depth == 0:\n            if self.is_comment:\n                raise ValueError(\"cannot append to standalone comments\")\n\n            if self.leaves and leaf.type == STANDALONE_COMMENT:\n                raise ValueError(\n                    \"cannot append standalone comments to a populated line\"\n                )\n\n        self.append(leaf, preformatted=preformatted)\n\n        return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT\n    @property\n    def contains_standalone_comments(self) -> bool:\n        \"\"\"If so, needs to be split before emitting.\"\"\"\n        for leaf in self.leaves:\n            if leaf.type == STANDALONE_COMMENT:\n                return True\n\n        return False\n\n            self.remove_trailing_comma()\n                self.remove_trailing_comma()\n            self.remove_trailing_comma()\n    def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Add an inline or standalone comment to the line.\"\"\"\n        if (\n            comment.prefix = ''\n        after = len(self.leaves) - 1\n        if after == -1:\n            self.comments.append((after, comment))\n    def comments_after(self, leaf: Leaf) -> Iterator[Leaf]:\n        \"\"\"Generate comments that should appear directly after `leaf`.\"\"\"\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n\n    def remove_trailing_comma(self) -> None:\n        \"\"\"Remove the trailing comma and moves the comments attached to it.\"\"\"\n        comma_index = len(self.leaves) - 1\n        for i in range(len(self.comments)):\n            comment_index, comment = self.comments[i]\n            if comment_index == comma_index:\n                self.comments[i] = (comma_index - 1, comment)\n        self.leaves.pop()\n        for _, comment in self.comments:\n    if isinstance(line, UnformattedLines) or line.is_comment:\n    if (\n        len(line_str) <= line_length\n        and '\\n' not in line_str  # multiline strings\n        and not line.contains_standalone_comments\n    ):\n    split_funcs: List[SplitFunc]\n        split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]\n            for l in split_func(line, py36):\n            for comment_after in line.comments_after(leaf):\n            for comment_after in line.comments_after(leaf):\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    \"\"\"Normalize prefix of the first leaf in every line returned by `split_func`.\n\n    This is a decorator over relevant split functions.\n    \"\"\"\n\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n\n    return split_wrapper\n\n\n@dont_increase_indentation\n\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n\n        yield from append_to_line(leaf)\n\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n\n        yield current_line\n\n\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n    \"\"\"Split standalone comments from the rest of the line.\"\"\"\n    for leaf in line.leaves:\n        if leaf.type == STANDALONE_COMMENT:\n            if leaf.bracket_depth == 0:\n                break\n\n    else:\n        raise CannotSplit(\"Line does not have any standalone comments\")\n\n    current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n\n    if current_line:", "description": ""}
{"id": "pandas-135", "project": "pandas", "bug_id": "135", "buggy_code": "return self._aggregate_series_pure_python(obj, func)", "fixed_code": "except TypeError as err:\n            if \"ndarray\" in str(err):\n                # raised in libreduction if obj's values is no ndarray\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)", "description": ""}
{"id": "pandas-61", "project": "pandas", "bug_id": "61", "buggy_code": "return self._get_values(key)", "fixed_code": "return self.iloc[key]", "description": ""}
{"id": "pandas-132", "project": "pandas", "bug_id": "132", "buggy_code": "return _wrap_results(result, values.dtype)\n@disallow(\"M8\")", "fixed_code": "orig_dtype = values.dtype\n    values, mask, dtype, dtype_max, fill_value = _get_values(values, skipna, mask=mask)\n\n    return _wrap_results(result, orig_dtype)\n@disallow(\"M8\", \"m8\")", "description": ""}
{"id": "pandas-59", "project": "pandas", "bug_id": "59", "buggy_code": "window = self._get_window(other)", "fixed_code": "window = self._get_window(other) if not self.is_freq_type else self.win_freq", "description": ""}
{"id": "pandas-92", "project": "pandas", "bug_id": "92", "buggy_code": "where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n        if isinstance(value, Period):\n            if value.freq != self.freq:\n                raise raise_on_incompatible(self, value)\n            value = value.ordinal\n                value = Period(value, freq=self.freq).ordinal\n        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n        bins = memb.searchsorted(rng, side=\"left\")\n        result = ts[2007]\n        result = ts[2007]\n        msg = \"Input has different freq=H from PeriodIndex\"\n        msg = \"Input has different freq=5D from PeriodIndex\"", "fixed_code": "where = Period(where, freq=self.index.freq)\n        if isinstance(value, Period) or value is NaT:\n            self._data._check_compatible_with(value)\n                value = Period(value, freq=self.freq)\n        elif not isinstance(value, PeriodArray):\n            raise TypeError(\n                \"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n\n        # Wrap in PeriodArray for PeriodArray.searchsorted\n        prng = type(memb._data)(rng, dtype=memb.dtype)\n        bins = memb.searchsorted(prng, side=\"left\")\n        result = ts[\"2007\"]\n        result = ts[\"2007\"]\n        assert pidx.searchsorted(pd.NaT) == 0\n\n        msg = \"Input has different freq=H from PeriodArray\"\n        msg = \"Input has different freq=5D from PeriodArray\"\n    def test_searchsorted_invalid(self):\n        pidx = pd.PeriodIndex(\n            [\"2014-01-01\", \"2014-01-02\", \"2014-01-03\", \"2014-01-04\", \"2014-01-05\"],\n            freq=\"D\",\n        )\n\n        other = np.array([0, 1], dtype=np.int64)\n\n        msg = \"requires either a Period or PeriodArray\"\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other)\n\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other.astype(\"timedelta64[ns]\"))\n\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(4))\n\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(\"NaT\", \"ms\"))\n\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(4, \"ns\"))\n\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(\"NaT\", \"ns\"))", "description": ""}
{"id": "pandas-66", "project": "pandas", "bug_id": "66", "buggy_code": "new_values = self._data.fast_xs(loc)\n            # may need to box a datelike-scalar\n            #\n            # if we encounter an array-like and we only have 1 dim\n            # that means that their are list/ndarrays inside the Series!\n            # so just return them (GH 6394)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n        return self._block.values[loc]", "fixed_code": "# In this case loc should be an integer\n            if self.ndim == 1:\n                # if we encounter an array-like and we only have 1 dim\n                # that means that their are list/ndarrays inside the Series!\n                # so just return them (GH 6394)\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n        raise NotImplementedError(\"Use series._values[loc] instead\")", "description": ""}
{"id": "pandas-104", "project": "pandas", "bug_id": "104", "buggy_code": "order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            # fix order.\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)", "fixed_code": "order = list(range(1, result.index.nlevels)) + [0]\n\n            # temporarily saves the index names\n            index_names = np.array(result.index.names)\n            # set index names to positions to avoid confusion\n            result.index.names = np.arange(len(index_names))\n\n            # place quantiles on the inside\n            result = result.reorder_levels(order)\n            # restore the index names in order\n            result.index.names = index_names[order]\n            # reorder rows to keep things sorted\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()", "description": ""}
{"id": "pandas-50", "project": "pandas", "bug_id": "50", "buggy_code": "ret[mask] = False", "fixed_code": "if opname == \"__ne__\":\n                    ret[(self._codes == -1) & (other_codes == -1)] = True\n                else:\n                    ret[mask] = False", "description": ""}
{"id": "pandas-57", "project": "pandas", "bug_id": "57", "buggy_code": "assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat.remove_categories(to_replace, inplace=True)\n                    categories[index] = value", "fixed_code": "check_category_order=True,\n    check_category_order : bool, default True\n        Whether to compare category order of internal Categoricals\n\n        .. versionadded:: 1.0.2\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )\n\n        # build a dict of (to replace -> value) pairs\n        if is_list_like(to_replace):\n            # if to_replace is list-like and value is scalar\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            # if both to_replace and value are scalar\n            replace_dict = {to_replace: value}\n\n        # other cases, like if both to_replace and value are list-like or if\n        # to_replace is a dict, are handled separately in NDFrame\n        for replace_value, new_value in replace_dict.items():\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                    cat.remove_categories(replace_value, inplace=True)\n                    categories[index] = new_value", "description": ""}
{"id": "pandas-168", "project": "pandas", "bug_id": "168", "buggy_code": "obj._check_label_or_level_ambiguity(gpr)\n            elif obj._is_level_reference(gpr):", "fixed_code": "obj._check_label_or_level_ambiguity(gpr, axis=axis)\n            elif obj._is_level_reference(gpr, axis=axis):", "description": ""}
{"id": "pandas-157", "project": "pandas", "bug_id": "157", "buggy_code": "is_datetime64_dtype,\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):", "fixed_code": "if is_datetimelike(lt):", "description": ""}
{"id": "pandas-150", "project": "pandas", "bug_id": "150", "buggy_code": "if left_value != right_value:", "fixed_code": "if np.any(left_value != right_value):", "description": ""}
{"id": "pandas-159", "project": "pandas", "bug_id": "159", "buggy_code": "result = _arith_op(this.values, other.values)", "fixed_code": "from pandas.core.ops.missing import dispatch_fill_zeros\n            with np.errstate(all=\"ignore\"):\n                result = _arith_op(this.values, other.values)\n            result = dispatch_fill_zeros(func, this.values, other.values, result)", "description": ""}
{"id": "pandas-32", "project": "pandas", "bug_id": "32", "buggy_code": "from io import BytesIO\n            # Copy to BytesIO, and ensure no encoding\n            contents = filepath_or_buffer.read()\n            try:\n                contents = contents.encode(self._encoding)\n            except UnicodeEncodeError:\n                pass\n            self.filepath_or_buffer = BytesIO(contents)", "fixed_code": "# Since xport files include non-text byte sequences, xport files\n            # should already be opened in binary mode in Python 3.\n            self.filepath_or_buffer = filepath_or_buffer", "description": ""}
{"id": "pandas-166", "project": "pandas", "bug_id": "166", "buggy_code": "res = concat(frames, axis=1, join=\"outer\", verify_integrity=True)\n                    return concat(frames, axis=1, join=how, verify_integrity=True)", "fixed_code": "res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )", "description": ""}
{"id": "pandas-35", "project": "pandas", "bug_id": "35", "buggy_code": "# To avoid a reference cycle, pass a weakref of self to _engine_type.\n        period = weakref.ref(self)", "fixed_code": "# To avoid a reference cycle, pass a weakref of self._values to _engine_type.\n        period = weakref.ref(self._values)", "description": ""}
{"id": "pandas-161", "project": "pandas", "bug_id": "161", "buggy_code": "indexer = np.where(values_codes != -1)\n                codes[indexer] = values_codes[values_codes != -1]", "fixed_code": "indexer = np.where(codes == -1)\n                codes[indexer] = values_codes[indexer]", "description": ""}
{"id": "pandas-102", "project": "pandas", "bug_id": "102", "buggy_code": "columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed_code": "if isinstance(values, np.ndarray) and values.ndim > 1:\n            # GH#12513 a EA dtype passed with a 2D array, split into\n            #  multiple EAs that view the values\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)", "description": ""}
{"id": "pandas-69", "project": "pandas", "bug_id": "69", "buggy_code": "if is_integer(i) and not ax.holds_integer():", "fixed_code": "if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):", "description": ""}
{"id": "pandas-56", "project": "pandas", "bug_id": "56", "buggy_code": "series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])", "fixed_code": "series = self._ixs(col, axis=1)\n            return series._values[index]", "description": ""}
{"id": "pandas-105", "project": "pandas", "bug_id": "105", "buggy_code": "def transpose(self, *args, **kwargs):\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with numpy.\n        return super().transpose(1, 0, **kwargs)\n    def transpose(self, *args, **kwargs):\n        \"\"\"\n        Permute the dimensions of the %(klass)s\n\n        Parameters\n        ----------\n        args : %(args_transpose)s\n        copy : bool, default False\n            Make a copy of the underlying data. Mixed-dtype data will\n            always result in a copy\n        **kwargs\n            Additional keyword arguments will be passed to the function.\n\n        Returns\n        -------\n        y : same as input\n\n        Examples\n        --------\n        >>> p.transpose(2, 0, 1)\n        >>> p.transpose(2, 0, 1, copy=True)\n        \"\"\"\n\n        # construct the args\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n\n        # we must have unique axes\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n\n@pytest.fixture(\n    params=[\n        (pd.Index, False),\n        (pd.Series, False),\n        (pd.DataFrame, False),\n        pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),\n        (tm.to_array, False),\n    ],\n    ids=id_func,\n)\ndef box_transpose_fail(request):\n    \"\"\"\n    Fixture similar to `box` but testing both transpose cases for DataFrame,\n    with the transpose=True case xfailed.\n    \"\"\"\n    # GH#23620\n    return request.param", "fixed_code": "def transpose(self, *args, copy: bool = False):\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n        # construct the args\n\n        dtypes = list(self.dtypes)\n        if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):\n            # We have EAs with the same dtype. We can preserve that dtype in transpose.\n            dtype = dtypes[0]\n            arr_type = dtype.construct_array_type()\n            values = self.values\n\n            new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]\n            result = self._constructor(\n                dict(zip(self.index, new_values)), index=self.columns\n            )\n\n        else:\n            new_values = self.values.T\n            if copy:\n                new_values = new_values.copy()\n            result = self._constructor(\n                new_values, index=self.columns, columns=self.index\n            )\n\n        return result.__finalize__(self)\n\n    def test_transpose(self, data):\n        df = pd.DataFrame({\"A\": data[:4], \"B\": data[:4]}, index=[\"a\", \"b\", \"c\", \"d\"])\n        result = df.T\n        expected = pd.DataFrame(\n            {\n                \"a\": type(data)._from_sequence([data[0]] * 2, dtype=data.dtype),\n                \"b\": type(data)._from_sequence([data[1]] * 2, dtype=data.dtype),\n                \"c\": type(data)._from_sequence([data[2]] * 2, dtype=data.dtype),\n                \"d\": type(data)._from_sequence([data[3]] * 2, dtype=data.dtype),\n            },\n            index=[\"A\", \"B\"],\n        )\n        self.assert_frame_equal(result, expected)\n        self.assert_frame_equal(np.transpose(np.transpose(df)), df)\n        self.assert_frame_equal(np.transpose(np.transpose(df[[\"A\"]])), df[[\"A\"]])\n    @pytest.mark.xfail(reason=\"Inconsistent sizes.\")\n    def test_transpose(self, data):\n        super().test_transpose(data)\n\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)", "description": ""}
{"id": "pandas-58", "project": "pandas", "bug_id": "58", "buggy_code": "codes = np.asarray(codes)  # #21767", "fixed_code": "if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            # Avoid the implicit conversion of Int to object\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)", "description": ""}
{"id": "pandas-133", "project": "pandas", "bug_id": "133", "buggy_code": "else:\n            _maybe_transposed_self = self", "fixed_code": "axis = self._get_axis_number(axis)", "description": ""}
{"id": "pandas-67", "project": "pandas", "bug_id": "67", "buggy_code": "from pandas._libs import NaT, algos as libalgos, lib, tslib, writers", "fixed_code": "from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n    def iget(self, key):\n        # GH#31649 we need to wrap scalars in Timestamp/Timedelta\n        # TODO: this can be removed if we ever have 2D EA\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result", "description": ""}
{"id": "pandas-93", "project": "pandas", "bug_id": "93", "buggy_code": "from datetime import datetime, timedelta\nfrom typing import Any, Sequence, Type, Union, cast\nimport warnings\nfrom pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib\nfrom pandas._libs.tslibs.c_timestamp import integer_op_not_supported\nfrom pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period\nfrom pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds\nfrom pandas._libs.tslibs.timestamps import RoundTo, round_nsint64\nfrom pandas._typing import DatetimeLikeScalar\nfrom pandas.compat import set_function_name\nfrom pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning\nfrom pandas.util._decorators import Appender, Substitution\nfrom pandas.util._validators import validate_fillna_kwargs\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_object_dtype,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\nfrom pandas.core.dtypes.generic import ABCIndexClass, ABCPeriodArray, ABCSeries\nfrom pandas.core.dtypes.inference import is_array_like\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n\nfrom pandas.core import missing, nanops, ops\nfrom pandas.core.algorithms import checked_add_with_arr, take, unique1d, value_counts\nimport pandas.core.common as com\nfrom pandas.core.indexers import check_bool_array_indexer\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.ops.invalid import invalid_comparison, make_invalid_op\n\nfrom pandas.tseries import frequencies\nfrom pandas.tseries.offsets import DateOffset, Tick\nfrom .base import ExtensionArray, ExtensionOpsMixin\ndef _datetimelike_array_cmp(cls, op):\n    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to\n    boxed scalars/arrays.\n    opname = f\"__{op.__name__}__\"\n    nat_result = opname == \"__ne__\"\n    @unpack_zerodim_and_defer(opname)\n    def wrapper(self, other):\n\n        if isinstance(other, str):\n            try:\n                # GH#18435 strings get a pass from tzawareness compat\n                other = self._scalar_from_string(other)\n            except ValueError:\n                # failed to parse as Timestamp/Timedelta/Period\n                return invalid_comparison(self, other, op)\n        if isinstance(other, self._recognized_scalars) or other is NaT:\n            other = self._scalar_type(other)\n            self._check_compatible_with(other)\n            other_i8 = self._unbox_scalar(other)\n            result = op(self.view(\"i8\"), other_i8)\n            if isna(other):\n                result.fill(nat_result)\n\n        elif not is_list_like(other):\n            return invalid_comparison(self, other, op)\n\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n\n        else:\n            if isinstance(other, list):\n                # TODO: could use pd.Index to do inference?\n                other = np.array(other)\n\n            if not isinstance(other, (np.ndarray, type(self))):\n                return invalid_comparison(self, other, op)\n\n            if is_object_dtype(other):\n                # We have to use comp_method_OBJECT_ARRAY instead of numpy\n                #  comparison otherwise it would fail to raise when\n                #  comparing tz-aware and tz-naive\n                with np.errstate(all=\"ignore\"):\n                    result = ops.comp_method_OBJECT_ARRAY(\n                        op, self.astype(object), other\n                    )\n                o_mask = isna(other)\n\n            elif not type(self)._is_recognized_dtype(other.dtype):\n                return invalid_comparison(self, other, op)\n\n            else:\n                # For PeriodDType this casting is unnecessary\n                other = type(self)._from_sequence(other)\n                self._check_compatible_with(other)\n                result = op(self.view(\"i8\"), other.view(\"i8\"))\n                o_mask = other._isnan\n\n            if o_mask.any():\n                result[o_mask] = nat_result\n\n        if self._hasnans:\n            result[self._isnan] = nat_result\n\n        return result\n\n    return set_function_name(wrapper, opname, cls)\nclass AttributesMixin:\n    _data: np.ndarray\n    @classmethod\n    def _simple_new(cls, values, **kwargs):\n        raise AbstractMethodError(cls)\n    def _scalar_type(self) -> Type[DatetimeLikeScalar]:\n        \"\"\"The scalar associated with this datelike\n        * PeriodArray : Period\n        * DatetimeArray : Timestamp\n        * TimedeltaArray : Timedelta\n        raise AbstractMethodError(self)\n\n    def _scalar_from_string(\n        self, value: str\n    ) -> Union[Period, Timestamp, Timedelta, NaTType]:\n        Construct a scalar type from a string.\n        Parameters\n        ----------\n        value : str\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.\n        raise AbstractMethodError(self)\n\n    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:\n        Unbox the integer value of a scalar `value`.\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n        Returns\n        -------\n        int\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\n        10000000000\n        raise AbstractMethodError(self)\n\n    def _check_compatible_with(\n        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False\n    ) -> None:\n        Verify that `self` and `other` are compatible.\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n        Parameters\n        ----------\n        other\n        setitem : bool, default False\n            For __setitem__ we may have stricter compatiblity resrictions than\n            for comparisons.\n\n        Raises\n        ------\n        Exception\n        \"\"\"\n        raise AbstractMethodError(self)\nclass DatelikeOps:\n    \"\"\"\n    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.\n    \"\"\"\n    @Substitution(\n        URL=\"https://docs.python.org/3/library/datetime.html\"\n        \"#strftime-and-strptime-behavior\"\n    )\n    def strftime(self, date_format):\n        Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n\n        Returns\n        -------\n        ndarray\n            NumPy ndarray of formatted strings.\n\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object)\n\n\nclass TimelikeOps:\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n\n    _round_doc = \"\"\"\n        Perform {op} operation on the data to the specified `freq`.\n\n        Parameters\n        ----------\n        freq : str or Offset\n            The frequency level to {op} the index to. Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end). See\n            :ref:`frequency aliases <timeseries.offset_aliases>` for\n            a list of possible `freq` values.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            Only relevant for DatetimeIndex:\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n            .. versionadded:: 0.24.0\n        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n            .. versionadded:: 0.24.0\n        Returns\n        -------\n        DatetimeIndex, TimedeltaIndex, or Series\n            Index of the same type for a DatetimeIndex or TimedeltaIndex,\n            or a Series with the same index for a Series.\n        Raises\n        ------\n        ValueError if the `freq` cannot be converted.\n        Examples\n        **DatetimeIndex**\n\n        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n        >>> rng\n        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:01:00'],\n                      dtype='datetime64[ns]', freq='T')\n    _round_example = \"\"\">>> rng.round('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n        **Series**\n        >>> pd.Series(rng).dt.round(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _floor_example = \"\"\">>> rng.floor('H')\n        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n        >>> pd.Series(rng).dt.floor(\"H\")\n        0   2018-01-01 11:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _ceil_example = \"\"\">>> rng.ceil('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 13:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n        >>> pd.Series(rng).dt.ceil(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 13:00:00\n        dtype: datetime64[ns]\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        values = _ensure_datetimelike_to_i8(self)\n        result = round_nsint64(values, mode, freq)\n        result = self._maybe_mask_results(result, fill_value=NaT)\n\n        dtype = self.dtype\n        if is_datetime64tz_dtype(self):\n            dtype = None\n        return self._ensure_localized(\n            self._simple_new(result, dtype=dtype), ambiguous, nonexistent\n        )\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\nclass DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray):\n    \"\"\"\n    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray\n    Assumes that __new__/__init__ defines:\n        _data\n        _freq\n    and that the inheriting class has methods:\n        _generate_range\n    \"\"\"\n    @property\n    def ndim(self) -> int:\n        return self._data.ndim\n    @property\n    def shape(self):\n        return self._data.shape\n    def reshape(self, *args, **kwargs):\n        # Note: we drop any freq\n        data = self._data.reshape(*args, **kwargs)\n        return type(self)(data, dtype=self.dtype)\n    def ravel(self, *args, **kwargs):\n        # Note: we drop any freq\n        data = self._data.ravel(*args, **kwargs)\n        return type(self)(data, dtype=self.dtype)\n    def _box_func(self):\n        \"\"\"\n        box function to get object from internal representation\n        \"\"\"\n    def _box_values(self, values):\n        apply box func to passed values\n        return lib.map_infer(values, self._box_func)\n    def __iter__(self):\n        return (self._box_func(v) for v in self.asi8)\n    @property\n    def asi8(self) -> np.ndarray:\n        Integer representation of the values.\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        # do not cache or you'll create a memory leak\n        return self._data.view(\"i8\")\n\n    @property\n    def _ndarray_values(self):\n        return self._data\n\n    # ----------------------------------------------------------------\n    # Rendering Methods\n    def _format_native_types(self, na_rep=\"NaT\", date_format=None):\n        \"\"\"\n        Helper method for astype when converting to strings.\n        ndarray[str]\n        raise AbstractMethodError(self)\n\n    def _formatter(self, boxed=False):\n        # TODO: Remove Datetime & DatetimeTZ formatters.\n        return \"'{}'\".format\n\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n\n    @property\n    def nbytes(self):\n        return self._data.nbytes\n\n    def __array__(self, dtype=None):\n        # used for Timedelta/DatetimeArray, overwritten by PeriodArray\n        if is_object_dtype(dtype):\n            return np.array(list(self), dtype=object)\n        return self._data\n\n    @property\n    def size(self) -> int:\n        \"\"\"The number of elements in this array.\"\"\"\n        return np.prod(self.shape)\n    def __len__(self) -> int:\n        return len(self._data)\n    def __getitem__(self, key):\n        \"\"\"\n        This getitem defers to the underlying array, which by-definition can\n        only handle list-likes, slices, and integer scalars\n        \"\"\"\n        is_int = lib.is_integer(key)\n        if lib.is_scalar(key) and not is_int:\n            raise IndexError(\n                \"only integers, slices (`:`), ellipsis (`...`), \"\n                \"numpy.newaxis (`None`) and integer or boolean \"\n                \"arrays are valid indices\"\n            )\n        getitem = self._data.__getitem__\n        if is_int:\n            val = getitem(key)\n            if lib.is_scalar(val):\n                # i.e. self.ndim == 1\n                return self._box_func(val)\n            return type(self)(val, dtype=self.dtype)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_array_indexer(self, key)\n            if key.all():\n                key = slice(0, None, None)\n            else:\n                key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n        is_period = is_period_dtype(self)\n        if is_period:\n            freq = self.freq\n        else:\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                # GH#21282 indexing with Ellipsis is similar to a full slice,\n                #  should preserve `freq` attribute\n                freq = self.freq\n\n        result = getitem(key)\n        if result.ndim > 1:\n            # To support MPL which performs slicing with 2 dim\n            # even though it only has 1 dim by definition\n            if is_period:\n                return self._simple_new(result, dtype=self.dtype, freq=freq)\n            return result\n        return self._simple_new(result, dtype=self.dtype, freq=freq)\n\n    def __setitem__(\n        self,\n        key: Union[int, Sequence[int], Sequence[bool], slice],\n        value: Union[NaTType, Any, Sequence[Any]],\n    ) -> None:\n        # I'm fudging the types a bit here. \"Any\" above really depends\n        # on type(self). For PeriodArray, it's Period (or stuff coercible\n        # to a period in from_sequence). For DatetimeArray, it's Timestamp...\n        # I don't know if mypy can do that, possibly with Generics.\n        # https://mypy.readthedocs.io/en/latest/generics.html\n        if lib.is_scalar(value) and not isna(value):\n            value = com.maybe_box_datetimelike(value)\n\n        if is_list_like(value):\n            is_slice = isinstance(key, slice)\n\n            if lib.is_scalar(key):\n                raise ValueError(\"setting an array element with a sequence.\")\n\n            if not is_slice:\n                key = cast(Sequence, key)\n                if len(key) != len(value) and not com.is_bool_indexer(key):\n                    msg = (\n                        f\"shape mismatch: value array of length '{len(key)}' \"\n                        \"does not match indexing result of length \"\n                        f\"'{len(value)}'.\"\n                    )\n                    raise ValueError(msg)\n                elif not len(key):\n                    return\n\n            value = type(self)._from_sequence(value, dtype=self.dtype)\n            self._check_compatible_with(value, setitem=True)\n            value = value.asi8\n        elif isinstance(value, self._scalar_type):\n            self._check_compatible_with(value, setitem=True)\n            value = self._unbox_scalar(value)\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            value = iNaT\n            msg = (\n                f\"'value' should be a '{self._scalar_type.__name__}', 'NaT', \"\n                f\"or array of those. Got '{type(value).__name__}' instead.\"\n            )\n            raise TypeError(msg)\n        self._data[key] = value\n        self._maybe_clear_freq()\n\n    def _maybe_clear_freq(self):\n        # inplace operations like __setitem__ may invalidate the freq of\n        # DatetimeArray and TimedeltaArray\n        pass\n    def astype(self, dtype, copy=True):\n        # Some notes on cases we don't have to handle here in the base class:\n        #   1. PeriodArray.astype handles period -> period\n        #   2. DatetimeArray.astype handles conversion between tz.\n        #   3. DatetimeArray.astype handles datetime -> period\n        from pandas import Categorical\n\n        dtype = pandas_dtype(dtype)\n\n        if is_object_dtype(dtype):\n            return self._box_values(self.asi8)\n        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):\n            return self._format_native_types()\n        elif is_integer_dtype(dtype):\n            # we deliberately ignore int32 vs. int64 here.\n            # See https://github.com/pandas-dev/pandas/issues/24381 for more.\n            values = self.asi8\n\n            if is_unsigned_integer_dtype(dtype):\n                # Again, we ignore int32 vs. int64\n                values = values.view(\"uint64\")\n\n            if copy:\n                values = values.copy()\n            return values\n        elif (\n            is_datetime_or_timedelta_dtype(dtype)\n            and not is_dtype_equal(self.dtype, dtype)\n        ) or is_float_dtype(dtype):\n            # disallow conversion between datetime/timedelta,\n            # and conversions for any datetimelike to float\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg)\n        elif is_categorical_dtype(dtype):\n            return Categorical(self, dtype=dtype)\n        else:\n            return np.asarray(self, dtype=dtype)\n    def view(self, dtype=None):\n        if dtype is None or dtype is self.dtype:\n            return type(self)(self._data, dtype=self.dtype)\n        return self._data.view(dtype=dtype)\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n    def unique(self):\n        result = unique1d(self.asi8)\n        return type(self)(result, dtype=self.dtype)\n    def _validate_fill_value(self, fill_value):\n        If a fill_value is passed to `take` convert it to an i8 representation,\n        raising ValueError if this is not possible.\n        fill_value : object\n        fill_value : np.int64\n\n        Raises\n        ------\n        ValueError\n        if isna(fill_value):\n            fill_value = iNaT\n        elif isinstance(fill_value, self._recognized_scalars):\n            self._check_compatible_with(fill_value)\n            fill_value = self._scalar_type(fill_value)\n            fill_value = self._unbox_scalar(fill_value)\n            raise ValueError(\n                f\"'fill_value' should be a {self._scalar_type}. Got '{fill_value}'.\"\n            )\n        return fill_value\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            fill_value = self._validate_fill_value(fill_value)\n        new_values = take(\n            self.asi8, indices, allow_fill=allow_fill, fill_value=fill_value\n        )\n        return type(self)(new_values, dtype=self.dtype)\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        dtypes = {x.dtype for x in to_concat}\n        assert len(dtypes) == 1\n        dtype = list(dtypes)[0]\n        values = np.concatenate([x.asi8 for x in to_concat])\n        return cls(values, dtype=dtype)\n    def copy(self):\n        values = self.asi8.copy()\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)\n    def _values_for_factorize(self):\n        return self.asi8, iNaT\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n    def _values_for_argsort(self):\n        return self._data\n    # ------------------------------------------------------------------\n    # Additional array methods\n    #  These are not part of the EA API, but we implement them because\n    #  pandas assumes they're there.\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        Find indices where elements should be inserted to maintain order.\n        Find the indices into a sorted array `self` such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort `self` into ascending\n            order. They are typically the result of ``np.argsort``.\n        Returns\n        -------\n        indices : array of ints\n            Array of insertion points with the same shape as `value`.\n        \"\"\"\n        if isinstance(value, str):\n            value = self._scalar_from_string(value)\n        if not (isinstance(value, (self._scalar_type, type(self))) or isna(value)):\n            raise ValueError(f\"Unexpected type for 'value': {type(value)}\")\n\n        self._check_compatible_with(value)\n        if isinstance(value, type(self)):\n            value = value.asi8\n        else:\n            value = self._unbox_scalar(value)\n\n        return self.asi8.searchsorted(value, side=side, sorter=sorter)\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an array.\n        numpy.ndarray.repeat\n        \"\"\"\n        nv.validate_repeat(args, kwargs)\n        values = self._data.repeat(repeats)\n        return type(self)(values.view(\"i8\"), dtype=self.dtype)\n\n    def value_counts(self, dropna=False):\n        Return a Series containing counts of unique values.\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaT values.\n        Returns\n        -------\n        Series\n        \"\"\"\n        from pandas import Series, Index\n        if dropna:\n            values = self[~self.isna()]._data\n            values = self._data\n\n        cls = type(self)\n\n        result = value_counts(values, sort=False, dropna=dropna)\n        index = Index(\n            cls(result.index.view(\"i8\"), dtype=self.dtype), name=result.index.name\n        )\n        return Series(result.values, index=index, name=result.name)\n    def map(self, mapper):\n        # TODO(GH-23179): Add ExtensionArray.map\n        # Need to figure out if we want ExtensionArray.map first.\n        # If so, then we can refactor IndexOpsMixin._map_values to\n        # a standalone function and call from here..\n        # Else, just rewrite _map_infer_values to do the right thing.\n        from pandas import Index\n        return Index(self).map(mapper).array\n    # ------------------------------------------------------------------\n    # Null Handling\n    def isna(self):\n        return self._isnan\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _isnan(self):\n        \"\"\"\n        return if each value is nan\n        return self.asi8 == iNaT\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _hasnans(self):\n        \"\"\"\n        return if I have any nans; enables various perf speedups\n        \"\"\"\n        return bool(self._isnan.any())\n\n    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):\n        \"\"\"\n        result : a ndarray\n        fill_value : object, default iNaT\n        convert : str, dtype or None\n\n        Returns\n        -------\n        result : ndarray with values replace by the fill_value\n\n        mask the result if needed, convert to the provided dtype if its not\n        None\n\n        This is an internal routine.\n        \"\"\"\n\n        if self._hasnans:\n            if convert:\n                result = result.astype(convert)\n            if fill_value is None:\n                fill_value = np.nan\n            result[self._isnan] = fill_value\n        return result\n\n    def fillna(self, value=None, method=None, limit=None):\n        # TODO(GH-20300): remove this\n        # Just overriding to ensure that we avoid an astype(object).\n        # Either 20300 or a `_values_for_fillna` would avoid this duplication.\n        if isinstance(value, ABCSeries):\n            value = value.array\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\" expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                if method == \"pad\":\n                    func = missing.pad_1d\n                else:\n                    func = missing.backfill_1d\n\n                values = self._data\n                if not is_period_dtype(self):\n                    # For PeriodArray self._data is i8, which gets copied\n                    #  by `func`.  Otherwise we need to make a copy manually\n                    # to avoid modifying `self` in-place.\n                    values = values.copy()\n\n                new_values = func(values, limit=limit, mask=mask)\n                if is_datetime64tz_dtype(self):\n                    # we need to pass int64 values to the constructor to avoid\n                    #  re-localizing incorrectly\n                    new_values = new_values.view(\"i8\")\n                new_values = type(self)(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n            new_values = self.copy()\n        return new_values\n    # ------------------------------------------------------------------\n    # Frequency Properties/Methods\n    @property\n    def freq(self):\n        \"\"\"\n        Return the frequency object if it is set, otherwise None.\n        \"\"\"\n        return self._freq\n    @freq.setter\n    def freq(self, value):\n        if value is not None:\n            value = frequencies.to_offset(value)\n            self._validate_frequency(self, value)\n        self._freq = value\n    @property\n    def freqstr(self):\n        \"\"\"\n        Return the frequency object as a string if its set, otherwise None\n        \"\"\"\n        if self.freq is None:\n            return None\n        return self.freq.freqstr\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def inferred_freq(self):\n        \"\"\"\n        Tryies to return a string representing a frequency guess,\n        generated by infer_freq.  Returns None if it can't autodetect the\n        frequency.\n        \"\"\"\n        if self.ndim != 1:\n            return None\n        try:\n            return frequencies.infer_freq(self)\n        except ValueError:\n            return None\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution(self):\n        return frequencies.Resolution.get_reso_from_freq(self.freqstr)\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def resolution(self):\n        \"\"\"\n        Returns day, hour, minute, second, millisecond or microsecond\n        return frequencies.Resolution.get_str(self._resolution)\n    @classmethod\n    def _validate_frequency(cls, index, freq, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        if is_period_dtype(cls):\n            # Frequency validation is not meaningful for Period Array/Index\n            return None\n\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n        try:\n            on_freq = cls._generate_range(\n                start=index[0], end=None, periods=len(index), freq=freq, **kwargs\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as e:\n            if \"non-fixed\" in str(e):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise e\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            )\n\n    # monotonicity/uniqueness properties are called via frequencies.infer_freq,\n    #  see GH#23789\n\n    @property\n    def _is_monotonic_increasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[0]\n    @property\n    def _is_monotonic_decreasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[1]\n    @property\n    def _is_unique(self):\n        return len(unique1d(self.asi8)) == len(self)\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n    _create_comparison_method = classmethod(_datetimelike_array_cmp)\n\n    # pow is invalid for all three subclasses; TimedeltaArray will override\n    #  the multiplication and division ops\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n\n    def _add_datetimelike_scalar(self, other):\n        # Overridden by TimedeltaArray\n        raise TypeError(f\"cannot add {type(self).__name__} and {type(other).__name__}\")\n\n    _add_datetime_arraylike = _add_datetimelike_scalar\n\n    def _sub_datetimelike_scalar(self, other):\n        # Overridden by DatetimeArray\n        assert other is not NaT\n        raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n    _sub_datetime_arraylike = _sub_datetimelike_scalar\n\n    def _sub_period(self, other):\n        # Overridden by PeriodArray\n        raise TypeError(f\"cannot subtract Period from a {type(self).__name__}\")\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n    def _add_delta(self, other):\n        \"\"\"\n        Add a timedelta-like, Tick or TimedeltaIndex-like object\n        to self, yielding an int64 numpy array\n        Parameters\n        ----------\n        delta : {timedelta, np.timedelta64, Tick,\n                 TimedeltaIndex, ndarray[timedelta64]}\n        result : ndarray[int64]\n\n        Notes\n        -----\n        The result's name is set outside of _add_delta by the calling\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\n        if isinstance(other, (Tick, timedelta, np.timedelta64)):\n            new_values = self._add_timedeltalike_scalar(other)\n        elif is_timedelta64_dtype(other):\n            # ndarray[timedelta64] or TimedeltaArray/index\n            new_values = self._add_delta_tdi(other)\n        return new_values\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Add a delta of a timedeltalike\n        return the i8 result view\n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\n            new_values = np.empty(self.shape, dtype=\"i8\")\n            new_values[:] = iNaT\n            return new_values\n\n        inc = delta_to_nanoseconds(other)\n        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(\n            \"i8\"\n        )\n        new_values = self._maybe_mask_results(new_values)\n        return new_values.view(\"i8\")\n\n    def _add_delta_tdi(self, other):\n        \"\"\"\n        Add a delta of a TimedeltaIndex\n        return the i8 result view\n        \"\"\"\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n        if isinstance(other, np.ndarray):\n            # ndarray[timedelta64]; wrap in TimedeltaIndex for op\n            from pandas.core.arrays import TimedeltaArray\n            other = TimedeltaArray._from_sequence(other)\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        new_values = checked_add_with_arr(\n            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = iNaT\n        return new_values.view(\"i8\")\n\n    def _add_nat(self):\n        \"\"\"\n        Add pd.NaT to self\n        \"\"\"\n        if is_period_dtype(self):\n            raise TypeError(\n                f\"Cannot add {type(self).__name__} and {type(NaT).__name__}\"\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return type(self)(result, dtype=self.dtype, freq=None)\n\n    def _sub_nat(self):\n        \"\"\"\n        Subtract pd.NaT from self\n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return result.view(\"timedelta64[ns]\")\n\n    def _sub_period_array(self, other):\n        \"\"\"\n        Subtract a Period Array/Index from self.  This is only valid if self\n        is itself a Period Array/Index, raises otherwise.  Both objects must\n        have the same frequency.\n        Parameters\n        ----------\n        other : PeriodIndex or PeriodArray\n        Returns\n        -------\n        result : np.ndarray[object]\n            Array of DateOffset objects; nulls represented by NaT.\n        \"\"\"\n        if not is_period_dtype(self):\n            raise TypeError(\n                f\"cannot subtract {other.dtype}-dtype from {type(self).__name__}\"\n            )\n        if self.freq != other.freq:\n            msg = DIFFERENT_FREQ.format(\n                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr\n            )\n            raise IncompatibleFrequency(msg)\n        new_values = checked_add_with_arr(\n            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        new_values = np.array([self.freq.base * x for x in new_values])\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = NaT\n        return new_values\n    def _addsub_object_array(self, other: np.ndarray, op):\n        \"\"\"\n        Add or subtract array-like of DateOffset objects\n        Parameters\n        ----------\n        other : np.ndarray[object]\n        op : {operator.add, operator.sub}\n        Returns\n        -------\n        result : same class as self\n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1:\n            return op(self, other[0])\n\n        warnings.warn(\n            \"Adding/subtracting array of DateOffsets to \"\n            f\"{type(self).__name__} not vectorized\",\n            PerformanceWarning,\n        )\n        # For EA self.astype('O') returns a numpy array, not an Index\n        left = self.astype(\"O\")\n        res_values = op(left, np.array(other))\n        kwargs = {}\n        if not is_period_dtype(self):\n            kwargs[\"freq\"] = \"infer\"\n            res = type(self)._from_sequence(res_values, **kwargs)\n            # e.g. we've passed a Timestamp to TimedeltaArray\n            res = res_values\n        return res\n\n    def _time_shift(self, periods, freq=None):\n        \"\"\"\n        Shift each value by `periods`.\n\n        Note this is different from ExtensionArray.shift, which\n        shifts the *position* of each element, padding the end with\n        missing values.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift by.\n        freq : pandas.DateOffset, pandas.Timedelta, or str\n            Frequency increment to shift by.\n        \"\"\"\n        if freq is not None and freq != self.freq:\n            if isinstance(freq, str):\n                freq = frequencies.to_offset(freq)\n            offset = periods * freq\n            result = self + offset\n            return result\n\n        if periods == 0:\n            # immutable so OK\n            return self.copy()\n\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n\n        start = self[0] + periods * self.freq\n        end = self[-1] + periods * self.freq\n\n        # Note: in the DatetimeTZ case, _generate_range will infer the\n        #  appropriate timezone from `start` and `end`, so tz does not need\n        #  to be passed explicitly.\n        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)\n\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n\n        # scalar others\n        if other is NaT:\n            result = self._add_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_delta(other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._add_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_delta(other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.add)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            return self._add_datetime_arraylike(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.add)\n            # Includes Categorical, other ExtensionArrays\n            # For PeriodDtype, if self is a TimedeltaArray and other is a\n            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this\n            #  operation is valid.  Defer to the PeriodArray implementation.\n            #  In remaining cases, this will end up raising TypeError.\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray(result)\n        return result\n    def __radd__(self, other):\n        # alias for __add__\n        return self.__add__(other)\n\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n\n        # scalar others\n        if other is NaT:\n            result = self._sub_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_delta(-other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(-other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._sub_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(-other)\n\n        elif isinstance(other, Period):\n            result = self._sub_period(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_delta(-other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.sub)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            result = self._sub_datetime_arraylike(other)\n        elif is_period_dtype(other):\n            # PeriodIndex\n            result = self._sub_period_array(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.sub)\n            # Includes ExtensionArrays, float_dtype\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray(result)\n        return result\n    def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):\n            # ndarray[datetime64] cannot be subtracted from self, so\n            # we need to wrap in DatetimeArray/Index and flip the operation\n            if lib.is_scalar(other):\n                # i.e. np.datetime64 object\n                return Timestamp(other) - self\n            if not isinstance(other, DatetimeLikeArrayMixin):\n                # Avoid down-casting DatetimeIndex\n                from pandas.core.arrays import DatetimeArray\n\n                other = DatetimeArray(other)\n            return other - self\n        elif (\n            is_datetime64_any_dtype(self.dtype)\n            and hasattr(other, \"dtype\")\n            and not is_datetime64_any_dtype(other.dtype)\n        ):\n            # GH#19959 datetime - datetime is well-defined as timedelta,\n            # but any other type - datetime is not well-defined.\n            raise TypeError(\n                f\"cannot subtract {type(self).__name__} from {type(other).__name__}\"\n            )\n        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):\n            # TODO: Can we simplify/generalize these cases at all?\n            raise TypeError(f\"cannot subtract {type(self).__name__} from {other.dtype}\")\n        elif is_timedelta64_dtype(self.dtype):\n            if lib.is_integer(other) or is_integer_dtype(other):\n                # need to subtract before negating, since that flips freq\n                # -self flips self.freq, messing up results\n                return -(self - other)\n\n            return (-self) + other\n\n        return -(self - other)\n\n    def __iadd__(self, other):  # type: ignore\n        result = self + other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    def __isub__(self, other):  # type: ignore\n        result = self - other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    # --------------------------------------------------------------\n    # Comparison Methods\n\n    def _ensure_localized(\n        self, arg, ambiguous=\"raise\", nonexistent=\"raise\", from_utc=False\n    ):\n        \"\"\"\n        Ensure that we are re-localized.\n        This is for compat as we can then call this on all datetimelike\n        arrays generally (ignored for Period/Timedelta)\n\n        Parameters\n        ----------\n        arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]\n        ambiguous : str, bool, or bool-ndarray, default 'raise'\n        nonexistent : str, default 'raise'\n        from_utc : bool, default False\n            If True, localize the i8 ndarray to UTC first before converting to\n            the appropriate tz. If False, localize directly to the tz.\n\n        Returns\n        -------\n        localized array\n        \"\"\"\n\n        # reconvert to local tz\n        tz = getattr(self, \"tz\", None)\n        if tz is not None:\n            if not isinstance(arg, type(self)):\n                arg = self._simple_new(arg)\n            if from_utc:\n                arg = arg.tz_localize(\"UTC\").tz_convert(self.tz)\n            else:\n                arg = arg.tz_localize(\n                    self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n                )\n        return arg\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def _reduce(self, name, axis=0, skipna=True, **kwargs):\n        op = getattr(self, name, None)\n        if op:\n            return op(skipna=skipna, **kwargs)\n            return super()._reduce(name, skipna, **kwargs)\n\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Array or minimum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.\n        \"\"\"\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\n        if isna(result):\n            # Period._from_ordinal does not handle np.nan gracefully\n            return NaT\n        return self._box_func(result)\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Array or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        # TODO: skipna is broken with max.\n        # See https://github.com/pandas-dev/pandas/issues/24265\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask].asi8\n        elif mask.any():\n            return NaT\n        else:\n            values = self.asi8\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmax(values, skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n    def mean(self, skipna=True):\n        Return the mean value of the Array.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Whether to ignore any NaT elements.\n        Returns\n        -------\n        scalar\n            Timestamp or Timedelta.\n        See Also\n        --------\n        numpy.ndarray.mean : Returns the average of array elements along a given axis.\n        Series.mean : Return the mean value in a Series.\n        Notes\n        -----\n        mean is only defined for Datetime and Timedelta dtypes, not for Period.\n        \"\"\"\n        if is_period_dtype(self):\n            # See discussion in GH#24757\n            raise TypeError(\n                f\"mean is not implemented for {type(self).__name__} since the \"\n                \"meaning is ambiguous.  An alternative is \"\n                \"obj.to_timestamp(how='start').mean()\"\n            )\n        mask = self.isna()\n        if skipna:\n            values = self[~mask]\n        elif mask.any():\n            return NaT\n            values = self\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmean(values.view(\"i8\"), skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n\nDatetimeLikeArrayMixin._add_comparison_ops()\n# -------------------------------------------------------------------\n# Shared Constructor Helpers\n\ndef validate_periods(periods):\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\n    constructor, cast it to an integer.\n\n    Parameters\n    ----------\n    periods : None, float, int\n\n    Returns\n    -------\n    periods : None or int\n\n    Raises\n    ------\n    TypeError\n        if periods is None, float, or int\n    if periods is not None:\n        if lib.is_float(periods):\n            periods = int(periods)\n        elif not lib.is_integer(periods):\n            raise TypeError(f\"periods must be a number, got {periods}\")\n    return periods\n\ndef validate_endpoints(closed):\n    \"\"\"\n    Check that the `closed` argument is among [None, \"left\", \"right\"]\n    Parameters\n    ----------\n    closed : {None, \"left\", \"right\"}\n\n    Returns\n    -------\n    left_closed : bool\n    right_closed : bool\n\n    Raises\n    ------\n    ValueError : if argument is not among valid values\n    \"\"\"\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == \"left\":\n        left_closed = True\n    elif closed == \"right\":\n        right_closed = True\n    else:\n        raise ValueError(\"Closed has to be either 'left', 'right' or None\")\n\n    return left_closed, right_closed\n\n\ndef validate_inferred_freq(freq, inferred_freq, freq_infer):\n    \"\"\"\n    If the user passes a freq and another freq is inferred from passed data,\n    require that they match.\n\n    Parameters\n    ----------\n    freq : DateOffset or None\n    inferred_freq : DateOffset or None\n    freq_infer : bool\n\n    Returns\n    -------\n    freq : DateOffset or None\n    freq_infer : bool\n\n    Notes\n    -----\n    We assume at this point that `maybe_infer_freq` has been called, so\n    `freq` is either a DateOffset object or None.\n    \"\"\"\n    if inferred_freq is not None:\n        if freq is not None and freq != inferred_freq:\n            raise ValueError(\n                f\"Inferred frequency {inferred_freq} from passed \"\n                \"values does not conform to passed frequency \"\n                f\"{freq.freqstr}\"\n            )\n        elif freq is None:\n            freq = inferred_freq\n        freq_infer = False\n\n    return freq, freq_infer\n\n\ndef maybe_infer_freq(freq):\n    \"\"\"\n    Comparing a DateOffset to the string \"infer\" raises, so we need to\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\n    signify the case where the given freq is \"infer\" and set freq to None\n    to avoid comparison trouble later on.\n\n    Parameters\n    ----------\n    freq : {DateOffset, None, str}\n\n    Returns\n    -------\n    freq : {DateOffset, None}\n    freq_infer : bool\n    \"\"\"\n    freq_infer = False\n    if not isinstance(freq, DateOffset):\n        # if a passed freq is None, don't infer automatically\n        if freq != \"infer\":\n            freq = frequencies.to_offset(freq)\n        else:\n            freq_infer = True\n            freq = None\n    return freq, freq_infer\ndef _ensure_datetimelike_to_i8(other, to_utc=False):\n    \"\"\"\n    Helper for coercing an input scalar or array to i8.\n\n    Parameters\n    ----------\n    other : 1d array\n    to_utc : bool, default False\n        If True, convert the values to UTC before extracting the i8 values\n        If False, extract the i8 values directly.\n\n    Returns\n    -------\n    i8 1d array\n    \"\"\"\n    from pandas import Index\n\n    if lib.is_scalar(other) and isna(other):\n        return iNaT\n    elif isinstance(other, (ABCPeriodArray, ABCIndexClass, DatetimeLikeArrayMixin)):\n        # convert tz if needed\n        if getattr(other, \"tz\", None) is not None:\n            if to_utc:\n                other = other.tz_convert(\"UTC\")\n            else:\n                other = other.tz_localize(None)\n    else:\n        try:\n            return np.array(other, copy=False).view(\"i8\")\n        except TypeError:\n            # period array cannot be coerced to int\n            other = Index(other)\n    return other.asi8", "fixed_code": "\"\"\"\nBase and utility classes for tseries type pandas objects.\n\"\"\"\nfrom typing import List, Optional, Set\nfrom pandas._libs import NaT, iNaT, join as libjoin, lib\nfrom pandas._libs.algos import unique_deltas\nfrom pandas._libs.tslibs import timezones\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, cache_readonly\n    ensure_int64,\n    is_bool_dtype,\n    is_float,\n    is_integer,\n    is_scalar,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import algorithms\nfrom pandas.core.accessor import PandasDelegate\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    ExtensionOpsMixin,\n    TimedeltaArray,\n)\nfrom pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas.core.indexes.numeric import Int64Index\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.tools.timedeltas import to_timedelta\n\nfrom pandas.tseries.frequencies import DateOffset, to_offset\n\nfrom .extension import (\n    ExtensionIndex,\n    inherit_names,\n    make_wrapped_arith_op,\n    make_wrapped_comparison_op,\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\ndef _join_i8_wrapper(joinf, with_indexers: bool = True):\n    Create the join wrapper methods.\n    @staticmethod  # type: ignore\n    def wrapper(left, right):\n        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            left = left.view(\"i8\")\n        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            right = right.view(\"i8\")\n        results = joinf(left, right)\n        if with_indexers:\n            # dtype should be timedelta64[ns] for TimedeltaIndex\n            #  and datetime64[ns] for DatetimeIndex\n            dtype = left.dtype.base\n            join_index, left_indexer, right_indexer = results\n            join_index = join_index.view(dtype)\n            return join_index, left_indexer, right_indexer\n        return results\n    return wrapper\n@inherit_names(\n    [\"inferred_freq\", \"_isnan\", \"_resolution\", \"resolution\"],\n    DatetimeLikeArrayMixin,\n    cache=True,\n)\n@inherit_names(\n    [\"__iter__\", \"mean\", \"freq\", \"freqstr\", \"_ndarray_values\", \"asi8\", \"_box_values\"],\n    DatetimeLikeArrayMixin,\n)\nclass DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n    _data: ExtensionArray\n    freq: Optional[DateOffset]\n    freqstr: Optional[str]\n    _resolution: int\n    _bool_ops: List[str] = []\n    _field_ops: List[str] = []\n    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore\n    _hasnans = hasnans  # for index / array -agnostic code\n    def is_all_dates(self) -> bool:\n        return True\n    @classmethod\n    def _create_comparison_method(cls, op):\n        Create a comparison method that dispatches to ``cls.values``.\n        return make_wrapped_comparison_op(f\"__{op.__name__}__\")\n    # ------------------------------------------------------------------------\n    # Abstract data attributes\n    @property\n    def values(self):\n        # Note: PeriodArray overrides this to return an ndarray of objects.\n        return self._data._data\n    def __array_wrap__(self, result, context=None):\n        Gets called after a ufunc.\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n        attrs = self._get_attributes_dict()\n        if not is_period_dtype(self) and attrs[\"freq\"]:\n            # no need to infer if freq is None\n            attrs[\"freq\"] = \"infer\"\n        return Index(result, **attrs)\n    # ------------------------------------------------------------------------\n    def equals(self, other) -> bool:\n        Determines if two Index objects contain the same elements.\n        if self.is_(other):\n            return True\n        if not isinstance(other, ABCIndexClass):\n            return False\n        elif not isinstance(other, type(self)):\n            try:\n                other = type(self)(other)\n            except (ValueError, TypeError, OverflowError):\n                # e.g.\n                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime\n                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex\n                #  OverflowError -> Index([very_large_timedeltas])\n                return False\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            # have different timezone\n            return False\n\n        return np.array_equal(self.asi8, other.asi8)\n\n    @Appender(_index_shared_docs[\"contains\"] % _index_doc_kwargs)\n    def __contains__(self, key):\n        try:\n            res = self.get_loc(key)\n            return (\n                is_scalar(res)\n                or isinstance(res, slice)\n                or (is_list_like(res) and len(res))\n            )\n        except (KeyError, TypeError, ValueError):\n            return False\n    # Try to run function on index first, and then on elements of index\n    # Especially important for group-by functionality\n    def map(self, mapper, na_action=None):\n        try:\n            result = mapper(self)\n            # Try to use this result if we can\n            if isinstance(result, np.ndarray):\n                result = Index(result)\n            if not isinstance(result, Index):\n                raise TypeError(\"The map function must return an Index object\")\n            return result\n        except Exception:\n            return self.astype(object).map(mapper)\n    def sort_values(self, return_indexer=False, ascending=True):\n        Return sorted copy of Index.\n        if return_indexer:\n            _as = self.argsort()\n            if not ascending:\n                _as = _as[::-1]\n            sorted_index = self.take(_as)\n            return sorted_index, _as\n        else:\n            # NB: using asi8 instead of _ndarray_values matters in numpy 1.18\n            #  because the treatment of NaT has been changed to put NaT last\n            #  instead of first.\n            sorted_values = np.sort(self.asi8)\n            attribs = self._get_attributes_dict()\n            freq = attribs[\"freq\"]\n\n            if freq is not None and not is_period_dtype(self):\n                if freq.n > 0 and not ascending:\n                    freq = freq * -1\n                elif freq.n < 0 and ascending:\n                    freq = freq * -1\n            attribs[\"freq\"] = freq\n\n            if not ascending:\n                sorted_values = sorted_values[::-1]\n\n            return self._simple_new(sorted_values, **attribs)\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_int64(indices)\n\n        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))\n        if isinstance(maybe_slice, slice):\n            return self[maybe_slice]\n\n        return ExtensionIndex.take(\n            self, indices, axis, allow_fill, fill_value, **kwargs\n        )\n    _can_hold_na = True\n    _na_value = NaT\n    \"\"\"The expected NA value to use with this index.\"\"\"\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n    def tolist(self) -> List:\n        \"\"\"\n        Return a list of the underlying data.\n        \"\"\"\n        return list(self.astype(object))\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Index or minimum along\n        an axis.\n        See Also\n        numpy.ndarray.min\n        Series.min : Return the minimum value in a Series.\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[0] != iNaT:\n                    return self._box_func(i8[0])\n\n            if self.hasnans:\n                if skipna:\n                    min_stamp = self[~self._isnan].asi8.min()\n                else:\n                    return self._na_value\n            else:\n                min_stamp = i8.min()\n            return self._box_func(min_stamp)\n        except ValueError:\n            return self._na_value\n    def argmin(self, axis=None, skipna=True, *args, **kwargs):\n        Returns the indices of the minimum values along an axis.\n        See `numpy.ndarray.argmin` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmin\n        nv.validate_argmin(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = np.iinfo(\"int64\").max\n        return i8.argmin()\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        Return the maximum value of the Index or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[-1] != iNaT:\n                    return self._box_func(i8[-1])\n\n            if self.hasnans:\n                if skipna:\n                    max_stamp = self[~self._isnan].asi8.max()\n                else:\n                    return self._na_value\n            else:\n                max_stamp = i8.max()\n            return self._box_func(max_stamp)\n        except ValueError:\n            return self._na_value\n    def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Returns the indices of the maximum values along an axis.\n        See `numpy.ndarray.argmax` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmax\n        \"\"\"\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = 0\n        return i8.argmax()\n    # --------------------------------------------------------------------\n    # Rendering Methods\n    def _format_with_header(self, header, na_rep=\"NaT\", **kwargs):\n        return header + list(self._format_native_types(na_rep, **kwargs))\n    def _formatter_func(self):\n    def _format_attrs(self):\n        Return a list of tuples of the (attr,formatted_value).\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            if attrib == \"freq\":\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)\n                attrs.append((\"freq\", freq))\n        return attrs\n    # --------------------------------------------------------------------\n    def _convert_scalar_indexer(self, key, kind=None):\n        We don't allow integer or float indexing on datetime-like when using\n        loc.\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\n        \"\"\"\n\n        assert kind in [\"ix\", \"loc\", \"getitem\", \"iloc\", None]\n\n        # we don't allow integer/float indexing for loc\n        # we don't allow float indexing for ix/getitem\n        if is_scalar(key):\n            is_int = is_integer(key)\n            is_flt = is_float(key)\n            if kind in [\"loc\"] and (is_int or is_flt):\n                self._invalid_indexer(\"index\", key)\n            elif kind in [\"ix\", \"getitem\"] and is_flt:\n                self._invalid_indexer(\"index\", key)\n\n        return super()._convert_scalar_indexer(key, kind=kind)\n\n    __add__ = make_wrapped_arith_op(\"__add__\")\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\n    __sub__ = make_wrapped_arith_op(\"__sub__\")\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Compute boolean array of whether each index value is found in the\n        passed set of values.\n        Parameters\n        ----------\n        values : set or sequence of values\n        is_contained : ndarray (boolean dtype)\n        if level is not None:\n            self._validate_index_level(level)\n        if not isinstance(values, type(self)):\n            try:\n                values = type(self)(values)\n            except ValueError:\n                return self.astype(object).isin(values)\n        return algorithms.isin(self.asi8, values.asi8)\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        nv.validate_repeat(tuple(), dict(axis=axis))\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n    @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n    def where(self, cond, other=None):\n        values = self.view(\"i8\")\n        if is_scalar(other) and isna(other):\n            other = NaT.value\n            # Do type inference if necessary up front\n            # e.g. we passed PeriodIndex.values and got an ndarray of Periods\n            other = Index(other)\n            if is_categorical_dtype(other):\n                # e.g. we have a Categorical holding self.dtype\n                if needs_i8_conversion(other.categories):\n                    other = other._internal_get_values()\n            if not is_dtype_equal(self.dtype, other.dtype):\n                raise TypeError(f\"Where requires matching dtype, not {other.dtype}\")\n            other = other.view(\"i8\")\n        result = np.where(cond, values, other).astype(\"i8\")\n        return self._shallow_copy(result)\n    def _summary(self, name=None):\n        Return a summarized representation.\n        name : str\n            Name to use in the summary representation.\n        str\n            Summarized representation of the index.\n        formatter = self._formatter_func\n        if len(self) > 0:\n            index_summary = f\", {formatter(self[0])} to {formatter(self[-1])}\"\n            index_summary = \"\"\n        if name is None:\n            name = type(self).__name__\n        result = f\"{name}: {len(self)} entries{index_summary}\"\n        if self.freq:\n            result += f\"\\nFreq: {self.freqstr}\"\n        # display as values, not quoted\n        result = result.replace(\"'\", \"\")\n        return result\n    def _concat_same_dtype(self, to_concat, name):\n        \"\"\"\n        Concatenate to_concat which has the same class.\n        \"\"\"\n        attribs = self._get_attributes_dict()\n        attribs[\"name\"] = name\n        # do not pass tz to set because tzlocal cannot be hashed\n        if len({str(x.dtype) for x in to_concat}) != 1:\n            raise ValueError(\"to_concat must have the same tz\")\n        new_data = type(self._values)._concat_same_type(to_concat).asi8\n        # GH 3232: If the concat result is evenly spaced, we can retain the\n        # original frequency\n        is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1\n        if not is_period_dtype(self) and not is_diff_evenly_spaced:\n            # reset freq\n            attribs[\"freq\"] = None\n        return self._simple_new(new_data, **attribs)\n    @Appender(_index_shared_docs[\"astype\"])\n    def astype(self, dtype, copy=True):\n        if is_dtype_equal(self.dtype, dtype) and copy is False:\n            # Ensure that self.astype(self.dtype) is self\n            return self\n        new_values = self._data.astype(dtype, copy=copy)\n        # pass copy=False because any copying will be done in the\n        #  _data.astype call above\n        return Index(new_values, dtype=new_values.dtype, name=self.name, copy=False)\n    def shift(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n            .. versionchanged:: 0.24.0\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        result = self._data._time_shift(periods, freq=freq)\n        return type(self)(result, name=self.name)\n    # --------------------------------------------------------------------\n    # List-like Methods\n    def delete(self, loc):\n        new_i8s = np.delete(self.asi8, loc)\n        freq = None\n        if is_period_dtype(self):\n            freq = self.freq\n        elif is_integer(loc):\n            if loc in (0, -len(self), -1, len(self) - 1):\n                freq = self.freq\n            if is_list_like(loc):\n                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))\n            if isinstance(loc, slice) and loc.step in (1, None):\n                if loc.start in (0, None) or loc.stop in (len(self), None):\n                    freq = self.freq\n        return self._shallow_copy(new_i8s, freq=freq)\nclass DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n    \"\"\"\n    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,\n    but not PeriodIndex\n    \"\"\"\n    # Compat for frequency inference, see GH#23789\n    _is_monotonic_increasing = Index.is_monotonic_increasing\n    _is_monotonic_decreasing = Index.is_monotonic_decreasing\n    _is_unique = Index.is_unique\n    def _set_freq(self, freq):\n        Set the _freq attribute on our underlying DatetimeArray.\n        freq : DateOffset, None, or \"infer\"\n        \"\"\"\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, DateOffset):\n            # Always valid.  In the TimedeltaIndex case, we assume this\n            #  is a Tick offset.\n            pass\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = to_offset(self.inferred_freq)\n        self._data._freq = freq\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n        attributes = self._get_attributes_dict()\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n    @Appender(Index.difference.__doc__)\n    def difference(self, other, sort=None):\n        new_idx = super().difference(other, sort=sort)\n        new_idx._set_freq(None)\n        return new_idx\n    def intersection(self, other, sort=False):\n        Specialized intersection for DatetimeIndex/TimedeltaIndex.\n        May be much faster than Index.intersection\n        other : Same type as self or array-like\n        sort : False or None, default False\n            Sort the resulting index if possible.\n            .. versionadded:: 0.24.0\n            .. versionchanged:: 0.24.1\n               Changed the default to ``False`` to match the behaviour\n               from before 0.24.0.\n            .. versionchanged:: 0.25.0\n               The `sort` keyword is added\n        y : Index or same type as self\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n        if len(self) == 0:\n            return self.copy()\n        if len(other) == 0:\n            return other.copy()\n\n        if not isinstance(other, type(self)):\n            result = Index.intersection(self, other, sort=sort)\n            if isinstance(result, type(self)):\n                if result.freq is None:\n                    result._set_freq(\"infer\")\n            return result\n        elif (\n            other.freq is None\n            or self.freq is None\n            or other.freq != self.freq\n            or not other.freq.is_anchored()\n            or (not self.is_monotonic or not other.is_monotonic)\n        ):\n            result = Index.intersection(self, other, sort=sort)\n            # Invalidate the freq of `result`, which may not be correct at\n            # this point, depending on the values.\n            result._set_freq(None)\n            result = self._shallow_copy(\n                result._data, name=result.name, dtype=result.dtype, freq=None\n            if result.freq is None:\n                result._set_freq(\"infer\")\n            return result\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        # after sorting, the intersection always starts with the right index\n        # and ends with the index of which the last elements is smallest\n        end = min(left[-1], right[-1])\n        start = right[0]\n        if end < start:\n            return type(self)(data=[])\n        else:\n            lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n            return self._shallow_copy(left_chunk)\n    def _can_fast_union(self, other) -> bool:\n        if not isinstance(other, type(self)):\n            return False\n        freq = self.freq\n        if freq is None or freq != other.freq:\n            return False\n        if not self.is_monotonic or not other.is_monotonic:\n            return False\n        if len(self) == 0 or len(other) == 0:\n            return True\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        right_start = right[0]\n        left_end = left[-1]\n        # Only need to \"adjoin\", not overlap\n            return (right_start == left_end + freq) or right_start in left\n            # if we are comparing a freq that does not propagate timezones\n            # this will raise\n            return False\n\n    def _fast_union(self, other, sort=None):\n        if len(other) == 0:\n            return self.view(type(self))\n\n        if len(self) == 0:\n            return other.view(type(self))\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        elif sort is False:\n            # TDIs are not in the \"correct\" order and we don't want\n            #  to sort but want to remove overlaps\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n            left, right = other, self\n        left_end = left[-1]\n        right_end = right[-1]\n        # concatenate\n        if left_end < right_end:\n            loc = right.searchsorted(left_end, side=\"right\")\n            right_chunk = right.values[loc:]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n            return left\n    def _union(self, other, sort):\n        if not len(other) or self.equals(other) or not len(self):\n            return super()._union(other, sort=sort)\n        # We are called by `union`, which is responsible for this validation\n        assert isinstance(other, type(self))\n        this, other = self._maybe_utc_convert(other)\n        if this._can_fast_union(other):\n            return this._fast_union(other, sort=sort)\n            result = Index._union(this, other, sort=sort)\n            if isinstance(result, type(self)):\n                assert result._data.dtype == this.dtype\n                if result.freq is None:\n                    result._set_freq(\"infer\")\n            return result\n    # --------------------------------------------------------------------\n    # Join Methods\n    _join_precedence = 10\n    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)\n    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)\n    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)\n    _left_indexer_unique = _join_i8_wrapper(\n        libjoin.left_join_indexer_unique, with_indexers=False\n    )\n    def join(\n        self, other, how: str = \"left\", level=None, return_indexers=False, sort=False\n    ):\n        See Index.join\n        if self._is_convertible_to_index_for_join(other):\n            try:\n                other = type(self)(other)\n            except (TypeError, ValueError):\n                pass\n\n        this, other = self._maybe_utc_convert(other)\n        return Index.join(\n            this,\n            other,\n            how=how,\n            level=level,\n            return_indexers=return_indexers,\n            sort=sort,\n        )\n    def _maybe_utc_convert(self, other):\n        this = self\n        if not hasattr(self, \"tz\"):\n            return this, other\n        if isinstance(other, type(self)):\n            if self.tz is not None:\n                if other.tz is None:\n                    raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            elif other.tz is not None:\n                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            if not timezones.tz_compare(self.tz, other.tz):\n                this = self.tz_convert(\"UTC\")\n                other = other.tz_convert(\"UTC\")\n        return this, other\n    @classmethod\n    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:\n        \"\"\"\n        return a boolean whether I can attempt conversion to a\n        DatetimeIndex/TimedeltaIndex\n        \"\"\"\n        if isinstance(other, cls):\n            return False\n        elif len(other) > 0 and other.inferred_type not in (\n            \"floating\",\n            \"mixed-integer\",\n            \"integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n            \"mixed\",\n        ):\n            return True\n        return False\n\n    def _wrap_joined_index(self, joined, other):\n        name = get_op_result_name(self, other)\n        if (\n            isinstance(other, type(self))\n            and self.freq == other.freq\n            and self._can_fast_union(other)\n        ):\n            joined = self._shallow_copy(joined)\n            joined.name = name\n            return joined\n            kwargs = {}\n            if hasattr(self, \"tz\"):\n                kwargs[\"tz\"] = getattr(other, \"tz\", None)\n            return self._simple_new(joined, name, **kwargs)\nclass DatetimelikeDelegateMixin(PandasDelegate):\n    Delegation mechanism, specific for Datetime, Timedelta, and Period types.\n\n    Functionality is delegated from the Index class to an Array class. A\n    few things can be customized\n\n    * _delegated_methods, delegated_properties : List\n        The list of property / method names being delagated.\n    * raw_methods : Set\n        The set of methods whose results should should *not* be\n        boxed in an index, after being returned from the array\n    * raw_properties : Set\n        The set of properties whose results should should *not* be\n        boxed in an index, after being returned from the array\n    # raw_methods : dispatch methods that shouldn't be boxed in an Index\n    _raw_methods: Set[str] = set()\n    # raw_properties : dispatch properties that shouldn't be boxed in an Index\n    _raw_properties: Set[str] = set()\n    _data: ExtensionArray\n    def _delegate_property_get(self, name, *args, **kwargs):\n        result = getattr(self._data, name)\n        if name not in self._raw_properties:\n            result = Index(result, name=self.name)\n        return result\n    def _delegate_property_set(self, name, value, *args, **kwargs):\n        setattr(self._data, name, value)\n    def _delegate_method(self, name, *args, **kwargs):\n        result = operator.methodcaller(name, *args, **kwargs)(self._data)\n        if name not in self._raw_methods:\n            result = Index(result, name=self.name)\n        return result", "description": ""}
{"id": "pandas-134", "project": "pandas", "bug_id": "134", "buggy_code": "# http://stackoverflow.com/questions/23814368/sorting-pandas-categorical-labels-after-groupby  # noqa: E501\n    end_date = Timestamp(datetime(2030, 12, 31))", "fixed_code": "# http://stackoverflow.com/questions/23814368/sorting-pandas-\n    #        categorical-labels-after-groupby\n    end_date = Timestamp(datetime(2200, 12, 31))", "description": ""}
{"id": "pandas-94", "project": "pandas", "bug_id": "94", "buggy_code": "from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin\n        taken = ExtensionIndex.take(\n        # keep freq in PeriodArray/Index, reset otherwise\n        freq = self.freq if is_period_dtype(self) else None\n        assert taken.freq == freq, (taken.freq, freq, taken)\n        return self._shallow_copy(taken, freq=freq)\n\n        freq = self.freq if is_period_dtype(self) else None\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)", "fixed_code": "from pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    ExtensionOpsMixin,\n    TimedeltaArray,\n)\n        return ExtensionIndex.take(\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n\n        attributes = self._get_attributes_dict()\n\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)", "description": ""}
{"id": "pandas-60", "project": "pandas", "bug_id": "60", "buggy_code": "# name=func for WindowGroupByMixin._apply", "fixed_code": "# name=func & raw=raw for WindowGroupByMixin._apply\n            raw=raw,", "description": ""}
{"id": "pandas-34", "project": "pandas", "bug_id": "34", "buggy_code": "ambiguous=\"infer\",", "fixed_code": "# GH 25758: If DST lands at midnight (e.g. 'America/Havana'), user feedback\n        # has noted that ambiguous=True provides the most sensible result\n            ambiguous=True,", "description": ""}
{"id": "pandas-160", "project": "pandas", "bug_id": "160", "buggy_code": "if hasattr(o, \"dtypes\"):\n                elif isinstance(o, np.ndarray):", "fixed_code": "# Series implements dtypes, check for dimension count as well\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                # ndarray and Series Case\n                elif hasattr(o, \"dtype\"):", "description": ""}
{"id": "pandas-33", "project": "pandas", "bug_id": "33", "buggy_code": "data[self._mask] = data.min() - 1", "fixed_code": "if self._mask.any():\n            data[self._mask] = data.min() - 1", "description": ""}
{"id": "pandas-158", "project": "pandas", "bug_id": "158", "buggy_code": "non_mapping = is_scalar(index) or (\n            is_list_like(index) and not is_dict_like(index)\n        )\n        if non_mapping:\n        return super().rename(index=index, **kwargs)", "fixed_code": "if callable(index) or is_dict_like(index):\n            return super().rename(index=index, **kwargs)\n        else:", "description": ""}
{"id": "pandas-167", "project": "pandas", "bug_id": "167", "buggy_code": "if isinstance(key, str) and labels.levels[0].is_all_dates:\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n        if idx.is_all_dates:", "fixed_code": "# whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n    _supports_partial_string_indexing = True\n    _supports_partial_string_indexing = True\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                # partial string indexing, df.loc['2000', 'A']\n                # should not be considered scalar\n                return False\n\n            if (\n                isinstance(key, str)\n                and labels.levels[0]._supports_partial_string_indexing\n            ):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n        if idx._supports_partial_string_indexing:", "description": ""}
{"id": "pandas-151", "project": "pandas", "bug_id": "151", "buggy_code": "values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value", "fixed_code": "value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value", "description": ""}
{"id": "pandas-156", "project": "pandas", "bug_id": "156", "buggy_code": "for col, series in this.items():\n            new_data[col] = func(series.values, other.values)\n            new_data[col] = func(left[col], float(right[col]))", "fixed_code": "for col in this.columns:\n            new_data[col] = func(this[col], other)\n            new_data[col] = func(left[col], right[col])", "description": ""}
{"id": "pandas-20", "project": "pandas", "bug_id": "20", "buggy_code": "# TODO: going through __new__ raises on call to _validate_frequency;\n        #  are we passing incorrect freq?\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n        # TODO: going through __new__ raises on call to _validate_frequency;\n        #  are we passing incorrect freq?\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n        # TODO: going through __new__ raises on call to _validate_frequency;\n        #  are we passing incorrect freq?\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )", "fixed_code": "return type(i)._simple_new(shifted, dtype=i.dtype)\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)", "description": ""}
{"id": "pandas-18", "project": "pandas", "bug_id": "18", "buggy_code": "if not isinstance(window, BaseIndexer):\n                            self.min_periods or 1,", "fixed_code": "\"skew\",\n                    if not isinstance(self.window, BaseIndexer):\n                            window_indexer.window_size,", "description": ""}
{"id": "pandas-27", "project": "pandas", "bug_id": "27", "buggy_code": "freq = get_period_alias(freq)", "fixed_code": "import pandas._libs.tslibs.frequencies as libfrequencies\n            res = get_period_alias(freq)\n\n            #  https://github.com/pandas-dev/pandas/issues/33358\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n\n            freq = res", "description": ""}
{"id": "pandas-9", "project": "pandas", "bug_id": "9", "buggy_code": "from pandas.core.dtypes.missing import isna, notna\n        if is_scalar(key) and isna(key):\nfrom pandas.core.dtypes.missing import isna\n        if is_scalar(key) and isna(key):\n        hash(key)", "fixed_code": "from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n        if is_valid_nat_for_dtype(key, self.categories.dtype):", "description": ""}
{"id": "pandas-145", "project": "pandas", "bug_id": "145", "buggy_code": "def column_op(a, b):\n            return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}", "fixed_code": "if right.dtype == \"timedelta64[ns]\":\n            # ensure we treat NaT values as the correct dtype\n            # Note: we do not do this unconditionally as it may be lossy or\n            #  expensive for EA dtypes.\n            right = np.asarray(right)\n\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}\n\n        else:\n\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}", "description": ""}
{"id": "pandas-11", "project": "pandas", "bug_id": "11", "buggy_code": "try:\n                    i = level.get_loc(key)\n                except KeyError as err:\n                    raise ValueError(f\"Key {key} not in level {level}\") from err", "fixed_code": "mask = level == key\n                if not mask.any():\n                    raise ValueError(f\"Key {key} not in level {level}\")\n                i = np.nonzero(level == key)[0][0]", "description": ""}
{"id": "pandas-142", "project": "pandas", "bug_id": "142", "buggy_code": "def test_diff(self):\n        # Just run the function\n        self.ts.diff()\n\n        # int dtype\n        a = 10000000000000000\n        b = a + 1\n        s = Series([a, b])\n\n        rs = s.diff()\n        assert rs[1] == 1\n\n        # neg n\n        rs = self.ts.diff(-1)\n        xp = self.ts - self.ts.shift(-1)\n        assert_series_equal(rs, xp)\n\n        # 0\n        rs = self.ts.diff(0)\n        xp = self.ts - self.ts\n        assert_series_equal(rs, xp)\n\n        # datetime diff (GH3100)\n        s = Series(date_range(\"20130102\", periods=5))\n        rs = s - s.shift(1)\n        xp = s.diff()\n        assert_series_equal(rs, xp)\n\n        # timedelta diff\n        nrs = rs - rs.shift(1)\n        nxp = xp.diff()\n        assert_series_equal(nrs, nxp)\n\n        # with tz\n        s = Series(\n            date_range(\"2000-01-01 09:00:00\", periods=5, tz=\"US/Eastern\"), name=\"foo\"\n        )\n        result = s.diff()\n        assert_series_equal(\n            result, Series(TimedeltaIndex([\"NaT\"] + [\"1 days\"] * 4), name=\"foo\")\n        )", "fixed_code": "is_bool = False\n        is_bool = True\n        elif is_bool:\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]", "description": ""}
{"id": "pandas-7", "project": "pandas", "bug_id": "7", "buggy_code": "target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)", "fixed_code": "left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)", "description": ""}
{"id": "pandas-29", "project": "pandas", "bug_id": "29", "buggy_code": "if needs_float_conversion:\n            left = left.astype(\"float\")\n        left.values[key] = value_left\n        if needs_float_conversion:\n            right = right.astype(\"float\")\n        right.values[key] = value_right", "fixed_code": "if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n\n\n        left._values[key] = value_left\n        right._values[key] = value_right", "description": ""}
{"id": "pandas-16", "project": "pandas", "bug_id": "16", "buggy_code": "new_freq = self._get_addsub_freq(other)\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n            # Only used for ops that stay PeriodDtype\n            return self.freq", "fixed_code": "new_freq = self._get_addsub_freq(other, result)\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n            if is_period_dtype(result.dtype):\n                # Only used for ops that stay PeriodDtype\n                return self.freq\n            return None", "description": ""}
{"id": "pandas-42", "project": "pandas", "bug_id": "42", "buggy_code": "elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):", "fixed_code": "elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):", "description": ""}
{"id": "pandas-89", "project": "pandas", "bug_id": "89", "buggy_code": "result = result.unstack(val)", "fixed_code": "result = result.unstack(val, fill_value=fill_value)", "description": ""}
{"id": "pandas-116", "project": "pandas", "bug_id": "116", "buggy_code": "left_keys = [self.left.index.values]", "fixed_code": "left_keys = [self.left.index._values]", "description": ""}
{"id": "pandas-111", "project": "pandas", "bug_id": "111", "buggy_code": "return self._invalid_indexer(\"label\", key)\n                    return self._invalid_indexer(\"label\", key)\n        if self.categories._defer_to_indexing:\n            return self.categories._convert_scalar_indexer(key, kind=kind)", "fixed_code": "self._invalid_indexer(\"label\", key)\n                    self._invalid_indexer(\"label\", key)\n        if kind == \"loc\":\n            try:\n                return self.categories._convert_scalar_indexer(key, kind=kind)\n            except TypeError:\n                self._invalid_indexer(\"label\", key)", "description": ""}
{"id": "pandas-73", "project": "pandas", "bug_id": "73", "buggy_code": "def _combine_frame(self, other, func, fill_value=None, level=None):\n    def _combine_match_index(self, other, func):\n                new_data = func(self.values.T, other.values).T\n        shape = result.shape\n        nan_mask = (zmask & (x == 0)).ravel()\n            neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()\n            posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()\n            result = result.astype(\"float64\", copy=False).ravel()\n\n            np.putmask(result, nan_mask, np.nan)\n            np.putmask(result, posinf_mask, np.inf)\n            np.putmask(result, neginf_mask, -np.inf)\n            result = result.reshape(shape)", "fixed_code": "def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n    def _combine_match_index(self, other: Series, func):\n            other_vals = other.values.reshape(-1, 1)\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n        nan_mask = zmask & (x == 0)\n            neginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))\n            posinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))\n            result = result.astype(\"float64\", copy=False)\n            result[nan_mask] = np.nan\n            result[posinf_mask] = np.inf\n            result[neginf_mask] = -np.inf", "description": ""}
{"id": "pandas-118", "project": "pandas", "bug_id": "118", "buggy_code": "missing = Index(np.ravel(id_vars)).difference(cols)\n            missing = Index(np.ravel(value_vars)).difference(cols)", "fixed_code": "import pandas.core.common as com\n            missing = Index(com.flatten(id_vars)).difference(cols)\n            missing = Index(com.flatten(value_vars)).difference(cols)", "description": ""}
{"id": "pandas-87", "project": "pandas", "bug_id": "87", "buggy_code": "\"__dummy__\",", "fixed_code": "original_df_cols = df.columns\n\n        [\"__dummy__\"],\n    # GH18321, after pivoting, an extra top level of column index of `__dummy__` is\n    # created, and this extra level should not be included in the further steps\n    if not table.empty:\n        cols_diff = df.columns.difference(original_df_cols)[0]\n        table = table[cols_diff]", "description": ""}
{"id": "pandas-80", "project": "pandas", "bug_id": "80", "buggy_code": "arr = operator.inv(com.values_from_object(self))\n        return self.__array_wrap__(arr)\n            result = result.to_dense()\nimport operator\nimport pandas.util._test_decorators as td\nfrom pandas.arrays import BooleanArray\nfrom pandas.core.arrays.boolean import coerce_to_array\nfrom pandas.tests.extension.base import BaseOpsUtil\n    return pd.BooleanDtype()\ndef test_boolean_array_constructor():\n    values = np.array([True, False, True, False], dtype=\"bool\")\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n\n    result = BooleanArray(values, mask)\n    expected = pd.array([True, False, True, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n    with pytest.raises(TypeError, match=\"values should be boolean numpy array\"):\n        BooleanArray(values.tolist(), mask)\n\n    with pytest.raises(TypeError, match=\"mask should be boolean numpy array\"):\n        BooleanArray(values, mask.tolist())\n\n    with pytest.raises(TypeError, match=\"values should be boolean numpy array\"):\n        BooleanArray(values.astype(int), mask)\n\n    with pytest.raises(TypeError, match=\"mask should be boolean numpy array\"):\n        BooleanArray(values, None)\n    with pytest.raises(ValueError, match=\"values must be a 1D array\"):\n        BooleanArray(values.reshape(1, -1), mask)\n\n    with pytest.raises(ValueError, match=\"mask must be a 1D array\"):\n        BooleanArray(values, mask.reshape(1, -1))\n\n\ndef test_boolean_array_constructor_copy():\n    values = np.array([True, False, True, False], dtype=\"bool\")\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n    result = BooleanArray(values, mask)\n    assert result._data is values\n    assert result._mask is mask\n\n    result = BooleanArray(values, mask, copy=True)\n    assert result._data is not values\n    assert result._mask is not mask\n\n\ndef test_to_boolean_array():\n    expected = BooleanArray(\n        np.array([True, False, True]), np.array([False, False, False])\n    )\n    result = pd.array([True, False, True], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n    result = pd.array(np.array([True, False, True]), dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n    result = pd.array(np.array([True, False, True], dtype=object), dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n    # with missing values\n    expected = BooleanArray(\n        np.array([True, False, True]), np.array([False, False, True])\n    )\n\n    result = pd.array([True, False, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n    result = pd.array(np.array([True, False, None], dtype=object), dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_to_boolean_array_all_none():\n    expected = BooleanArray(np.array([True, True, True]), np.array([True, True, True]))\n\n    result = pd.array([None, None, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n    result = pd.array(np.array([None, None, None], dtype=object), dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"a, b\",\n    [\n        ([True, False, None, np.nan, pd.NA], [True, False, None, None, None]),\n        ([True, np.nan], [True, None]),\n        ([True, pd.NA], [True, None]),\n        ([np.nan, np.nan], [None, None]),\n        (np.array([np.nan, np.nan], dtype=float), [None, None]),\n    ],\n)\ndef test_to_boolean_array_missing_indicators(a, b):\n    result = pd.array(a, dtype=\"boolean\")\n    expected = pd.array(b, dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"values\",\n    [\n        [\"foo\", \"bar\"],\n        [\"1\", \"2\"],\n        # \"foo\",\n        [1, 2],\n        [1.0, 2.0],\n        pd.date_range(\"20130101\", periods=2),\n        np.array([\"foo\"]),\n        np.array([1, 2]),\n        np.array([1.0, 2.0]),\n        [np.nan, {\"a\": 1}],\n    ],\n)\ndef test_to_boolean_array_error(values):\n    # error in converting existing arrays to BooleanArray\n    with pytest.raises(TypeError):\n        pd.array(values, dtype=\"boolean\")\n\n\ndef test_to_boolean_array_from_integer_array():\n    result = pd.array(np.array([1, 0, 1, 0]), dtype=\"boolean\")\n    expected = pd.array([True, False, True, False], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n    # with missing values\n    result = pd.array(np.array([1, 0, 1, None]), dtype=\"boolean\")\n    expected = pd.array([True, False, True, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_to_boolean_array_from_float_array():\n    result = pd.array(np.array([1.0, 0.0, 1.0, 0.0]), dtype=\"boolean\")\n    expected = pd.array([True, False, True, False], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n    # with missing values\n    result = pd.array(np.array([1.0, 0.0, 1.0, np.nan]), dtype=\"boolean\")\n    expected = pd.array([True, False, True, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_to_boolean_array_integer_like():\n    # integers of 0's and 1's\n    result = pd.array([1, 0, 1, 0], dtype=\"boolean\")\n    expected = pd.array([True, False, True, False], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n    # with missing values\n    result = pd.array([1, 0, 1, None], dtype=\"boolean\")\n    expected = pd.array([True, False, True, None], dtype=\"boolean\")\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_coerce_to_array():\n    # TODO this is currently not public API\n    values = np.array([True, False, True, False], dtype=\"bool\")\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n    result = BooleanArray(*coerce_to_array(values, mask=mask))\n    expected = BooleanArray(values, mask)\n    tm.assert_extension_array_equal(result, expected)\n    assert result._data is values\n    assert result._mask is mask\n    result = BooleanArray(*coerce_to_array(values, mask=mask, copy=True))\n    expected = BooleanArray(values, mask)\n    tm.assert_extension_array_equal(result, expected)\n    assert result._data is not values\n    assert result._mask is not mask\n\n    # mixed missing from values and mask\n    values = [True, False, None, False]\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n    result = BooleanArray(*coerce_to_array(values, mask=mask))\n    expected = BooleanArray(\n        np.array([True, False, True, True]), np.array([False, False, True, True])\n    )\n    tm.assert_extension_array_equal(result, expected)\n    result = BooleanArray(*coerce_to_array(np.array(values, dtype=object), mask=mask))\n    tm.assert_extension_array_equal(result, expected)\n    result = BooleanArray(*coerce_to_array(values, mask=mask.tolist()))\n    tm.assert_extension_array_equal(result, expected)\n\n    # raise errors for wrong dimension\n    values = np.array([True, False, True, False], dtype=\"bool\")\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n\n    with pytest.raises(ValueError, match=\"values must be a 1D list-like\"):\n        coerce_to_array(values.reshape(1, -1))\n\n    with pytest.raises(ValueError, match=\"mask must be a 1D list-like\"):\n        coerce_to_array(values, mask=mask.reshape(1, -1))\n\n\ndef test_coerce_to_array_from_boolean_array():\n    # passing BooleanArray to coerce_to_array\n    values = np.array([True, False, True, False], dtype=\"bool\")\n    mask = np.array([False, False, False, True], dtype=\"bool\")\n    arr = BooleanArray(values, mask)\n    result = BooleanArray(*coerce_to_array(arr))\n    tm.assert_extension_array_equal(result, arr)\n    # no copy\n    assert result._data is arr._data\n    assert result._mask is arr._mask\n\n    result = BooleanArray(*coerce_to_array(arr), copy=True)\n    tm.assert_extension_array_equal(result, arr)\n    assert result._data is not arr._data\n    assert result._mask is not arr._mask\n\n    with pytest.raises(ValueError, match=\"cannot pass mask for BooleanArray input\"):\n        coerce_to_array(arr, mask=mask)\n\n\ndef test_coerce_to_numpy_array():\n    # with missing values -> object dtype\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n    result = np.array(arr)\n    expected = np.array([True, False, pd.NA], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    # also with no missing values -> object dtype\n    arr = pd.array([True, False, True], dtype=\"boolean\")\n    result = np.array(arr)\n    expected = np.array([True, False, True], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    # force bool dtype\n    result = np.array(arr, dtype=\"bool\")\n    expected = np.array([True, False, True], dtype=\"bool\")\n    tm.assert_numpy_array_equal(result, expected)\n    # with missing values will raise error\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n    with pytest.raises(ValueError):\n        np.array(arr, dtype=\"bool\")\n\n\ndef test_to_boolean_array_from_strings():\n    result = BooleanArray._from_sequence_of_strings(\n        np.array([\"True\", \"False\", np.nan], dtype=object)\n    )\n    expected = BooleanArray(\n        np.array([True, False, False]), np.array([False, False, True])\n    )\n\n    tm.assert_extension_array_equal(result, expected)\n\n\ndef test_to_boolean_array_from_strings_invalid_string():\n    with pytest.raises(ValueError, match=\"cannot be cast\"):\n        BooleanArray._from_sequence_of_strings([\"donkey\"])\n\n\ndef test_repr():\n    df = pd.DataFrame({\"A\": pd.array([True, False, None], dtype=\"boolean\")})\n    expected = \"       A\\n0   True\\n1  False\\n2   <NA>\"\n    assert repr(df) == expected\n\n    expected = \"0     True\\n1    False\\n2     <NA>\\nName: A, dtype: boolean\"\n    assert repr(df.A) == expected\n\n    expected = \"<BooleanArray>\\n[True, False, <NA>]\\nLength: 3, dtype: boolean\"\n    assert repr(df.A.array) == expected\n\n\n@pytest.mark.parametrize(\"box\", [True, False], ids=[\"series\", \"array\"])\ndef test_to_numpy(box):\n    con = pd.Series if box else pd.array\n    # default (with or without missing values) -> object dtype\n    arr = con([True, False, True], dtype=\"boolean\")\n    result = arr.to_numpy()\n    expected = np.array([True, False, True], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    arr = con([True, False, None], dtype=\"boolean\")\n    result = arr.to_numpy()\n    expected = np.array([True, False, pd.NA], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    arr = con([True, False, None], dtype=\"boolean\")\n    result = arr.to_numpy(dtype=\"str\")\n    expected = np.array([True, False, pd.NA], dtype=\"<U5\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    # no missing values -> can convert to bool, otherwise raises\n    arr = con([True, False, True], dtype=\"boolean\")\n    result = arr.to_numpy(dtype=\"bool\")\n    expected = np.array([True, False, True], dtype=\"bool\")\n    tm.assert_numpy_array_equal(result, expected)\n    arr = con([True, False, None], dtype=\"boolean\")\n    with pytest.raises(ValueError, match=\"cannot convert to 'bool'-dtype\"):\n        result = arr.to_numpy(dtype=\"bool\")\n\n    # specify dtype and na_value\n    arr = con([True, False, None], dtype=\"boolean\")\n    result = arr.to_numpy(dtype=object, na_value=None)\n    expected = np.array([True, False, None], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    result = arr.to_numpy(dtype=bool, na_value=False)\n    expected = np.array([True, False, False], dtype=\"bool\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    result = arr.to_numpy(dtype=\"int64\", na_value=-99)\n    expected = np.array([1, 0, -99], dtype=\"int64\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    result = arr.to_numpy(dtype=\"float64\", na_value=np.nan)\n    expected = np.array([1, 0, np.nan], dtype=\"float64\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    # converting to int or float without specifying na_value raises\n    with pytest.raises(ValueError, match=\"cannot convert to 'int64'-dtype\"):\n        arr.to_numpy(dtype=\"int64\")\n    with pytest.raises(ValueError, match=\"cannot convert to 'float64'-dtype\"):\n        arr.to_numpy(dtype=\"float64\")\n\n\ndef test_to_numpy_copy():\n    # to_numpy can be zero-copy if no missing values\n    arr = pd.array([True, False, True], dtype=\"boolean\")\n    result = arr.to_numpy(dtype=bool)\n    result[0] = False\n    tm.assert_extension_array_equal(\n        arr, pd.array([False, False, True], dtype=\"boolean\")\n    )\n\n    arr = pd.array([True, False, True], dtype=\"boolean\")\n    result = arr.to_numpy(dtype=bool, copy=True)\n    result[0] = False\n    tm.assert_extension_array_equal(arr, pd.array([True, False, True], dtype=\"boolean\"))\n\n\ndef test_astype():\n    # with missing values\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n\n    with pytest.raises(ValueError, match=\"cannot convert NA to integer\"):\n        arr.astype(\"int64\")\n\n    with pytest.raises(ValueError, match=\"cannot convert float NaN to\"):\n        arr.astype(\"bool\")\n\n    result = arr.astype(\"float64\")\n    expected = np.array([1, 0, np.nan], dtype=\"float64\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    result = arr.astype(\"str\")\n    expected = np.array([\"True\", \"False\", \"<NA>\"], dtype=\"object\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    # no missing values\n    arr = pd.array([True, False, True], dtype=\"boolean\")\n    result = arr.astype(\"int64\")\n    expected = np.array([1, 0, 1], dtype=\"int64\")\n    tm.assert_numpy_array_equal(result, expected)\n\n    result = arr.astype(\"bool\")\n    expected = np.array([True, False, True], dtype=\"bool\")\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_astype_to_boolean_array():\n    # astype to BooleanArray\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n\n    result = arr.astype(\"boolean\")\n    tm.assert_extension_array_equal(result, arr)\n    result = arr.astype(pd.BooleanDtype())\n    tm.assert_extension_array_equal(result, arr)\n\n\ndef test_astype_to_integer_array():\n    # astype to IntegerArray\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n\n    result = arr.astype(\"Int64\")\n    expected = pd.array([1, 0, None], dtype=\"Int64\")\n    tm.assert_extension_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"na\", [None, np.nan, pd.NA])\ndef test_setitem_missing_values(na):\n    arr = pd.array([True, False, None], dtype=\"boolean\")\n    expected = pd.array([True, None, None], dtype=\"boolean\")\n    arr[1] = na\n    tm.assert_extension_array_equal(arr, expected)\n\n\n@pytest.mark.parametrize(\n    \"ufunc\", [np.add, np.logical_or, np.logical_and, np.logical_xor]\n)\ndef test_ufuncs_binary(ufunc):\n    # two BooleanArrays\n    a = pd.array([True, False, None], dtype=\"boolean\")\n    result = ufunc(a, a)\n    expected = pd.array(ufunc(a._data, a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    s = pd.Series(a)\n    result = ufunc(s, a)\n    expected = pd.Series(ufunc(a._data, a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_series_equal(result, expected)\n\n    # Boolean with numpy array\n    arr = np.array([True, True, False])\n    result = ufunc(a, arr)\n    expected = pd.array(ufunc(a._data, arr), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    result = ufunc(arr, a)\n    expected = pd.array(ufunc(arr, a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    # BooleanArray with scalar\n    result = ufunc(a, True)\n    expected = pd.array(ufunc(a._data, True), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    result = ufunc(True, a)\n    expected = pd.array(ufunc(True, a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    # not handled types\n    with pytest.raises(TypeError):\n        ufunc(a, \"test\")\n\n\n@pytest.mark.parametrize(\"ufunc\", [np.logical_not])\ndef test_ufuncs_unary(ufunc):\n    a = pd.array([True, False, None], dtype=\"boolean\")\n    result = ufunc(a)\n    expected = pd.array(ufunc(a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_extension_array_equal(result, expected)\n\n    s = pd.Series(a)\n    result = ufunc(s)\n    expected = pd.Series(ufunc(a._data), dtype=\"boolean\")\n    expected[a._mask] = np.nan\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"values\", [[True, False], [True, None]])\ndef test_ufunc_reduce_raises(values):\n    a = pd.array(values, dtype=\"boolean\")\n    with pytest.raises(NotImplementedError):\n        np.add.reduce(a)\n\n\nclass TestLogicalOps(BaseOpsUtil):\n    def test_numpy_scalars_ok(self, all_logical_operators):\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        op = getattr(a, all_logical_operators)\n\n        tm.assert_extension_array_equal(op(True), op(np.bool(True)))\n        tm.assert_extension_array_equal(op(False), op(np.bool(False)))\n\n    def get_op_from_name(self, op_name):\n        short_opname = op_name.strip(\"_\")\n        short_opname = short_opname if \"xor\" in short_opname else short_opname + \"_\"\n        try:\n            op = getattr(operator, short_opname)\n        except AttributeError:\n            # Assume it is the reverse operator\n            rop = getattr(operator, short_opname[1:])\n            op = lambda x, y: rop(y, x)\n\n        return op\n\n    def test_empty_ok(self, all_logical_operators):\n        a = pd.array([], dtype=\"boolean\")\n        op_name = all_logical_operators\n        result = getattr(a, op_name)(True)\n        tm.assert_extension_array_equal(a, result)\n\n        result = getattr(a, op_name)(False)\n        tm.assert_extension_array_equal(a, result)\n\n        # TODO: pd.NA\n        # result = getattr(a, op_name)(pd.NA)\n        # tm.assert_extension_array_equal(a, result)\n\n    def test_logical_length_mismatch_raises(self, all_logical_operators):\n        op_name = all_logical_operators\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        msg = \"Lengths must match to compare\"\n\n        with pytest.raises(ValueError, match=msg):\n            getattr(a, op_name)([True, False])\n\n        with pytest.raises(ValueError, match=msg):\n            getattr(a, op_name)(np.array([True, False]))\n\n        with pytest.raises(ValueError, match=msg):\n            getattr(a, op_name)(pd.array([True, False], dtype=\"boolean\"))\n\n    def test_logical_nan_raises(self, all_logical_operators):\n        op_name = all_logical_operators\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        msg = \"Got float instead\"\n\n        with pytest.raises(TypeError, match=msg):\n            getattr(a, op_name)(np.nan)\n\n    @pytest.mark.parametrize(\"other\", [\"a\", 1])\n    def test_non_bool_or_na_other_raises(self, other, all_logical_operators):\n        a = pd.array([True, False], dtype=\"boolean\")\n        with pytest.raises(TypeError, match=str(type(other).__name__)):\n            getattr(a, all_logical_operators)(other)\n\n    def test_kleene_or(self):\n        # A clear test of behavior.\n        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        b = pd.array([True, False, None] * 3, dtype=\"boolean\")\n        result = a | b\n        expected = pd.array(\n            [True, True, True, True, False, None, True, None, None], dtype=\"boolean\"\n        )\n        tm.assert_extension_array_equal(result, expected)\n        result = b | a\n        tm.assert_extension_array_equal(result, expected)\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        )\n        tm.assert_extension_array_equal(\n            b, pd.array([True, False, None] * 3, dtype=\"boolean\")\n        )\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (pd.NA, [True, None, None]),\n            (True, [True, True, True]),\n            (np.bool_(True), [True, True, True]),\n            (False, [True, False, None]),\n            (np.bool_(False), [True, False, None]),\n        ],\n    )\n    def test_kleene_or_scalar(self, other, expected):\n        # TODO: test True & False\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        result = a | other\n        expected = pd.array(expected, dtype=\"boolean\")\n        tm.assert_extension_array_equal(result, expected)\n\n        result = other | a\n        tm.assert_extension_array_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True, False, None], dtype=\"boolean\")\n        )\n    def test_kleene_and(self):\n        # A clear test of behavior.\n        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        b = pd.array([True, False, None] * 3, dtype=\"boolean\")\n        result = a & b\n        expected = pd.array(\n            [True, False, None, False, False, False, None, False, None], dtype=\"boolean\"\n        )\n        tm.assert_extension_array_equal(result, expected)\n        result = b & a\n        tm.assert_extension_array_equal(result, expected)\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        )\n        tm.assert_extension_array_equal(\n            b, pd.array([True, False, None] * 3, dtype=\"boolean\")\n        )\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (pd.NA, [None, False, None]),\n            (True, [True, False, None]),\n            (False, [False, False, False]),\n            (np.bool_(True), [True, False, None]),\n            (np.bool_(False), [False, False, False]),\n        ],\n    )\n    def test_kleene_and_scalar(self, other, expected):\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        result = a & other\n        expected = pd.array(expected, dtype=\"boolean\")\n        tm.assert_extension_array_equal(result, expected)\n\n        result = other & a\n        tm.assert_extension_array_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True, False, None], dtype=\"boolean\")\n        )\n    def test_kleene_xor(self):\n        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        b = pd.array([True, False, None] * 3, dtype=\"boolean\")\n        result = a ^ b\n        expected = pd.array(\n            [False, True, None, True, False, None, None, None, None], dtype=\"boolean\"\n        )\n        tm.assert_extension_array_equal(result, expected)\n        result = b ^ a\n        tm.assert_extension_array_equal(result, expected)\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        )\n        tm.assert_extension_array_equal(\n            b, pd.array([True, False, None] * 3, dtype=\"boolean\")\n        )\n    @pytest.mark.parametrize(\n        \"other, expected\",\n        [\n            (pd.NA, [None, None, None]),\n            (True, [False, True, None]),\n            (np.bool_(True), [False, True, None]),\n            (np.bool_(False), [True, False, None]),\n        ],\n    )\n    def test_kleene_xor_scalar(self, other, expected):\n        a = pd.array([True, False, None], dtype=\"boolean\")\n        result = a ^ other\n        expected = pd.array(expected, dtype=\"boolean\")\n        tm.assert_extension_array_equal(result, expected)\n\n        result = other ^ a\n        tm.assert_extension_array_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        tm.assert_extension_array_equal(\n            a, pd.array([True, False, None], dtype=\"boolean\")\n        )\n    @pytest.mark.parametrize(\n        \"other\", [True, False, pd.NA, [True, False, None] * 3],\n    )\n    def test_no_masked_assumptions(self, other, all_logical_operators):\n        # The logical operations should not assume that masked values are False!\n        a = pd.arrays.BooleanArray(\n            np.array([True, True, True, False, False, False, True, False, True]),\n            np.array([False] * 6 + [True, True, True]),\n        )\n        b = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        if isinstance(other, list):\n            other = pd.array(other, dtype=\"boolean\")\n        result = getattr(a, all_logical_operators)(other)\n        expected = getattr(b, all_logical_operators)(other)\n        tm.assert_extension_array_equal(result, expected)\n        if isinstance(other, BooleanArray):\n            other._data[other._mask] = True\n            a._data[a._mask] = False\n            result = getattr(a, all_logical_operators)(other)\n            expected = getattr(b, all_logical_operators)(other)\n            tm.assert_extension_array_equal(result, expected)\nclass TestComparisonOps(BaseOpsUtil):\n    def _compare_other(self, data, op_name, other):\n        op = self.get_op_from_name(op_name)\n        # array\n        result = pd.Series(op(data, other))\n        expected = pd.Series(op(data._data, other), dtype=\"boolean\")\n        # propagate NAs\n        expected[data._mask] = pd.NA\n        tm.assert_series_equal(result, expected)\n        # series\n        s = pd.Series(data)\n        result = op(s, other)\n        expected = pd.Series(data._data)\n        expected = op(expected, other)\n        expected = expected.astype(\"boolean\")\n        # propagate NAs\n        expected[data._mask] = pd.NA\n        tm.assert_series_equal(result, expected)\n        op_name = all_compare_operators\n        self._compare_other(data, op_name, True)\n        op_name = all_compare_operators\n        other = pd.array([True] * len(data), dtype=\"boolean\")\n        self._compare_other(data, op_name, other)\n        other = np.array([True] * len(data))\n        self._compare_other(data, op_name, other)\n        other = pd.Series([True] * len(data))\n        self._compare_other(data, op_name, other)\n\n    @pytest.mark.parametrize(\"other\", [True, False, pd.NA])\n    def test_scalar(self, other, all_compare_operators):\n        op = self.get_op_from_name(all_compare_operators)\n        a = pd.array([True, False, None], dtype=\"boolean\")\n\n        result = op(a, other)\n\n        if other is pd.NA:\n            expected = pd.array([None, None, None], dtype=\"boolean\")\n        else:\n            values = op(a._data, other)\n            expected = BooleanArray(values, a._mask, copy=True)\n        tm.assert_extension_array_equal(result, expected)\n\n        # ensure we haven't mutated anything inplace\n        result[0] = None\n        tm.assert_extension_array_equal(\n            a, pd.array([True, False, None], dtype=\"boolean\")\n    def test_array(self, all_compare_operators):\n        op = self.get_op_from_name(all_compare_operators)\n        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        b = pd.array([True, False, None] * 3, dtype=\"boolean\")\n        result = op(a, b)\n        values = op(a._data, b._data)\n        mask = a._mask | b._mask\n        expected = BooleanArray(values, mask)\n        tm.assert_extension_array_equal(result, expected)\n        # ensure we haven't mutated anything inplace\n        result[0] = None\n        tm.assert_extension_array_equal(\n            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=\"boolean\")\n        tm.assert_extension_array_equal(\n            b, pd.array([True, False, None] * 3, dtype=\"boolean\")\nclass TestArithmeticOps(BaseOpsUtil):\n    def test_error(self, data, all_arithmetic_operators):\n        # invalid ops\n\n        op = all_arithmetic_operators\n        s = pd.Series(data)\n        ops = getattr(s, op)\n        opa = getattr(data, op)\n\n        # invalid scalars\n        with pytest.raises(TypeError):\n            ops(\"foo\")\n        with pytest.raises(TypeError):\n            ops(pd.Timestamp(\"20180101\"))\n\n        # invalid array-likes\n        if op not in (\"__mul__\", \"__rmul__\"):\n            # TODO(extension) numpy's mul with object array sees booleans as numbers\n            with pytest.raises(TypeError):\n                ops(pd.Series(\"foo\", index=s.index))\n\n        # 2d\n        result = opa(pd.DataFrame({\"A\": s}))\n        assert result is NotImplemented\n\n        with pytest.raises(NotImplementedError):\n            opa(np.arange(len(s)).reshape(-1, len(s)))\n\n\n@pytest.mark.parametrize(\"dropna\", [True, False])\ndef test_reductions_return_types(dropna, data, all_numeric_reductions):\n    op = all_numeric_reductions\n    s = pd.Series(data)\n    if dropna:\n        s = s.dropna()\n\n    if op in (\"sum\", \"prod\"):\n        assert isinstance(getattr(s, op)(), np.int64)\n    elif op in (\"min\", \"max\"):\n        assert isinstance(getattr(s, op)(), np.bool_)\n    else:\n        # \"mean\", \"std\", \"var\", \"median\", \"kurt\", \"skew\"\n        assert isinstance(getattr(s, op)(), np.float64)\n\n\n@pytest.mark.parametrize(\n    \"values, exp_any, exp_all, exp_any_noskip, exp_all_noskip\",\n    [\n        ([True, pd.NA], True, True, True, pd.NA),\n        ([False, pd.NA], False, False, pd.NA, False),\n        ([pd.NA], False, True, pd.NA, pd.NA),\n        ([], False, True, False, True),\n    ],\n)\ndef test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):\n    # the methods return numpy scalars\n    exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)\n    exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)\n    exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)\n    exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)\n\n    for con in [pd.array, pd.Series]:\n        a = con(values, dtype=\"boolean\")\n        assert a.any() is exp_any\n        assert a.all() is exp_all\n        assert a.any(skipna=False) is exp_any_noskip\n        assert a.all(skipna=False) is exp_all_noskip\n\n        assert np.any(a.any()) is exp_any\n        assert np.all(a.all()) is exp_all\n\n\n# TODO when BooleanArray coerces to object dtype numpy array, need to do conversion\n# manually in the indexing code\n# def test_indexing_boolean_mask():\n#     arr = pd.array([1, 2, 3, 4], dtype=\"Int64\")\n#     mask = pd.array([True, False, True, False], dtype=\"boolean\")\n#     result = arr[mask]\n#     expected = pd.array([1, 3], dtype=\"Int64\")\n#     tm.assert_extension_array_equal(result, expected)\n\n#     # missing values -> error\n#     mask = pd.array([True, False, True, None], dtype=\"boolean\")\n#     with pytest.raises(IndexError):\n#         result = arr[mask]\n\n\n@td.skip_if_no(\"pyarrow\", min_version=\"0.15.0\")\ndef test_arrow_array(data):\n    # protocol added in 0.15.0\n    import pyarrow as pa\n\n    arr = pa.array(data)\n\n    # TODO use to_numpy(na_value=None) here\n    data_object = np.array(data, dtype=object)\n    data_object[data.isna()] = None\n    expected = pa.array(data_object, type=pa.bool_(), from_pandas=True)\n    assert arr.equals(expected)\n\n\n@td.skip_if_no(\"pyarrow\", min_version=\"0.15.1.dev\")\ndef test_arrow_roundtrip():\n    # roundtrip possible from arrow 1.0.0\n    import pyarrow as pa\n\n    data = pd.array([True, False, None], dtype=\"boolean\")\n    df = pd.DataFrame({\"a\": data})\n    table = pa.table(df)\n    assert table.field(\"a\").type == \"bool\"\n    result = table.to_pandas()\n    assert isinstance(result[\"a\"].dtype, pd.BooleanDtype)\n    tm.assert_frame_equal(result, df)\n\n\ndef test_value_counts_na():\n    arr = pd.array([True, False, pd.NA], dtype=\"boolean\")\n    result = arr.value_counts(dropna=False)\n    expected = pd.Series([1, 1, 1], index=[True, False, pd.NA], dtype=\"Int64\")\n    tm.assert_series_equal(result, expected)\n\n    result = arr.value_counts(dropna=True)\n    expected = pd.Series([1, 1], index=[True, False], dtype=\"Int64\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_diff():\n    a = pd.array(\n        [True, True, False, False, True, None, True, None, False], dtype=\"boolean\"\n    )\n    result = pd.core.algorithms.diff(a, 1)\n    expected = pd.array(\n        [None, False, True, False, True, None, None, None, None], dtype=\"boolean\"\n    )\n    tm.assert_extension_array_equal(result, expected)\n\n    s = pd.Series(a)\n    result = s.diff()\n    expected = pd.Series(expected)\n    tm.assert_series_equal(result, expected)\nfrom .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil  # noqa", "fixed_code": "def __invert__(self):\n        return type(self)(~self._data, self._mask)\n\n        new_data = self._data.apply(operator.invert)\n        result = self._constructor(new_data).__finalize__(self)\n        return result\n            result = np.asarray(result)\n\"\"\"\nThis file contains a minimal set of tests for compliance with the extension\narray interface test suite, and should contain no other tests.\nThe test suite for the full functionality of the array is located in\n`pandas/tests/arrays/`.\nThe tests in this file are inherited from the BaseExtensionTests, and only\nminimal tweaks should be applied to get the tests passing (by overwriting a\nparent method).\n\nAdditional tests should either be added to one of the BaseExtensionTests\nclasses (if they are relevant for the extension interface for all dtypes), or\nbe added to the array-specific tests in `pandas/tests/arrays/`.\n\n\"\"\"\nfrom pandas.compat.numpy import _np_version_under1p14\nfrom pandas.core.arrays.boolean import BooleanDtype\nfrom pandas.tests.extension import base\n    return BooleanDtype()\n@pytest.fixture\ndef data_for_twos(dtype):\n    return pd.array(np.ones(100), dtype=dtype)\n@pytest.fixture\ndef data_missing(dtype):\n    return pd.array([np.nan, True], dtype=dtype)\n@pytest.fixture\ndef data_for_sorting(dtype):\n    return pd.array([True, True, False], dtype=dtype)\n@pytest.fixture\ndef data_missing_for_sorting(dtype):\n    return pd.array([True, np.nan, False], dtype=dtype)\n@pytest.fixture\ndef na_cmp():\n    # we are pd.NA\n    return lambda x, y: x is pd.NA and y is pd.NA\n@pytest.fixture\ndef na_value():\n    return pd.NA\n@pytest.fixture\ndef data_for_grouping(dtype):\n    b = True\n    a = False\n    na = np.nan\n    return pd.array([b, b, na, na, a, a, b], dtype=dtype)\nclass TestDtype(base.BaseDtypeTests):\n    pass\nclass TestInterface(base.BaseInterfaceTests):\n    pass\n\n\nclass TestConstructors(base.BaseConstructorsTests):\n    pass\n\n\nclass TestGetitem(base.BaseGetitemTests):\n    pass\n\n\nclass TestSetitem(base.BaseSetitemTests):\n    pass\n\n\nclass TestMissing(base.BaseMissingTests):\n    pass\n\nclass TestArithmeticOps(base.BaseArithmeticOpsTests):\n    def check_opname(self, s, op_name, other, exc=None):\n        # overwriting to indicate ops don't raise an error\n        super().check_opname(s, op_name, other, exc=None)\n    def _check_op(self, s, op, other, op_name, exc=NotImplementedError):\n        if exc is None:\n            if op_name in (\"__sub__\", \"__rsub__\"):\n                # subtraction for bools raises TypeError (but not yet in 1.13)\n                if _np_version_under1p14:\n                    pytest.skip(\"__sub__ does not yet raise in numpy 1.13\")\n                with pytest.raises(TypeError):\n                    op(s, other)\n                return\n            result = op(s, other)\n            expected = s.combine(other, op)\n            if op_name in (\n                \"__floordiv__\",\n                \"__rfloordiv__\",\n                \"__pow__\",\n                \"__rpow__\",\n                \"__mod__\",\n                \"__rmod__\",\n            ):\n                # combine keeps boolean type\n                expected = expected.astype(\"Int8\")\n            elif op_name in (\"__truediv__\", \"__rtruediv__\"):\n                # combine with bools does not generate the correct result\n                #  (numpy behaviour for div is to regard the bools as numeric)\n                expected = s.astype(float).combine(other, op)\n            if op_name == \"__rpow__\":\n                # for rpow, combine does not propagate NaN\n                expected[result.isna()] = np.nan\n            self.assert_series_equal(result, expected)\n        else:\n            with pytest.raises(exc):\n                op(s, other)\n    def _check_divmod_op(self, s, op, other, exc=None):\n        # override to not raise an error\n        super()._check_divmod_op(s, op, other, None)\n    @pytest.mark.skip(reason=\"BooleanArray does not error on ops\")\n    def test_error(self, data, all_arithmetic_operators):\n        # other specific errors tested in the boolean array specific tests\n        pass\nclass TestComparisonOps(base.BaseComparisonOpsTests):\n    def check_opname(self, s, op_name, other, exc=None):\n        # overwriting to indicate ops don't raise an error\n        super().check_opname(s, op_name, other, exc=None)\n    def _compare_other(self, s, data, op_name, other):\n        self.check_opname(s, op_name, other)\n    @pytest.mark.skip(reason=\"Tested in tests/arrays/test_boolean.py\")\n        pass\n    @pytest.mark.skip(reason=\"Tested in tests/arrays/test_boolean.py\")\n        pass\n\n\nclass TestReshaping(base.BaseReshapingTests):\n    pass\n\n\nclass TestMethods(base.BaseMethodsTests):\n    @pytest.mark.parametrize(\"na_sentinel\", [-1, -2])\n    def test_factorize(self, data_for_grouping, na_sentinel):\n        # override because we only have 2 unique values\n        labels, uniques = pd.factorize(data_for_grouping, na_sentinel=na_sentinel)\n        expected_labels = np.array(\n            [0, 0, na_sentinel, na_sentinel, 1, 1, 0], dtype=np.intp\n        )\n        expected_uniques = data_for_grouping.take([0, 4])\n\n        tm.assert_numpy_array_equal(labels, expected_labels)\n        self.assert_extension_array_equal(uniques, expected_uniques)\n\n    def test_combine_le(self, data_repeated):\n        # override because expected needs to be boolean instead of bool dtype\n        orig_data1, orig_data2 = data_repeated(2)\n        s1 = pd.Series(orig_data1)\n        s2 = pd.Series(orig_data2)\n        result = s1.combine(s2, lambda x1, x2: x1 <= x2)\n        expected = pd.Series(\n            [a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],\n            dtype=\"boolean\",\n        self.assert_series_equal(result, expected)\n\n        val = s1.iloc[0]\n        result = s1.combine(val, lambda x1, x2: x1 <= x2)\n        expected = pd.Series([a <= val for a in list(orig_data1)], dtype=\"boolean\")\n        self.assert_series_equal(result, expected)\n\n    def test_searchsorted(self, data_for_sorting, as_series):\n        # override because we only have 2 unique values\n        data_for_sorting = pd.array([True, False], dtype=\"boolean\")\n        b, a = data_for_sorting\n        arr = type(data_for_sorting)._from_sequence([a, b])\n\n        if as_series:\n            arr = pd.Series(arr)\n        assert arr.searchsorted(a) == 0\n        assert arr.searchsorted(a, side=\"right\") == 1\n\n        assert arr.searchsorted(b) == 1\n        assert arr.searchsorted(b, side=\"right\") == 2\n\n        result = arr.searchsorted(arr.take([0, 1]))\n        expected = np.array([0, 1], dtype=np.intp)\n\n        tm.assert_numpy_array_equal(result, expected)\n\n        # sorter\n        sorter = np.array([1, 0])\n        assert data_for_sorting.searchsorted(a, sorter=sorter) == 0\n\n    @pytest.mark.skip(reason=\"uses nullable integer\")\n    def test_value_counts(self, all_data, dropna):\n        return super().test_value_counts(all_data, dropna)\nclass TestCasting(base.BaseCastingTests):\n    pass\nclass TestGroupby(base.BaseGroupbyTests):\n    \"\"\"\n    Groupby-specific tests are overridden because boolean only has 2\n    unique values, base tests uses 3 groups.\n    \"\"\"\n\n    def test_grouping_grouper(self, data_for_grouping):\n        df = pd.DataFrame(\n            {\"A\": [\"B\", \"B\", None, None, \"A\", \"A\", \"B\"], \"B\": data_for_grouping}\n        )\n        gr1 = df.groupby(\"A\").grouper.groupings[0]\n        gr2 = df.groupby(\"B\").grouper.groupings[0]\n\n        tm.assert_numpy_array_equal(gr1.grouper, df.A.values)\n        tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)\n\n    @pytest.mark.parametrize(\"as_index\", [True, False])\n    def test_groupby_extension_agg(self, as_index, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1], \"B\": data_for_grouping})\n        result = df.groupby(\"B\", as_index=as_index).A.mean()\n        _, index = pd.factorize(data_for_grouping, sort=True)\n\n        index = pd.Index(index, name=\"B\")\n        expected = pd.Series([3, 1], index=index, name=\"A\")\n        if as_index:\n            self.assert_series_equal(result, expected)\n        else:\n            expected = expected.reset_index()\n            self.assert_frame_equal(result, expected)\n\n    def test_groupby_extension_no_sort(self, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1], \"B\": data_for_grouping})\n        result = df.groupby(\"B\", sort=False).A.mean()\n        _, index = pd.factorize(data_for_grouping, sort=False)\n\n        index = pd.Index(index, name=\"B\")\n        expected = pd.Series([1, 3], index=index, name=\"A\")\n        self.assert_series_equal(result, expected)\n\n    def test_groupby_extension_transform(self, data_for_grouping):\n        valid = data_for_grouping[~data_for_grouping.isna()]\n        df = pd.DataFrame({\"A\": [1, 1, 3, 3, 1], \"B\": valid})\n\n        result = df.groupby(\"B\").A.transform(len)\n        expected = pd.Series([3, 3, 2, 2, 3], name=\"A\")\n\n        self.assert_series_equal(result, expected)\n\n    def test_groupby_extension_apply(self, data_for_grouping, groupby_apply_op):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1], \"B\": data_for_grouping})\n        df.groupby(\"B\").apply(groupby_apply_op)\n        df.groupby(\"B\").A.apply(groupby_apply_op)\n        df.groupby(\"A\").apply(groupby_apply_op)\n        df.groupby(\"A\").B.apply(groupby_apply_op)\n\n    def test_groupby_apply_identity(self, data_for_grouping):\n        df = pd.DataFrame({\"A\": [1, 1, 2, 2, 3, 3, 1], \"B\": data_for_grouping})\n        result = df.groupby(\"A\").B.apply(lambda x: x.array)\n        expected = pd.Series(\n            [\n                df.B.iloc[[0, 1, 6]].array,\n                df.B.iloc[[2, 3]].array,\n                df.B.iloc[[4, 5]].array,\n            ],\n            index=pd.Index([1, 2, 3], name=\"A\"),\n            name=\"B\",\n        self.assert_series_equal(result, expected)\n\n    def test_in_numeric_groupby(self, data_for_grouping):\n        df = pd.DataFrame(\n            {\n                \"A\": [1, 1, 2, 2, 3, 3, 1],\n                \"B\": data_for_grouping,\n                \"C\": [1, 1, 1, 1, 1, 1, 1],\n            }\n        result = df.groupby(\"A\").sum().columns\n\n        if data_for_grouping.dtype._is_numeric:\n            expected = pd.Index([\"B\", \"C\"])\n        else:\n            expected = pd.Index([\"C\"])\n        tm.assert_index_equal(result, expected)\n\nclass TestNumericReduce(base.BaseNumericReduceTests):\n    def check_reduce(self, s, op_name, skipna):\n        result = getattr(s, op_name)(skipna=skipna)\n        expected = getattr(s.astype(\"float64\"), op_name)(skipna=skipna)\n        # override parent function to cast to bool for min/max\n        if np.isnan(expected):\n            expected = pd.NA\n        elif op_name in (\"min\", \"max\"):\n            expected = bool(expected)\n        tm.assert_almost_equal(result, expected)\n\n\nclass TestBooleanReduce(base.BaseBooleanReduceTests):\n    pass\n\n\nclass TestPrinting(base.BasePrintingTests):\n    pass\n\n\nclass TestUnaryOps(base.BaseUnaryOpsTests):\n    pass\n\n\n# TODO parsing not yet supported\n# class TestParsing(base.BaseParsingTests):\n#     pass\nfrom .ops import (  # noqa\n    BaseArithmeticOpsTests,\n    BaseComparisonOpsTests,\n    BaseOpsUtil,\n    BaseUnaryOpsTests,\n)\n\n\nclass BaseUnaryOpsTests(BaseOpsUtil):\n    def test_invert(self, data):\n        s = pd.Series(data, name=\"name\")\n        result = ~s\n        expected = pd.Series(~data, name=\"name\")\n        self.assert_series_equal(result, expected)\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)", "description": ""}
{"id": "pandas-74", "project": "pandas", "bug_id": "74", "buggy_code": "if isinstance(data, TimedeltaArray):", "fixed_code": "if isinstance(data, TimedeltaArray) and freq is None:", "description": ""}
{"id": "pandas-120", "project": "pandas", "bug_id": "120", "buggy_code": "return Series(res, index=ri, name=self._selection_name)\n        return Series(\n        return result\n    def _reindex_output(self, output):\n        output: Series or DataFrame\n            d = {self.obj._get_axis_name(self.axis): index, \"copy\": False}\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed_code": "result = Series(res, index=ri, name=self._selection_name)\n        return self._reindex_output(result, fill_value=0)\n        result = Series(\n        return self._reindex_output(result, fill_value=0)\nfrom pandas._typing import FrameOrSeries, Scalar\n        return self._reindex_output(result, fill_value=0)\n            out = self._reindex_output(out)\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n        output : Series or DataFrame\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n            d = {\n                self.obj._get_axis_name(self.axis): index,\n                \"copy\": False,\n                \"fill_value\": fill_value,\n            }\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )", "description": ""}
{"id": "pandas-6", "project": "pandas", "bug_id": "6", "buggy_code": "except (KeyError, IndexError):", "fixed_code": "except (KeyError, IndexError, ValueError):\n            # TODO: ValueError: Given date string not likely a datetime.\n            # should be KeyError?", "description": ""}
{"id": "pandas-28", "project": "pandas", "bug_id": "28", "buggy_code": "return [Series(others._values, index=others)]", "fixed_code": "return [Series(others._values, index=idx)]", "description": ""}
{"id": "pandas-143", "project": "pandas", "bug_id": "143", "buggy_code": "if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)", "fixed_code": "if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )", "description": ""}
{"id": "pandas-17", "project": "pandas", "bug_id": "17", "buggy_code": "import operator\nfrom typing import Any, Sequence, Type, Union, cast\nimport warnings\nfrom pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib\nfrom pandas._libs.tslibs.c_timestamp import integer_op_not_supported\nfrom pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period\nfrom pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds\nfrom pandas._libs.tslibs.timestamps import RoundTo, round_nsint64\nfrom pandas._typing import DatetimeLikeScalar\nfrom pandas.compat import set_function_name\nfrom pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning\nfrom pandas.util._decorators import Appender, Substitution\nfrom pandas.util._validators import validate_fillna_kwargs\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_string_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\nfrom pandas.core.dtypes.generic import ABCSeries\nfrom pandas.core.dtypes.inference import is_array_like\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n\nfrom pandas.core import missing, nanops, ops\nfrom pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts\nfrom pandas.core.array_algos.transforms import shift\nfrom pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray\nfrom pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin\nimport pandas.core.common as com\nfrom pandas.core.construction import array, extract_array\nfrom pandas.core.indexers import check_array_indexer\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.ops.invalid import invalid_comparison, make_invalid_op\n\nfrom pandas.tseries import frequencies\nfrom pandas.tseries.offsets import DateOffset, Tick\n\n\ndef _datetimelike_array_cmp(cls, op):\n    \"\"\"\n    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to\n    boxed scalars/arrays.\n    \"\"\"\n    opname = f\"__{op.__name__}__\"\n    nat_result = opname == \"__ne__\"\n\n    class InvalidComparison(Exception):\n        pass\n\n    def _validate_comparison_value(self, other):\n        if isinstance(other, str):\n            try:\n                # GH#18435 strings get a pass from tzawareness compat\n                other = self._scalar_from_string(other)\n            except ValueError:\n                # failed to parse as Timestamp/Timedelta/Period\n                raise InvalidComparison(other)\n\n        if isinstance(other, self._recognized_scalars) or other is NaT:\n            other = self._scalar_type(other)\n            self._check_compatible_with(other)\n\n        elif not is_list_like(other):\n            raise InvalidComparison(other)\n\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n        else:\n            if isinstance(other, list):\n                # TODO: could use pd.Index to do inference?\n                other = np.array(other)\n            if not isinstance(other, (np.ndarray, type(self))):\n                raise InvalidComparison(other)\n            elif is_object_dtype(other.dtype):\n                pass\n            elif not type(self)._is_recognized_dtype(other.dtype):\n                raise InvalidComparison(other)\n            else:\n                # For PeriodDType this casting is unnecessary\n                # TODO: use Index to do inference?\n                other = type(self)._from_sequence(other)\n                self._check_compatible_with(other)\n        return other\n    @unpack_zerodim_and_defer(opname)\n    def wrapper(self, other):\n        try:\n            other = _validate_comparison_value(self, other)\n        except InvalidComparison:\n            return invalid_comparison(self, other, op)\n\n        dtype = getattr(other, \"dtype\", None)\n        if is_object_dtype(dtype):\n            # We have to use comp_method_OBJECT_ARRAY instead of numpy\n            #  comparison otherwise it would fail to raise when\n            #  comparing tz-aware and tz-naive\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)\n            return result\n        if isinstance(other, self._scalar_type) or other is NaT:\n            other_i8 = self._unbox_scalar(other)\n        else:\n            # Then type(other) == type(self)\n            other_i8 = other.asi8\n        result = op(self.asi8, other_i8)\n        o_mask = isna(other)\n        if self._hasnans | np.any(o_mask):\n            result[self._isnan | o_mask] = nat_result\n    return set_function_name(wrapper, opname, cls)\nclass AttributesMixin:\n    _data: np.ndarray\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, **kwargs):\n        raise AbstractMethodError(cls)\n    @property\n    def _scalar_type(self) -> Type[DatetimeLikeScalar]:\n        \"\"\"\n        The scalar associated with this datelike\n        * PeriodArray : Period\n        * DatetimeArray : Timestamp\n        * TimedeltaArray : Timedelta\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _scalar_from_string(\n        self, value: str\n    ) -> Union[Period, Timestamp, Timedelta, NaTType]:\n        \"\"\"\n        Construct a scalar type from a string.\n        Parameters\n        ----------\n        value : str\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.\n        raise AbstractMethodError(self)\n\n    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:\n        Unbox the integer value of a scalar `value`.\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n        Returns\n        -------\n        int\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta(\"10s\"))  # doctest: +SKIP\n        10000000000\n        raise AbstractMethodError(self)\n\n    def _check_compatible_with(\n        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False\n    ) -> None:\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n        In each case, NaT is considered compatible.\n        Parameters\n        ----------\n        other\n        setitem : bool, default False\n            For __setitem__ we may have stricter compatibility resrictions than\n            for comparisons.\n\n        Raises\n        ------\n        Exception\n        raise AbstractMethodError(self)\n\n\nclass DatelikeOps:\n    \"\"\"\n    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.\n    \"\"\"\n\n    @Substitution(\n        URL=\"https://docs.python.org/3/library/datetime.html\"\n        \"#strftime-and-strptime-behavior\"\n    )\n    def strftime(self, date_format):\n        Convert to Index using specified date_format.\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n        Returns\n        -------\n        ndarray\n            NumPy ndarray of formatted strings.\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        \"\"\"\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object)\nclass TimelikeOps:\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n    _round_doc = \"\"\"\n        Perform {op} operation on the data to the specified `freq`.\n        Parameters\n        ----------\n        freq : str or Offset\n            The frequency level to {op} the index to. Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end). See\n            :ref:`frequency aliases <timeseries.offset_aliases>` for\n            a list of possible `freq` values.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            Only relevant for DatetimeIndex:\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n            .. versionadded:: 0.24.0\n        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n            .. versionadded:: 0.24.0\n        Returns\n        -------\n        DatetimeIndex, TimedeltaIndex, or Series\n            Index of the same type for a DatetimeIndex or TimedeltaIndex,\n            or a Series with the same index for a Series.\n        Raises\n        ------\n        ValueError if the `freq` cannot be converted.\n        Examples\n        **DatetimeIndex**\n\n        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n        >>> rng\n        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:01:00'],\n                      dtype='datetime64[ns]', freq='T')\n    _round_example = \"\"\">>> rng.round('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n        **Series**\n        >>> pd.Series(rng).dt.round(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _floor_example = \"\"\">>> rng.floor('H')\n        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n        **Series**\n\n        >>> pd.Series(rng).dt.floor(\"H\")\n        0   2018-01-01 11:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _ceil_example = \"\"\">>> rng.ceil('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 13:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n        >>> pd.Series(rng).dt.ceil(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 13:00:00\n        dtype: datetime64[ns]\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        if is_datetime64tz_dtype(self):\n            # operate on naive timestamps, then convert back to aware\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            aware = result.tz_localize(\n                self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n            return aware\n\n        values = self.view(\"i8\")\n        result = round_nsint64(values, mode, freq)\n        result = self._maybe_mask_results(result, fill_value=NaT)\n        return self._simple_new(result, dtype=self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n    def _with_freq(self, freq):\n        Helper to set our freq in-place, returning self to allow method chaining.\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n        Returns\n        -------\n        self\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, DateOffset):\n            # Always valid.  In the TimedeltaArray case, we assume this\n            #  is a Tick offset.\n            pass\n        else:\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = frequencies.to_offset(self.inferred_freq)\n\n        self._freq = freq\n        return self\n\n\nclass DatetimeLikeArrayMixin(\n    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray\n):\n    \"\"\"\n    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray\n\n    Assumes that __new__/__init__ defines:\n        _data\n        _freq\n\n    and that the inheriting class has methods:\n        _generate_range\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n\n    @property\n    def _ndarray(self) -> np.ndarray:\n        # NB: A bunch of Interval tests fail if we use ._data\n        return self.asi8\n\n    def _from_backing_data(self: _T, arr: np.ndarray) -> _T:\n        # Note: we do not retain `freq`\n        return type(self)(arr, dtype=self.dtype)  # type: ignore\n\n    # ------------------------------------------------------------------\n\n    @property\n    def ndim(self) -> int:\n        return self._data.ndim\n    @property\n    def shape(self):\n        return self._data.shape\n    def reshape(self, *args, **kwargs):\n        # Note: we drop any freq\n        data = self._data.reshape(*args, **kwargs)\n        return type(self)(data, dtype=self.dtype)\n    def ravel(self, *args, **kwargs):\n        # Note: we drop any freq\n        data = self._data.ravel(*args, **kwargs)\n        return type(self)(data, dtype=self.dtype)\n    def _box_func(self):\n        \"\"\"\n        box function to get object from internal representation\n        \"\"\"\n    def _box_values(self, values):\n        apply box func to passed values\n        return lib.map_infer(values, self._box_func)\n    def __iter__(self):\n        return (self._box_func(v) for v in self.asi8)\n    @property\n    def asi8(self) -> np.ndarray:\n        \"\"\"\n        Integer representation of the values.\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        # do not cache or you'll create a memory leak\n        return self._data.view(\"i8\")\n    # ----------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_native_types(self, na_rep=\"NaT\", date_format=None):\n        Helper method for astype when converting to strings.\n        ndarray[str]\n        raise AbstractMethodError(self)\n\n    def _formatter(self, boxed=False):\n        # TODO: Remove Datetime & DatetimeTZ formatters.\n        return \"'{}'\".format\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n    @property\n    def nbytes(self):\n        return self._data.nbytes\n    def __array__(self, dtype=None) -> np.ndarray:\n        # used for Timedelta/DatetimeArray, overwritten by PeriodArray\n        if is_object_dtype(dtype):\n            return np.array(list(self), dtype=object)\n        return self._data\n    @property\n    def size(self) -> int:\n        \"\"\"The number of elements in this array.\"\"\"\n        return np.prod(self.shape)\n    def __len__(self) -> int:\n        return len(self._data)\n\n    def __getitem__(self, key):\n        \"\"\"\n        This getitem defers to the underlying array, which by-definition can\n        only handle list-likes, slices, and integer scalars\n        \"\"\"\n        if com.is_bool_indexer(key):\n            # first convert to boolean, because check_array_indexer doesn't\n            # allow object dtype\n            if is_object_dtype(key):\n                key = np.asarray(key, dtype=bool)\n\n            key = check_array_indexer(self, key)\n            key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n        elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):\n            # see https://github.com/pandas-dev/pandas/issues/31299, need to allow\n            # this for now (would otherwise raise in check_array_indexer)\n            pass\n            key = check_array_indexer(self, key)\n        freq = self._get_getitem_freq(key)\n        result = self._data[key]\n        if lib.is_scalar(result):\n            return self._box_func(result)\n        return self._simple_new(result, dtype=self.dtype, freq=freq)\n    def _get_getitem_freq(self, key):\n        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\n        is_period = is_period_dtype(self.dtype)\n        if is_period:\n            freq = self.freq\n        else:\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                # GH#21282 indexing with Ellipsis is similar to a full slice,\n                #  should preserve `freq` attribute\n                freq = self.freq\n        return freq\n\n    def __setitem__(\n        self,\n        key: Union[int, Sequence[int], Sequence[bool], slice],\n        value: Union[NaTType, Any, Sequence[Any]],\n    ) -> None:\n        # I'm fudging the types a bit here. \"Any\" above really depends\n        # on type(self). For PeriodArray, it's Period (or stuff coercible\n        # to a period in from_sequence). For DatetimeArray, it's Timestamp...\n        # I don't know if mypy can do that, possibly with Generics.\n        # https://mypy.readthedocs.io/en/latest/generics.html\n        if is_list_like(value):\n            is_slice = isinstance(key, slice)\n\n            if lib.is_scalar(key):\n                raise ValueError(\"setting an array element with a sequence.\")\n\n            if not is_slice:\n                key = cast(Sequence, key)\n                if len(key) != len(value) and not com.is_bool_indexer(key):\n                    msg = (\n                        f\"shape mismatch: value array of length '{len(key)}' \"\n                        \"does not match indexing result of length \"\n                        f\"'{len(value)}'.\"\n                    )\n                    raise ValueError(msg)\n                elif not len(key):\n                    return\n\n        value = self._validate_setitem_value(value)\n        key = check_array_indexer(self, key)\n        self._data[key] = value\n        self._maybe_clear_freq()\n\n    def _maybe_clear_freq(self):\n        # inplace operations like __setitem__ may invalidate the freq of\n        # DatetimeArray and TimedeltaArray\n        pass\n\n    def astype(self, dtype, copy=True):\n        # Some notes on cases we don't have to handle here in the base class:\n        #   1. PeriodArray.astype handles period -> period\n        #   2. DatetimeArray.astype handles conversion between tz.\n        #   3. DatetimeArray.astype handles datetime -> period\n        dtype = pandas_dtype(dtype)\n\n        if is_object_dtype(dtype):\n            return self._box_values(self.asi8.ravel()).reshape(self.shape)\n        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):\n            return self._format_native_types()\n        elif is_integer_dtype(dtype):\n            # we deliberately ignore int32 vs. int64 here.\n            # See https://github.com/pandas-dev/pandas/issues/24381 for more.\n            values = self.asi8\n\n            if is_unsigned_integer_dtype(dtype):\n                # Again, we ignore int32 vs. int64\n                values = values.view(\"uint64\")\n\n            if copy:\n                values = values.copy()\n            return values\n        elif (\n            is_datetime_or_timedelta_dtype(dtype)\n            and not is_dtype_equal(self.dtype, dtype)\n        ) or is_float_dtype(dtype):\n            # disallow conversion between datetime/timedelta,\n            # and conversions for any datetimelike to float\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg)\n        elif is_categorical_dtype(dtype):\n            arr_cls = dtype.construct_array_type()\n            return arr_cls(self, dtype=dtype)\n        else:\n            return np.asarray(self, dtype=dtype)\n\n    def view(self, dtype=None):\n        if dtype is None or dtype is self.dtype:\n            return type(self)(self._data, dtype=self.dtype)\n        return self._data.view(dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def unique(self):\n        result = unique1d(self.asi8)\n        return type(self)(result, dtype=self.dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat, axis: int = 0):\n\n        # do not pass tz to set because tzlocal cannot be hashed\n        dtypes = {str(x.dtype) for x in to_concat}\n        if len(dtypes) != 1:\n            raise ValueError(\"to_concat must have the same dtype (tz)\", dtypes)\n\n        obj = to_concat[0]\n        dtype = obj.dtype\n\n        i8values = [x.asi8 for x in to_concat]\n        values = np.concatenate(i8values, axis=axis)\n\n        new_freq = None\n        if is_period_dtype(dtype):\n            new_freq = obj.freq\n        elif axis == 0:\n            # GH 3232: If the concat result is evenly spaced, we can retain the\n            # original frequency\n            to_concat = [x for x in to_concat if len(x)]\n\n            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):\n                    new_freq = obj.freq\n\n        return cls._simple_new(values, dtype=dtype, freq=new_freq)\n\n    def copy(self):\n        values = self.asi8.copy()\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)\n\n    def _values_for_factorize(self):\n        return self.asi8, iNaT\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    def _values_for_argsort(self):\n        return self._data\n\n    @Appender(ExtensionArray.shift.__doc__)\n    def shift(self, periods=1, fill_value=None, axis=0):\n        if not self.size or periods == 0:\n            return self.copy()\n\n        fill_value = self._validate_shift_value(fill_value)\n        new_values = shift(self._data, periods, axis, fill_value)\n        return type(self)._simple_new(new_values, dtype=self.dtype)\n    # ------------------------------------------------------------------\n    # Validation Methods\n    # TODO: try to de-duplicate these, ensure identical behavior\n    def _validate_fill_value(self, fill_value):\n        \"\"\"\n        If a fill_value is passed to `take` convert it to an i8 representation,\n        raising ValueError if this is not possible.\n        fill_value : object\n        fill_value : np.int64\n\n        Raises\n        ------\n        ValueError\n        if is_valid_nat_for_dtype(fill_value, self.dtype):\n            fill_value = iNaT\n        elif isinstance(fill_value, self._recognized_scalars):\n            self._check_compatible_with(fill_value)\n            fill_value = self._scalar_type(fill_value)\n            fill_value = self._unbox_scalar(fill_value)\n        else:\n            raise ValueError(\n                f\"'fill_value' should be a {self._scalar_type}. \"\n                f\"Got '{str(fill_value)}'.\"\n            )\n        return fill_value\n\n    def _validate_shift_value(self, fill_value):\n        # TODO(2.0): once this deprecation is enforced, used _validate_fill_value\n        if is_valid_nat_for_dtype(fill_value, self.dtype):\n            fill_value = NaT\n        elif not isinstance(fill_value, self._recognized_scalars):\n            # only warn if we're not going to raise\n            if self._scalar_type is Period and lib.is_integer(fill_value):\n                # kludge for #31971 since Period(integer) tries to cast to str\n                new_fill = Period._from_ordinal(fill_value, freq=self.freq)\n            else:\n                new_fill = self._scalar_type(fill_value)\n\n            # stacklevel here is chosen to be correct when called from\n            #  DataFrame.shift or Series.shift\n            warnings.warn(\n                f\"Passing {type(fill_value)} to shift is deprecated and \"\n                \"will raise in a future version, pass \"\n                f\"{self._scalar_type.__name__} instead.\",\n                FutureWarning,\n                stacklevel=10,\n            )\n            fill_value = new_fill\n\n        fill_value = self._unbox_scalar(fill_value)\n        return fill_value\n    def _validate_searchsorted_value(self, value):\n        if isinstance(value, str):\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar\"\n                ) from err\n\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            value = NaT\n\n        elif isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n\n        elif is_list_like(value) and not isinstance(value, type(self)):\n            value = array(value)\n\n            if not type(self)._is_recognized_dtype(value):\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar, \"\n                    f\"not {type(value).__name__}\"\n                )\n\n        if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):\n            raise TypeError(f\"Unexpected type for 'value': {type(value)}\")\n\n        if isinstance(value, type(self)):\n            self._check_compatible_with(value)\n            value = value.asi8\n        else:\n            value = self._unbox_scalar(value)\n\n        return value\n\n    def _validate_setitem_value(self, value):\n        if lib.is_scalar(value) and not isna(value):\n            value = com.maybe_box_datetimelike(value)\n\n        if is_list_like(value):\n            value = type(self)._from_sequence(value, dtype=self.dtype)\n            self._check_compatible_with(value, setitem=True)\n            value = value.asi8\n        elif isinstance(value, self._scalar_type):\n            self._check_compatible_with(value, setitem=True)\n            value = self._unbox_scalar(value)\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            value = iNaT\n        else:\n            msg = (\n                f\"'value' should be a '{self._scalar_type.__name__}', 'NaT', \"\n                f\"or array of those. Got '{type(value).__name__}' instead.\"\n            )\n            raise TypeError(msg)\n\n        return value\n\n    def _validate_insert_value(self, value):\n        if isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n        elif lib.is_scalar(value) and isna(value):\n            raise TypeError(\n                f\"cannot insert {type(self).__name__} with incompatible label\"\n            )\n\n        return value\n    def _validate_where_value(self, other):\n        if is_valid_nat_for_dtype(other, self.dtype):\n            other = NaT\n        elif isinstance(other, self._recognized_scalars):\n            other = self._scalar_type(other)\n            self._check_compatible_with(other, setitem=True)\n        elif not is_list_like(other):\n            raise TypeError(f\"Where requires matching dtype, not {type(other)}\")\n        else:\n            # Do type inference if necessary up front\n            # e.g. we passed PeriodIndex.values and got an ndarray of Periods\n            other = array(other)\n            other = extract_array(other, extract_numpy=True)\n\n            if is_categorical_dtype(other.dtype):\n                # e.g. we have a Categorical holding self.dtype\n                if is_dtype_equal(other.categories.dtype, self.dtype):\n                    other = other._internal_get_values()\n\n            if not type(self)._is_recognized_dtype(other.dtype):\n                raise TypeError(f\"Where requires matching dtype, not {other.dtype}\")\n            self._check_compatible_with(other, setitem=True)\n\n        if lib.is_scalar(other):\n            other = self._unbox_scalar(other)\n        else:\n            other = other.view(\"i8\")\n        return other\n    # ------------------------------------------------------------------\n    # Additional array methods\n    #  These are not part of the EA API, but we implement them because\n    #  pandas assumes they're there.\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        Find indices where elements should be inserted to maintain order.\n\n        Find the indices into a sorted array `self` such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort `self` into ascending\n            order. They are typically the result of ``np.argsort``.\n        indices : array of ints\n            Array of insertion points with the same shape as `value`.\n        value = self._validate_searchsorted_value(value)\n        # TODO: Use datetime64 semantics for sorting, xref GH#29844\n        return self.asi8.searchsorted(value, side=side, sorter=sorter)\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an array.\n        See Also\n        --------\n        numpy.ndarray.repeat\n        nv.validate_repeat(args, kwargs)\n        values = self._data.repeat(repeats)\n        return type(self)(values.view(\"i8\"), dtype=self.dtype)\n    def value_counts(self, dropna=False):\n        \"\"\"\n        Return a Series containing counts of unique values.\n        dropna : bool, default True\n            Don't include counts of NaT values.\n        Returns\n        -------\n        Series\n        \"\"\"\n        from pandas import Series, Index\n        if dropna:\n            values = self[~self.isna()]._data\n        else:\n            values = self._data\n\n        cls = type(self)\n        result = value_counts(values, sort=False, dropna=dropna)\n        index = Index(\n            cls(result.index.view(\"i8\"), dtype=self.dtype), name=result.index.name\n        )\n        return Series(result._values, index=index, name=result.name)\n    def map(self, mapper):\n        # TODO(GH-23179): Add ExtensionArray.map\n        # Need to figure out if we want ExtensionArray.map first.\n        # If so, then we can refactor IndexOpsMixin._map_values to\n        # a standalone function and call from here..\n        # Else, just rewrite _map_infer_values to do the right thing.\n        from pandas import Index\n        return Index(self).map(mapper).array\n    # ------------------------------------------------------------------\n    # Null Handling\n    def isna(self):\n        return self._isnan\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _isnan(self):\n        \"\"\"\n        return if each value is nan\n        \"\"\"\n        return self.asi8 == iNaT\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _hasnans(self):\n        \"\"\"\n        return if I have any nans; enables various perf speedups\n        \"\"\"\n        return bool(self._isnan.any())\n    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):\n        \"\"\"\n        Parameters\n        ----------\n        result : a ndarray\n        fill_value : object, default iNaT\n        convert : str, dtype or None\n        Returns\n        -------\n        result : ndarray with values replace by the fill_value\n        mask the result if needed, convert to the provided dtype if its not\n        None\n        This is an internal routine.\n        \"\"\"\n        if self._hasnans:\n            if convert:\n                result = result.astype(convert)\n            if fill_value is None:\n                fill_value = np.nan\n            result[self._isnan] = fill_value\n        return result\n    def fillna(self, value=None, method=None, limit=None):\n        # TODO(GH-20300): remove this\n        # Just overriding to ensure that we avoid an astype(object).\n        # Either 20300 or a `_values_for_fillna` would avoid this duplication.\n        if isinstance(value, ABCSeries):\n            value = value.array\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\" expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                if method == \"pad\":\n                    func = missing.pad_1d\n                else:\n                    func = missing.backfill_1d\n\n                values = self._data\n                if not is_period_dtype(self):\n                    # For PeriodArray self._data is i8, which gets copied\n                    #  by `func`.  Otherwise we need to make a copy manually\n                    # to avoid modifying `self` in-place.\n                    values = values.copy()\n\n                new_values = func(values, limit=limit, mask=mask)\n                if is_datetime64tz_dtype(self):\n                    # we need to pass int64 values to the constructor to avoid\n                    #  re-localizing incorrectly\n                    new_values = new_values.view(\"i8\")\n                new_values = type(self)(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n        else:\n            new_values = self.copy()\n        return new_values\n    # ------------------------------------------------------------------\n    # Frequency Properties/Methods\n        Return the frequency object if it is set, otherwise None.\n        return self._freq\n\n    @freq.setter\n    def freq(self, value):\n        if value is not None:\n            value = frequencies.to_offset(value)\n            self._validate_frequency(self, value)\n\n        self._freq = value\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def inferred_freq(self):\n        \"\"\"\n        Tryies to return a string representing a frequency guess,\n        generated by infer_freq.  Returns None if it can't autodetect the\n        frequency.\n        \"\"\"\n        if self.ndim != 1:\n            return None\n        try:\n            return frequencies.infer_freq(self)\n        except ValueError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution(self):\n        return frequencies.Resolution.get_reso_from_freq(self.freqstr)\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def resolution(self):\n        \"\"\"\n        Returns day, hour, minute, second, millisecond or microsecond\n        \"\"\"\n        return frequencies.Resolution.get_str(self._resolution)\n\n    @classmethod\n    def _validate_frequency(cls, index, freq, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        if is_period_dtype(cls):\n            # Frequency validation is not meaningful for Period Array/Index\n            return None\n\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(\n                start=index[0], end=None, periods=len(index), freq=freq, **kwargs\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as e:\n            if \"non-fixed\" in str(e):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise e\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            ) from e\n\n    # monotonicity/uniqueness properties are called via frequencies.infer_freq,\n    #  see GH#23789\n\n    @property\n    def _is_monotonic_increasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[0]\n\n    @property\n    def _is_monotonic_decreasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[1]\n\n    @property\n    def _is_unique(self):\n        return len(unique1d(self.asi8)) == len(self)\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n    _create_comparison_method = classmethod(_datetimelike_array_cmp)\n\n    # pow is invalid for all three subclasses; TimedeltaArray will override\n    #  the multiplication and division ops\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n\n    def _add_datetimelike_scalar(self, other):\n        # Overridden by TimedeltaArray\n        raise TypeError(f\"cannot add {type(self).__name__} and {type(other).__name__}\")\n\n    _add_datetime_arraylike = _add_datetimelike_scalar\n\n    def _sub_datetimelike_scalar(self, other):\n        # Overridden by DatetimeArray\n        assert other is not NaT\n        raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n    _sub_datetime_arraylike = _sub_datetimelike_scalar\n\n    def _sub_period(self, other):\n        # Overridden by PeriodArray\n        raise TypeError(f\"cannot subtract Period from a {type(self).__name__}\")\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Add a delta of a timedeltalike\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\n            new_values = np.empty(self.shape, dtype=\"i8\")\n            new_values[:] = iNaT\n            return type(self)(new_values, dtype=self.dtype)\n\n        inc = delta_to_nanoseconds(other)\n        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(\n            \"i8\"\n        )\n        new_values = self._maybe_mask_results(new_values)\n\n        new_freq = None\n        if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):\n            # adding a scalar preserves freq\n            new_freq = self.freq\n\n        return type(self)(new_values, dtype=self.dtype, freq=new_freq)\n    def _add_timedelta_arraylike(self, other):\n        \"\"\"\n        Add a delta of a TimedeltaIndex\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # overridden by PeriodArray\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n        if isinstance(other, np.ndarray):\n            # ndarray[timedelta64]; wrap in TimedeltaIndex for op\n            from pandas.core.arrays import TimedeltaArray\n            other = TimedeltaArray._from_sequence(other)\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        new_values = checked_add_with_arr(\n            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = iNaT\n        return type(self)(new_values, dtype=self.dtype)\n    def _add_nat(self):\n        Add pd.NaT to self\n        \"\"\"\n        if is_period_dtype(self):\n            raise TypeError(\n                f\"Cannot add {type(self).__name__} and {type(NaT).__name__}\"\n            )\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return type(self)(result, dtype=self.dtype, freq=None)\n\n    def _sub_nat(self):\n        \"\"\"\n        Subtract pd.NaT from self\n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return result.view(\"timedelta64[ns]\")\n\n    def _sub_period_array(self, other):\n        \"\"\"\n        Subtract a Period Array/Index from self.  This is only valid if self\n        is itself a Period Array/Index, raises otherwise.  Both objects must\n        have the same frequency.\n        other : PeriodIndex or PeriodArray\n\n        Returns\n        -------\n        result : np.ndarray[object]\n            Array of DateOffset objects; nulls represented by NaT.\n        \"\"\"\n        if not is_period_dtype(self):\n            raise TypeError(\n                f\"cannot subtract {other.dtype}-dtype from {type(self).__name__}\"\n            )\n        if self.freq != other.freq:\n            msg = DIFFERENT_FREQ.format(\n                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr\n            )\n            raise IncompatibleFrequency(msg)\n        new_values = checked_add_with_arr(\n            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        new_values = np.array([self.freq.base * x for x in new_values])\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = NaT\n        return new_values\n    def _addsub_object_array(self, other: np.ndarray, op):\n        \"\"\"\n        Add or subtract array-like of DateOffset objects\n        Parameters\n        ----------\n        other : np.ndarray[object]\n        op : {operator.add, operator.sub}\n        result : same class as self\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1:\n            return op(self, other[0])\n\n        warnings.warn(\n            \"Adding/subtracting array of DateOffsets to \"\n            f\"{type(self).__name__} not vectorized\",\n            PerformanceWarning,\n        )\n        # Caller is responsible for broadcasting if necessary\n        assert self.shape == other.shape, (self.shape, other.shape)\n        res_values = op(self.astype(\"O\"), np.array(other))\n        result = array(res_values.ravel())\n        result = extract_array(result, extract_numpy=True).reshape(self.shape)\n        return result\n\n    def _time_shift(self, periods, freq=None):\n        \"\"\"\n        Shift each value by `periods`.\n\n        Note this is different from ExtensionArray.shift, which\n        shifts the *position* of each element, padding the end with\n        missing values.\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift by.\n        freq : pandas.DateOffset, pandas.Timedelta, or str\n            Frequency increment to shift by.\n        \"\"\"\n        if freq is not None and freq != self.freq:\n            if isinstance(freq, str):\n                freq = frequencies.to_offset(freq)\n            offset = periods * freq\n            result = self + offset\n        if periods == 0:\n            # immutable so OK\n            return self.copy()\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n        start = self[0] + periods * self.freq\n        end = self[-1] + periods * self.freq\n        # Note: in the DatetimeTZ case, _generate_range will infer the\n        #  appropriate timezone from `start` and `end`, so tz does not need\n        #  to be passed explicitly.\n        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n        # scalar others\n        if other is NaT:\n            result = self._add_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._add_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.add)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            return self._add_datetime_arraylike(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.add)\n        else:\n            # Includes Categorical, other ExtensionArrays\n            # For PeriodDtype, if self is a TimedeltaArray and other is a\n            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this\n            #  operation is valid.  Defer to the PeriodArray implementation.\n            #  In remaining cases, this will end up raising TypeError.\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray(result)\n        return result\n    def __radd__(self, other):\n        # alias for __add__\n        return self.__add__(other)\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n        # scalar others\n        if other is NaT:\n            result = self._sub_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(-other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(-other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._sub_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(-other)\n\n        elif isinstance(other, Period):\n            result = self._sub_period(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(-other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.sub)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            result = self._sub_datetime_arraylike(other)\n        elif is_period_dtype(other):\n            # PeriodIndex\n            result = self._sub_period_array(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.sub)\n            # Includes ExtensionArrays, float_dtype\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray(result)\n        return result\n    def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):\n            # ndarray[datetime64] cannot be subtracted from self, so\n            # we need to wrap in DatetimeArray/Index and flip the operation\n            if lib.is_scalar(other):\n                # i.e. np.datetime64 object\n                return Timestamp(other) - self\n            if not isinstance(other, DatetimeLikeArrayMixin):\n                # Avoid down-casting DatetimeIndex\n                from pandas.core.arrays import DatetimeArray\n\n                other = DatetimeArray(other)\n            return other - self\n        elif (\n            is_datetime64_any_dtype(self.dtype)\n            and hasattr(other, \"dtype\")\n            and not is_datetime64_any_dtype(other.dtype)\n        ):\n            # GH#19959 datetime - datetime is well-defined as timedelta,\n            # but any other type - datetime is not well-defined.\n            raise TypeError(\n                f\"cannot subtract {type(self).__name__} from {type(other).__name__}\"\n            )\n        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):\n            # TODO: Can we simplify/generalize these cases at all?\n            raise TypeError(f\"cannot subtract {type(self).__name__} from {other.dtype}\")\n        elif is_timedelta64_dtype(self.dtype):\n            if lib.is_integer(other) or is_integer_dtype(other):\n                # need to subtract before negating, since that flips freq\n                # -self flips self.freq, messing up results\n                return -(self - other)\n\n            return (-self) + other\n\n        return -(self - other)\n\n    def __iadd__(self, other):\n        result = self + other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    def __isub__(self, other):\n        result = self - other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def _reduce(self, name, axis=0, skipna=True, **kwargs):\n        op = getattr(self, name, None)\n        if op:\n            return op(skipna=skipna, **kwargs)\n            return super()._reduce(name, skipna, **kwargs)\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Array or minimum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\n        if isna(result):\n            # Period._from_ordinal does not handle np.nan gracefully\n            return NaT\n        return self._box_func(result)\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Array or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        # TODO: skipna is broken with max.\n        # See https://github.com/pandas-dev/pandas/issues/24265\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        mask = self.isna()\n        if skipna:\n            values = self[~mask].asi8\n        elif mask.any():\n            return NaT\n        else:\n            values = self.asi8\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n        result = nanops.nanmax(values, skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n    def mean(self, skipna=True):\n        Return the mean value of the Array.\n\n        .. versionadded:: 0.25.0\n        skipna : bool, default True\n            Whether to ignore any NaT elements.\n        scalar\n            Timestamp or Timedelta.\n\n        See Also\n        --------\n        numpy.ndarray.mean : Returns the average of array elements along a given axis.\n        Series.mean : Return the mean value in a Series.\n\n        Notes\n        -----\n        mean is only defined for Datetime and Timedelta dtypes, not for Period.\n        \"\"\"\n        if is_period_dtype(self):\n            # See discussion in GH#24757\n            raise TypeError(\n                f\"mean is not implemented for {type(self).__name__} since the \"\n                \"meaning is ambiguous.  An alternative is \"\n                \"obj.to_timestamp(how='start').mean()\"\n            )\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask]\n        elif mask.any():\n            return NaT\n        else:\n            values = self\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmean(values.view(\"i8\"), skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n\nDatetimeLikeArrayMixin._add_comparison_ops()\n\n# -------------------------------------------------------------------\n# Shared Constructor Helpers\n\n\ndef validate_periods(periods):\n    \"\"\"\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\n    constructor, cast it to an integer.\n\n    Parameters\n    ----------\n    periods : None, float, int\n\n    Returns\n    -------\n    periods : None or int\n\n    Raises\n    ------\n    TypeError\n        if periods is None, float, or int\n    \"\"\"\n    if periods is not None:\n        if lib.is_float(periods):\n            periods = int(periods)\n        elif not lib.is_integer(periods):\n            raise TypeError(f\"periods must be a number, got {periods}\")\n    return periods\n\n\ndef validate_endpoints(closed):\n    \"\"\"\n    Check that the `closed` argument is among [None, \"left\", \"right\"]\n\n    Parameters\n    ----------\n    closed : {None, \"left\", \"right\"}\n\n    Returns\n    -------\n    left_closed : bool\n    right_closed : bool\n\n    Raises\n    ------\n    ValueError : if argument is not among valid values\n    \"\"\"\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == \"left\":\n        left_closed = True\n    elif closed == \"right\":\n        right_closed = True\n    else:\n        raise ValueError(\"Closed has to be either 'left', 'right' or None\")\n\n    return left_closed, right_closed\n\n\ndef validate_inferred_freq(freq, inferred_freq, freq_infer):\n    \"\"\"\n    If the user passes a freq and another freq is inferred from passed data,\n    require that they match.\n\n    Parameters\n    ----------\n    freq : DateOffset or None\n    inferred_freq : DateOffset or None\n    freq_infer : bool\n\n    Returns\n    -------\n    freq : DateOffset or None\n    freq_infer : bool\n\n    Notes\n    -----\n    We assume at this point that `maybe_infer_freq` has been called, so\n    `freq` is either a DateOffset object or None.\n    \"\"\"\n    if inferred_freq is not None:\n        if freq is not None and freq != inferred_freq:\n            raise ValueError(\n                f\"Inferred frequency {inferred_freq} from passed \"\n                \"values does not conform to passed frequency \"\n                f\"{freq.freqstr}\"\n            )\n        elif freq is None:\n            freq = inferred_freq\n        freq_infer = False\n\n    return freq, freq_infer\ndef maybe_infer_freq(freq):\n    \"\"\"\n    Comparing a DateOffset to the string \"infer\" raises, so we need to\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\n    signify the case where the given freq is \"infer\" and set freq to None\n    to avoid comparison trouble later on.\n\n    Parameters\n    ----------\n    freq : {DateOffset, None, str}\n\n    Returns\n    -------\n    freq : {DateOffset, None}\n    freq_infer : bool\n        Whether we should inherit the freq of passed data.\n    \"\"\"\n    freq_infer = False\n    if not isinstance(freq, DateOffset):\n        # if a passed freq is None, don't infer automatically\n        if freq != \"infer\":\n            freq = frequencies.to_offset(freq)\n        else:\n            freq_infer = True\n            freq = None\n    return freq, freq_infer\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n        msg = \"cannot insert DatetimeIndex with incompatible label\"", "fixed_code": "\"\"\"\nBase and utility classes for tseries type pandas objects.\n\"\"\"\nfrom typing import Any, List, Optional, Union, cast\nfrom pandas._libs import NaT, iNaT, join as libjoin, lib\nfrom pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n    ensure_int64,\n    is_bool_dtype,\n    is_integer,\n    is_scalar,\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import algorithms\nfrom pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\nfrom pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\nfrom pandas.core.base import IndexOpsMixin\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas.core.indexes.extension import (\n    ExtensionIndex,\n    inherit_names,\n    make_wrapped_arith_op,\n)\nfrom pandas.core.indexes.numeric import Int64Index\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.tools.timedeltas import to_timedelta\nfrom pandas.tseries.frequencies import DateOffset, to_offset\nfrom pandas.tseries.offsets import Tick\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\ndef _join_i8_wrapper(joinf, with_indexers: bool = True):\n    \"\"\"\n    Create the join wrapper methods.\n    \"\"\"\n    @staticmethod  # type: ignore\n    def wrapper(left, right):\n        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            left = left.view(\"i8\")\n        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            right = right.view(\"i8\")\n        results = joinf(left, right)\n        if with_indexers:\n            # dtype should be timedelta64[ns] for TimedeltaIndex\n            #  and datetime64[ns] for DatetimeIndex\n            dtype = left.dtype.base\n            join_index, left_indexer, right_indexer = results\n            join_index = join_index.view(dtype)\n            return join_index, left_indexer, right_indexer\n        return results\n    return wrapper\ndef _make_wrapped_arith_op_with_freq(opname: str):\n    \"\"\"\n    Dispatch the operation to the underlying ExtensionArray, and infer\n    the appropriate frequency for the result.\n    \"\"\"\n    meth = make_wrapped_arith_op(opname)\n    def wrapped(self, other):\n        result = meth(self, other)\n        if result is NotImplemented:\n            return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n        result._freq = new_freq\n    wrapped.__name__ = opname\n    return wrapped\n@inherit_names(\n    [\"inferred_freq\", \"_isnan\", \"_resolution\", \"resolution\"],\n    DatetimeLikeArrayMixin,\n    cache=True,\n)\n@inherit_names(\n    [\"mean\", \"asi8\", \"_box_func\"], DatetimeLikeArrayMixin,\n)\nclass DatetimeIndexOpsMixin(ExtensionIndex):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]\n    freq: Optional[DateOffset]\n    freqstr: Optional[str]\n    _resolution: int\n    _bool_ops: List[str] = []\n    _field_ops: List[str] = []\n    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore\n    _hasnans = hasnans  # for index / array -agnostic code\n    @property\n    def is_all_dates(self) -> bool:\n        return True\n    # ------------------------------------------------------------------------\n    # Abstract data attributes\n    @property\n    def values(self):\n        # Note: PeriodArray overrides this to return an ndarray of objects.\n        return self._data._data\n    def __array_wrap__(self, result, context=None):\n        Gets called after a ufunc.\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n        attrs = self._get_attributes_dict()\n        if not is_period_dtype(self) and attrs[\"freq\"]:\n            # no need to infer if freq is None\n            attrs[\"freq\"] = \"infer\"\n        return Index(result, **attrs)\n    # ------------------------------------------------------------------------\n    def equals(self, other) -> bool:\n        Determines if two Index objects contain the same elements.\n        if self.is_(other):\n            return True\n        if not isinstance(other, ABCIndexClass):\n            return False\n        elif not isinstance(other, type(self)):\n            try:\n                other = type(self)(other)\n            except (ValueError, TypeError, OverflowError):\n                # e.g.\n                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime\n                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex\n                #  OverflowError -> Index([very_large_timedeltas])\n                return False\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            # have different timezone\n            return False\n\n        return np.array_equal(self.asi8, other.asi8)\n\n    @Appender(Index.__contains__.__doc__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            res = self.get_loc(key)\n        except (KeyError, TypeError, ValueError):\n            return False\n        return bool(\n            is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))\n        )\n    def sort_values(self, return_indexer=False, ascending=True):\n        Return sorted copy of Index.\n        if return_indexer:\n            _as = self.argsort()\n            if not ascending:\n                _as = _as[::-1]\n            sorted_index = self.take(_as)\n            return sorted_index, _as\n        else:\n            # NB: using asi8 instead of _data matters in numpy 1.18\n            #  because the treatment of NaT has been changed to put NaT last\n            #  instead of first.\n            sorted_values = np.sort(self.asi8)\n            freq = self.freq\n            if freq is not None and not is_period_dtype(self):\n                if freq.n > 0 and not ascending:\n                    freq = freq * -1\n                elif freq.n < 0 and ascending:\n                    freq = freq * -1\n            if not ascending:\n                sorted_values = sorted_values[::-1]\n            arr = type(self._data)._simple_new(\n                sorted_values, dtype=self.dtype, freq=freq\n            )\n            return type(self)._simple_new(arr, name=self.name)\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_int64(indices)\n        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))\n        if isinstance(maybe_slice, slice):\n            return self[maybe_slice]\n        return ExtensionIndex.take(\n            self, indices, axis, allow_fill, fill_value, **kwargs\n        )\n    @doc(IndexOpsMixin.searchsorted, klass=\"Datetime-like Index\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, str):\n            raise TypeError(\n                \"searchsorted requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n        if isinstance(value, Index):\n            value = value._data\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n    _can_hold_na = True\n    _na_value = NaT\n    \"\"\"The expected NA value to use with this index.\"\"\"\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n    def tolist(self) -> List:\n        \"\"\"\n        Return a list of the underlying data.\n        \"\"\"\n        return list(self.astype(object))\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Index or minimum along\n        an axis.\n        See Also\n        numpy.ndarray.min\n        Series.min : Return the minimum value in a Series.\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[0] != iNaT:\n                    return self._box_func(i8[0])\n\n            if self.hasnans:\n                if skipna:\n                    min_stamp = self[~self._isnan].asi8.min()\n                else:\n                    return self._na_value\n            else:\n                min_stamp = i8.min()\n            return self._box_func(min_stamp)\n        except ValueError:\n            return self._na_value\n    def argmin(self, axis=None, skipna=True, *args, **kwargs):\n        Returns the indices of the minimum values along an axis.\n        See `numpy.ndarray.argmin` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmin\n        nv.validate_argmin(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = np.iinfo(\"int64\").max\n        return i8.argmin()\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        Return the maximum value of the Index or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[-1] != iNaT:\n                    return self._box_func(i8[-1])\n\n            if self.hasnans:\n                if skipna:\n                    max_stamp = self[~self._isnan].asi8.max()\n                else:\n                    return self._na_value\n            else:\n                max_stamp = i8.max()\n            return self._box_func(max_stamp)\n        except ValueError:\n            return self._na_value\n    def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        Returns the indices of the maximum values along an axis.\n        See `numpy.ndarray.argmax` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmax\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = 0\n        return i8.argmax()\n    # --------------------------------------------------------------------\n    # Rendering Methods\n    def _format_with_header(self, header, na_rep=\"NaT\", **kwargs):\n        return header + list(self._format_native_types(na_rep, **kwargs))\n    def _formatter_func(self):\n    def _format_attrs(self):\n        Return a list of tuples of the (attr,formatted_value).\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            if attrib == \"freq\":\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)\n                attrs.append((\"freq\", freq))\n        return attrs\n    # --------------------------------------------------------------------\n    # Indexing Methods\n    def _validate_partial_date_slice(self, reso: str):\n        raise NotImplementedError\n    def _parsed_string_to_bounds(self, reso: str, parsed: datetime):\n        raise NotImplementedError\n    def _partial_date_slice(\n        self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True\n    ):\n        Parameters\n        ----------\n        reso : str\n        parsed : datetime\n        use_lhs : bool, default True\n        use_rhs : bool, default True\n        slice or ndarray[intp]\n        self._validate_partial_date_slice(reso)\n        t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n        i8vals = self.asi8\n        unbox = self._data._unbox_scalar\n        if self.is_monotonic:\n            if len(self) and (\n                (use_lhs and t1 < self[0] and t2 < self[0])\n                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))\n            ):\n                # we are out of range\n                raise KeyError\n            # TODO: does this depend on being monotonic _increasing_?\n            # a monotonic (sorted) series can be sliced\n            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps\n            left = i8vals.searchsorted(unbox(t1), side=\"left\") if use_lhs else None\n            right = i8vals.searchsorted(unbox(t2), side=\"right\") if use_rhs else None\n            return slice(left, right)\n            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True\n            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True\n            # try to find the dates\n            return (lhs_mask & rhs_mask).nonzero()[0]\n    # --------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n        Find the freq we expect the result of an addition/subtraction operation\n        to have.\n        if is_period_dtype(self.dtype):\n            # Only used for ops that stay PeriodDtype\n            return self.freq\n        elif self.freq is None:\n            return None\n        elif lib.is_scalar(other) and isna(other):\n            return None\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            new_freq = None\n            if isinstance(self.freq, Tick):\n                new_freq = self.freq\n            return new_freq\n        elif isinstance(other, DateOffset):\n            # otherwise just DatetimeArray\n            return None  # TODO: Should we infer if it matches self.freq * n?\n        elif isinstance(other, (datetime, np.datetime64)):\n            return self.freq\n        elif is_timedelta64_dtype(other):\n            return None  # TODO: shouldnt we be able to do self.freq + other.freq?\n        elif is_object_dtype(other):\n            return None  # TODO: is this quite right?  sometimes we unpack singletons\n        elif is_datetime64_any_dtype(other):\n            return None  # TODO: shouldnt we be able to do self.freq + other.freq?\n        else:\n            raise NotImplementedError\n\n    __add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\n    __sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Compute boolean array of whether each index value is found in the\n        passed set of values.\n        values : set or sequence of values\n        is_contained : ndarray (boolean dtype)\n        if level is not None:\n            self._validate_index_level(level)\n        if not isinstance(values, type(self)):\n                values = type(self)(values)\n            except ValueError:\n                return self.astype(object).isin(values)\n        return algorithms.isin(self.asi8, values.asi8)\n    @Appender(Index.where.__doc__)\n    def where(self, cond, other=None):\n        values = self.view(\"i8\")\n        try:\n            other = self._data._validate_where_value(other)\n        except (TypeError, ValueError) as err:\n            # Includes tzawareness mismatch and IncompatibleFrequencyError\n            oth = getattr(other, \"dtype\", other)\n            raise TypeError(f\"Where requires matching dtype, not {oth}\") from err\n        result = np.where(cond, values, other).astype(\"i8\")\n        arr = type(self._data)._simple_new(result, dtype=self.dtype)\n        return type(self)._simple_new(arr, name=self.name)\n    def _summary(self, name=None) -> str:\n        Return a summarized representation.\n        name : str\n            Name to use in the summary representation.\n        str\n            Summarized representation of the index.\n        formatter = self._formatter_func\n        if len(self) > 0:\n            index_summary = f\", {formatter(self[0])} to {formatter(self[-1])}\"\n        else:\n            index_summary = \"\"\n        if name is None:\n            name = type(self).__name__\n        result = f\"{name}: {len(self)} entries{index_summary}\"\n        if self.freq:\n            result += f\"\\nFreq: {self.freqstr}\"\n        # display as values, not quoted\n        result = result.replace(\"'\", \"\")\n        return result\n    def shift(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n            .. versionchanged:: 0.24.0\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        \"\"\"\n        arr = self._data.view()\n        arr._freq = self.freq\n        result = arr._time_shift(periods, freq=freq)\n        return type(self)(result, name=self.name)\n    # --------------------------------------------------------------------\n    # List-like Methods\n    def delete(self, loc):\n        new_i8s = np.delete(self.asi8, loc)\n        freq = None\n        if is_period_dtype(self):\n            freq = self.freq\n        elif is_integer(loc):\n            if loc in (0, -len(self), -1, len(self) - 1):\n                freq = self.freq\n        else:\n            if is_list_like(loc):\n                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))\n            if isinstance(loc, slice) and loc.step in (1, None):\n                if loc.start in (0, None) or loc.stop in (len(self), None):\n                    freq = self.freq\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n    # --------------------------------------------------------------------\n    # Join/Set Methods\n    def _wrap_joined_index(self, joined: np.ndarray, other):\n        assert other.dtype == self.dtype, (other.dtype, self.dtype)\n        name = get_op_result_name(self, other)\n        if is_period_dtype(self.dtype):\n            freq = self.freq\n        else:\n            self = cast(DatetimeTimedeltaMixin, self)\n            freq = self.freq if self._can_fast_union(other) else None\n        new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(new_data, name=name)\nclass DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n    \"\"\"\n    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,\n    but not PeriodIndex\n    \"\"\"\n    # Compat for frequency inference, see GH#23789\n    _is_monotonic_increasing = Index.is_monotonic_increasing\n    _is_monotonic_decreasing = Index.is_monotonic_decreasing\n    _is_unique = Index.is_unique\n    _freq = lib.no_default\n        In limited circumstances, our freq may differ from that of our _data.\n        if self._freq is not lib.no_default:\n            return self._freq\n        return self._data.freq\n    def _with_freq(self, freq):\n        index = self.copy(deep=False)\n        if freq is None:\n            # Even if we _can_ have a freq, we might want to set it to None\n            index._freq = None\n        elif len(self) == 0 and isinstance(freq, DateOffset):\n            # Always valid.  In the TimedeltaArray case, we assume this\n            #  is a Tick offset.\n            index._freq = freq\n        else:\n            assert freq == \"infer\", freq\n            freq = to_offset(self.inferred_freq)\n            index._freq = freq\n        return index\n    def _shallow_copy(self, values=None, name: Label = lib.no_default):\n        name = self.name if name is lib.no_default else name\n        cache = self._cache.copy() if values is None else {}\n        if values is None:\n            values = self._data\n        if isinstance(values, np.ndarray):\n            # TODO: We would rather not get here\n            values = type(self._data)(values, dtype=self.dtype)\n        result = type(self)._simple_new(values, name=name)\n        result._cache = cache\n        return result\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n    @Appender(Index.difference.__doc__)\n    def difference(self, other, sort=None):\n        new_idx = super().difference(other, sort=sort)._with_freq(None)\n        return new_idx\n    def intersection(self, other, sort=False):\n        Specialized intersection for DatetimeIndex/TimedeltaIndex.\n        May be much faster than Index.intersection\n        other : Same type as self or array-like\n        sort : False or None, default False\n            Sort the resulting index if possible.\n            .. versionadded:: 0.24.0\n            .. versionchanged:: 0.24.1\n               Changed the default to ``False`` to match the behaviour\n               from before 0.24.0.\n            .. versionchanged:: 0.25.0\n               The `sort` keyword is added\n        y : Index or same type as self\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n        if len(self) == 0:\n            return self.copy()\n        if len(other) == 0:\n            return other.copy()\n\n        if not isinstance(other, type(self)):\n            result = Index.intersection(self, other, sort=sort)\n            if isinstance(result, type(self)):\n                if result.freq is None:\n                    result = result._with_freq(\"infer\")\n            return result\n        elif (\n            other.freq is None\n            or self.freq is None\n            or other.freq != self.freq\n            or not other.freq.is_anchored()\n            or (not self.is_monotonic or not other.is_monotonic)\n        ):\n            result = Index.intersection(self, other, sort=sort)\n            result = result._with_freq(\"infer\")\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        # after sorting, the intersection always starts with the right index\n        # and ends with the index of which the last elements is smallest\n        end = min(left[-1], right[-1])\n        start = right[0]\n        if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n        else:\n            lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n            return self._shallow_copy(left_chunk)\n    def _can_fast_union(self, other) -> bool:\n        if not isinstance(other, type(self)):\n            return False\n        freq = self.freq\n        if freq is None or freq != other.freq:\n            return False\n        if not self.is_monotonic or not other.is_monotonic:\n            return False\n        if len(self) == 0 or len(other) == 0:\n            return True\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        right_start = right[0]\n        left_end = left[-1]\n        # Only need to \"adjoin\", not overlap\n        try:\n            return (right_start == left_end + freq) or right_start in left\n        except ValueError:\n            # if we are comparing a freq that does not propagate timezones\n            # this will raise\n            return False\n\n    def _fast_union(self, other, sort=None):\n        if len(other) == 0:\n            return self.view(type(self))\n\n        if len(self) == 0:\n            return other.view(type(self))\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        elif sort is False:\n            # TDIs are not in the \"correct\" order and we don't want\n            #  to sort but want to remove overlaps\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right._values[:loc]\n            dates = concat_compat((left._values, right_chunk))\n            # TODO: can we infer that it has self.freq?\n            result = self._shallow_copy(dates)._with_freq(\"infer\")\n            return result\n            left, right = other, self\n\n        left_end = left[-1]\n        right_end = right[-1]\n\n        # concatenate\n        if left_end < right_end:\n            loc = right.searchsorted(left_end, side=\"right\")\n            right_chunk = right._values[loc:]\n            dates = concat_compat([left._values, right_chunk])\n            # TODO: can we infer that it has self.freq?\n            result = self._shallow_copy(dates)._with_freq(\"infer\")\n            return result\n        else:\n            return left\n    def _union(self, other, sort):\n        if not len(other) or self.equals(other) or not len(self):\n            return super()._union(other, sort=sort)\n        # We are called by `union`, which is responsible for this validation\n        assert isinstance(other, type(self))\n        this, other = self._maybe_utc_convert(other)\n\n        if this._can_fast_union(other):\n            result = this._fast_union(other, sort=sort)\n            if result.freq is None:\n                result = result._with_freq(\"infer\")\n            return result\n            i8self = Int64Index._simple_new(self.asi8, name=self.name)\n            i8other = Int64Index._simple_new(other.asi8, name=other.name)\n            i8result = i8self._union(i8other, sort=sort)\n            result = type(self)(i8result, dtype=self.dtype, freq=\"infer\")\n            return result\n    # --------------------------------------------------------------------\n    # Join Methods\n    _join_precedence = 10\n    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)\n    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)\n    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)\n    _left_indexer_unique = _join_i8_wrapper(\n        libjoin.left_join_indexer_unique, with_indexers=False\n    )\n\n    def join(\n        self, other, how: str = \"left\", level=None, return_indexers=False, sort=False\n    ):\n        See Index.join\n        \"\"\"\n        if self._is_convertible_to_index_for_join(other):\n            try:\n                other = type(self)(other)\n            except (TypeError, ValueError):\n                pass\n        this, other = self._maybe_utc_convert(other)\n        return Index.join(\n            this,\n            other,\n            how=how,\n            level=level,\n            return_indexers=return_indexers,\n            sort=sort,\n        )\n    def _maybe_utc_convert(self, other):\n        this = self\n        if not hasattr(self, \"tz\"):\n            return this, other\n        if isinstance(other, type(self)):\n            if self.tz is not None:\n                if other.tz is None:\n                    raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            elif other.tz is not None:\n                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            if not timezones.tz_compare(self.tz, other.tz):\n                this = self.tz_convert(\"UTC\")\n                other = other.tz_convert(\"UTC\")\n        return this, other\n    @classmethod\n    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:\n        \"\"\"\n        return a boolean whether I can attempt conversion to a\n        DatetimeIndex/TimedeltaIndex\n        \"\"\"\n        if isinstance(other, cls):\n            return False\n        elif len(other) > 0 and other.inferred_type not in (\n            \"floating\",\n            \"mixed-integer\",\n            \"integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n            \"mixed\",\n        ):\n            return True\n        return False\n    # --------------------------------------------------------------------\n    # List-Like Methods\n    def insert(self, loc, item):\n        Make new Index inserting new item at location\n        loc : int\n        item : object\n            if not either a Python datetime or a numpy integer-like, returned\n            Index dtype will be object rather than datetime.\n        new_index : Index\n        \"\"\"\n        if isinstance(item, str):\n            # TODO: Why are strings special?\n            # TODO: Should we attempt _scalar_from_string?\n            return self.astype(object).insert(loc, item)\n\n        item = self._data._validate_insert_value(item)\n\n        freq = None\n        # check freq can be preserved on edge cases\n        if self.freq is not None:\n            if self.size:\n                if item is NaT:\n                    pass\n                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\n                    freq = self.freq\n                elif (loc == len(self)) and item - self.freq == self[-1]:\n                    freq = self.freq\n            else:\n                # Adding a single item to an empty index may preserve freq\n                if self.freq.is_on_offset(item):\n                    freq = self.freq\n        item = self._data._unbox_scalar(item)\n        new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n        msg = \"cannot insert DatetimeArray with incompatible label\"", "description": ""}
{"id": "pandas-144", "project": "pandas", "bug_id": "144", "buggy_code": "ax.set_xticks(self.tick_pos)\n        ax.set_xticklabels(ticklabels)", "fixed_code": "if self.xticks is not None:\n            ax.set_xticks(np.array(self.xticks))\n        else:\n            ax.set_xticks(self.tick_pos)\n            ax.set_xticklabels(ticklabels)", "description": ""}
{"id": "pandas-1", "project": "pandas", "bug_id": "1", "buggy_code": "is_excluded_checks = (is_period_dtype, is_interval_dtype)", "fixed_code": "is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)", "description": ""}
{"id": "pandas-10", "project": "pandas", "bug_id": "10", "buggy_code": "if isinstance(new, np.ndarray) and len(new) == len(mask):", "fixed_code": "if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):", "description": ""}
{"id": "pandas-26", "project": "pandas", "bug_id": "26", "buggy_code": "if skipna:\n            if skipna:", "fixed_code": "if skipna and good.any():\n            if skipna and good.any():", "description": ""}
{"id": "pandas-21", "project": "pandas", "bug_id": "21", "buggy_code": "if isinstance(key, list):\n            # handle the dup indexing case GH#4246\n            return self.loc[key]\n\n        return self.reindex(key)", "fixed_code": "# handle the dup indexing case GH#4246\n        return self.loc[key]", "description": ""}
{"id": "pandas-75", "project": "pandas", "bug_id": "75", "buggy_code": "except (TypeError, ValueError):\n                return self._get_string_slice(key)\n            except (TypeError, KeyError, ValueError, OverflowError):\n                key = asdt", "fixed_code": "except (TypeError, ValueError, OverflowError):\n\n                loc = self._get_string_slice(key)\n                return loc\n            except (TypeError, ValueError):\n            grp = resolution.Resolution.get_freq_group(reso)\n            freqn = resolution.get_freq_group(self.freq)\n\n            # _get_string_slice will handle cases where grp < freqn\n            assert grp >= freqn\n\n            if grp == freqn:\n                key = Period(asdt, freq=self.freq)\n                loc = self.get_loc(key, method=method, tolerance=tolerance)\n                return loc\n            elif method is None:\n                raise KeyError(key)\n            else:\n                key = asdt", "description": ""}
{"id": "pandas-121", "project": "pandas", "bug_id": "121", "buggy_code": "if m.any():", "fixed_code": "if convert:\n            return [self.convert(numeric=False, copy=True)]\n                    if m.any() or convert:", "description": ""}
{"id": "pandas-119", "project": "pandas", "bug_id": "119", "buggy_code": "margin_dummy[cols] = margin_dummy[cols].astype(dtype)", "fixed_code": "# check the result column and leave floats\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )", "description": ""}
{"id": "pandas-72", "project": "pandas", "bug_id": "72", "buggy_code": "# if we are an exact match (ex-broadcasting),\n        # then use the resultant dtype\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size", "fixed_code": "exact_match = (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n        )\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n            # GH25495 - If the current dtype is not categorical,\n            # we need to create a new categorical block\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n\n        # if we are an exact match (ex-broadcasting),\n        # then use the resultant dtype\n        elif exact_match:\n            values[indexer] = value", "description": ""}
{"id": "pandas-126", "project": "pandas", "bug_id": "126", "buggy_code": "elif isinstance(other, list) and not isinstance(other[0], DataFrame):\n            other = DataFrame(other)\n            if (self.columns.get_indexer(other.columns) >= 0).all():\n                other = other.reindex(columns=self.columns)", "fixed_code": "elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if (self.columns.get_indexer(other.columns) >= 0).all():\n                    other = other.reindex(columns=self.columns)", "description": ""}
{"id": "pandas-44", "project": "pandas", "bug_id": "44", "buggy_code": "is_categorical,\n        if is_categorical(target):\n        elif self.is_all_dates and target.is_all_dates:  # GH 30399\n            tgt_values = target.asi8\nfrom pandas._typing import Label\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas._typing import Label\nfrom pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar\nfrom pandas._typing import Label\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\nfrom pandas._typing import Label", "fixed_code": "# Remove tz so Index will try non-DatetimeIndex inference\n        attributes.pop(\"tz\", None)\n        if is_categorical_dtype(target.dtype):\nfrom pandas._typing import DtypeObj, Label\n    ensure_platform_int,\nfrom pandas.core.indexes.base import Index, _index_shared_docs, ensure_index\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\nfrom pandas._typing import DtypeObj, Label\nfrom pandas.core.dtypes.common import (\n    _NS_DTYPE,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_float,\n    is_integer,\n    is_scalar,\n)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            # If we have tz, we can compare to tzaware\n            return is_datetime64tz_dtype(dtype)\n        # if we dont have tz, we can only compare to tznaive\n        return is_datetime64_dtype(dtype)\n\nfrom pandas._typing import DtypeObj, Label\nfrom pandas.core.dtypes.dtypes import PeriodDtype\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq\n\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\nfrom pandas._typing import DtypeObj, Label\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return is_timedelta64_dtype(dtype)", "description": ""}
{"id": "pandas-110", "project": "pandas", "bug_id": "110", "buggy_code": "is_positional = is_index_slice and not self.is_integer()", "fixed_code": "is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        if kind == \"loc\":\n            return label\n\n        return super()._maybe_cast_slice_bound(label, side, kind)", "description": ""}
{"id": "pandas-43", "project": "pandas", "bug_id": "43", "buggy_code": "left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):", "fixed_code": "left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n    if op is operator.pow or op is rpow:\n        # GH#32685 pow has special semantics for operating with null values\n        return False\n\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):", "description": ""}
{"id": "pandas-88", "project": "pandas", "bug_id": "88", "buggy_code": "if table.index.nlevels > 1:", "fixed_code": "# GH17038, this check should only happen if index is defined (not None)\n    if table.index.nlevels > 1 and index:", "description": ""}
{"id": "pandas-117", "project": "pandas", "bug_id": "117", "buggy_code": "elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):", "fixed_code": "elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):", "description": ""}
{"id": "pandas-153", "project": "pandas", "bug_id": "153", "buggy_code": "from pandas._libs import NaT, Timestamp, lib, tslib\n            values = values.astype(str)", "fixed_code": "from pandas._libs import NaT, Timestamp, lib, tslib, writers\n            itemsize = writers.word_len(na_rep)\n            values = values.astype(\"<U{size}\".format(size=itemsize))", "description": ""}
{"id": "pandas-38", "project": "pandas", "bug_id": "38", "buggy_code": "if rlocs == []:\n                clocs = [v if i > v else v - 1 for v in clocs]", "fixed_code": "if not rlocs:\n                clocs = [v if v < val else v - 1 for v in clocs]", "description": ""}
{"id": "pandas-154", "project": "pandas", "bug_id": "154", "buggy_code": "result_sz = len(obj.values)\n                cython_dtype = obj.values.dtype\n                vals = obj.values\n                mask = isna(obj.values).view(np.uint8)\n                result = algorithms.take_nd(obj.values, result)", "fixed_code": "values = obj._data._values\n\n                result_sz = len(values)\n                cython_dtype = values.dtype\n                vals = values\n                mask = isna(values).view(np.uint8)\n                result = algorithms.take_nd(values, result)", "description": ""}
{"id": "pandas-36", "project": "pandas", "bug_id": "36", "buggy_code": "is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_period_dtype,\n    is_timedelta64_dtype,\n    ABCDatetimeArray,\n    ABCTimedeltaArray,\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n        return obj is None\n        return obj is None\n    is_extension = is_extension_array_dtype(obj)\n\n    if not is_extension:\n        # Avoid accessing `.values` on things like\n        # PeriodIndex, which may be expensive.\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n        # Working around NumPy ticket 1542\n        shape = values.shape\n\n        if is_string_like_dtype(dtype):\n            # object array of strings\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            # object array of non-strings\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n        # Working around NumPy ticket 1542\n        shape = values.shape\n\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n    if (\n        is_datetime64_dtype(dtype)\n        or is_datetime64tz_dtype(dtype)\n        or is_timedelta64_dtype(dtype)\n        or is_period_dtype(dtype)\n    ):", "fixed_code": "elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return False\n        return False\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n        result = _isna_string_dtype(values, dtype, old=False)\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\ndef _isna_string_dtype(values: np.ndarray, dtype: np.dtype, old: bool) -> np.ndarray:\n    # Working around NumPy ticket 1542\n    shape = values.shape\n\n    if is_string_like_dtype(dtype):\n        result = np.zeros(values.shape, dtype=bool)\n    else:\n        result = np.empty(shape, dtype=bool)\n        if old:\n            vec = libmissing.isnaobj_old(values.ravel())\n        else:\n            vec = libmissing.isnaobj(values.ravel())\n\n        result[...] = vec.reshape(shape)\n\n    return result\n\n\n    if needs_i8_conversion(dtype):", "description": ""}
{"id": "pandas-162", "project": "pandas", "bug_id": "162", "buggy_code": "column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        # to keep index and columns names\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n        table.index.names = table_index_names\n        table.columns.names = table_columns_names", "fixed_code": "# keep index and column of pivoted table\n        table_index = table.index\n        table_columns = table.columns\n\n        # check if margin name is in (for MI cases) or equal to last\n        # index/column and save the column and index margin\n        if (margins_name not in table.iloc[-1, :].name) | (\n            margins_name != table.iloc[:, -1].name\n        ):\n            raise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\n        column_margin = table.iloc[:-1, -1]\n        index_margin = table.iloc[-1, :-1]\n\n        # keep the core table\n        table = table.iloc[:-1, :-1]\n            table.columns = table_columns\n            table.index = table_index\n            table.index = table_index\n            table.columns = table_columns", "description": ""}
{"id": "pandas-31", "project": "pandas", "bug_id": "31", "buggy_code": "if is_integer_dtype(vals):\n            elif is_datetime64_dtype(vals):", "fixed_code": "is_bool_dtype,\n    is_extension_array_dtype,\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):", "description": ""}
{"id": "pandas-165", "project": "pandas", "bug_id": "165", "buggy_code": "if isinstance(other, (ABCSeries, ABCDataFrame)):\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):", "fixed_code": "if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n    # ---------------------------------------------------------------\n    # Unsorted\n\n    def test_parr_add_sub_index(self):\n        # Check that PeriodArray defers to Index on arithmetic ops\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n\n    def test_tda_add_sub_index(self):\n        # Check that TimedeltaArray defers to Index on arithmetic ops\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)", "description": ""}
{"id": "pandas-131", "project": "pandas", "bug_id": "131", "buggy_code": "from pandas.core.algorithms import take_1d\n        # blow up if we operate on categories\n            result = take_1d(result, self.orig.cat.codes)\n            data = Series(orig.values.categories, name=orig.name, copy=False)", "fixed_code": "data = Series(\n                orig.array,\n                name=orig.name,\n                copy=False,\n                dtype=orig.values.categories.dtype,\n            )", "description": ""}
{"id": "pandas-91", "project": "pandas", "bug_id": "91", "buggy_code": "value = np.array(value, dtype=_TD_DTYPE, copy=False)\n        else:\n            value = Timedelta(value).asm8.view(_TD_DTYPE)\n        return self.values.searchsorted(value, side=side, sorter=sorter)", "fixed_code": "if not type(self._data)._is_recognized_dtype(value):\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar, \"\n                    f\"not {type(value).__name__}\"\n                )\n            value = type(self._data)(value)\n            self._data._check_compatible_with(value)\n\n        elif isinstance(value, self._data._recognized_scalars):\n            self._data._check_compatible_with(value)\n            value = self._data._scalar_type(value)\n\n        elif not isinstance(value, TimedeltaArray):\n            raise TypeError(\n                \"searchsorted requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)", "description": ""}
{"id": "pandas-65", "project": "pandas", "bug_id": "65", "buggy_code": "from io import BufferedIOBase, BytesIO\n        need_text_wrapping = (BufferedIOBase, S3File)\n        need_text_wrapping = BufferedIOBase  # type: ignore\n        if not isinstance(f, BufferedIOBase):\nfrom io import BufferedIOBase, StringIO, TextIOWrapper\n            if isinstance(src, BufferedIOBase):", "fixed_code": "from io import BufferedIOBase, BytesIO, RawIOBase\n        need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)\n        need_text_wrapping = (BufferedIOBase, RawIOBase)  # type: ignore\n        if not isinstance(f, (BufferedIOBase, RawIOBase)):\nfrom io import BufferedIOBase, RawIOBase, StringIO, TextIOWrapper\n            if isinstance(src, (BufferedIOBase, RawIOBase)):", "description": ""}
{"id": "pandas-136", "project": "pandas", "bug_id": "136", "buggy_code": "is_int64_dtype,\n            elif is_int64_dtype(lt):", "fixed_code": "elif is_integer_dtype(lt):", "description": ""}
{"id": "pandas-96", "project": "pandas", "bug_id": "96", "buggy_code": "skip_bd = BusinessDay(n=bd)", "fixed_code": "if isinstance(self, _CustomMixin):  # GH 30593\n                    skip_bd = CustomBusinessDay(\n                        n=bd,\n                        weekmask=self.weekmask,\n                        holidays=self.holidays,\n                        calendar=self.calendar,\n                    )\n                else:\n                    skip_bd = BusinessDay(n=bd)", "description": ""}
{"id": "pandas-100", "project": "pandas", "bug_id": "100", "buggy_code": "rs = rs.loc[~rs.index.duplicated()]\n        rs = rs.reindex_like(data)\n        if freq is None:\n            mask = isna(com.values_from_object(data))\n            np.putmask(rs.values, mask, np.nan)", "fixed_code": "if freq is not None:\n            # Shift method is implemented differently when freq is not None\n            # We want to restore the original index\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)", "description": ""}
{"id": "pandas-54", "project": "pandas", "bug_id": "54", "buggy_code": "if not isinstance(indices, RangeIndex):", "fixed_code": "elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n        if not isinstance(indices, (RangeIndex, CategoricalIndex)):\n            # TODO: CategoricalIndex can be re-allowed following GH#32167", "description": ""}
{"id": "pandas-107", "project": "pandas", "bug_id": "107", "buggy_code": "if other.name is None:\n                index = None\n            else:\n                # other must have the same index name as self, otherwise\n                # index name will be reset\n                index = Index([other.name], name=self.index.name)\n\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n            other = other._convert(datetime=True, timedelta=True)", "fixed_code": "index = Index([other.name], name=self.index.name)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)", "description": ""}
{"id": "pandas-98", "project": "pandas", "bug_id": "98", "buggy_code": "elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            return PeriodIndex(data, copy=copy, name=name, **kwargs)", "fixed_code": "elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n        elif is_period_dtype(data) or is_period_dtype(dtype):\n            if is_dtype_equal(_o_dtype, dtype):\n                return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)\n            return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)", "description": ""}
{"id": "pandas-138", "project": "pandas", "bug_id": "138", "buggy_code": "if the passed data is of datetime/timedelta type,\n    this method converts it to numeric so that cut method can", "fixed_code": "is_bool_dtype,\n    if the passed data is of datetime/timedelta or bool type,\n    this method converts it to numeric so that cut or qcut method can\n    elif is_bool_dtype(x):\n        # GH 20303\n        x = x.astype(np.int64)\n\n\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    # issue 20303\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)", "description": ""}
{"id": "pandas-53", "project": "pandas", "bug_id": "53", "buggy_code": "if not self.holds_integer():\n        return self.index.get_value(self, label)\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n        with pytest.raises(TypeError, match=msg):\n    def test_frame_raises_type_error(self):\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n        with pytest.raises(TypeError, match=msg):", "fixed_code": "if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n        # Similar to Index.get_value, but we do not fall back to positional\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n        with pytest.raises(KeyError, match=\"^0$\"):\n        with pytest.raises(KeyError, match=\"^0$\"):\n    def test_frame_raises_key_error(self):\n        with pytest.raises(KeyError, match=\"^0$\"):\n        with pytest.raises(KeyError, match=\"^0$\"):", "description": ""}
{"id": "pandas-30", "project": "pandas", "bug_id": "30", "buggy_code": "except (ValueError, OverflowError):", "fixed_code": "except (ValueError, OverflowError, TypeError):", "description": ""}
{"id": "pandas-37", "project": "pandas", "bug_id": "37", "buggy_code": "from pandas.core.arrays import PandasArray", "fixed_code": "from pandas.core.arrays import IntegerArray, PandasArray\nfrom pandas.core.arrays.integer import _IntegerDtype\n        elif isinstance(dtype, _IntegerDtype):\n            arr = self._ndarray.copy()\n            mask = self.isna()\n            arr[mask] = 0\n            values = arr.astype(dtype.numpy_dtype)\n            return IntegerArray(values, mask, copy=False)", "description": ""}
{"id": "pandas-163", "project": "pandas", "bug_id": "163", "buggy_code": "# Always convert inf to nan\n        values[np.isinf(values)] = np.NaN", "fixed_code": "# Convert inf to nan for C funcs\n        inf = np.isinf(values)\n        if inf.any():\n            values = np.where(inf, np.nan, values)", "description": ""}
{"id": "pandas-155", "project": "pandas", "bug_id": "155", "buggy_code": "return self.obj.index", "fixed_code": "if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns", "description": ""}
{"id": "pandas-152", "project": "pandas", "bug_id": "152", "buggy_code": "to_concat = [self] + to_append", "fixed_code": "to_concat = [self]\n            to_concat.extend(to_append)", "description": ""}
{"id": "pandas-106", "project": "pandas", "bug_id": "106", "buggy_code": "elif self.is_all_dates:", "fixed_code": "elif self.is_all_dates and target.is_all_dates:  # GH 30399", "description": ""}
{"id": "pandas-99", "project": "pandas", "bug_id": "99", "buggy_code": "arg = getattr(arg, \"values\", arg)\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)", "fixed_code": "from pandas.arrays import IntegerArray\n        arg = getattr(arg, \"_values\", arg)\n\n        # GH 30050 pass an ndarray to tslib.array_with_unit_to_datetime\n        # because it expects an ndarray argument\n        if isinstance(arg, IntegerArray):\n            # Explicitly pass NaT mask to array_with_unit_to_datetime\n            mask = arg.isna()\n            arg = arg._ndarray_values\n        else:\n            mask = None\n\n        result, tz_parsed = tslib.array_with_unit_to_datetime(\n            arg, mask, unit, errors=errors\n        )", "description": ""}
{"id": "pandas-52", "project": "pandas", "bug_id": "52", "buggy_code": "# GH 27951\n        # temporary fix while we wait for NumPy bug 12629 to be fixed\n        val[isna(val)] = np.datetime64(\"NaT\")\n\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:  # catches object dtypes\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n\n        ids, val = ids[sorter], val[sorter]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)", "fixed_code": "codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1", "description": ""}
{"id": "pandas-55", "project": "pandas", "bug_id": "55", "buggy_code": "def _iget_item_cache(self, item):\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False", "fixed_code": "def _iget_item_cache(self, item: int):\n            return self._ixs(item, axis=1)", "description": ""}
{"id": "pandas-137", "project": "pandas", "bug_id": "137", "buggy_code": "from pandas.core.construction import extract_array, sanitize_array", "fixed_code": "from pandas.core.construction import array, extract_array, sanitize_array\n        if is_extension_array_dtype(dtype):\n            return array(self, dtype=dtype, copy=copy)  # type: ignore # GH 28770", "description": ""}
{"id": "pandas-97", "project": "pandas", "bug_id": "97", "buggy_code": "return this._fast_union(other)\n    def _fast_union(self, other):", "fixed_code": "return this._fast_union(other, sort=sort)\n    def _fast_union(self, other, sort=None):\n        elif sort is False:\n            # TDIs are not in the \"correct\" order and we don't want\n            #  to sort but want to remove overlaps\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)", "description": ""}
{"id": "pandas-108", "project": "pandas", "bug_id": "108", "buggy_code": "from .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype", "fixed_code": "from .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype\n        elif lib.is_interval(val):\n            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]\n            dtype = IntervalDtype(subtype=subtype)", "description": ""}
{"id": "pandas-63", "project": "pandas", "bug_id": "63", "buggy_code": "for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key", "fixed_code": "lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n\n        return tuple(lkey)", "description": ""}
{"id": "pandas-64", "project": "pandas", "bug_id": "64", "buggy_code": "self.df = df", "fixed_code": "self.df = df.reindex(columns=cols)", "description": ""}
{"id": "pandas-90", "project": "pandas", "bug_id": "90", "buggy_code": "from typing import List, Optional, Union, cast\nfrom pandas._typing import FrameOrSeries\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n    obj : pandas object\n    path : str, default None\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\nfrom pandas.io.common import get_handle, stringify_path\ndef to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n    path : str\n        File path where the pickled object will be stored.\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n    path = stringify_path(path)\n    f, fh = get_handle(path, \"wb\", compression=compression, is_text=False)\ndef read_pickle(path, compression=\"infer\"):\n    path : str\n        File path where the pickled object will be loaded.\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n    path = stringify_path(path)\n    f, fh = get_handle(path, \"rb\", compression=compression, is_text=False)", "fixed_code": "from typing import Any, List, Optional, Union, cast\nfrom pandas._typing import FilePathOrBuffer, FrameOrSeries\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n    obj : any object\n    path : str, path object or file-like object, default None\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\nfrom typing import Any, Optional\nfrom pandas._typing import FilePathOrBuffer\nfrom pandas.io.common import get_filepath_or_buffer, get_handle\ndef to_pickle(\n    obj: Any,\n    filepath_or_buffer: FilePathOrBuffer,\n    compression: Optional[str] = \"infer\",\n    protocol: int = pickle.HIGHEST_PROTOCOL,\n):\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n        filepath_or_buffer, compression=compression, mode=\"wb\"\n    )\n    if not isinstance(fp_or_buf, str) and compression == \"infer\":\n        compression = None\n    f, fh = get_handle(fp_or_buf, \"wb\", compression=compression, is_text=False)\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass\ndef read_pickle(\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\n):\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n        filepath_or_buffer, compression=compression\n    )\n    if not isinstance(fp_or_buf, str) and compression == \"infer\":\n        compression = None\n    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass", "description": ""}
{"id": "pandas-46", "project": "pandas", "bug_id": "46", "buggy_code": "import pandas.core.common as com\n\n    return [\n        np.tile(\n            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])\n        )\n        for i, x in enumerate(X)\n    ]", "fixed_code": "# codes are all ndarrays, so cartesian_product is lossless\n    return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]\n\n\ndef _tile_compat(arr, num: int):\n    \"\"\"\n    Index compat for np.tile.\n\n    Notes\n    -----\n    Does not support multi-dimensional `num`.\n    \"\"\"\n    if isinstance(arr, np.ndarray):\n        return np.tile(arr, num)\n\n    # Otherwise we have an Index\n    taker = np.tile(np.arange(len(arr)), num)\n    return arr.take(taker)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)", "description": ""}
{"id": "pandas-79", "project": "pandas", "bug_id": "79", "buggy_code": "except (KeyError, TypeError):\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n            except TypeError:", "fixed_code": "from pandas.core.indexes.base import InvalidIndexError\n            except (KeyError, TypeError, InvalidIndexError):\nfrom pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n\n            except (TypeError, InvalidIndexError):\n        except InvalidIndexError:\n            # e.g. slice\n            self._set_with(key, value)", "description": ""}
{"id": "pandas-112", "project": "pandas", "bug_id": "112", "buggy_code": "from pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range", "fixed_code": "is_categorical,\nfrom pandas.core.algorithms import take_1d\n        elif is_categorical(target_as_index):\n            # get an indexer for unique categories then propogate to codes via take_1d\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\nfrom pandas import (\n    CategoricalIndex,\n    Interval,\n    IntervalIndex,\n    Timedelta,\n    date_range,\n    timedelta_range,\n)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        # GH 30063: categorical and non-categorical results should be consistent\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)", "description": ""}
{"id": "pandas-41", "project": "pandas", "bug_id": "41", "buggy_code": "Modify Block in-place with new item value\n        Returns\n        -------\n        None\n        Set the value inplace, returning a a maybe different typed block.\n    def should_store(self, value):\n    def set(self, locs, values, check=False):\n        self.values = values\n        Set the value inplace, returning a same-typed block.\n    def should_store(self, value) -> bool:\n    def should_store(self, value) -> bool:\n    def should_store(self, value) -> bool:\n    def should_store(self, value) -> bool:\n        return is_datetime64_dtype(value.dtype)\n\n        Modify Block in-place with new item value\n\n        Returns\n        -------\n        None\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n\n    def should_store(self, value) -> bool:\n    def should_store(self, value) -> bool:", "fixed_code": "from pandas._typing import ArrayLike\n        Modify block values in-place with new item value.\n        Notes\n        -----\n        `set` never creates a new array or new Block, whereas `setitem` _may_\n        create a new array and always creates a new Block.\n        Attempt self.values[indexer] = value, possibly creating a new array.\n    def should_store(self, value: ArrayLike) -> bool:\n        \"\"\"\n        Can we set the given array-like value inplace?\n        \"\"\"\n    def set(self, locs, values):\n        self.values[:] = values\n        Attempt self.values[indexer] = value, possibly creating a new array.\n    def should_store(self, value: ArrayLike) -> bool:\n    def should_store(self, value: ArrayLike) -> bool:\n    def should_store(self, value: ArrayLike) -> bool:\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n\n        See Block.set.__doc__\n    should_store = DatetimeBlock.should_store\n    def should_store(self, value: ArrayLike) -> bool:\n    def should_store(self, value: ArrayLike) -> bool:\n    def should_store(self, arr: ArrayLike):\n        return isinstance(arr, self._holder) and is_dtype_equal(self.dtype, arr.dtype)", "description": ""}
{"id": "pandas-115", "project": "pandas", "bug_id": "115", "buggy_code": "result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])", "fixed_code": "# np.interp requires sorted X values, #21037\n        indexer = np.argsort(inds[valid])\n        result[invalid] = np.interp(\n            inds[invalid], inds[valid][indexer], yvalues[valid][indexer]\n        )", "description": ""}
{"id": "pandas-83", "project": "pandas", "bug_id": "83", "buggy_code": "objs, intersect: bool = False, axis=0, sort: bool = True\n    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort", "fixed_code": "objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n    copy : bool, default False\n        If True, return a copy of the combined index.\n    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n    copy : bool, default False\n        If True, return a copy of the combined index.\n\n    # GH 29879\n    if copy:\n        index = index.copy()\n\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,", "description": ""}
{"id": "pandas-77", "project": "pandas", "bug_id": "77", "buggy_code": "result = libops.vec_binop(x, y, op)\n    return result", "fixed_code": "result = libops.vec_binop(x.ravel(), y.ravel(), op)\n    return result.reshape(x.shape)", "description": ""}
{"id": "pandas-123", "project": "pandas", "bug_id": "123", "buggy_code": "return Float64Index(data, copy=copy, dtype=dtype, name=name)\n\n    is_int64_dtype,\n    @staticmethod\n    def _validate_dtype(dtype):\n        \"\"\" require dtype to be None or int64 \"\"\"\n        if not (dtype is None or is_int64_dtype(dtype)):\n            raise TypeError(\"Invalid to pass a non-int64 dtype to RangeIndex\")", "fixed_code": "return Float64Index(data, copy=copy, name=name)\n    is_signed_integer_dtype,\n    is_unsigned_integer_dtype,\nfrom pandas._typing import Dtype\n        cls._validate_dtype(dtype)\n    @classmethod\n    def _validate_dtype(cls, dtype: Dtype) -> None:\n        if dtype is None:\n            return\n        validation_metadata = {\n            \"int64index\": (is_signed_integer_dtype, \"signed integer\"),\n            \"uint64index\": (is_unsigned_integer_dtype, \"unsigned integer\"),\n            \"float64index\": (is_float_dtype, \"float\"),\n            \"rangeindex\": (is_signed_integer_dtype, \"signed integer\"),\n        }\n\n        validation_func, expected = validation_metadata[cls._typ]\n        if not validation_func(dtype):\n            msg = f\"Incorrect `dtype` passed: expected {expected}, received {dtype}\"\n            raise ValueError(msg)", "description": ""}
{"id": "pandas-48", "project": "pandas", "bug_id": "48", "buggy_code": "except ValueError:", "fixed_code": "except (ValueError, TypeError):", "description": ""}
{"id": "pandas-70", "project": "pandas", "bug_id": "70", "buggy_code": "# return the same type (Series) as our caller\n                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n        [1, 4, 7], index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"), dtype=\"Int64\"\n    expected[\"Group\"] = expected[\"Group_obj\"].astype(\"category\")", "fixed_code": "if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n\n        elif (\n            how == \"add\"\n            and is_integer_dtype(orig_values.dtype)\n            and is_extension_array_dtype(orig_values.dtype)\n        ):\n            # We need this to ensure that Series[Int64Dtype].resample().sum()\n            # remains int64 dtype.\n            # Two options for avoiding this special case\n            # 1. mask-aware ops and avoid casting to float with NaN above\n            # 2. specify the result dtype when calling this method\n            result = result.astype(\"int64\")\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    # https://github.com/pandas-dev/pandas/pull/31359\n    # This is currently failing to cast back to Int64Dtype.\n    # The presence of the NA causes two problems\n    # 1. NA is not an instance of Int64Dtype.type (numpy.int64)\n    # 2. The presence of an NA forces object type, so the non-NA values is\n    #    a Python int rather than a NumPy int64. Python ints aren't\n    #    instances of numpy.int64.\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n\n\n        [1, 4, 7],\n        index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"),\n        dtype=\"float64\",\n    expected[\"Group\"] = expected[\"Group_obj\"]", "description": ""}
{"id": "pandas-84", "project": "pandas", "bug_id": "84", "buggy_code": "from pandas import DataFrame, MultiIndex, Series\n    def test_unstack(self):\n\n        index = MultiIndex(\n            levels=[[\"bar\", \"foo\"], [\"one\", \"three\", \"two\"]],\n            codes=[[1, 1, 0, 0], [0, 1, 0, 2]],\n        )\n\n        s = Series(np.arange(4.0), index=index)\n        unstacked = s.unstack()\n\n        expected = DataFrame(\n            [[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],\n            index=[\"bar\", \"foo\"],\n            columns=[\"one\", \"three\", \"two\"],\n        )\n\n        tm.assert_frame_equal(unstacked, expected)\n\n        unstacked = s.unstack(level=0)\n        tm.assert_frame_equal(unstacked, expected.T)\n\n        index = MultiIndex(\n            levels=[[\"bar\"], [\"one\", \"two\", \"three\"], [0, 1]],\n            codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],\n        )\n        s = Series(np.random.randn(6), index=index)\n        exp_index = MultiIndex(\n            levels=[[\"one\", \"two\", \"three\"], [0, 1]],\n            codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],\n        )\n        expected = DataFrame({\"bar\": s.values}, index=exp_index).sort_index(level=0)\n        unstacked = s.unstack(0).sort_index()\n        tm.assert_frame_equal(unstacked, expected)\n\n        # GH5873\n        idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])\n        ts = pd.Series([1, 2], index=idx)\n        left = ts.unstack()\n        right = DataFrame(\n            [[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]\n        )\n        tm.assert_frame_equal(left, right)\n\n        idx = pd.MultiIndex.from_arrays(\n            [\n                [\"cat\", \"cat\", \"cat\", \"dog\", \"dog\"],\n                [\"a\", \"a\", \"b\", \"a\", \"b\"],\n                [1, 2, 1, 1, np.nan],\n            ]\n        )\n        ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)\n        right = DataFrame(\n            [[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],\n            columns=[\"cat\", \"dog\"],\n        )\n        tpls = [(\"a\", 1), (\"a\", 2), (\"b\", np.nan), (\"b\", 1)]\n        right.index = pd.MultiIndex.from_tuples(tpls)\n        tm.assert_frame_equal(ts.unstack(level=0), right)", "fixed_code": "# GH 19966 Make sure if MultiIndexed index has tuple name, they will be\n    # recognised as a whole\n    if clocs in index.names:\n        clocs = [clocs]\nfrom pandas import DataFrame, Series", "description": ""}
{"id": "pandas-24", "project": "pandas", "bug_id": "24", "buggy_code": "dtype='datetime64[ns, US/Eastern]', freq='D')\n                      dtype='datetime64[ns]', freq='D')\n        return self._simple_new(new_dates, dtype=dtype, freq=self.freq)", "fixed_code": "dtype='datetime64[ns, US/Eastern]', freq=None)\n                      dtype='datetime64[ns]', freq=None)\n\n        freq = None\n        if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):\n            # we can preserve freq\n            # TODO: Also for fixed-offsets\n            freq = self.freq\n        elif tz is None and self.tz is None:\n            # no-op\n            freq = self.freq\n        return self._simple_new(new_dates, dtype=dtype, freq=freq)\n        dti._set_freq(\"infer\")  # freq not preserved by tz_localize", "description": ""}
{"id": "pandas-148", "project": "pandas", "bug_id": "148", "buggy_code": "EMPTY_SERIES = Series([])\n                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)\n            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)", "fixed_code": "r = self.f(Series([]))\n            if len(self.agg_axis):\n                r = self.f(Series([]))\n            else:\n                r = np.nan\n\n            return self.obj._constructor_sliced(r, index=self.agg_axis)", "description": ""}
{"id": "pandas-23", "project": "pandas", "bug_id": "23", "buggy_code": "return type(self)(data=[])\n            left_chunk = left.values[lslice]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])", "fixed_code": "return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n            left_chunk = left._values[lslice]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"B\")\n        assert smaller.freq == exp.freq\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n        assert smaller.freq == exp.freq", "description": ""}
{"id": "pandas-141", "project": "pandas", "bug_id": "141", "buggy_code": "# Work on reversed range for simplicity:\n            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)", "fixed_code": "# GH 28678: work on reversed range for simplicity\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step", "description": ""}
{"id": "pandas-4", "project": "pandas", "bug_id": "4", "buggy_code": "return multi_join_idx, lidx, ridx", "fixed_code": "if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx", "description": ""}
{"id": "pandas-15", "project": "pandas", "bug_id": "15", "buggy_code": "import operator\nfrom typing import Any, Sequence, Type, Union, cast\nimport warnings\nfrom pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib\nfrom pandas._libs.tslibs.c_timestamp import integer_op_not_supported\nfrom pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period\nfrom pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds\nfrom pandas._libs.tslibs.timestamps import RoundTo, round_nsint64\nfrom pandas._typing import DatetimeLikeScalar\nfrom pandas.compat import set_function_name\nfrom pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning\nfrom pandas.util._decorators import Appender, Substitution\nfrom pandas.util._validators import validate_fillna_kwargs\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_string_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\nfrom pandas.core.dtypes.generic import ABCSeries\nfrom pandas.core.dtypes.inference import is_array_like\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n\nfrom pandas.core import missing, nanops, ops\nfrom pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts\nfrom pandas.core.array_algos.transforms import shift\nfrom pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray\nfrom pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin\nimport pandas.core.common as com\nfrom pandas.core.construction import array, extract_array\nfrom pandas.core.indexers import check_array_indexer\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.ops.invalid import invalid_comparison, make_invalid_op\n\nfrom pandas.tseries import frequencies\nfrom pandas.tseries.offsets import DateOffset, Tick\n\n\ndef _datetimelike_array_cmp(cls, op):\n    \"\"\"\n    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to\n    boxed scalars/arrays.\n    \"\"\"\n    opname = f\"__{op.__name__}__\"\n    nat_result = opname == \"__ne__\"\n\n    class InvalidComparison(Exception):\n        pass\n\n    def _validate_comparison_value(self, other):\n        if isinstance(other, str):\n            try:\n                # GH#18435 strings get a pass from tzawareness compat\n                other = self._scalar_from_string(other)\n            except ValueError:\n                # failed to parse as Timestamp/Timedelta/Period\n                raise InvalidComparison(other)\n        if isinstance(other, self._recognized_scalars) or other is NaT:\n            other = self._scalar_type(other)\n            self._check_compatible_with(other)\n        elif not is_list_like(other):\n            raise InvalidComparison(other)\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n        else:\n            if isinstance(other, list):\n                # TODO: could use pd.Index to do inference?\n                other = np.array(other)\n            if not isinstance(other, (np.ndarray, type(self))):\n                raise InvalidComparison(other)\n            elif is_object_dtype(other.dtype):\n                pass\n            elif not type(self)._is_recognized_dtype(other.dtype):\n                raise InvalidComparison(other)\n            else:\n                # For PeriodDType this casting is unnecessary\n                # TODO: use Index to do inference?\n                other = type(self)._from_sequence(other)\n                self._check_compatible_with(other)\n        return other\n    @unpack_zerodim_and_defer(opname)\n    def wrapper(self, other):\n        try:\n            other = _validate_comparison_value(self, other)\n        except InvalidComparison:\n            return invalid_comparison(self, other, op)\n\n        dtype = getattr(other, \"dtype\", None)\n        if is_object_dtype(dtype):\n            # We have to use comp_method_OBJECT_ARRAY instead of numpy\n            #  comparison otherwise it would fail to raise when\n            #  comparing tz-aware and tz-naive\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)\n            return result\n        if isinstance(other, self._scalar_type) or other is NaT:\n            other_i8 = self._unbox_scalar(other)\n        else:\n            # Then type(other) == type(self)\n            other_i8 = other.asi8\n        result = op(self.asi8, other_i8)\n        o_mask = isna(other)\n        if self._hasnans | np.any(o_mask):\n            result[self._isnan | o_mask] = nat_result\n        return result\n    return set_function_name(wrapper, opname, cls)\nclass AttributesMixin:\n    _data: np.ndarray\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, **kwargs):\n        raise AbstractMethodError(cls)\n    def _scalar_type(self) -> Type[DatetimeLikeScalar]:\n        \"\"\"\n        The scalar associated with this datelike\n        * PeriodArray : Period\n        * DatetimeArray : Timestamp\n        * TimedeltaArray : Timedelta\n        raise AbstractMethodError(self)\n\n    def _scalar_from_string(\n        self, value: str\n    ) -> Union[Period, Timestamp, Timedelta, NaTType]:\n        Construct a scalar type from a string.\n        Parameters\n        ----------\n        value : str\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.\n        raise AbstractMethodError(self)\n\n    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:\n        Unbox the integer value of a scalar `value`.\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta(\"10s\"))  # doctest: +SKIP\n        10000000000\n        \"\"\"\n        raise AbstractMethodError(self)\n    def _check_compatible_with(\n        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False\n    ) -> None:\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n        setitem : bool, default False\n            For __setitem__ we may have stricter compatibility resrictions than\n            for comparisons.\n\n        Raises\n        ------\n        Exception\n        raise AbstractMethodError(self)\n\n\nclass DatelikeOps:\n    \"\"\"\n    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.\n    \"\"\"\n\n    @Substitution(\n        URL=\"https://docs.python.org/3/library/datetime.html\"\n        \"#strftime-and-strptime-behavior\"\n    )\n    def strftime(self, date_format):\n        \"\"\"\n        Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n        Returns\n        -------\n        ndarray\n            NumPy ndarray of formatted strings.\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        \"\"\"\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object)\nclass TimelikeOps:\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n    _round_doc = \"\"\"\n        Perform {op} operation on the data to the specified `freq`.\n        Parameters\n        ----------\n        freq : str or Offset\n            The frequency level to {op} the index to. Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end). See\n            :ref:`frequency aliases <timeseries.offset_aliases>` for\n            a list of possible `freq` values.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            Only relevant for DatetimeIndex:\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n            .. versionadded:: 0.24.0\n        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n            .. versionadded:: 0.24.0\n        Returns\n        -------\n        DatetimeIndex, TimedeltaIndex, or Series\n            Index of the same type for a DatetimeIndex or TimedeltaIndex,\n            or a Series with the same index for a Series.\n        Raises\n        ------\n        ValueError if the `freq` cannot be converted.\n        Examples\n        **DatetimeIndex**\n\n        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n        >>> rng\n        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:01:00'],\n                      dtype='datetime64[ns]', freq='T')\n    _round_example = \"\"\">>> rng.round('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n        **Series**\n        >>> pd.Series(rng).dt.round(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _floor_example = \"\"\">>> rng.floor('H')\n        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n        **Series**\n\n        >>> pd.Series(rng).dt.floor(\"H\")\n        0   2018-01-01 11:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n    _ceil_example = \"\"\">>> rng.ceil('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 13:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n        >>> pd.Series(rng).dt.ceil(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 13:00:00\n        dtype: datetime64[ns]\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        if is_datetime64tz_dtype(self):\n            # operate on naive timestamps, then convert back to aware\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            aware = result.tz_localize(\n                self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n            return aware\n\n        values = self.view(\"i8\")\n        result = round_nsint64(values, mode, freq)\n        result = self._maybe_mask_results(result, fill_value=NaT)\n        return self._simple_new(result, dtype=self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n    def _with_freq(self, freq):\n        Helper to set our freq in-place, returning self to allow method chaining.\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n        Returns\n        -------\n        self\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, DateOffset):\n            # Always valid.  In the TimedeltaArray case, we assume this\n            #  is a Tick offset.\n            pass\n        else:\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = frequencies.to_offset(self.inferred_freq)\n\n        self._freq = freq\n        return self\n\n\nclass DatetimeLikeArrayMixin(\n    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray\n):\n    \"\"\"\n    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray\n\n    Assumes that __new__/__init__ defines:\n        _data\n        _freq\n\n    and that the inheriting class has methods:\n        _generate_range\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n    @property\n    def _ndarray(self) -> np.ndarray:\n        # NB: A bunch of Interval tests fail if we use ._data\n        return self.asi8\n    def _from_backing_data(self: _T, arr: np.ndarray) -> _T:\n        # Note: we do not retain `freq`\n        return type(self)(arr, dtype=self.dtype)  # type: ignore\n    # ------------------------------------------------------------------\n    def _box_func(self):\n        \"\"\"\n        box function to get object from internal representation\n        \"\"\"\n    def _box_values(self, values):\n        apply box func to passed values\n        return lib.map_infer(values, self._box_func)\n    def __iter__(self):\n        return (self._box_func(v) for v in self.asi8)\n    @property\n    def asi8(self) -> np.ndarray:\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        # do not cache or you'll create a memory leak\n        return self._data.view(\"i8\")\n    # ----------------------------------------------------------------\n    # Rendering Methods\n    def _format_native_types(self, na_rep=\"NaT\", date_format=None):\n        Helper method for astype when converting to strings.\n        ndarray[str]\n        raise AbstractMethodError(self)\n\n    def _formatter(self, boxed=False):\n        # TODO: Remove Datetime & DatetimeTZ formatters.\n        return \"'{}'\".format\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n    def __array__(self, dtype=None) -> np.ndarray:\n        # used for Timedelta/DatetimeArray, overwritten by PeriodArray\n        if is_object_dtype(dtype):\n            return np.array(list(self), dtype=object)\n        return self._data\n    def __getitem__(self, key):\n        \"\"\"\n        This getitem defers to the underlying array, which by-definition can\n        only handle list-likes, slices, and integer scalars\n        \"\"\"\n        if com.is_bool_indexer(key):\n            # first convert to boolean, because check_array_indexer doesn't\n            # allow object dtype\n            if is_object_dtype(key):\n                key = np.asarray(key, dtype=bool)\n\n            key = check_array_indexer(self, key)\n            key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n        elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):\n            # see https://github.com/pandas-dev/pandas/issues/31299, need to allow\n            # this for now (would otherwise raise in check_array_indexer)\n            pass\n        else:\n            key = check_array_indexer(self, key)\n        freq = self._get_getitem_freq(key)\n        result = self._data[key]\n        if lib.is_scalar(result):\n            return self._box_func(result)\n        return self._simple_new(result, dtype=self.dtype, freq=freq)\n    def _get_getitem_freq(self, key):\n        \"\"\"\n        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\n        \"\"\"\n        is_period = is_period_dtype(self.dtype)\n        if is_period:\n            freq = self.freq\n        else:\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                # GH#21282 indexing with Ellipsis is similar to a full slice,\n                #  should preserve `freq` attribute\n                freq = self.freq\n        return freq\n\n    def __setitem__(\n        self,\n        key: Union[int, Sequence[int], Sequence[bool], slice],\n        value: Union[NaTType, Any, Sequence[Any]],\n    ) -> None:\n        # I'm fudging the types a bit here. \"Any\" above really depends\n        # on type(self). For PeriodArray, it's Period (or stuff coercible\n        # to a period in from_sequence). For DatetimeArray, it's Timestamp...\n        # I don't know if mypy can do that, possibly with Generics.\n        # https://mypy.readthedocs.io/en/latest/generics.html\n        if is_list_like(value):\n            is_slice = isinstance(key, slice)\n\n            if lib.is_scalar(key):\n                raise ValueError(\"setting an array element with a sequence.\")\n\n            if not is_slice:\n                key = cast(Sequence, key)\n                if len(key) != len(value) and not com.is_bool_indexer(key):\n                    msg = (\n                        f\"shape mismatch: value array of length '{len(key)}' \"\n                        \"does not match indexing result of length \"\n                        f\"'{len(value)}'.\"\n                    )\n                    raise ValueError(msg)\n                elif not len(key):\n                    return\n\n        value = self._validate_setitem_value(value)\n        key = check_array_indexer(self, key)\n        self._data[key] = value\n        self._maybe_clear_freq()\n\n    def _maybe_clear_freq(self):\n        # inplace operations like __setitem__ may invalidate the freq of\n        # DatetimeArray and TimedeltaArray\n        pass\n\n    def astype(self, dtype, copy=True):\n        # Some notes on cases we don't have to handle here in the base class:\n        #   1. PeriodArray.astype handles period -> period\n        #   2. DatetimeArray.astype handles conversion between tz.\n        #   3. DatetimeArray.astype handles datetime -> period\n        dtype = pandas_dtype(dtype)\n\n        if is_object_dtype(dtype):\n            return self._box_values(self.asi8.ravel()).reshape(self.shape)\n        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):\n            return self._format_native_types()\n        elif is_integer_dtype(dtype):\n            # we deliberately ignore int32 vs. int64 here.\n            # See https://github.com/pandas-dev/pandas/issues/24381 for more.\n            values = self.asi8\n\n            if is_unsigned_integer_dtype(dtype):\n                # Again, we ignore int32 vs. int64\n                values = values.view(\"uint64\")\n\n            if copy:\n                values = values.copy()\n            return values\n        elif (\n            is_datetime_or_timedelta_dtype(dtype)\n            and not is_dtype_equal(self.dtype, dtype)\n        ) or is_float_dtype(dtype):\n            # disallow conversion between datetime/timedelta,\n            # and conversions for any datetimelike to float\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg)\n        elif is_categorical_dtype(dtype):\n            arr_cls = dtype.construct_array_type()\n            return arr_cls(self, dtype=dtype)\n            return np.asarray(self, dtype=dtype)\n\n    def view(self, dtype=None):\n        if dtype is None or dtype is self.dtype:\n            return type(self)(self._data, dtype=self.dtype)\n        return self._data.view(dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    @classmethod\n    def _concat_same_type(cls, to_concat, axis: int = 0):\n        # do not pass tz to set because tzlocal cannot be hashed\n        dtypes = {str(x.dtype) for x in to_concat}\n        if len(dtypes) != 1:\n            raise ValueError(\"to_concat must have the same dtype (tz)\", dtypes)\n        obj = to_concat[0]\n        dtype = obj.dtype\n\n        i8values = [x.asi8 for x in to_concat]\n        values = np.concatenate(i8values, axis=axis)\n\n        new_freq = None\n        if is_period_dtype(dtype):\n            new_freq = obj.freq\n        elif axis == 0:\n            # GH 3232: If the concat result is evenly spaced, we can retain the\n            # original frequency\n            to_concat = [x for x in to_concat if len(x)]\n\n            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):\n                    new_freq = obj.freq\n\n        return cls._simple_new(values, dtype=dtype, freq=new_freq)\n\n    def copy(self):\n        values = self.asi8.copy()\n        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)\n\n    def _values_for_factorize(self):\n        return self.asi8, iNaT\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    def _values_for_argsort(self):\n        return self._data\n\n    @Appender(ExtensionArray.shift.__doc__)\n    def shift(self, periods=1, fill_value=None, axis=0):\n        if not self.size or periods == 0:\n            return self.copy()\n        fill_value = self._validate_shift_value(fill_value)\n        new_values = shift(self._data, periods, axis, fill_value)\n        return type(self)._simple_new(new_values, dtype=self.dtype)\n    # ------------------------------------------------------------------\n    # Validation Methods\n    # TODO: try to de-duplicate these, ensure identical behavior\n    def _validate_fill_value(self, fill_value):\n        \"\"\"\n        If a fill_value is passed to `take` convert it to an i8 representation,\n        raising ValueError if this is not possible.\n        fill_value : object\n        fill_value : np.int64\n\n        Raises\n        ------\n        ValueError\n        if is_valid_nat_for_dtype(fill_value, self.dtype):\n            fill_value = iNaT\n        elif isinstance(fill_value, self._recognized_scalars):\n            self._check_compatible_with(fill_value)\n            fill_value = self._scalar_type(fill_value)\n            fill_value = self._unbox_scalar(fill_value)\n        else:\n            raise ValueError(\n                f\"'fill_value' should be a {self._scalar_type}. \"\n                f\"Got '{str(fill_value)}'.\"\n            )\n        return fill_value\n\n    def _validate_shift_value(self, fill_value):\n        # TODO(2.0): once this deprecation is enforced, used _validate_fill_value\n        if is_valid_nat_for_dtype(fill_value, self.dtype):\n            fill_value = NaT\n        elif not isinstance(fill_value, self._recognized_scalars):\n            # only warn if we're not going to raise\n            if self._scalar_type is Period and lib.is_integer(fill_value):\n                # kludge for #31971 since Period(integer) tries to cast to str\n                new_fill = Period._from_ordinal(fill_value, freq=self.freq)\n            else:\n                new_fill = self._scalar_type(fill_value)\n\n            # stacklevel here is chosen to be correct when called from\n            #  DataFrame.shift or Series.shift\n            warnings.warn(\n                f\"Passing {type(fill_value)} to shift is deprecated and \"\n                \"will raise in a future version, pass \"\n                f\"{self._scalar_type.__name__} instead.\",\n                FutureWarning,\n                stacklevel=10,\n            )\n            fill_value = new_fill\n\n        fill_value = self._unbox_scalar(fill_value)\n        return fill_value\n    def _validate_searchsorted_value(self, value):\n        if isinstance(value, str):\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar\"\n                ) from err\n\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            value = NaT\n\n        elif isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n\n        elif is_list_like(value) and not isinstance(value, type(self)):\n            value = array(value)\n\n            if not type(self)._is_recognized_dtype(value):\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar, \"\n                    f\"not {type(value).__name__}\"\n                )\n\n        if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):\n            raise TypeError(f\"Unexpected type for 'value': {type(value)}\")\n\n        if isinstance(value, type(self)):\n            self._check_compatible_with(value)\n            value = value.asi8\n        else:\n            value = self._unbox_scalar(value)\n\n        return value\n\n    def _validate_setitem_value(self, value):\n        if lib.is_scalar(value) and not isna(value):\n            value = com.maybe_box_datetimelike(value)\n\n        if is_list_like(value):\n            value = type(self)._from_sequence(value, dtype=self.dtype)\n            self._check_compatible_with(value, setitem=True)\n            value = value.asi8\n        elif isinstance(value, self._scalar_type):\n            self._check_compatible_with(value, setitem=True)\n            value = self._unbox_scalar(value)\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            value = iNaT\n        else:\n            msg = (\n                f\"'value' should be a '{self._scalar_type.__name__}', 'NaT', \"\n                f\"or array of those. Got '{type(value).__name__}' instead.\"\n            )\n            raise TypeError(msg)\n\n        return value\n\n    def _validate_insert_value(self, value):\n        if isinstance(value, self._recognized_scalars):\n            value = self._scalar_type(value)\n            self._check_compatible_with(value, setitem=True)\n            # TODO: if we dont have compat, should we raise or astype(object)?\n            #  PeriodIndex does astype(object)\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n        else:\n            raise TypeError(\n                f\"cannot insert {type(self).__name__} with incompatible label\"\n            )\n\n        return value\n    def _validate_where_value(self, other):\n        if is_valid_nat_for_dtype(other, self.dtype):\n            other = NaT\n        elif isinstance(other, self._recognized_scalars):\n            other = self._scalar_type(other)\n            self._check_compatible_with(other, setitem=True)\n        elif not is_list_like(other):\n            raise TypeError(f\"Where requires matching dtype, not {type(other)}\")\n        else:\n            # Do type inference if necessary up front\n            # e.g. we passed PeriodIndex.values and got an ndarray of Periods\n            other = array(other)\n            other = extract_array(other, extract_numpy=True)\n\n            if is_categorical_dtype(other.dtype):\n                # e.g. we have a Categorical holding self.dtype\n                if is_dtype_equal(other.categories.dtype, self.dtype):\n                    other = other._internal_get_values()\n\n            if not type(self)._is_recognized_dtype(other.dtype):\n                raise TypeError(f\"Where requires matching dtype, not {other.dtype}\")\n            self._check_compatible_with(other, setitem=True)\n\n        if lib.is_scalar(other):\n            other = self._unbox_scalar(other)\n        else:\n            other = other.view(\"i8\")\n        return other\n    # ------------------------------------------------------------------\n    # Additional array methods\n    #  These are not part of the EA API, but we implement them because\n    #  pandas assumes they're there.\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        Find indices where elements should be inserted to maintain order.\n\n        Find the indices into a sorted array `self` such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort `self` into ascending\n            order. They are typically the result of ``np.argsort``.\n        indices : array of ints\n            Array of insertion points with the same shape as `value`.\n        value = self._validate_searchsorted_value(value)\n        # TODO: Use datetime64 semantics for sorting, xref GH#29844\n        return self.asi8.searchsorted(value, side=side, sorter=sorter)\n    def value_counts(self, dropna=False):\n        Return a Series containing counts of unique values.\n        dropna : bool, default True\n            Don't include counts of NaT values.\n        Returns\n        -------\n        Series\n        \"\"\"\n        from pandas import Series, Index\n        if dropna:\n            values = self[~self.isna()]._data\n        else:\n            values = self._data\n        cls = type(self)\n        result = value_counts(values, sort=False, dropna=dropna)\n        index = Index(\n            cls(result.index.view(\"i8\"), dtype=self.dtype), name=result.index.name\n        )\n        return Series(result._values, index=index, name=result.name)\n    def map(self, mapper):\n        # TODO(GH-23179): Add ExtensionArray.map\n        # Need to figure out if we want ExtensionArray.map first.\n        # If so, then we can refactor IndexOpsMixin._map_values to\n        # a standalone function and call from here..\n        # Else, just rewrite _map_infer_values to do the right thing.\n        from pandas import Index\n        return Index(self).map(mapper).array\n    # ------------------------------------------------------------------\n    # Null Handling\n    def isna(self):\n        return self._isnan\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _isnan(self):\n        \"\"\"\n        return if each value is nan\n        \"\"\"\n        return self.asi8 == iNaT\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _hasnans(self):\n        \"\"\"\n        return if I have any nans; enables various perf speedups\n        \"\"\"\n        return bool(self._isnan.any())\n    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):\n        \"\"\"\n        Parameters\n        ----------\n        result : a ndarray\n        fill_value : object, default iNaT\n        convert : str, dtype or None\n        Returns\n        -------\n        result : ndarray with values replace by the fill_value\n        mask the result if needed, convert to the provided dtype if its not\n        None\n        This is an internal routine.\n        \"\"\"\n        if self._hasnans:\n            if convert:\n                result = result.astype(convert)\n            if fill_value is None:\n                fill_value = np.nan\n            result[self._isnan] = fill_value\n        return result\n\n    def fillna(self, value=None, method=None, limit=None):\n        # TODO(GH-20300): remove this\n        # Just overriding to ensure that we avoid an astype(object).\n        # Either 20300 or a `_values_for_fillna` would avoid this duplication.\n        if isinstance(value, ABCSeries):\n            value = value.array\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\" expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                if method == \"pad\":\n                    func = missing.pad_1d\n                else:\n                    func = missing.backfill_1d\n\n                values = self._data\n                if not is_period_dtype(self):\n                    # For PeriodArray self._data is i8, which gets copied\n                    #  by `func`.  Otherwise we need to make a copy manually\n                    # to avoid modifying `self` in-place.\n                    values = values.copy()\n\n                new_values = func(values, limit=limit, mask=mask)\n                if is_datetime64tz_dtype(self):\n                    # we need to pass int64 values to the constructor to avoid\n                    #  re-localizing incorrectly\n                    new_values = new_values.view(\"i8\")\n                new_values = type(self)(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n        else:\n            new_values = self.copy()\n        return new_values\n    # ------------------------------------------------------------------\n    # Frequency Properties/Methods\n        Return the frequency object if it is set, otherwise None.\n        return self._freq\n\n    @freq.setter\n    def freq(self, value):\n        if value is not None:\n            value = frequencies.to_offset(value)\n            self._validate_frequency(self, value)\n\n        self._freq = value\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def inferred_freq(self):\n        \"\"\"\n        Tryies to return a string representing a frequency guess,\n        generated by infer_freq.  Returns None if it can't autodetect the\n        frequency.\n        \"\"\"\n        if self.ndim != 1:\n            return None\n        try:\n            return frequencies.infer_freq(self)\n        except ValueError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution(self):\n        return frequencies.Resolution.get_reso_from_freq(self.freqstr)\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def resolution(self):\n        \"\"\"\n        Returns day, hour, minute, second, millisecond or microsecond\n        \"\"\"\n        return frequencies.Resolution.get_str(self._resolution)\n\n    @classmethod\n    def _validate_frequency(cls, index, freq, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        if is_period_dtype(cls):\n            # Frequency validation is not meaningful for Period Array/Index\n            return None\n\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(\n                start=index[0], end=None, periods=len(index), freq=freq, **kwargs\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as e:\n            if \"non-fixed\" in str(e):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise e\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            ) from e\n\n    # monotonicity/uniqueness properties are called via frequencies.infer_freq,\n    #  see GH#23789\n\n    @property\n    def _is_monotonic_increasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[0]\n\n    @property\n    def _is_monotonic_decreasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[1]\n\n    @property\n    def _is_unique(self):\n        return len(unique1d(self.asi8)) == len(self)\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n    _create_comparison_method = classmethod(_datetimelike_array_cmp)\n\n    # pow is invalid for all three subclasses; TimedeltaArray will override\n    #  the multiplication and division ops\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n\n    def _add_datetimelike_scalar(self, other):\n        # Overridden by TimedeltaArray\n        raise TypeError(f\"cannot add {type(self).__name__} and {type(other).__name__}\")\n\n    _add_datetime_arraylike = _add_datetimelike_scalar\n\n    def _sub_datetimelike_scalar(self, other):\n        # Overridden by DatetimeArray\n        assert other is not NaT\n        raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n    _sub_datetime_arraylike = _sub_datetimelike_scalar\n\n    def _sub_period(self, other):\n        # Overridden by PeriodArray\n        raise TypeError(f\"cannot subtract Period from a {type(self).__name__}\")\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Add a delta of a timedeltalike\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\n            new_values = np.empty(self.shape, dtype=\"i8\")\n            new_values[:] = iNaT\n            return type(self)(new_values, dtype=self.dtype)\n\n        inc = delta_to_nanoseconds(other)\n        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(\n            \"i8\"\n        )\n        new_values = self._maybe_mask_results(new_values)\n\n        new_freq = None\n        if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):\n            # adding a scalar preserves freq\n            new_freq = self.freq\n\n        return type(self)(new_values, dtype=self.dtype, freq=new_freq)\n    def _add_timedelta_arraylike(self, other):\n        \"\"\"\n        Add a delta of a TimedeltaIndex\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # overridden by PeriodArray\n\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n        if isinstance(other, np.ndarray):\n            # ndarray[timedelta64]; wrap in TimedeltaIndex for op\n            from pandas.core.arrays import TimedeltaArray\n            other = TimedeltaArray._from_sequence(other)\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        new_values = checked_add_with_arr(\n            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = iNaT\n        return type(self)(new_values, dtype=self.dtype)\n    def _add_nat(self):\n        Add pd.NaT to self\n        \"\"\"\n        if is_period_dtype(self):\n            raise TypeError(\n                f\"Cannot add {type(self).__name__} and {type(NaT).__name__}\"\n            )\n\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return type(self)(result, dtype=self.dtype, freq=None)\n    def _sub_nat(self):\n        \"\"\"\n        Subtract pd.NaT from self\n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return result.view(\"timedelta64[ns]\")\n\n    def _sub_period_array(self, other):\n        \"\"\"\n        Subtract a Period Array/Index from self.  This is only valid if self\n        is itself a Period Array/Index, raises otherwise.  Both objects must\n        have the same frequency.\n        other : PeriodIndex or PeriodArray\n\n        Returns\n        -------\n        result : np.ndarray[object]\n            Array of DateOffset objects; nulls represented by NaT.\n        \"\"\"\n        if not is_period_dtype(self):\n            raise TypeError(\n                f\"cannot subtract {other.dtype}-dtype from {type(self).__name__}\"\n            )\n        if self.freq != other.freq:\n            msg = DIFFERENT_FREQ.format(\n                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr\n            )\n            raise IncompatibleFrequency(msg)\n        new_values = checked_add_with_arr(\n            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        new_values = np.array([self.freq.base * x for x in new_values])\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = NaT\n        return new_values\n    def _addsub_object_array(self, other: np.ndarray, op):\n        \"\"\"\n        Add or subtract array-like of DateOffset objects\n        Parameters\n        ----------\n        other : np.ndarray[object]\n        op : {operator.add, operator.sub}\n        result : same class as self\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1:\n            return op(self, other[0])\n\n        warnings.warn(\n            \"Adding/subtracting array of DateOffsets to \"\n            f\"{type(self).__name__} not vectorized\",\n            PerformanceWarning,\n        )\n        # Caller is responsible for broadcasting if necessary\n        assert self.shape == other.shape, (self.shape, other.shape)\n\n        res_values = op(self.astype(\"O\"), np.array(other))\n        result = array(res_values.ravel())\n        result = extract_array(result, extract_numpy=True).reshape(self.shape)\n        return result\n    def _time_shift(self, periods, freq=None):\n        \"\"\"\n        Shift each value by `periods`.\n\n        Note this is different from ExtensionArray.shift, which\n        shifts the *position* of each element, padding the end with\n        missing values.\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift by.\n        freq : pandas.DateOffset, pandas.Timedelta, or str\n            Frequency increment to shift by.\n        \"\"\"\n        if freq is not None and freq != self.freq:\n            if isinstance(freq, str):\n                freq = frequencies.to_offset(freq)\n            offset = periods * freq\n            result = self + offset\n        if periods == 0:\n            # immutable so OK\n            return self.copy()\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n        start = self[0] + periods * self.freq\n        end = self[-1] + periods * self.freq\n        # Note: in the DatetimeTZ case, _generate_range will infer the\n        #  appropriate timezone from `start` and `end`, so tz does not need\n        #  to be passed explicitly.\n        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n        # scalar others\n        if other is NaT:\n            result = self._add_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._add_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.add)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            return self._add_datetime_arraylike(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.add)\n        else:\n            # Includes Categorical, other ExtensionArrays\n            # For PeriodDtype, if self is a TimedeltaArray and other is a\n            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this\n            #  operation is valid.  Defer to the PeriodArray implementation.\n            #  In remaining cases, this will end up raising TypeError.\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray(result)\n        return result\n    def __radd__(self, other):\n        # alias for __add__\n        return self.__add__(other)\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n        # scalar others\n        if other is NaT:\n            result = self._sub_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(-other)\n        elif isinstance(other, DateOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(-other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._sub_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(-other)\n\n        elif isinstance(other, Period):\n            result = self._sub_period(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(-other)\n        elif is_object_dtype(other):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.sub)\n        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):\n            # DatetimeIndex, ndarray[datetime64]\n            result = self._sub_datetime_arraylike(other)\n        elif is_period_dtype(other):\n            # PeriodIndex\n            result = self._sub_period_array(other)\n        elif is_integer_dtype(other):\n            if not is_period_dtype(self):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.sub)\n            # Includes ExtensionArrays, float_dtype\n            return NotImplemented\n        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):\n            from pandas.core.arrays import TimedeltaArray\n            return TimedeltaArray(result)\n        return result\n    def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):\n            # ndarray[datetime64] cannot be subtracted from self, so\n            # we need to wrap in DatetimeArray/Index and flip the operation\n            if lib.is_scalar(other):\n                # i.e. np.datetime64 object\n                return Timestamp(other) - self\n            if not isinstance(other, DatetimeLikeArrayMixin):\n                # Avoid down-casting DatetimeIndex\n                from pandas.core.arrays import DatetimeArray\n\n                other = DatetimeArray(other)\n            return other - self\n        elif (\n            is_datetime64_any_dtype(self.dtype)\n            and hasattr(other, \"dtype\")\n            and not is_datetime64_any_dtype(other.dtype)\n        ):\n            # GH#19959 datetime - datetime is well-defined as timedelta,\n            # but any other type - datetime is not well-defined.\n            raise TypeError(\n                f\"cannot subtract {type(self).__name__} from {type(other).__name__}\"\n            )\n        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):\n            # TODO: Can we simplify/generalize these cases at all?\n            raise TypeError(f\"cannot subtract {type(self).__name__} from {other.dtype}\")\n        elif is_timedelta64_dtype(self.dtype):\n            if lib.is_integer(other) or is_integer_dtype(other):\n                # need to subtract before negating, since that flips freq\n                # -self flips self.freq, messing up results\n                return -(self - other)\n\n            return (-self) + other\n\n        return -(self - other)\n\n    def __iadd__(self, other):\n        result = self + other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    def __isub__(self, other):\n        result = self - other\n        self[:] = result[:]\n\n        if not is_period_dtype(self):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def _reduce(self, name, axis=0, skipna=True, **kwargs):\n        op = getattr(self, name, None)\n        if op:\n            return op(skipna=skipna, **kwargs)\n            return super()._reduce(name, skipna, **kwargs)\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Array or minimum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\n        if isna(result):\n            # Period._from_ordinal does not handle np.nan gracefully\n            return NaT\n        return self._box_func(result)\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Array or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        # TODO: skipna is broken with max.\n        # See https://github.com/pandas-dev/pandas/issues/24265\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        mask = self.isna()\n        if skipna:\n            values = self[~mask].asi8\n        elif mask.any():\n            return NaT\n        else:\n            values = self.asi8\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n        result = nanops.nanmax(values, skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n    def mean(self, skipna=True):\n        Return the mean value of the Array.\n\n        .. versionadded:: 0.25.0\n        skipna : bool, default True\n            Whether to ignore any NaT elements.\n        scalar\n            Timestamp or Timedelta.\n\n        See Also\n        --------\n        numpy.ndarray.mean : Returns the average of array elements along a given axis.\n        Series.mean : Return the mean value in a Series.\n\n        Notes\n        -----\n        mean is only defined for Datetime and Timedelta dtypes, not for Period.\n        \"\"\"\n        if is_period_dtype(self):\n            # See discussion in GH#24757\n            raise TypeError(\n                f\"mean is not implemented for {type(self).__name__} since the \"\n                \"meaning is ambiguous.  An alternative is \"\n                \"obj.to_timestamp(how='start').mean()\"\n            )\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask]\n        elif mask.any():\n            return NaT\n        else:\n            values = self\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmean(values.view(\"i8\"), skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n\nDatetimeLikeArrayMixin._add_comparison_ops()\n\n# -------------------------------------------------------------------\n# Shared Constructor Helpers\n\n\ndef validate_periods(periods):\n    \"\"\"\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\n    constructor, cast it to an integer.\n\n    Parameters\n    ----------\n    periods : None, float, int\n\n    Returns\n    -------\n    periods : None or int\n\n    Raises\n    ------\n    TypeError\n        if periods is None, float, or int\n    \"\"\"\n    if periods is not None:\n        if lib.is_float(periods):\n            periods = int(periods)\n        elif not lib.is_integer(periods):\n            raise TypeError(f\"periods must be a number, got {periods}\")\n    return periods\n\n\ndef validate_endpoints(closed):\n    \"\"\"\n    Check that the `closed` argument is among [None, \"left\", \"right\"]\n\n    Parameters\n    ----------\n    closed : {None, \"left\", \"right\"}\n\n    Returns\n    -------\n    left_closed : bool\n    right_closed : bool\n\n    Raises\n    ------\n    ValueError : if argument is not among valid values\n    \"\"\"\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == \"left\":\n        left_closed = True\n    elif closed == \"right\":\n        right_closed = True\n    else:\n        raise ValueError(\"Closed has to be either 'left', 'right' or None\")\n\n    return left_closed, right_closed\n\n\ndef validate_inferred_freq(freq, inferred_freq, freq_infer):\n    \"\"\"\n    If the user passes a freq and another freq is inferred from passed data,\n    require that they match.\n\n    Parameters\n    ----------\n    freq : DateOffset or None\n    inferred_freq : DateOffset or None\n    freq_infer : bool\n\n    Returns\n    -------\n    freq : DateOffset or None\n    freq_infer : bool\n\n    Notes\n    -----\n    We assume at this point that `maybe_infer_freq` has been called, so\n    `freq` is either a DateOffset object or None.\n    \"\"\"\n    if inferred_freq is not None:\n        if freq is not None and freq != inferred_freq:\n            raise ValueError(\n                f\"Inferred frequency {inferred_freq} from passed \"\n                \"values does not conform to passed frequency \"\n                f\"{freq.freqstr}\"\n            )\n        elif freq is None:\n            freq = inferred_freq\n        freq_infer = False\n\n    return freq, freq_infer\ndef maybe_infer_freq(freq):\n    \"\"\"\n    Comparing a DateOffset to the string \"infer\" raises, so we need to\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\n    signify the case where the given freq is \"infer\" and set freq to None\n    to avoid comparison trouble later on.\n\n    Parameters\n    ----------\n    freq : {DateOffset, None, str}\n\n    Returns\n    -------\n    freq : {DateOffset, None}\n    freq_infer : bool\n        Whether we should inherit the freq of passed data.\n    \"\"\"\n    freq_infer = False\n    if not isinstance(freq, DateOffset):\n        # if a passed freq is None, don't infer automatically\n        if freq != \"infer\":\n            freq = frequencies.to_offset(freq)\n        else:\n            freq_infer = True\n            freq = None\n    return freq, freq_infer", "fixed_code": "\"\"\"\nBase and utility classes for tseries type pandas objects.\n\"\"\"\nfrom typing import Any, List, Optional, Union, cast\nfrom pandas._libs import NaT, iNaT, join as libjoin, lib\nfrom pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n    ensure_int64,\n    is_bool_dtype,\n    is_integer,\n    is_scalar,\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import algorithms\nfrom pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\nfrom pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\nfrom pandas.core.base import IndexOpsMixin\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas.core.indexes.extension import (\n    ExtensionIndex,\n    inherit_names,\n    make_wrapped_arith_op,\n)\nfrom pandas.core.indexes.numeric import Int64Index\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.tools.timedeltas import to_timedelta\nfrom pandas.tseries.frequencies import DateOffset\nfrom pandas.tseries.offsets import Tick\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\ndef _join_i8_wrapper(joinf, with_indexers: bool = True):\n    \"\"\"\n    Create the join wrapper methods.\n    \"\"\"\n    @staticmethod  # type: ignore\n    def wrapper(left, right):\n        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            left = left.view(\"i8\")\n        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            right = right.view(\"i8\")\n        results = joinf(left, right)\n        if with_indexers:\n            # dtype should be timedelta64[ns] for TimedeltaIndex\n            #  and datetime64[ns] for DatetimeIndex\n            dtype = left.dtype.base\n            join_index, left_indexer, right_indexer = results\n            join_index = join_index.view(dtype)\n            return join_index, left_indexer, right_indexer\n        return results\n    return wrapper\ndef _make_wrapped_arith_op_with_freq(opname: str):\n    \"\"\"\n    Dispatch the operation to the underlying ExtensionArray, and infer\n    the appropriate frequency for the result.\n    \"\"\"\n    meth = make_wrapped_arith_op(opname)\n    def wrapped(self, other):\n        result = meth(self, other)\n        if result is NotImplemented:\n            return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n        result._freq = new_freq\n        return result\n    wrapped.__name__ = opname\n    return wrapped\n@inherit_names(\n    [\"inferred_freq\", \"_isnan\", \"_resolution\", \"resolution\"],\n    DatetimeLikeArrayMixin,\n    cache=True,\n)\n@inherit_names(\n    [\"mean\", \"asi8\", \"_box_func\"], DatetimeLikeArrayMixin,\n)\nclass DatetimeIndexOpsMixin(ExtensionIndex):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]\n    freq: Optional[DateOffset]\n    freqstr: Optional[str]\n    _resolution: int\n    _bool_ops: List[str] = []\n    _field_ops: List[str] = []\n    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore\n    _hasnans = hasnans  # for index / array -agnostic code\n    @property\n    def is_all_dates(self) -> bool:\n        return True\n    # ------------------------------------------------------------------------\n    # Abstract data attributes\n    def values(self):\n        # Note: PeriodArray overrides this to return an ndarray of objects.\n        return self._data._data\n    def __array_wrap__(self, result, context=None):\n        Gets called after a ufunc.\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n        attrs = self._get_attributes_dict()\n        if not is_period_dtype(self) and attrs[\"freq\"]:\n            # no need to infer if freq is None\n            attrs[\"freq\"] = \"infer\"\n        return Index(result, **attrs)\n    # ------------------------------------------------------------------------\n    def equals(self, other) -> bool:\n        Determines if two Index objects contain the same elements.\n        if self.is_(other):\n            return True\n        if not isinstance(other, ABCIndexClass):\n            return False\n        elif not isinstance(other, type(self)):\n            try:\n                other = type(self)(other)\n            except (ValueError, TypeError, OverflowError):\n                # e.g.\n                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime\n                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex\n                #  OverflowError -> Index([very_large_timedeltas])\n                return False\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            # have different timezone\n            return False\n\n        return np.array_equal(self.asi8, other.asi8)\n\n    @Appender(Index.__contains__.__doc__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            res = self.get_loc(key)\n        except (KeyError, TypeError, ValueError):\n            return False\n        return bool(\n            is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))\n        )\n    def sort_values(self, return_indexer=False, ascending=True):\n        Return sorted copy of Index.\n        if return_indexer:\n            _as = self.argsort()\n            if not ascending:\n                _as = _as[::-1]\n            sorted_index = self.take(_as)\n            return sorted_index, _as\n        else:\n            # NB: using asi8 instead of _data matters in numpy 1.18\n            #  because the treatment of NaT has been changed to put NaT last\n            #  instead of first.\n            sorted_values = np.sort(self.asi8)\n            freq = self.freq\n            if freq is not None and not is_period_dtype(self):\n                if freq.n > 0 and not ascending:\n                    freq = freq * -1\n                elif freq.n < 0 and ascending:\n                    freq = freq * -1\n            if not ascending:\n                sorted_values = sorted_values[::-1]\n            arr = type(self._data)._simple_new(\n                sorted_values, dtype=self.dtype, freq=freq\n            )\n            return type(self)._simple_new(arr, name=self.name)\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_int64(indices)\n        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))\n        if isinstance(maybe_slice, slice):\n            return self[maybe_slice]\n        return ExtensionIndex.take(\n            self, indices, axis, allow_fill, fill_value, **kwargs\n        )\n    @doc(IndexOpsMixin.searchsorted, klass=\"Datetime-like Index\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, str):\n            raise TypeError(\n                \"searchsorted requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n        if isinstance(value, Index):\n            value = value._data\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n    _can_hold_na = True\n    _na_value = NaT\n    \"\"\"The expected NA value to use with this index.\"\"\"\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n    def tolist(self) -> List:\n        \"\"\"\n        Return a list of the underlying data.\n        \"\"\"\n        return list(self.astype(object))\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Index or minimum along\n        an axis.\n        See Also\n        numpy.ndarray.min\n        Series.min : Return the minimum value in a Series.\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[0] != iNaT:\n                    return self._box_func(i8[0])\n\n            if self.hasnans:\n                if skipna:\n                    min_stamp = self[~self._isnan].asi8.min()\n                else:\n                    return self._na_value\n            else:\n                min_stamp = i8.min()\n            return self._box_func(min_stamp)\n        except ValueError:\n            return self._na_value\n    def argmin(self, axis=None, skipna=True, *args, **kwargs):\n        Returns the indices of the minimum values along an axis.\n        See `numpy.ndarray.argmin` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmin\n        nv.validate_argmin(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = np.iinfo(\"int64\").max\n        return i8.argmin()\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        Return the maximum value of the Index or maximum along\n        an axis.\n        See Also\n        --------\n        numpy.ndarray.max\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        if not len(self):\n            return self._na_value\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[-1] != iNaT:\n                    return self._box_func(i8[-1])\n\n            if self.hasnans:\n                if skipna:\n                    max_stamp = self[~self._isnan].asi8.max()\n                else:\n                    return self._na_value\n            else:\n                max_stamp = i8.max()\n            return self._box_func(max_stamp)\n        except ValueError:\n            return self._na_value\n    def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        Returns the indices of the maximum values along an axis.\n        See `numpy.ndarray.argmax` for more information on the\n        `axis` parameter.\n        See Also\n        --------\n        numpy.ndarray.argmax\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = 0\n        return i8.argmax()\n    # --------------------------------------------------------------------\n    # Rendering Methods\n    def _format_with_header(self, header, na_rep=\"NaT\", **kwargs):\n        return header + list(self._format_native_types(na_rep, **kwargs))\n    def _formatter_func(self):\n    def _format_attrs(self):\n        Return a list of tuples of the (attr,formatted_value).\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            if attrib == \"freq\":\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)\n                attrs.append((\"freq\", freq))\n        return attrs\n    # --------------------------------------------------------------------\n    # Indexing Methods\n    def _validate_partial_date_slice(self, reso: str):\n        raise NotImplementedError\n    def _parsed_string_to_bounds(self, reso: str, parsed: datetime):\n        raise NotImplementedError\n    def _partial_date_slice(\n        self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True\n    ):\n        Parameters\n        ----------\n        reso : str\n        parsed : datetime\n        use_lhs : bool, default True\n        use_rhs : bool, default True\n        slice or ndarray[intp]\n        self._validate_partial_date_slice(reso)\n        t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n        i8vals = self.asi8\n        unbox = self._data._unbox_scalar\n        if self.is_monotonic:\n            if len(self) and (\n                (use_lhs and t1 < self[0] and t2 < self[0])\n                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))\n            ):\n                # we are out of range\n                raise KeyError\n            # TODO: does this depend on being monotonic _increasing_?\n            # a monotonic (sorted) series can be sliced\n            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps\n            left = i8vals.searchsorted(unbox(t1), side=\"left\") if use_lhs else None\n            right = i8vals.searchsorted(unbox(t2), side=\"right\") if use_rhs else None\n            return slice(left, right)\n            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True\n            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True\n            # try to find the dates\n            return (lhs_mask & rhs_mask).nonzero()[0]\n    # --------------------------------------------------------------------\n    # Arithmetic Methods\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n        \"\"\"\n        Find the freq we expect the result of an addition/subtraction operation\n        to have.\n        \"\"\"\n        if is_period_dtype(self.dtype):\n            if is_period_dtype(result.dtype):\n                # Only used for ops that stay PeriodDtype\n                return self.freq\n            return None\n        elif self.freq is None:\n            return None\n        elif lib.is_scalar(other) and isna(other):\n            return None\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            new_freq = None\n            if isinstance(self.freq, Tick):\n                new_freq = self.freq\n            return new_freq\n        elif isinstance(other, DateOffset):\n            # otherwise just DatetimeArray\n            return None  # TODO: Should we infer if it matches self.freq * n?\n        elif isinstance(other, (datetime, np.datetime64)):\n            return self.freq\n        elif is_timedelta64_dtype(other):\n            return None  # TODO: shouldnt we be able to do self.freq + other.freq?\n        elif is_object_dtype(other):\n            return None  # TODO: is this quite right?  sometimes we unpack singletons\n        elif is_datetime64_any_dtype(other):\n            return None  # TODO: shouldnt we be able to do self.freq + other.freq?\n        else:\n            raise NotImplementedError\n\n    __add__ = _make_wrapped_arith_op_with_freq(\"__add__\")\n    __sub__ = _make_wrapped_arith_op_with_freq(\"__sub__\")\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Compute boolean array of whether each index value is found in the\n        passed set of values.\n        values : set or sequence of values\n        is_contained : ndarray (boolean dtype)\n        if level is not None:\n            self._validate_index_level(level)\n        if not isinstance(values, type(self)):\n                values = type(self)(values)\n            except ValueError:\n                return self.astype(object).isin(values)\n        return algorithms.isin(self.asi8, values.asi8)\n    @Appender(Index.where.__doc__)\n    def where(self, cond, other=None):\n        values = self.view(\"i8\")\n        try:\n            other = self._data._validate_where_value(other)\n        except (TypeError, ValueError) as err:\n            # Includes tzawareness mismatch and IncompatibleFrequencyError\n            oth = getattr(other, \"dtype\", other)\n            raise TypeError(f\"Where requires matching dtype, not {oth}\") from err\n        result = np.where(cond, values, other).astype(\"i8\")\n        arr = type(self._data)._simple_new(result, dtype=self.dtype)\n        return type(self)._simple_new(arr, name=self.name)\n    def _summary(self, name=None) -> str:\n        Return a summarized representation.\n        name : str\n            Name to use in the summary representation.\n        str\n            Summarized representation of the index.\n        formatter = self._formatter_func\n        if len(self) > 0:\n            index_summary = f\", {formatter(self[0])} to {formatter(self[-1])}\"\n        else:\n            index_summary = \"\"\n        if name is None:\n            name = type(self).__name__\n        result = f\"{name}: {len(self)} entries{index_summary}\"\n        if self.freq:\n            result += f\"\\nFreq: {self.freqstr}\"\n        # display as values, not quoted\n        result = result.replace(\"'\", \"\")\n        return result\n\n    def shift(self, periods=1, freq=None):\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n            .. versionchanged:: 0.24.0\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        \"\"\"\n        arr = self._data.view()\n        arr._freq = self.freq\n        result = arr._time_shift(periods, freq=freq)\n        return type(self)(result, name=self.name)\n    # --------------------------------------------------------------------\n    # List-like Methods\n    def delete(self, loc):\n        new_i8s = np.delete(self.asi8, loc)\n        freq = None\n        if is_period_dtype(self):\n            freq = self.freq\n        elif is_integer(loc):\n            if loc in (0, -len(self), -1, len(self) - 1):\n                freq = self.freq\n        else:\n            if is_list_like(loc):\n                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))\n            if isinstance(loc, slice) and loc.step in (1, None):\n                if loc.start in (0, None) or loc.stop in (len(self), None):\n                    freq = self.freq\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n    # --------------------------------------------------------------------\n    # Join/Set Methods\n    def _wrap_joined_index(self, joined: np.ndarray, other):\n        assert other.dtype == self.dtype, (other.dtype, self.dtype)\n        name = get_op_result_name(self, other)\n        if is_period_dtype(self.dtype):\n            freq = self.freq\n        else:\n            self = cast(DatetimeTimedeltaMixin, self)\n            freq = self.freq if self._can_fast_union(other) else None\n        new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(new_data, name=name)\nclass DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n    \"\"\"\n    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,\n    but not PeriodIndex\n    \"\"\"\n    # Compat for frequency inference, see GH#23789\n    _is_monotonic_increasing = Index.is_monotonic_increasing\n    _is_monotonic_decreasing = Index.is_monotonic_decreasing\n    _is_unique = Index.is_unique\n    _freq = lib.no_default\n        In limited circumstances, our freq may differ from that of our _data.\n        if self._freq is not lib.no_default:\n            return self._freq\n        return self._data.freq\n    def _with_freq(self, freq):\n        arr = self._data._with_freq(freq)\n        return type(self)._simple_new(arr, name=self.name)\n    def _shallow_copy(self, values=None, name: Label = lib.no_default):\n        name = self.name if name is lib.no_default else name\n        cache = self._cache.copy() if values is None else {}\n        if values is None:\n            values = self._data\n        if isinstance(values, np.ndarray):\n            # TODO: We would rather not get here\n            values = type(self._data)(values, dtype=self.dtype)\n        result = type(self)._simple_new(values, name=name)\n        result._cache = cache\n        return result\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n    @Appender(Index.difference.__doc__)\n    def difference(self, other, sort=None):\n        new_idx = super().difference(other, sort=sort)._with_freq(None)\n        return new_idx\n    def intersection(self, other, sort=False):\n        Specialized intersection for DatetimeIndex/TimedeltaIndex.\n        May be much faster than Index.intersection\n        other : Same type as self or array-like\n        sort : False or None, default False\n            Sort the resulting index if possible.\n            .. versionadded:: 0.24.0\n            .. versionchanged:: 0.24.1\n               Changed the default to ``False`` to match the behaviour\n               from before 0.24.0.\n            .. versionchanged:: 0.25.0\n               The `sort` keyword is added\n        y : Index or same type as self\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n        if len(self) == 0:\n            return self.copy()\n        if len(other) == 0:\n            return other.copy()\n\n        if not isinstance(other, type(self)):\n            result = Index.intersection(self, other, sort=sort)\n            if isinstance(result, type(self)):\n                if result.freq is None:\n                    result = result._with_freq(\"infer\")\n            return result\n        elif (\n            other.freq is None\n            or self.freq is None\n            or other.freq != self.freq\n            or not other.freq.is_anchored()\n            or (not self.is_monotonic or not other.is_monotonic)\n        ):\n            result = Index.intersection(self, other, sort=sort)\n            result = result._with_freq(\"infer\")\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        # after sorting, the intersection always starts with the right index\n        # and ends with the index of which the last elements is smallest\n        end = min(left[-1], right[-1])\n        start = right[0]\n        if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n        else:\n            lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n            return self._shallow_copy(left_chunk)\n    def _can_fast_union(self, other) -> bool:\n        if not isinstance(other, type(self)):\n            return False\n        freq = self.freq\n        if freq is None or freq != other.freq:\n            return False\n        if not self.is_monotonic or not other.is_monotonic:\n            return False\n        if len(self) == 0 or len(other) == 0:\n            return True\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n        right_start = right[0]\n        left_end = left[-1]\n        # Only need to \"adjoin\", not overlap\n        try:\n            return (right_start == left_end + freq) or right_start in left\n        except ValueError:\n            # if we are comparing a freq that does not propagate timezones\n            # this will raise\n            return False\n\n    def _fast_union(self, other, sort=None):\n        if len(other) == 0:\n            return self.view(type(self))\n\n        if len(self) == 0:\n            return other.view(type(self))\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        elif sort is False:\n            # TDIs are not in the \"correct\" order and we don't want\n            #  to sort but want to remove overlaps\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right._values[:loc]\n            dates = concat_compat((left._values, right_chunk))\n            # TODO: can we infer that it has self.freq?\n            result = self._shallow_copy(dates)._with_freq(\"infer\")\n            return result\n            left, right = other, self\n\n        left_end = left[-1]\n        right_end = right[-1]\n\n        # concatenate\n        if left_end < right_end:\n            loc = right.searchsorted(left_end, side=\"right\")\n            right_chunk = right._values[loc:]\n            dates = concat_compat([left._values, right_chunk])\n            # TODO: can we infer that it has self.freq?\n            result = self._shallow_copy(dates)._with_freq(\"infer\")\n            return result\n        else:\n            return left\n    def _union(self, other, sort):\n        if not len(other) or self.equals(other) or not len(self):\n            return super()._union(other, sort=sort)\n        # We are called by `union`, which is responsible for this validation\n        assert isinstance(other, type(self))\n        this, other = self._maybe_utc_convert(other)\n\n        if this._can_fast_union(other):\n            result = this._fast_union(other, sort=sort)\n            if result.freq is None:\n                result = result._with_freq(\"infer\")\n            return result\n            i8self = Int64Index._simple_new(self.asi8, name=self.name)\n            i8other = Int64Index._simple_new(other.asi8, name=other.name)\n            i8result = i8self._union(i8other, sort=sort)\n            result = type(self)(i8result, dtype=self.dtype, freq=\"infer\")\n            return result\n    # --------------------------------------------------------------------\n    # Join Methods\n    _join_precedence = 10\n    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)\n    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)\n    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)\n    _left_indexer_unique = _join_i8_wrapper(\n        libjoin.left_join_indexer_unique, with_indexers=False\n    )\n\n    def join(\n        self, other, how: str = \"left\", level=None, return_indexers=False, sort=False\n    ):\n        See Index.join\n        \"\"\"\n        if self._is_convertible_to_index_for_join(other):\n            try:\n                other = type(self)(other)\n            except (TypeError, ValueError):\n                pass\n        this, other = self._maybe_utc_convert(other)\n        return Index.join(\n            this,\n            other,\n            how=how,\n            level=level,\n            return_indexers=return_indexers,\n            sort=sort,\n        )\n    def _maybe_utc_convert(self, other):\n        this = self\n        if not hasattr(self, \"tz\"):\n            return this, other\n        if isinstance(other, type(self)):\n            if self.tz is not None:\n                if other.tz is None:\n                    raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            elif other.tz is not None:\n                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            if not timezones.tz_compare(self.tz, other.tz):\n                this = self.tz_convert(\"UTC\")\n                other = other.tz_convert(\"UTC\")\n        return this, other\n    @classmethod\n    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:\n        \"\"\"\n        return a boolean whether I can attempt conversion to a\n        DatetimeIndex/TimedeltaIndex\n        \"\"\"\n        if isinstance(other, cls):\n            return False\n        elif len(other) > 0 and other.inferred_type not in (\n            \"floating\",\n            \"mixed-integer\",\n            \"integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n            \"mixed\",\n        ):\n            return True\n        return False\n    # --------------------------------------------------------------------\n    # List-Like Methods\n    def insert(self, loc, item):\n        Make new Index inserting new item at location\n        loc : int\n        item : object\n            if not either a Python datetime or a numpy integer-like, returned\n            Index dtype will be object rather than datetime.\n        new_index : Index\n        \"\"\"\n        if isinstance(item, str):\n            # TODO: Why are strings special?\n            # TODO: Should we attempt _scalar_from_string?\n            return self.astype(object).insert(loc, item)\n\n        item = self._data._validate_insert_value(item)\n\n        freq = None\n        # check freq can be preserved on edge cases\n        if self.freq is not None:\n            if self.size:\n                if item is NaT:\n                    pass\n                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\n                    freq = self.freq\n                elif (loc == len(self)) and item - self.freq == self[-1]:\n                    freq = self.freq\n            else:\n                # Adding a single item to an empty index may preserve freq\n                if self.freq.is_on_offset(item):\n                    freq = self.freq\n        item = self._data._unbox_scalar(item)\n        new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)", "description": ""}
{"id": "pandas-3", "project": "pandas", "bug_id": "3", "buggy_code": "assert isinstance(self.index, PeriodIndex)\n        assert isinstance(self.index, DatetimeIndex)", "fixed_code": "if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")", "description": ""}
{"id": "pandas-146", "project": "pandas", "bug_id": "146", "buggy_code": "if np.any(left_value != right_value):\n                    return False\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False", "fixed_code": "try:\n                    if np.any(left_value != right_value):\n                        return False\n                except TypeError as err:\n                    if \"Cannot compare tz-naive\" in str(err):\n                        # tzawareness compat failure, see GH#28507\n                        return False\n                    raise\n        return array_equivalent(\n            com.values_from_object(self), com.values_from_object(other)\n        )", "description": ""}
{"id": "pandas-12", "project": "pandas", "bug_id": "12", "buggy_code": "ensure_float64,\n        mat = numeric_df.values\n            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)\n            correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)\n            mat = ensure_float64(mat).T\n        mat = numeric_df.values\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)", "fixed_code": "mat = numeric_df.astype(float, copy=False).to_numpy()\n            correl = libalgos.nancorr(mat, minp=min_periods)\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n            mat = mat.T\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)", "description": ""}
{"id": "pandas-76", "project": "pandas", "bug_id": "76", "buggy_code": "except (TypeError, ValueError):", "fixed_code": "except (TypeError, ValueError, OverflowError):", "description": ""}
{"id": "pandas-82", "project": "pandas", "bug_id": "82", "buggy_code": "return np.dtype(\"M8[ns]\"), tslibs.iNaT", "fixed_code": "return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")", "description": ""}
{"id": "pandas-122", "project": "pandas", "bug_id": "122", "buggy_code": "# canonicalize block order, using a tuple combining the type\n        # name and then mgr_locs because there might be unconsolidated\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())", "fixed_code": "# canonicalize block order, using a tuple combining the mgr_locs\n        # then type name because there might be unconsolidated\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)", "description": ""}
{"id": "pandas-40", "project": "pandas", "bug_id": "40", "buggy_code": "from typing import TYPE_CHECKING, Optional, Tuple, Union\nfrom pandas._typing import FrameOrSeries\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort)\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\ndef _factorize_keys(lk, rk, sort=True):\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)", "fixed_code": "from typing import TYPE_CHECKING, Optional, Tuple, Union, cast\nfrom pandas._typing import ArrayLike, FrameOrSeries\n    is_categorical,\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n    \"\"\"\n    Encode left and right keys as enumerated types.\n\n    This is used to get the join indexers to be used when merging DataFrames.\n\n    Parameters\n    ----------\n    lk : array-like\n        Left key.\n    rk : array-like\n        Right key.\n    sort : bool, defaults to True\n        If True, the encoding is done such that the unique elements in the\n        keys are sorted.\n    how : {\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019}, default \u2018inner\u2019\n        Type of merge.\n\n    Returns\n    -------\n    array\n        Left (resp. right if called with `key='right'`) labels, as enumerated type.\n    array\n        Right (resp. left if called with `key='right'`) labels, as enumerated type.\n    int\n        Number of unique elements in union of left and right labels.\n\n    See Also\n    --------\n    merge : Merge DataFrame or named Series objects\n        with a database-style join.\n    algorithms.factorize : Encode the object as an enumerated type\n        or categorical variable.\n\n    Examples\n    --------\n    >>> lk = np.array([\"a\", \"c\", \"b\"])\n    >>> rk = np.array([\"a\", \"c\"])\n\n    Here, the unique values are `'a', 'b', 'c'`. With the default\n    `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:\n\n    >>> pd.core.reshape.merge._factorize_keys(lk, rk)\n    (array([0, 2, 1]), array([0, 2]), 3)\n\n    With the `sort=False`, the encoding will correspond to the order\n    in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:\n\n    >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)\n    (array([0, 1, 2]), array([0, 1]), 3)\n    \"\"\"\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n    if how == \"right\":\n        return rlab, llab, count", "description": ""}
{"id": "pandas-114", "project": "pandas", "bug_id": "114", "buggy_code": "s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            # GH 20882, 21257\n            # Unify Index and ExtensionArray treatment\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]", "fixed_code": "from pandas.core.construction import extract_array\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                # GH 20882, 21257\n                # First try to convert the key to a location\n                # If that fails, raise a KeyError if an integer\n                # index, otherwise, see if key is an integer, and\n                # try that\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                # if key is not a scalar, directly raise an error (the code below\n                # would convert to numpy arrays and raise later any way) - GH29926\n                raise InvalidIndexError(key)", "description": ""}
{"id": "pandas-113", "project": "pandas", "bug_id": "113", "buggy_code": "result = op(self._data, other)", "fixed_code": "@pytest.fixture(params=ALL_EA_INT_DTYPES)\ndef any_nullable_int_dtype(request):\n    \"\"\"\n    Parameterized fixture for any nullable integer dtype.\n\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    \"\"\"\n\n    return request.param\n\n\nfrom pandas.core.ops import invalid_comparison\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)", "description": ""}
{"id": "pandas-78", "project": "pandas", "bug_id": "78", "buggy_code": "result = Series(result, index=labels)", "fixed_code": "result = self._constructor_sliced(result, index=labels)", "description": ""}
{"id": "pandas-147", "project": "pandas", "bug_id": "147", "buggy_code": "elif tz is None:\n            try:\n                match = cls._match.match(string)\n                if match:\n                    d = match.groupdict()\n            except Exception:\n                # TODO(py3): Change this pass to `raise TypeError(msg) from e`\n                pass", "fixed_code": "if tz is None:\n            match = cls._match.match(string)\n            if match:\n                d = match.groupdict()\n                try:\n                except (KeyError, TypeError, ValueError) as err:\n                    # KeyError if maybe_get_tz tries and fails to get a\n                    #  pytz timezone (actually pytz.UnknownTimeZoneError).\n                    # TypeError if we pass a nonsense tz;\n                    # ValueError if we pass a unit other than \"ns\"\n                    raise TypeError(msg.format(string)) from err", "description": ""}
{"id": "pandas-2", "project": "pandas", "bug_id": "2", "buggy_code": "key = list(self._convert_key(key, is_setter=True))", "fixed_code": "key = list(self._convert_key(key, is_setter=True))\n        # GH 26989\n        # For series, unpacking key needs to result in the label.\n        # This is already the case for len(key) == 1; e.g. (1,)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)", "description": ""}
{"id": "pandas-13", "project": "pandas", "bug_id": "13", "buggy_code": "return _isna_ndarraylike(obj)\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n        return _isna_ndarraylike(np.asarray(obj))\n        return _isna_ndarraylike_old(obj)\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\n        return _isna_ndarraylike_old(np.asarray(obj))\ndef _isna_ndarraylike(obj):\n    values = getattr(obj, \"_values\", obj)\n    dtype = values.dtype\n\n    if is_extension_array_dtype(dtype):\n        result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n\n    elif needs_i8_conversion(dtype):\n        # this is the NaT pattern\n        result = values.view(\"i8\") == iNaT\n    else:\n        result = np.isnan(values)\n\n    # box\n    if isinstance(obj, ABCSeries):\n        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)\n\n    return result\ndef _isna_ndarraylike_old(obj):\n    if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n\n        result = ~np.isfinite(values)", "fixed_code": "return _isna_ndarraylike(obj, old=False)\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n        return _isna_ndarraylike(obj, old=True)\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)\n        return _isna_ndarraylike(np.asarray(obj), old=True)\ndef _isna_ndarraylike(obj, old: bool = False):\n    \"\"\"\n    Return an array indicating which values of the input array are NaN / NA.\n    Parameters\n    ----------\n    obj: array-like\n        The input array whose elements are to be checked.\n    old: bool\n        Whether or not to treat infinite values as NA.\n    Returns\n    -------\n    array-like\n        Array of boolean values denoting the NA status of each element.\n    \"\"\"\n    if is_extension_array_dtype(dtype):\n        if old:\n            result = values.isna() | (values == -np.inf) | (values == np.inf)\n        else:\n            result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=old)\n        if old:\n            result = ~np.isfinite(values)\n        else:\n            result = np.isnan(values)", "description": ""}
{"id": "pandas-5", "project": "pandas", "bug_id": "5", "buggy_code": "return multi_join_idx, lidx, ridx\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)", "fixed_code": "if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n\n\ndef test_join_multi_return_indexers():\n    # GH 34074\n\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)", "description": ""}
{"id": "pandas-140", "project": "pandas", "bug_id": "140", "buggy_code": "idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])", "fixed_code": "idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])", "description": ""}
{"id": "pandas-14", "project": "pandas", "bug_id": "14", "buggy_code": "if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):\n        # in particular case where right is an array of DateOffsets", "fixed_code": "if (is_datetime64_dtype(ldtype) and is_object_dtype(rdtype)) or (\n        is_datetime64_dtype(rdtype) and is_object_dtype(ldtype)\n    ):\n        # in particular case where one is an array of DateOffsets\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    \"\"\"\n    A 3-tuple of names, the first two for operands, the last for a result.\n    \"\"\"\n    return request.param", "description": ""}
{"id": "pandas-22", "project": "pandas", "bug_id": "22", "buggy_code": "if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n        if self.is_freq_type:", "fixed_code": "\"count\",\n        # GH 32865. Using count with custom BaseIndexer subclass\n        # implementations shouldn't end up here\n        assert not isinstance(self.window, BaseIndexer)\n        # GH 32865. Use a custom count function implementation\n        # when using a BaseIndexer subclass as a window\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):", "description": ""}
{"id": "pandas-149", "project": "pandas", "bug_id": "149", "buggy_code": "from pandas.io.common import get_filepath_or_buffer, is_s3_url\n        if is_s3_url(path):\n            # path is s3:// so we need to open the s3file in 'wb' mode.\n            # And pass the opened s3file to the fastparquet internal impl.", "fixed_code": "from pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url\n        if is_s3_url(path) or is_gcs_url(path):\n            # if path is s3:// or gs:// we need to open the file in 'wb' mode.\n            # And pass the opened file to the fastparquet internal impl.", "description": ""}
{"id": "pandas-25", "project": "pandas", "bug_id": "25", "buggy_code": "sarray = fields.build_isocalendar_sarray(self.asi8)", "fixed_code": "if self.tz is not None and not timezones.is_utc(self.tz):\n            values = self._local_timestamps()\n        else:\n            values = self.asi8\n        sarray = fields.build_isocalendar_sarray(values)", "description": ""}
