[
  {
    "code": "def learn(env,\n          network,\n          seed=None,\n          lr=5e-4,\n          total_timesteps=100000,\n          buffer_size=50000,\n          exploration_fraction=0.1,\n          exploration_final_eps=0.02,\n          train_freq=1,\n          batch_size=32,\n          print_freq=100,\n          checkpoint_freq=10000,\n          checkpoint_path=None,\n          learning_starts=1000,\n          gamma=1.0,\n          target_network_update_freq=500,\n          prioritized_replay=False,\n          prioritized_replay_alpha=0.6,\n          prioritized_replay_beta0=0.4,\n          prioritized_replay_beta_iters=None,\n          prioritized_replay_eps=1e-6,\n          param_noise=False,\n          callback=None,\n          load_path=None,\n          **network_kwargs\n            ):\n    \"\"\"Train a deepq model.\n\n    Parameters\n    -------\n    env: gym.Env\n        environment to train on\n    network: string or a function\n        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n    seed: int or None\n        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n    lr: float\n        learning rate for adam optimizer\n    total_timesteps: int\n        number of env steps to optimizer for\n    buffer_size: int\n        size of the replay buffer\n    exploration_fraction: float\n        fraction of entire training period over which the exploration rate is annealed\n    exploration_final_eps: float\n        final value of random action probability\n    train_freq: int\n        update the model every `train_freq` steps.\n        set to None to disable printing\n    batch_size: int\n        size of a batched sampled from replay buffer for training\n    print_freq: int\n        how often to print out training progress\n        set to None to disable printing\n    checkpoint_freq: int\n        how often to save the model. This is so that the best version is restored\n        at the end of the training. If you do not wish to restore the best version at\n        the end of the training set this variable to None.\n    learning_starts: int\n        how many steps of the model to collect transitions for before learning starts\n    gamma: float\n        discount factor\n    target_network_update_freq: int\n        update the target network every `target_network_update_freq` steps.\n    prioritized_replay: True\n        if True prioritized replay buffer will be used.\n    prioritized_replay_alpha: float\n        alpha parameter for prioritized replay buffer\n    prioritized_replay_beta0: float\n        initial value of beta for prioritized replay buffer\n    prioritized_replay_beta_iters: int\n        number of iterations over which beta will be annealed from initial value\n        to 1.0. If set to None equals to total_timesteps.\n    prioritized_replay_eps: float\n        epsilon to add to the TD errors when updating priorities.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    callback: (locals, globals) -> None\n        function called at every steps with state of the algorithm.\n        If callback returns true training stops.\n    load_path: str\n        path to load the model from. (default: None)\n    **network_kwargs\n        additional keyword arguments to pass to the network builder.\n\n    Returns\n    -------\n    act: ActWrapper\n        Wrapper over act function. Adds ability to save it and load it.\n        See header of baselines/deepq/categorical.py for details on the act function.\n    \"\"\"\n    # Create all the functions necessary to train the model\n\n    sess = get_session()\n    set_global_seeds(seed)\n\n    q_func = build_q_func(network, **network_kwargs)\n\n    # capture the shape outside the closure so that the env object is not serialized\n    # by cloudpickle when serializing make_obs_ph\n\n    observation_space = env.observation_space\n    def make_obs_ph(name):\n        return ObservationInput(observation_space, name=name)\n\n    act, train, update_target, debug = deepq.build_train(\n        make_obs_ph=make_obs_ph,\n        q_func=q_func,\n        num_actions=env.action_space.n,\n        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n        gamma=gamma,\n        grad_norm_clipping=10,\n        param_noise=param_noise\n    )\n\n    act_params = {\n        'make_obs_ph': make_obs_ph,\n        'q_func': q_func,\n        'num_actions': env.action_space.n,\n    }\n\n    act = ActWrapper(act, act_params)\n\n    # Create the replay buffer\n    if prioritized_replay:\n        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n        if prioritized_replay_beta_iters is None:\n            prioritized_replay_beta_iters = total_timesteps\n        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n                                       initial_p=prioritized_replay_beta0,\n                                       final_p=1.0)\n    else:\n        replay_buffer = ReplayBuffer(buffer_size)\n        beta_schedule = None\n    # Create the schedule for exploration starting from 1.\n    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n                                 initial_p=1.0,\n                                 final_p=exploration_final_eps)\n\n    # Initialize the parameters and copy them to the target network.\n    U.initialize()\n    update_target()\n\n    episode_rewards = [0.0]\n    saved_mean_reward = None\n    obs = env.reset()\n    reset = True\n\n    with tempfile.TemporaryDirectory() as td:\n        td = checkpoint_path or td\n\n        model_file = os.path.join(td, \"model\")\n        model_saved = False\n\n        if tf.train.latest_checkpoint(td) is not None:\n            load_variables(model_file)\n            logger.log('Loaded model from {}'.format(model_file))\n            model_saved = True\n        elif load_path is not None:\n            load_variables(load_path)\n            logger.log('Loaded model from {}'.format(load_path))\n\n\n        for t in range(total_timesteps):\n            if callback is not None:\n                if callback(locals(), globals()):\n                    break\n            # Take action and update exploration to the newest value\n            kwargs = {}\n            if not param_noise:\n                update_eps = exploration.value(t)\n                update_param_noise_threshold = 0.\n            else:\n                update_eps = 0.\n                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n                # for detailed explanation.\n                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))\n                kwargs['reset'] = reset\n                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n                kwargs['update_param_noise_scale'] = True\n            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n            env_action = action\n            reset = False\n            new_obs, rew, done, _ = env.step(env_action)\n            # Store transition in the replay buffer.\n            replay_buffer.add(obs, action, rew, new_obs, float(done))\n            obs = new_obs\n\n            episode_rewards[-1] += rew\n            if done:\n                obs = env.reset()\n                episode_rewards.append(0.0)\n                reset = True\n\n            if t > learning_starts and t % train_freq == 0:\n                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n                if prioritized_replay:\n                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n                else:\n                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n                    weights, batch_idxes = np.ones_like(rewards), None\n                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n                if prioritized_replay:\n                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n\n            if t > learning_starts and t % target_network_update_freq == 0:\n                # Update target network periodically.\n                update_target()\n\n            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n            num_episodes = len(episode_rewards)\n            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:\n                logger.record_tabular(\"steps\", t)\n                logger.record_tabular(\"episodes\", num_episodes)\n                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n                logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n                logger.dump_tabular()\n\n            if (checkpoint_freq is not None and t > learning_starts and\n                    num_episodes > 100 and t % checkpoint_freq == 0):\n                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n                    if print_freq is not None:\n                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n                                   saved_mean_reward, mean_100ep_reward))\n                    save_variables(model_file)\n                    model_saved = True\n                    saved_mean_reward = mean_100ep_reward\n        if model_saved:\n            if print_freq is not None:\n                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n            load_variables(model_file)\n\n    return act",
    "doc": "Train a deepq model.\n\n    Parameters\n    -------\n    env: gym.Env\n        environment to train on\n    network: string or a function\n        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n    seed: int or None\n        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n    lr: float\n        learning rate for adam optimizer\n    total_timesteps: int\n        number of env steps to optimizer for\n    buffer_size: int\n        size of the replay buffer\n    exploration_fraction: float\n        fraction of entire training period over which the exploration rate is annealed\n    exploration_final_eps: float\n        final value of random action probability\n    train_freq: int\n        update the model every `train_freq` steps.\n        set to None to disable printing\n    batch_size: int\n        size of a batched sampled from replay buffer for training\n    print_freq: int\n        how often to print out training progress\n        set to None to disable printing\n    checkpoint_freq: int\n        how often to save the model. This is so that the best version is restored\n        at the end of the training. If you do not wish to restore the best version at\n        the end of the training set this variable to None.\n    learning_starts: int\n        how many steps of the model to collect transitions for before learning starts\n    gamma: float\n        discount factor\n    target_network_update_freq: int\n        update the target network every `target_network_update_freq` steps.\n    prioritized_replay: True\n        if True prioritized replay buffer will be used.\n    prioritized_replay_alpha: float\n        alpha parameter for prioritized replay buffer\n    prioritized_replay_beta0: float\n        initial value of beta for prioritized replay buffer\n    prioritized_replay_beta_iters: int\n        number of iterations over which beta will be annealed from initial value\n        to 1.0. If set to None equals to total_timesteps.\n    prioritized_replay_eps: float\n        epsilon to add to the TD errors when updating priorities.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    callback: (locals, globals) -> None\n        function called at every steps with state of the algorithm.\n        If callback returns true training stops.\n    load_path: str\n        path to load the model from. (default: None)\n    **network_kwargs\n        additional keyword arguments to pass to the network builder.\n\n    Returns\n    -------\n    act: ActWrapper\n        Wrapper over act function. Adds ability to save it and load it.\n        See header of baselines/deepq/categorical.py for details on the act function."
  },
  {
    "code": "def save_act(self, path=None):\n        \"\"\"Save model to a pickle located at `path`\"\"\"\n        if path is None:\n            path = os.path.join(logger.get_dir(), \"model.pkl\")\n\n        with tempfile.TemporaryDirectory() as td:\n            save_variables(os.path.join(td, \"model\"))\n            arc_name = os.path.join(td, \"packed.zip\")\n            with zipfile.ZipFile(arc_name, 'w') as zipf:\n                for root, dirs, files in os.walk(td):\n                    for fname in files:\n                        file_path = os.path.join(root, fname)\n                        if file_path != arc_name:\n                            zipf.write(file_path, os.path.relpath(file_path, td))\n            with open(arc_name, \"rb\") as f:\n                model_data = f.read()\n        with open(path, \"wb\") as f:\n            cloudpickle.dump((model_data, self._act_params), f)",
    "doc": "Save model to a pickle located at `path`"
  },
  {
    "code": "def nature_cnn(unscaled_images, **conv_kwargs):\n    \"\"\"\n    CNN from Nature paper.\n    \"\"\"\n    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\n    activ = tf.nn.relu\n    h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2),\n                   **conv_kwargs))\n    h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs))\n    h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2), **conv_kwargs))\n    h3 = conv_to_fc(h3)\n    return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))",
    "doc": "CNN from Nature paper."
  },
  {
    "code": "def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n    \"\"\"\n    Stack of fully-connected layers to be used in a policy / q-function approximator\n\n    Parameters:\n    ----------\n\n    num_layers: int                 number of fully-connected layers (default: 2)\n\n    num_hidden: int                 size of fully-connected layers (default: 64)\n\n    activation:                     activation function (default: tf.tanh)\n\n    Returns:\n    -------\n\n    function that builds fully connected network with a given input tensor / placeholder\n    \"\"\"\n    def network_fn(X):\n        h = tf.layers.flatten(X)\n        for i in range(num_layers):\n            h = fc(h, 'mlp_fc{}'.format(i), nh=num_hidden, init_scale=np.sqrt(2))\n            if layer_norm:\n                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n            h = activation(h)\n\n        return h\n\n    return network_fn",
    "doc": "Stack of fully-connected layers to be used in a policy / q-function approximator\n\n    Parameters:\n    ----------\n\n    num_layers: int                 number of fully-connected layers (default: 2)\n\n    num_hidden: int                 size of fully-connected layers (default: 64)\n\n    activation:                     activation function (default: tf.tanh)\n\n    Returns:\n    -------\n\n    function that builds fully connected network with a given input tensor / placeholder"
  },
  {
    "code": "def lstm(nlstm=128, layer_norm=False):\n    \"\"\"\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters:\n    ----------\n\n    nlstm: int          LSTM hidden state size\n\n    layer_norm: bool    if True, layer-normalized version of LSTM is used\n\n    Returns:\n    -------\n\n    function that builds LSTM with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X, nenv=1):\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2*nlstm]) #states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            h5, snew = utils.lnlstm(xs, ms, S, scope='lnlstm', nh=nlstm)\n        else:\n            h5, snew = utils.lstm(xs, ms, S, scope='lstm', nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {'S':S, 'M':M, 'state':snew, 'initial_state':initial_state}\n\n    return network_fn",
    "doc": "Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters:\n    ----------\n\n    nlstm: int          LSTM hidden state size\n\n    layer_norm: bool    if True, layer-normalized version of LSTM is used\n\n    Returns:\n    -------\n\n    function that builds LSTM with a given input tensor / placeholder"
  },
  {
    "code": "def conv_only(convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)], **conv_kwargs):\n    '''\n    convolutions-only net\n\n    Parameters:\n    ----------\n\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\n\n    Returns:\n\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer\n\n    '''\n\n    def network_fn(X):\n        out = tf.cast(X, tf.float32) / 255.\n        with tf.variable_scope(\"convnet\"):\n            for num_outputs, kernel_size, stride in convs:\n                out = layers.convolution2d(out,\n                                           num_outputs=num_outputs,\n                                           kernel_size=kernel_size,\n                                           stride=stride,\n                                           activation_fn=tf.nn.relu,\n                                           **conv_kwargs)\n\n        return out\n    return network_fn",
    "doc": "convolutions-only net\n\n    Parameters:\n    ----------\n\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\n\n    Returns:\n\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer"
  },
  {
    "code": "def get_network_builder(name):\n    \"\"\"\n    If you want to register your own network outside models.py, you just need:\n\n    Usage Example:\n    -------------\n    from baselines.common.models import register\n    @register(\"your_network_name\")\n    def your_network_define(**net_kwargs):\n        ...\n        return network_fn\n\n    \"\"\"\n    if callable(name):\n        return name\n    elif name in mapping:\n        return mapping[name]\n    else:\n        raise ValueError('Unknown network type: {}'.format(name))",
    "doc": "If you want to register your own network outside models.py, you just need:\n\n    Usage Example:\n    -------------\n    from baselines.common.models import register\n    @register(\"your_network_name\")\n    def your_network_define(**net_kwargs):\n        ...\n        return network_fn"
  },
  {
    "code": "def mlp(hiddens=[], layer_norm=False):\n    \"\"\"This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    hiddens: [int]\n        list of sizes of hidden layers\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm.\n    \"\"\"\n    return lambda *args, **kwargs: _mlp(hiddens, layer_norm=layer_norm, *args, **kwargs)",
    "doc": "This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    hiddens: [int]\n        list of sizes of hidden layers\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm."
  },
  {
    "code": "def cnn_to_mlp(convs, hiddens, dueling=False, layer_norm=False):\n    \"\"\"This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    convs: [(int, int, int)]\n        list of convolutional layers in form of\n        (num_outputs, kernel_size, stride)\n    hiddens: [int]\n        list of sizes of hidden layers\n    dueling: bool\n        if true double the output MLP to compute a baseline\n        for action scores\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm.\n    \"\"\"\n\n    return lambda *args, **kwargs: _cnn_to_mlp(convs, hiddens, dueling, layer_norm=layer_norm, *args, **kwargs)",
    "doc": "This model takes as input an observation and returns values of all actions.\n\n    Parameters\n    ----------\n    convs: [(int, int, int)]\n        list of convolutional layers in form of\n        (num_outputs, kernel_size, stride)\n    hiddens: [int]\n        list of sizes of hidden layers\n    dueling: bool\n        if true double the output MLP to compute a baseline\n        for action scores\n    layer_norm: bool\n        if true applies layer normalization for every layer\n        as described in https://arxiv.org/abs/1607.06450\n\n    Returns\n    -------\n    q_func: function\n        q_function for DQN algorithm."
  },
  {
    "code": "def make_vec_env(env_id, env_type, num_env, seed,\n                 wrapper_kwargs=None,\n                 start_index=0,\n                 reward_scale=1.0,\n                 flatten_dict_observations=True,\n                 gamestate=None):\n    \"\"\"\n    Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.\n    \"\"\"\n    wrapper_kwargs = wrapper_kwargs or {}\n    mpi_rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\n    seed = seed + 10000 * mpi_rank if seed is not None else None\n    logger_dir = logger.get_dir()\n    def make_thunk(rank):\n        return lambda: make_env(\n            env_id=env_id,\n            env_type=env_type,\n            mpi_rank=mpi_rank,\n            subrank=rank,\n            seed=seed,\n            reward_scale=reward_scale,\n            gamestate=gamestate,\n            flatten_dict_observations=flatten_dict_observations,\n            wrapper_kwargs=wrapper_kwargs,\n            logger_dir=logger_dir\n        )\n\n    set_global_seeds(seed)\n    if num_env > 1:\n        return SubprocVecEnv([make_thunk(i + start_index) for i in range(num_env)])\n    else:\n        return DummyVecEnv([make_thunk(start_index)])",
    "doc": "Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo."
  },
  {
    "code": "def make_mujoco_env(env_id, seed, reward_scale=1.0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    rank = MPI.COMM_WORLD.Get_rank()\n    myseed = seed  + 1000 * rank if seed is not None else None\n    set_global_seeds(myseed)\n    env = gym.make(env_id)\n    logger_path = None if logger.get_dir() is None else os.path.join(logger.get_dir(), str(rank))\n    env = Monitor(env, logger_path, allow_early_resets=True)\n    env.seed(seed)\n    if reward_scale != 1.0:\n        from baselines.common.retro_wrappers import RewardScaler\n        env = RewardScaler(env, reward_scale)\n    return env",
    "doc": "Create a wrapped, monitored gym.Env for MuJoCo."
  },
  {
    "code": "def make_robotics_env(env_id, seed, rank=0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    set_global_seeds(seed)\n    env = gym.make(env_id)\n    env = FlattenDictWrapper(env, ['observation', 'desired_goal'])\n    env = Monitor(\n        env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n        info_keywords=('is_success',))\n    env.seed(seed)\n    return env",
    "doc": "Create a wrapped, monitored gym.Env for MuJoCo."
  },
  {
    "code": "def common_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument('--env', help='environment ID', type=str, default='Reacher-v2')\n    parser.add_argument('--env_type', help='type of environment, used when the environment type cannot be automatically determined', type=str)\n    parser.add_argument('--seed', help='RNG seed', type=int, default=None)\n    parser.add_argument('--alg', help='Algorithm', type=str, default='ppo2')\n    parser.add_argument('--num_timesteps', type=float, default=1e6),\n    parser.add_argument('--network', help='network type (mlp, cnn, lstm, cnn_lstm, conv_only)', default=None)\n    parser.add_argument('--gamestate', help='game state to load (so far only used in retro games)', default=None)\n    parser.add_argument('--num_env', help='Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco', default=None, type=int)\n    parser.add_argument('--reward_scale', help='Reward scale factor. Default: 1.0', default=1.0, type=float)\n    parser.add_argument('--save_path', help='Path to save trained model to', default=None, type=str)\n    parser.add_argument('--save_video_interval', help='Save video every x steps (0 = disabled)', default=0, type=int)\n    parser.add_argument('--save_video_length', help='Length of recorded video. Default: 200', default=200, type=int)\n    parser.add_argument('--play', default=False, action='store_true')\n    return parser",
    "doc": "Create an argparse.ArgumentParser for run_mujoco.py."
  },
  {
    "code": "def robotics_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument('--env', help='environment ID', type=str, default='FetchReach-v0')\n    parser.add_argument('--seed', help='RNG seed', type=int, default=None)\n    parser.add_argument('--num-timesteps', type=int, default=int(1e6))\n    return parser",
    "doc": "Create an argparse.ArgumentParser for run_mujoco.py."
  },
  {
    "code": "def parse_unknown_args(args):\n    \"\"\"\n    Parse arguments not consumed by arg parser into a dicitonary\n    \"\"\"\n    retval = {}\n    preceded_by_key = False\n    for arg in args:\n        if arg.startswith('--'):\n            if '=' in arg:\n                key = arg.split('=')[0][2:]\n                value = arg.split('=')[1]\n                retval[key] = value\n            else:\n                key = arg[2:]\n                preceded_by_key = True\n        elif preceded_by_key:\n            retval[key] = arg\n            preceded_by_key = False\n\n    return retval",
    "doc": "Parse arguments not consumed by arg parser into a dicitonary"
  },
  {
    "code": "def clear_mpi_env_vars():\n    \"\"\"\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes.\n    \"\"\"\n    removed_environment = {}\n    for k, v in list(os.environ.items()):\n        for prefix in ['OMPI_', 'PMI_']:\n            if k.startswith(prefix):\n                removed_environment[k] = v\n                del os.environ[k]\n    try:\n        yield\n    finally:\n        os.environ.update(removed_environment)",
    "doc": "from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes."
  },
  {
    "code": "def learn(*, network, env, total_timesteps, eval_env = None, seed=None, nsteps=2048, ent_coef=0.0, lr=3e-4,\n            vf_coef=0.5,  max_grad_norm=0.5, gamma=0.99, lam=0.95,\n            log_interval=10, nminibatches=4, noptepochs=4, cliprange=0.2,\n            save_interval=0, load_path=None, model_fn=None, **network_kwargs):\n    '''\n    Learn policy using PPO algorithm (https://arxiv.org/abs/1707.06347)\n\n    Parameters:\n    ----------\n\n    network:                          policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                                      specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                                      tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                                      neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                                      See common/models.py/lstm for more details on using recurrent nets in policies\n\n    env: baselines.common.vec_env.VecEnv     environment. Needs to be vectorized for parallel environment simulation.\n                                      The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n\n    nsteps: int                       number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                                      nenv is number of environment copies simulated in parallel)\n\n    total_timesteps: int              number of timesteps (i.e. number of actions taken in the environment)\n\n    ent_coef: float                   policy entropy coefficient in the optimization objective\n\n    lr: float or function             learning rate, constant or a schedule function [0,1] -> R+ where 1 is beginning of the\n                                      training and 0 is the end of the training.\n\n    vf_coef: float                    value function loss coefficient in the optimization objective\n\n    max_grad_norm: float or None      gradient norm clipping coefficient\n\n    gamma: float                      discounting factor\n\n    lam: float                        advantage estimation discounting factor (lambda in the paper)\n\n    log_interval: int                 number of timesteps between logging events\n\n    nminibatches: int                 number of training minibatches per update. For recurrent policies,\n                                      should be smaller or equal than number of environments run in parallel.\n\n    noptepochs: int                   number of training epochs per update\n\n    cliprange: float or function      clipping range, constant or schedule function [0,1] -> R+ where 1 is beginning of the training\n                                      and 0 is the end of the training\n\n    save_interval: int                number of timesteps between saving events\n\n    load_path: str                    path to load the model from\n\n    **network_kwargs:                 keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                      For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n\n\n\n    '''\n\n    set_global_seeds(seed)\n\n    if isinstance(lr, float): lr = constfn(lr)\n    else: assert callable(lr)\n    if isinstance(cliprange, float): cliprange = constfn(cliprange)\n    else: assert callable(cliprange)\n    total_timesteps = int(total_timesteps)\n\n    policy = build_policy(env, network, **network_kwargs)\n\n    # Get the nb of env\n    nenvs = env.num_envs\n\n    # Get state_space and action_space\n    ob_space = env.observation_space\n    ac_space = env.action_space\n\n    # Calculate the batch_size\n    nbatch = nenvs * nsteps\n    nbatch_train = nbatch // nminibatches\n\n    # Instantiate the model object (that creates act_model and train_model)\n    if model_fn is None:\n        from baselines.ppo2.model import Model\n        model_fn = Model\n\n    model = model_fn(policy=policy, ob_space=ob_space, ac_space=ac_space, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                    nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n                    max_grad_norm=max_grad_norm)\n\n    if load_path is not None:\n        model.load(load_path)\n    # Instantiate the runner object\n    runner = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)\n    if eval_env is not None:\n        eval_runner = Runner(env = eval_env, model = model, nsteps = nsteps, gamma = gamma, lam= lam)\n\n    epinfobuf = deque(maxlen=100)\n    if eval_env is not None:\n        eval_epinfobuf = deque(maxlen=100)\n\n    # Start total timer\n    tfirststart = time.perf_counter()\n\n    nupdates = total_timesteps//nbatch\n    for update in range(1, nupdates+1):\n        assert nbatch % nminibatches == 0\n        # Start timer\n        tstart = time.perf_counter()\n        frac = 1.0 - (update - 1.0) / nupdates\n        # Calculate the learning rate\n        lrnow = lr(frac)\n        # Calculate the cliprange\n        cliprangenow = cliprange(frac)\n        # Get minibatch\n        obs, returns, masks, actions, values, neglogpacs, states, epinfos = runner.run() #pylint: disable=E0632\n        if eval_env is not None:\n            eval_obs, eval_returns, eval_masks, eval_actions, eval_values, eval_neglogpacs, eval_states, eval_epinfos = eval_runner.run() #pylint: disable=E0632\n\n        epinfobuf.extend(epinfos)\n        if eval_env is not None:\n            eval_epinfobuf.extend(eval_epinfos)\n\n        # Here what we're going to do is for each minibatch calculate the loss and append it.\n        mblossvals = []\n        if states is None: # nonrecurrent version\n            # Index of each element of batch_size\n            # Create the indices array\n            inds = np.arange(nbatch)\n            for _ in range(noptepochs):\n                # Randomize the indexes\n                np.random.shuffle(inds)\n                # 0 to batch_size with batch_train_size step\n                for start in range(0, nbatch, nbatch_train):\n                    end = start + nbatch_train\n                    mbinds = inds[start:end]\n                    slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices))\n        else: # recurrent version\n            assert nenvs % nminibatches == 0\n            envsperbatch = nenvs // nminibatches\n            envinds = np.arange(nenvs)\n            flatinds = np.arange(nenvs * nsteps).reshape(nenvs, nsteps)\n            for _ in range(noptepochs):\n                np.random.shuffle(envinds)\n                for start in range(0, nenvs, envsperbatch):\n                    end = start + envsperbatch\n                    mbenvinds = envinds[start:end]\n                    mbflatinds = flatinds[mbenvinds].ravel()\n                    slices = (arr[mbflatinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    mbstates = states[mbenvinds]\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices, mbstates))\n\n        # Feedforward --> get losses --> update\n        lossvals = np.mean(mblossvals, axis=0)\n        # End timer\n        tnow = time.perf_counter()\n        # Calculate the fps (frame per second)\n        fps = int(nbatch / (tnow - tstart))\n        if update % log_interval == 0 or update == 1:\n            # Calculates if value function is a good predicator of the returns (ev > 1)\n            # or if it's just worse than predicting nothing (ev =< 0)\n            ev = explained_variance(values, returns)\n            logger.logkv(\"serial_timesteps\", update*nsteps)\n            logger.logkv(\"nupdates\", update)\n            logger.logkv(\"total_timesteps\", update*nbatch)\n            logger.logkv(\"fps\", fps)\n            logger.logkv(\"explained_variance\", float(ev))\n            logger.logkv('eprewmean', safemean([epinfo['r'] for epinfo in epinfobuf]))\n            logger.logkv('eplenmean', safemean([epinfo['l'] for epinfo in epinfobuf]))\n            if eval_env is not None:\n                logger.logkv('eval_eprewmean', safemean([epinfo['r'] for epinfo in eval_epinfobuf]) )\n                logger.logkv('eval_eplenmean', safemean([epinfo['l'] for epinfo in eval_epinfobuf]) )\n            logger.logkv('time_elapsed', tnow - tfirststart)\n            for (lossval, lossname) in zip(lossvals, model.loss_names):\n                logger.logkv(lossname, lossval)\n            if MPI is None or MPI.COMM_WORLD.Get_rank() == 0:\n                logger.dumpkvs()\n        if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir() and (MPI is None or MPI.COMM_WORLD.Get_rank() == 0):\n            checkdir = osp.join(logger.get_dir(), 'checkpoints')\n            os.makedirs(checkdir, exist_ok=True)\n            savepath = osp.join(checkdir, '%.5i'%update)\n            print('Saving to', savepath)\n            model.save(savepath)\n    return model",
    "doc": "Learn policy using PPO algorithm (https://arxiv.org/abs/1707.06347)\n\n    Parameters:\n    ----------\n\n    network:                          policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                                      specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                                      tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                                      neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                                      See common/models.py/lstm for more details on using recurrent nets in policies\n\n    env: baselines.common.vec_env.VecEnv     environment. Needs to be vectorized for parallel environment simulation.\n                                      The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n\n    nsteps: int                       number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                                      nenv is number of environment copies simulated in parallel)\n\n    total_timesteps: int              number of timesteps (i.e. number of actions taken in the environment)\n\n    ent_coef: float                   policy entropy coefficient in the optimization objective\n\n    lr: float or function             learning rate, constant or a schedule function [0,1] -> R+ where 1 is beginning of the\n                                      training and 0 is the end of the training.\n\n    vf_coef: float                    value function loss coefficient in the optimization objective\n\n    max_grad_norm: float or None      gradient norm clipping coefficient\n\n    gamma: float                      discounting factor\n\n    lam: float                        advantage estimation discounting factor (lambda in the paper)\n\n    log_interval: int                 number of timesteps between logging events\n\n    nminibatches: int                 number of training minibatches per update. For recurrent policies,\n                                      should be smaller or equal than number of environments run in parallel.\n\n    noptepochs: int                   number of training epochs per update\n\n    cliprange: float or function      clipping range, constant or schedule function [0,1] -> R+ where 1 is beginning of the training\n                                      and 0 is the end of the training\n\n    save_interval: int                number of timesteps between saving events\n\n    load_path: str                    path to load the model from\n\n    **network_kwargs:                 keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                      For instance, 'mlp' network architecture has arguments num_hidden and num_layers."
  },
  {
    "code": "def cg(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10):\n    \"\"\"\n    Demmel p 312\n    \"\"\"\n    p = b.copy()\n    r = b.copy()\n    x = np.zeros_like(b)\n    rdotr = r.dot(r)\n\n    fmtstr =  \"%10i %10.3g %10.3g\"\n    titlestr =  \"%10s %10s %10s\"\n    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\"))\n\n    for i in range(cg_iters):\n        if callback is not None:\n            callback(x)\n        if verbose: print(fmtstr % (i, rdotr, np.linalg.norm(x)))\n        z = f_Ax(p)\n        v = rdotr / p.dot(z)\n        x += v*p\n        r -= v*z\n        newrdotr = r.dot(r)\n        mu = newrdotr/rdotr\n        p = r + mu*p\n\n        rdotr = newrdotr\n        if rdotr < residual_tol:\n            break\n\n    if callback is not None:\n        callback(x)\n    if verbose: print(fmtstr % (i+1, rdotr, np.linalg.norm(x)))  # pylint: disable=W0631\n    return x",
    "doc": "Demmel p 312"
  },
  {
    "code": "def observation_placeholder(ob_space, batch_size=None, name='Ob'):\n    '''\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor\n    '''\n\n    assert isinstance(ob_space, Discrete) or isinstance(ob_space, Box) or isinstance(ob_space, MultiDiscrete), \\\n        'Can only deal with Discrete and Box observation spaces for now'\n\n    dtype = ob_space.dtype\n    if dtype == np.int8:\n        dtype = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)",
    "doc": "Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor"
  },
  {
    "code": "def observation_input(ob_space, batch_size=None, name='Ob'):\n    '''\n    Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type.\n    '''\n\n    placeholder = observation_placeholder(ob_space, batch_size, name)\n    return placeholder, encode_observation(ob_space, placeholder)",
    "doc": "Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type."
  },
  {
    "code": "def encode_observation(ob_space, placeholder):\n    '''\n    Encode input in the way that is appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space             observation space\n\n    placeholder: tf.placeholder     observation input placeholder\n    '''\n    if isinstance(ob_space, Discrete):\n        return tf.to_float(tf.one_hot(placeholder, ob_space.n))\n    elif isinstance(ob_space, Box):\n        return tf.to_float(placeholder)\n    elif isinstance(ob_space, MultiDiscrete):\n        placeholder = tf.cast(placeholder, tf.int32)\n        one_hots = [tf.to_float(tf.one_hot(placeholder[..., i], ob_space.nvec[i])) for i in range(placeholder.shape[-1])]\n        return tf.concat(one_hots, axis=-1)\n    else:\n        raise NotImplementedError",
    "doc": "Encode input in the way that is appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space             observation space\n\n    placeholder: tf.placeholder     observation input placeholder"
  },
  {
    "code": "def generate_rollouts(self):\n        \"\"\"Performs `rollout_batch_size` rollouts in parallel for time horizon `T` with the current\n        policy acting on it accordingly.\n        \"\"\"\n        self.reset_all_rollouts()\n\n        # compute observations\n        o = np.empty((self.rollout_batch_size, self.dims['o']), np.float32)  # observations\n        ag = np.empty((self.rollout_batch_size, self.dims['g']), np.float32)  # achieved goals\n        o[:] = self.initial_o\n        ag[:] = self.initial_ag\n\n        # generate episodes\n        obs, achieved_goals, acts, goals, successes = [], [], [], [], []\n        dones = []\n        info_values = [np.empty((self.T - 1, self.rollout_batch_size, self.dims['info_' + key]), np.float32) for key in self.info_keys]\n        Qs = []\n        for t in range(self.T):\n            policy_output = self.policy.get_actions(\n                o, ag, self.g,\n                compute_Q=self.compute_Q,\n                noise_eps=self.noise_eps if not self.exploit else 0.,\n                random_eps=self.random_eps if not self.exploit else 0.,\n                use_target_net=self.use_target_net)\n\n            if self.compute_Q:\n                u, Q = policy_output\n                Qs.append(Q)\n            else:\n                u = policy_output\n\n            if u.ndim == 1:\n                # The non-batched case should still have a reasonable shape.\n                u = u.reshape(1, -1)\n\n            o_new = np.empty((self.rollout_batch_size, self.dims['o']))\n            ag_new = np.empty((self.rollout_batch_size, self.dims['g']))\n            success = np.zeros(self.rollout_batch_size)\n            # compute new states and observations\n            obs_dict_new, _, done, info = self.venv.step(u)\n            o_new = obs_dict_new['observation']\n            ag_new = obs_dict_new['achieved_goal']\n            success = np.array([i.get('is_success', 0.0) for i in info])\n\n            if any(done):\n                # here we assume all environments are done is ~same number of steps, so we terminate rollouts whenever any of the envs returns done\n                # trick with using vecenvs is not to add the obs from the environments that are \"done\", because those are already observations\n                # after a reset\n                break\n\n            for i, info_dict in enumerate(info):\n                for idx, key in enumerate(self.info_keys):\n                    info_values[idx][t, i] = info[i][key]\n\n            if np.isnan(o_new).any():\n                self.logger.warn('NaN caught during rollout generation. Trying again...')\n                self.reset_all_rollouts()\n                return self.generate_rollouts()\n\n            dones.append(done)\n            obs.append(o.copy())\n            achieved_goals.append(ag.copy())\n            successes.append(success.copy())\n            acts.append(u.copy())\n            goals.append(self.g.copy())\n            o[...] = o_new\n            ag[...] = ag_new\n        obs.append(o.copy())\n        achieved_goals.append(ag.copy())\n\n        episode = dict(o=obs,\n                       u=acts,\n                       g=goals,\n                       ag=achieved_goals)\n        for key, value in zip(self.info_keys, info_values):\n            episode['info_{}'.format(key)] = value\n\n        # stats\n        successful = np.array(successes)[-1, :]\n        assert successful.shape == (self.rollout_batch_size,)\n        success_rate = np.mean(successful)\n        self.success_history.append(success_rate)\n        if self.compute_Q:\n            self.Q_history.append(np.mean(Qs))\n        self.n_episodes += self.rollout_batch_size\n\n        return convert_episode_to_batch_major(episode)",
    "doc": "Performs `rollout_batch_size` rollouts in parallel for time horizon `T` with the current\n        policy acting on it accordingly."
  },
  {
    "code": "def save_policy(self, path):\n        \"\"\"Pickles the current policy for later inspection.\n        \"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump(self.policy, f)",
    "doc": "Pickles the current policy for later inspection."
  },
  {
    "code": "def logs(self, prefix='worker'):\n        \"\"\"Generates a dictionary that contains all collected statistics.\n        \"\"\"\n        logs = []\n        logs += [('success_rate', np.mean(self.success_history))]\n        if self.compute_Q:\n            logs += [('mean_Q', np.mean(self.Q_history))]\n        logs += [('episode', self.n_episodes)]\n\n        if prefix != '' and not prefix.endswith('/'):\n            return [(prefix + '/' + key, val) for key, val in logs]\n        else:\n            return logs",
    "doc": "Generates a dictionary that contains all collected statistics."
  },
  {
    "code": "def smooth(y, radius, mode='two_sided', valid_only=False):\n    '''\n    Smooth signal y, where radius is determines the size of the window\n\n    mode='twosided':\n        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]\n    mode='causal':\n        average over the window [max(index - radius, 0), index]\n\n    valid_only: put nan in entries where the full-sized window is not available\n\n    '''\n    assert mode in ('two_sided', 'causal')\n    if len(y) < 2*radius+1:\n        return np.ones_like(y) * y.mean()\n    elif mode == 'two_sided':\n        convkernel = np.ones(2 * radius+1)\n        out = np.convolve(y, convkernel,mode='same') / np.convolve(np.ones_like(y), convkernel, mode='same')\n        if valid_only:\n            out[:radius] = out[-radius:] = np.nan\n    elif mode == 'causal':\n        convkernel = np.ones(radius)\n        out = np.convolve(y, convkernel,mode='full') / np.convolve(np.ones_like(y), convkernel, mode='full')\n        out = out[:-radius+1]\n        if valid_only:\n            out[:radius] = np.nan\n    return out",
    "doc": "Smooth signal y, where radius is determines the size of the window\n\n    mode='twosided':\n        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]\n    mode='causal':\n        average over the window [max(index - radius, 0), index]\n\n    valid_only: put nan in entries where the full-sized window is not available"
  },
  {
    "code": "def one_sided_ema(xolds, yolds, low=None, high=None, n=512, decay_steps=1., low_counts_threshold=1e-8):\n    '''\n    perform one-sided (causal) EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid\n\n    '''\n\n    low = xolds[0] if low is None else low\n    high = xolds[-1] if high is None else high\n\n    assert xolds[0] <= low, 'low = {} < xolds[0] = {} - extrapolation not permitted!'.format(low, xolds[0])\n    assert xolds[-1] >= high, 'high = {} > xolds[-1] = {}  - extrapolation not permitted!'.format(high, xolds[-1])\n    assert len(xolds) == len(yolds), 'length of xolds ({}) and yolds ({}) do not match!'.format(len(xolds), len(yolds))\n\n\n    xolds = xolds.astype('float64')\n    yolds = yolds.astype('float64')\n\n    luoi = 0 # last unused old index\n    sum_y = 0.\n    count_y = 0.\n    xnews = np.linspace(low, high, n)\n    decay_period = (high - low) / (n - 1) * decay_steps\n    interstep_decay = np.exp(- 1. / decay_steps)\n    sum_ys = np.zeros_like(xnews)\n    count_ys = np.zeros_like(xnews)\n    for i in range(n):\n        xnew = xnews[i]\n        sum_y *= interstep_decay\n        count_y *= interstep_decay\n        while True:\n            xold = xolds[luoi]\n            if xold <= xnew:\n                decay = np.exp(- (xnew - xold) / decay_period)\n                sum_y += decay * yolds[luoi]\n                count_y += decay\n                luoi += 1\n            else:\n                break\n            if luoi >= len(xolds):\n                break\n        sum_ys[i] = sum_y\n        count_ys[i] = count_y\n\n    ys = sum_ys / count_ys\n    ys[count_ys < low_counts_threshold] = np.nan\n\n    return xnews, ys, count_ys",
    "doc": "perform one-sided (causal) EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid"
  },
  {
    "code": "def symmetric_ema(xolds, yolds, low=None, high=None, n=512, decay_steps=1., low_counts_threshold=1e-8):\n    '''\n    perform symmetric EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid\n\n    '''\n    xs, ys1, count_ys1 = one_sided_ema(xolds, yolds, low, high, n, decay_steps, low_counts_threshold=0)\n    _,  ys2, count_ys2 = one_sided_ema(-xolds[::-1], yolds[::-1], -high, -low, n, decay_steps, low_counts_threshold=0)\n    ys2 = ys2[::-1]\n    count_ys2 = count_ys2[::-1]\n    count_ys = count_ys1 + count_ys2\n    ys = (ys1 * count_ys1 + ys2 * count_ys2) / count_ys\n    ys[count_ys < low_counts_threshold] = np.nan\n    return xs, ys, count_ys",
    "doc": "perform symmetric EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid"
  },
  {
    "code": "def load_results(root_dir_or_dirs, enable_progress=True, enable_monitor=True, verbose=False):\n    '''\n    load summaries of runs from a list of directories (including subdirectories)\n    Arguments:\n\n    enable_progress: bool - if True, will attempt to load data from progress.csv files (data saved by logger). Default: True\n\n    enable_monitor: bool - if True, will attempt to load data from monitor.csv files (data saved by Monitor environment wrapper). Default: True\n\n    verbose: bool - if True, will print out list of directories from which the data is loaded. Default: False\n\n\n    Returns:\n    List of Result objects with the following fields:\n         - dirname - path to the directory data was loaded from\n         - metadata - run metadata (such as command-line arguments and anything else in metadata.json file\n         - monitor - if enable_monitor is True, this field contains pandas dataframe with loaded monitor.csv file (or aggregate of all *.monitor.csv files in the directory)\n         - progress - if enable_progress is True, this field contains pandas dataframe with loaded progress.csv file\n    '''\n    import re\n    if isinstance(root_dir_or_dirs, str):\n        rootdirs = [osp.expanduser(root_dir_or_dirs)]\n    else:\n        rootdirs = [osp.expanduser(d) for d in root_dir_or_dirs]\n    allresults = []\n    for rootdir in rootdirs:\n        assert osp.exists(rootdir), \"%s doesn't exist\"%rootdir\n        for dirname, dirs, files in os.walk(rootdir):\n            if '-proc' in dirname:\n                files[:] = []\n                continue\n            monitor_re = re.compile(r'(\\d+\\.)?(\\d+\\.)?monitor\\.csv')\n            if set(['metadata.json', 'monitor.json', 'progress.json', 'progress.csv']).intersection(files) or \\\n               any([f for f in files if monitor_re.match(f)]):  # also match monitor files like 0.1.monitor.csv\n                # used to be uncommented, which means do not go deeper than current directory if any of the data files\n                # are found\n                # dirs[:] = []\n                result = {'dirname' : dirname}\n                if \"metadata.json\" in files:\n                    with open(osp.join(dirname, \"metadata.json\"), \"r\") as fh:\n                        result['metadata'] = json.load(fh)\n                progjson = osp.join(dirname, \"progress.json\")\n                progcsv = osp.join(dirname, \"progress.csv\")\n                if enable_progress:\n                    if osp.exists(progjson):\n                        result['progress'] = pandas.DataFrame(read_json(progjson))\n                    elif osp.exists(progcsv):\n                        try:\n                            result['progress'] = read_csv(progcsv)\n                        except pandas.errors.EmptyDataError:\n                            print('skipping progress file in ', dirname, 'empty data')\n                    else:\n                        if verbose: print('skipping %s: no progress file'%dirname)\n\n                if enable_monitor:\n                    try:\n                        result['monitor'] = pandas.DataFrame(monitor.load_results(dirname))\n                    except monitor.LoadMonitorResultsError:\n                        print('skipping %s: no monitor files'%dirname)\n                    except Exception as e:\n                        print('exception loading monitor file in %s: %s'%(dirname, e))\n\n                if result.get('monitor') is not None or result.get('progress') is not None:\n                    allresults.append(Result(**result))\n                    if verbose:\n                        print('successfully loaded %s'%dirname)\n\n    if verbose: print('loaded %i results'%len(allresults))\n    return allresults",
    "doc": "load summaries of runs from a list of directories (including subdirectories)\n    Arguments:\n\n    enable_progress: bool - if True, will attempt to load data from progress.csv files (data saved by logger). Default: True\n\n    enable_monitor: bool - if True, will attempt to load data from monitor.csv files (data saved by Monitor environment wrapper). Default: True\n\n    verbose: bool - if True, will print out list of directories from which the data is loaded. Default: False\n\n\n    Returns:\n    List of Result objects with the following fields:\n         - dirname - path to the directory data was loaded from\n         - metadata - run metadata (such as command-line arguments and anything else in metadata.json file\n         - monitor - if enable_monitor is True, this field contains pandas dataframe with loaded monitor.csv file (or aggregate of all *.monitor.csv files in the directory)\n         - progress - if enable_progress is True, this field contains pandas dataframe with loaded progress.csv file"
  },
  {
    "code": "def plot_results(\n    allresults, *,\n    xy_fn=default_xy_fn,\n    split_fn=default_split_fn,\n    group_fn=default_split_fn,\n    average_group=False,\n    shaded_std=True,\n    shaded_err=True,\n    figsize=None,\n    legend_outside=False,\n    resample=0,\n    smooth_step=1.0\n):\n    '''\n    Plot multiple Results objects\n\n    xy_fn: function Result -> x,y           - function that converts results objects into tuple of x and y values.\n                                              By default, x is cumsum of episode lengths, and y is episode rewards\n\n    split_fn: function Result -> hashable   - function that converts results objects into keys to split curves into sub-panels by.\n                                              That is, the results r for which split_fn(r) is different will be put on different sub-panels.\n                                              By default, the portion of r.dirname between last / and -<digits> is returned. The sub-panels are\n                                              stacked vertically in the figure.\n\n    group_fn: function Result -> hashable   - function that converts results objects into keys to group curves by.\n                                              That is, the results r for which group_fn(r) is the same will be put into the same group.\n                                              Curves in the same group have the same color (if average_group is False), or averaged over\n                                              (if average_group is True). The default value is the same as default value for split_fn\n\n    average_group: bool                     - if True, will average the curves in the same group and plot the mean. Enables resampling\n                                              (if resample = 0, will use 512 steps)\n\n    shaded_std: bool                        - if True (default), the shaded region corresponding to standard deviation of the group of curves will be\n                                              shown (only applicable if average_group = True)\n\n    shaded_err: bool                        - if True (default), the shaded region corresponding to error in mean estimate of the group of curves\n                                              (that is, standard deviation divided by square root of number of curves) will be\n                                              shown (only applicable if average_group = True)\n\n    figsize: tuple or None                  - size of the resulting figure (including sub-panels). By default, width is 6 and height is 6 times number of\n                                              sub-panels.\n\n\n    legend_outside: bool                    - if True, will place the legend outside of the sub-panels.\n\n    resample: int                           - if not zero, size of the uniform grid in x direction to resample onto. Resampling is performed via symmetric\n                                              EMA smoothing (see the docstring for symmetric_ema).\n                                              Default is zero (no resampling). Note that if average_group is True, resampling is necessary; in that case, default\n                                              value is 512.\n\n    smooth_step: float                      - when resampling (i.e. when resample > 0 or average_group is True), use this EMA decay parameter (in units of the new grid step).\n                                              See docstrings for decay_steps in symmetric_ema or one_sided_ema functions.\n\n    '''\n\n    if split_fn is None: split_fn = lambda _ : ''\n    if group_fn is None: group_fn = lambda _ : ''\n    sk2r = defaultdict(list) # splitkey2results\n    for result in allresults:\n        splitkey = split_fn(result)\n        sk2r[splitkey].append(result)\n    assert len(sk2r) > 0\n    assert isinstance(resample, int), \"0: don't resample. <integer>: that many samples\"\n    nrows = len(sk2r)\n    ncols = 1\n    figsize = figsize or (6, 6 * nrows)\n    f, axarr = plt.subplots(nrows, ncols, sharex=False, squeeze=False, figsize=figsize)\n\n    groups = list(set(group_fn(result) for result in allresults))\n\n    default_samples = 512\n    if average_group:\n        resample = resample or default_samples\n\n    for (isplit, sk) in enumerate(sorted(sk2r.keys())):\n        g2l = {}\n        g2c = defaultdict(int)\n        sresults = sk2r[sk]\n        gresults = defaultdict(list)\n        ax = axarr[isplit][0]\n        for result in sresults:\n            group = group_fn(result)\n            g2c[group] += 1\n            x, y = xy_fn(result)\n            if x is None: x = np.arange(len(y))\n            x, y = map(np.asarray, (x, y))\n            if average_group:\n                gresults[group].append((x,y))\n            else:\n                if resample:\n                    x, y, counts = symmetric_ema(x, y, x[0], x[-1], resample, decay_steps=smooth_step)\n                l, = ax.plot(x, y, color=COLORS[groups.index(group) % len(COLORS)])\n                g2l[group] = l\n        if average_group:\n            for group in sorted(groups):\n                xys = gresults[group]\n                if not any(xys):\n                    continue\n                color = COLORS[groups.index(group) % len(COLORS)]\n                origxs = [xy[0] for xy in xys]\n                minxlen = min(map(len, origxs))\n                def allequal(qs):\n                    return all((q==qs[0]).all() for q in qs[1:])\n                if resample:\n                    low  = max(x[0] for x in origxs)\n                    high = min(x[-1] for x in origxs)\n                    usex = np.linspace(low, high, resample)\n                    ys = []\n                    for (x, y) in xys:\n                        ys.append(symmetric_ema(x, y, low, high, resample, decay_steps=smooth_step)[1])\n                else:\n                    assert allequal([x[:minxlen] for x in origxs]),\\\n                        'If you want to average unevenly sampled data, set resample=<number of samples you want>'\n                    usex = origxs[0]\n                    ys = [xy[1][:minxlen] for xy in xys]\n                ymean = np.mean(ys, axis=0)\n                ystd = np.std(ys, axis=0)\n                ystderr = ystd / np.sqrt(len(ys))\n                l, = axarr[isplit][0].plot(usex, ymean, color=color)\n                g2l[group] = l\n                if shaded_err:\n                    ax.fill_between(usex, ymean - ystderr, ymean + ystderr, color=color, alpha=.4)\n                if shaded_std:\n                    ax.fill_between(usex, ymean - ystd,    ymean + ystd,    color=color, alpha=.2)\n\n\n        # https://matplotlib.org/users/legend_guide.html\n        plt.tight_layout()\n        if any(g2l.keys()):\n            ax.legend(\n                g2l.values(),\n                ['%s (%i)'%(g, g2c[g]) for g in g2l] if average_group else g2l.keys(),\n                loc=2 if legend_outside else None,\n                bbox_to_anchor=(1,1) if legend_outside else None)\n        ax.set_title(sk)\n    return f, axarr",
    "doc": "Plot multiple Results objects\n\n    xy_fn: function Result -> x,y           - function that converts results objects into tuple of x and y values.\n                                              By default, x is cumsum of episode lengths, and y is episode rewards\n\n    split_fn: function Result -> hashable   - function that converts results objects into keys to split curves into sub-panels by.\n                                              That is, the results r for which split_fn(r) is different will be put on different sub-panels.\n                                              By default, the portion of r.dirname between last / and -<digits> is returned. The sub-panels are\n                                              stacked vertically in the figure.\n\n    group_fn: function Result -> hashable   - function that converts results objects into keys to group curves by.\n                                              That is, the results r for which group_fn(r) is the same will be put into the same group.\n                                              Curves in the same group have the same color (if average_group is False), or averaged over\n                                              (if average_group is True). The default value is the same as default value for split_fn\n\n    average_group: bool                     - if True, will average the curves in the same group and plot the mean. Enables resampling\n                                              (if resample = 0, will use 512 steps)\n\n    shaded_std: bool                        - if True (default), the shaded region corresponding to standard deviation of the group of curves will be\n                                              shown (only applicable if average_group = True)\n\n    shaded_err: bool                        - if True (default), the shaded region corresponding to error in mean estimate of the group of curves\n                                              (that is, standard deviation divided by square root of number of curves) will be\n                                              shown (only applicable if average_group = True)\n\n    figsize: tuple or None                  - size of the resulting figure (including sub-panels). By default, width is 6 and height is 6 times number of\n                                              sub-panels.\n\n\n    legend_outside: bool                    - if True, will place the legend outside of the sub-panels.\n\n    resample: int                           - if not zero, size of the uniform grid in x direction to resample onto. Resampling is performed via symmetric\n                                              EMA smoothing (see the docstring for symmetric_ema).\n                                              Default is zero (no resampling). Note that if average_group is True, resampling is necessary; in that case, default\n                                              value is 512.\n\n    smooth_step: float                      - when resampling (i.e. when resample > 0 or average_group is True), use this EMA decay parameter (in units of the new grid step).\n                                              See docstrings for decay_steps in symmetric_ema or one_sided_ema functions."
  },
  {
    "code": "def check_synced(localval, comm=None):\n    \"\"\"\n    It's common to forget to initialize your variables to the same values, or\n    (less commonly) if you update them in some other way than adam, to get them out of sync.\n    This function checks that variables on all MPI workers are the same, and raises\n    an AssertionError otherwise\n\n    Arguments:\n        comm: MPI communicator\n        localval: list of local variables (list of variables on current worker to be compared with the other workers)\n    \"\"\"\n    comm = comm or MPI.COMM_WORLD\n    vals = comm.gather(localval)\n    if comm.rank == 0:\n        assert all(val==vals[0] for val in vals[1:])",
    "doc": "It's common to forget to initialize your variables to the same values, or\n    (less commonly) if you update them in some other way than adam, to get them out of sync.\n    This function checks that variables on all MPI workers are the same, and raises\n    an AssertionError otherwise\n\n    Arguments:\n        comm: MPI communicator\n        localval: list of local variables (list of variables on current worker to be compared with the other workers)"
  },
  {
    "code": "def copy_obs_dict(obs):\n    \"\"\"\n    Deep-copy an observation dict.\n    \"\"\"\n    return {k: np.copy(v) for k, v in obs.items()}",
    "doc": "Deep-copy an observation dict."
  },
  {
    "code": "def obs_space_info(obs_space):\n    \"\"\"\n    Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    \"\"\"\n    if isinstance(obs_space, gym.spaces.Dict):\n        assert isinstance(obs_space.spaces, OrderedDict)\n        subspaces = obs_space.spaces\n    else:\n        subspaces = {None: obs_space}\n    keys = []\n    shapes = {}\n    dtypes = {}\n    for key, box in subspaces.items():\n        keys.append(key)\n        shapes[key] = box.shape\n        dtypes[key] = box.dtype\n    return keys, shapes, dtypes",
    "doc": "Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes."
  },
  {
    "code": "def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):\n    \"\"\"\n    Calculates q_retrace targets\n\n    :param R: Rewards\n    :param D: Dones\n    :param q_i: Q values for actions taken\n    :param v: V values\n    :param rho_i: Importance weight for each action\n    :return: Q_retrace values\n    \"\"\"\n    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]\n    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]\n    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]\n    q_is = batch_to_seq(q_i, nenvs, nsteps, True)\n    vs = batch_to_seq(v, nenvs, nsteps + 1, True)\n    v_final = vs[-1]\n    qret = v_final\n    qrets = []\n    for i in range(nsteps - 1, -1, -1):\n        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)\n        qret = rs[i] + gamma * qret * (1.0 - ds[i])\n        qrets.append(qret)\n        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]\n    qrets = qrets[::-1]\n    qret = seq_to_batch(qrets, flat=True)\n    return qret",
    "doc": "Calculates q_retrace targets\n\n    :param R: Rewards\n    :param D: Dones\n    :param q_i: Q values for actions taken\n    :param v: V values\n    :param rho_i: Importance weight for each action\n    :return: Q_retrace values"
  },
  {
    "code": "def learn(network, env, seed=None, nsteps=20, total_timesteps=int(80e6), q_coef=0.5, ent_coef=0.01,\n          max_grad_norm=10, lr=7e-4, lrschedule='linear', rprop_epsilon=1e-5, rprop_alpha=0.99, gamma=0.99,\n          log_interval=100, buffer_size=50000, replay_ratio=4, replay_start=10000, c=10.0,\n          trust_region=True, alpha=0.99, delta=1, load_path=None, **network_kwargs):\n\n    '''\n    Main entrypoint for ACER (Actor-Critic with Experience Replay) algorithm (https://arxiv.org/pdf/1611.01224.pdf)\n    Train an agent with given network architecture on a given environment using ACER.\n\n    Parameters:\n    ----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n    env:                environment. Needs to be vectorized for parallel environment simulation.\n                        The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel) (default: 20)\n\n    nstack:             int, size of the frame stack, i.e. number of the frames passed to the step model. Frames are stacked along channel dimension\n                        (last image dimension) (default: 4)\n\n    total_timesteps:    int, number of timesteps (i.e. number of actions taken in the environment) (default: 80M)\n\n    q_coef:             float, value function loss coefficient in the optimization objective (analog of vf_coef for other actor-critic methods)\n\n    ent_coef:           float, policy entropy coefficient in the optimization objective (default: 0.01)\n\n    max_grad_norm:      float, gradient norm clipping coefficient. If set to None, no clipping. (default: 10),\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    rprop_epsilon:      float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    rprop_alpha:        float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting factor (default: 0.99)\n\n    log_interval:       int, number of updates between logging events (default: 100)\n\n    buffer_size:        int, size of the replay buffer (default: 50k)\n\n    replay_ratio:       int, now many (on average) batches of data to sample from the replay buffer take after batch from the environment (default: 4)\n\n    replay_start:       int, the sampling from the replay buffer does not start until replay buffer has at least that many samples (default: 10k)\n\n    c:                  float, importance weight clipping factor (default: 10)\n\n    trust_region        bool, whether or not algorithms estimates the gradient KL divergence between the old and updated policy and uses it to determine step size  (default: True)\n\n    delta:              float, max KL divergence between the old policy and updated policy (default: 1)\n\n    alpha:              float, momentum factor in the Polyak (exponential moving average) averaging of the model parameters (default: 0.99)\n\n    load_path:          str, path to load the model from (default: None)\n\n    **network_kwargs:               keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                    For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n\n    '''\n\n    print(\"Running Acer Simple\")\n    print(locals())\n    set_global_seeds(seed)\n    if not isinstance(env, VecFrameStack):\n        env = VecFrameStack(env, 1)\n\n    policy = build_policy(env, network, estimate_q=True, **network_kwargs)\n    nenvs = env.num_envs\n    ob_space = env.observation_space\n    ac_space = env.action_space\n\n    nstack = env.nstack\n    model = Model(policy=policy, ob_space=ob_space, ac_space=ac_space, nenvs=nenvs, nsteps=nsteps,\n                  ent_coef=ent_coef, q_coef=q_coef, gamma=gamma,\n                  max_grad_norm=max_grad_norm, lr=lr, rprop_alpha=rprop_alpha, rprop_epsilon=rprop_epsilon,\n                  total_timesteps=total_timesteps, lrschedule=lrschedule, c=c,\n                  trust_region=trust_region, alpha=alpha, delta=delta)\n\n    runner = Runner(env=env, model=model, nsteps=nsteps)\n    if replay_ratio > 0:\n        buffer = Buffer(env=env, nsteps=nsteps, size=buffer_size)\n    else:\n        buffer = None\n    nbatch = nenvs*nsteps\n    acer = Acer(runner, model, buffer, log_interval)\n    acer.tstart = time.time()\n\n    for acer.steps in range(0, total_timesteps, nbatch): #nbatch samples, 1 on_policy call and multiple off-policy calls\n        acer.call(on_policy=True)\n        if replay_ratio > 0 and buffer.has_atleast(replay_start):\n            n = np.random.poisson(replay_ratio)\n            for _ in range(n):\n                acer.call(on_policy=False)  # no simulation steps in this\n\n    return model",
    "doc": "Main entrypoint for ACER (Actor-Critic with Experience Replay) algorithm (https://arxiv.org/pdf/1611.01224.pdf)\n    Train an agent with given network architecture on a given environment using ACER.\n\n    Parameters:\n    ----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n    env:                environment. Needs to be vectorized for parallel environment simulation.\n                        The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel) (default: 20)\n\n    nstack:             int, size of the frame stack, i.e. number of the frames passed to the step model. Frames are stacked along channel dimension\n                        (last image dimension) (default: 4)\n\n    total_timesteps:    int, number of timesteps (i.e. number of actions taken in the environment) (default: 80M)\n\n    q_coef:             float, value function loss coefficient in the optimization objective (analog of vf_coef for other actor-critic methods)\n\n    ent_coef:           float, policy entropy coefficient in the optimization objective (default: 0.01)\n\n    max_grad_norm:      float, gradient norm clipping coefficient. If set to None, no clipping. (default: 10),\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    rprop_epsilon:      float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    rprop_alpha:        float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting factor (default: 0.99)\n\n    log_interval:       int, number of updates between logging events (default: 100)\n\n    buffer_size:        int, size of the replay buffer (default: 50k)\n\n    replay_ratio:       int, now many (on average) batches of data to sample from the replay buffer take after batch from the environment (default: 4)\n\n    replay_start:       int, the sampling from the replay buffer does not start until replay buffer has at least that many samples (default: 10k)\n\n    c:                  float, importance weight clipping factor (default: 10)\n\n    trust_region        bool, whether or not algorithms estimates the gradient KL divergence between the old and updated policy and uses it to determine step size  (default: True)\n\n    delta:              float, max KL divergence between the old policy and updated policy (default: 1)\n\n    alpha:              float, momentum factor in the Polyak (exponential moving average) averaging of the model parameters (default: 0.99)\n\n    load_path:          str, path to load the model from (default: None)\n\n    **network_kwargs:               keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                    For instance, 'mlp' network architecture has arguments num_hidden and num_layers."
  },
  {
    "code": "def apply_stats(self, statsUpdates):\n        \"\"\" compute stats and update/apply the new stats to the running average\n        \"\"\"\n\n        def updateAccumStats():\n            if self._full_stats_init:\n                return tf.cond(tf.greater(self.sgd_step, self._cold_iter), lambda: tf.group(*self._apply_stats(statsUpdates, accumulate=True, accumulateCoeff=1. / self._stats_accum_iter)), tf.no_op)\n            else:\n                return tf.group(*self._apply_stats(statsUpdates, accumulate=True, accumulateCoeff=1. / self._stats_accum_iter))\n\n        def updateRunningAvgStats(statsUpdates, fac_iter=1):\n            # return tf.cond(tf.greater_equal(self.factor_step,\n            # tf.convert_to_tensor(fac_iter)), lambda:\n            # tf.group(*self._apply_stats(stats_list, varlist)), tf.no_op)\n            return tf.group(*self._apply_stats(statsUpdates))\n\n        if self._async_stats:\n            # asynchronous stats update\n            update_stats = self._apply_stats(statsUpdates)\n\n            queue = tf.FIFOQueue(1, [item.dtype for item in update_stats], shapes=[\n                                 item.get_shape() for item in update_stats])\n            enqueue_op = queue.enqueue(update_stats)\n\n            def dequeue_stats_op():\n                return queue.dequeue()\n            self.qr_stats = tf.train.QueueRunner(queue, [enqueue_op])\n            update_stats_op = tf.cond(tf.equal(queue.size(), tf.convert_to_tensor(\n                0)), tf.no_op, lambda: tf.group(*[dequeue_stats_op(), ]))\n        else:\n            # synchronous stats update\n            update_stats_op = tf.cond(tf.greater_equal(\n                self.stats_step, self._stats_accum_iter), lambda: updateRunningAvgStats(statsUpdates), updateAccumStats)\n        self._update_stats_op = update_stats_op\n        return update_stats_op",
    "doc": "compute stats and update/apply the new stats to the running average"
  },
  {
    "code": "def tile_images(img_nhwc):\n    \"\"\"\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3\n    \"\"\"\n    img_nhwc = np.asarray(img_nhwc)\n    N, h, w, c = img_nhwc.shape\n    H = int(np.ceil(np.sqrt(N)))\n    W = int(np.ceil(float(N)/H))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0]*0 for _ in range(N, H*W)])\n    img_HWhwc = img_nhwc.reshape(H, W, h, w, c)\n    img_HhWwc = img_HWhwc.transpose(0, 2, 1, 3, 4)\n    img_Hh_Ww_c = img_HhWwc.reshape(H*h, W*w, c)\n    return img_Hh_Ww_c",
    "doc": "Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3"
  },
  {
    "code": "def sum(self, start=0, end=None):\n        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n        return super(SumSegmentTree, self).reduce(start, end)",
    "doc": "Returns arr[start] + ... + arr[end]"
  },
  {
    "code": "def find_prefixsum_idx(self, prefixsum):\n        \"\"\"Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        \"\"\"\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity",
    "doc": "Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint"
  },
  {
    "code": "def min(self, start=0, end=None):\n        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n\n        return super(MinSegmentTree, self).reduce(start, end)",
    "doc": "Returns min(arr[start], ...,  arr[end])"
  },
  {
    "code": "def value(self, t):\n        \"\"\"See Schedule.value\"\"\"\n        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n            if l_t <= t and t < r_t:\n                alpha = float(t - l_t) / (r_t - l_t)\n                return self._interpolation(l, r, alpha)\n\n        # t does not belong to any of the pieces, so doom.\n        assert self._outside_value is not None\n        return self._outside_value",
    "doc": "See Schedule.value"
  },
  {
    "code": "def _subproc_worker(pipe, parent_pipe, env_fn_wrapper, obs_bufs, obs_shapes, obs_dtypes, keys):\n    \"\"\"\n    Control a single environment instance using IPC and\n    shared memory.\n    \"\"\"\n    def _write_obs(maybe_dict_obs):\n        flatdict = obs_to_dict(maybe_dict_obs)\n        for k in keys:\n            dst = obs_bufs[k].get_obj()\n            dst_np = np.frombuffer(dst, dtype=obs_dtypes[k]).reshape(obs_shapes[k])  # pylint: disable=W0212\n            np.copyto(dst_np, flatdict[k])\n\n    env = env_fn_wrapper.x()\n    parent_pipe.close()\n    try:\n        while True:\n            cmd, data = pipe.recv()\n            if cmd == 'reset':\n                pipe.send(_write_obs(env.reset()))\n            elif cmd == 'step':\n                obs, reward, done, info = env.step(data)\n                if done:\n                    obs = env.reset()\n                pipe.send((_write_obs(obs), reward, done, info))\n            elif cmd == 'render':\n                pipe.send(env.render(mode='rgb_array'))\n            elif cmd == 'close':\n                pipe.send(None)\n                break\n            else:\n                raise RuntimeError('Got unrecognized cmd %s' % cmd)\n    except KeyboardInterrupt:\n        print('ShmemVecEnv worker: got KeyboardInterrupt')\n    finally:\n        env.close()",
    "doc": "Control a single environment instance using IPC and\n    shared memory."
  },
  {
    "code": "def learn(\n    network,\n    env,\n    seed=None,\n    nsteps=5,\n    total_timesteps=int(80e6),\n    vf_coef=0.5,\n    ent_coef=0.01,\n    max_grad_norm=0.5,\n    lr=7e-4,\n    lrschedule='linear',\n    epsilon=1e-5,\n    alpha=0.99,\n    gamma=0.99,\n    log_interval=100,\n    load_path=None,\n    **network_kwargs):\n\n    '''\n    Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\n\n    Parameters:\n    -----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n\n    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\n\n\n    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel)\n\n    total_timesteps:    int, total number of timesteps to train on (default: 80M)\n\n    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\n\n    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\n\n    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    alpha:              float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting parameter (default: 0.99)\n\n    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\n\n    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                        For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n\n    '''\n\n\n\n    set_global_seeds(seed)\n\n    # Get the nb of env\n    nenvs = env.num_envs\n    policy = build_policy(env, network, **network_kwargs)\n\n    # Instantiate the model object (that creates step_model and train_model)\n    model = Model(policy=policy, env=env, nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n        max_grad_norm=max_grad_norm, lr=lr, alpha=alpha, epsilon=epsilon, total_timesteps=total_timesteps, lrschedule=lrschedule)\n    if load_path is not None:\n        model.load(load_path)\n\n    # Instantiate the runner object\n    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\n    epinfobuf = deque(maxlen=100)\n\n    # Calculate the batch_size\n    nbatch = nenvs*nsteps\n\n    # Start total timer\n    tstart = time.time()\n\n    for update in range(1, total_timesteps//nbatch+1):\n        # Get mini batch of experiences\n        obs, states, rewards, masks, actions, values, epinfos = runner.run()\n        epinfobuf.extend(epinfos)\n\n        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\n        nseconds = time.time()-tstart\n\n        # Calculate the fps (frame per second)\n        fps = int((update*nbatch)/nseconds)\n        if update % log_interval == 0 or update == 1:\n            # Calculates if value function is a good predicator of the returns (ev > 1)\n            # or if it's just worse than predicting nothing (ev =< 0)\n            ev = explained_variance(values, rewards)\n            logger.record_tabular(\"nupdates\", update)\n            logger.record_tabular(\"total_timesteps\", update*nbatch)\n            logger.record_tabular(\"fps\", fps)\n            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n            logger.record_tabular(\"value_loss\", float(value_loss))\n            logger.record_tabular(\"explained_variance\", float(ev))\n            logger.record_tabular(\"eprewmean\", safemean([epinfo['r'] for epinfo in epinfobuf]))\n            logger.record_tabular(\"eplenmean\", safemean([epinfo['l'] for epinfo in epinfobuf]))\n            logger.dump_tabular()\n    return model",
    "doc": "Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\n\n    Parameters:\n    -----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n\n    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\n\n\n    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel)\n\n    total_timesteps:    int, total number of timesteps to train on (default: 80M)\n\n    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\n\n    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\n\n    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    alpha:              float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting parameter (default: 0.99)\n\n    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\n\n    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                        For instance, 'mlp' network architecture has arguments num_hidden and num_layers."
  },
  {
    "code": "def sf01(arr):\n    \"\"\"\n    swap and then flatten axes 0 and 1\n    \"\"\"\n    s = arr.shape\n    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])",
    "doc": "swap and then flatten axes 0 and 1"
  },
  {
    "code": "def step(self, observation, **extra_feed):\n        \"\"\"\n        Compute next action(s) given the observation(s)\n\n        Parameters:\n        ----------\n\n        observation     observation data (either single or a batch)\n\n        **extra_feed    additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns:\n        -------\n        (action, value estimate, next state, negative log likelihood of the action under current policy parameters) tuple\n        \"\"\"\n\n        a, v, state, neglogp = self._evaluate([self.action, self.vf, self.state, self.neglogp], observation, **extra_feed)\n        if state.size == 0:\n            state = None\n        return a, v, state, neglogp",
    "doc": "Compute next action(s) given the observation(s)\n\n        Parameters:\n        ----------\n\n        observation     observation data (either single or a batch)\n\n        **extra_feed    additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns:\n        -------\n        (action, value estimate, next state, negative log likelihood of the action under current policy parameters) tuple"
  },
  {
    "code": "def value(self, ob, *args, **kwargs):\n        \"\"\"\n        Compute value estimate(s) given the observation(s)\n\n        Parameters:\n        ----------\n\n        observation     observation data (either single or a batch)\n\n        **extra_feed    additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns:\n        -------\n        value estimate\n        \"\"\"\n        return self._evaluate(self.vf, ob, *args, **kwargs)",
    "doc": "Compute value estimate(s) given the observation(s)\n\n        Parameters:\n        ----------\n\n        observation     observation data (either single or a batch)\n\n        **extra_feed    additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns:\n        -------\n        value estimate"
  },
  {
    "code": "def pretty_eta(seconds_left):\n    \"\"\"Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    \"\"\"\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return \"{} {}{}\".format(str(cnt), name, ('s' if cnt > 1 else ''))\n\n    if days_left > 0:\n        msg = helper(days_left, 'day')\n        if hours_left > 0:\n            msg += ' and ' + helper(hours_left, 'hour')\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, 'hour')\n        if minutes_left > 0:\n            msg += ' and ' + helper(minutes_left, 'minute')\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, 'minute')\n    return 'less than a minute'",
    "doc": "Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA."
  },
  {
    "code": "def boolean_flag(parser, name, default=False, help=None):\n    \"\"\"Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    \"\"\"\n    dest = name.replace('-', '_')\n    parser.add_argument(\"--\" + name, action=\"store_true\", default=default, dest=dest, help=help)\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)",
    "doc": "Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag"
  },
  {
    "code": "def get_wrapper_by_name(env, classname):\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    \"\"\"\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(\"Couldn't find wrapper named %s\" % classname)",
    "doc": "Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname"
  },
  {
    "code": "def relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)",
    "doc": "This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed"
  },
  {
    "code": "def pickle_load(path, compression=False):\n    \"\"\"Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    \"\"\"\n\n    if compression:\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(\"data\") as f:\n                return pickle.load(f)\n    else:\n        with open(path, \"rb\") as f:\n            return pickle.load(f)",
    "doc": "Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object"
  },
  {
    "code": "def update(self, new_val):\n        \"\"\"Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        \"\"\"\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val",
    "doc": "Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity."
  },
  {
    "code": "def store_args(method):\n    \"\"\"Stores provided method args as instance attributes.\n    \"\"\"\n    argspec = inspect.getfullargspec(method)\n    defaults = {}\n    if argspec.defaults is not None:\n        defaults = dict(\n            zip(argspec.args[-len(argspec.defaults):], argspec.defaults))\n    if argspec.kwonlydefaults is not None:\n        defaults.update(argspec.kwonlydefaults)\n    arg_names = argspec.args[1:]\n\n    @functools.wraps(method)\n    def wrapper(*positional_args, **keyword_args):\n        self = positional_args[0]\n        # Get default arg values\n        args = defaults.copy()\n        # Add provided arg values\n        for name, value in zip(arg_names, positional_args[1:]):\n            args[name] = value\n        args.update(keyword_args)\n        self.__dict__.update(args)\n        return method(*positional_args, **keyword_args)\n\n    return wrapper",
    "doc": "Stores provided method args as instance attributes."
  },
  {
    "code": "def import_function(spec):\n    \"\"\"Import a function identified by a string like \"pkg.module:fn_name\".\n    \"\"\"\n    mod_name, fn_name = spec.split(':')\n    module = importlib.import_module(mod_name)\n    fn = getattr(module, fn_name)\n    return fn",
    "doc": "Import a function identified by a string like \"pkg.module:fn_name\"."
  },
  {
    "code": "def flatten_grads(var_list, grads):\n    \"\"\"Flattens a variables and their gradients.\n    \"\"\"\n    return tf.concat([tf.reshape(grad, [U.numel(v)])\n                      for (v, grad) in zip(var_list, grads)], 0)",
    "doc": "Flattens a variables and their gradients."
  },
  {
    "code": "def nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n    \"\"\"Creates a simple neural network\n    \"\"\"\n    for i, size in enumerate(layers_sizes):\n        activation = tf.nn.relu if i < len(layers_sizes) - 1 else None\n        input = tf.layers.dense(inputs=input,\n                                units=size,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                reuse=reuse,\n                                name=name + '_' + str(i))\n        if activation:\n            input = activation(input)\n    if flatten:\n        assert layers_sizes[-1] == 1\n        input = tf.reshape(input, [-1])\n    return input",
    "doc": "Creates a simple neural network"
  },
  {
    "code": "def mpi_fork(n, extra_mpi_args=[]):\n    \"\"\"Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children\n    \"\"\"\n    if n <= 1:\n        return \"child\"\n    if os.getenv(\"IN_MPI\") is None:\n        env = os.environ.copy()\n        env.update(\n            MKL_NUM_THREADS=\"1\",\n            OMP_NUM_THREADS=\"1\",\n            IN_MPI=\"1\"\n        )\n        # \"-bind-to core\" is crucial for good performance\n        args = [\"mpirun\", \"-np\", str(n)] + \\\n            extra_mpi_args + \\\n            [sys.executable]\n\n        args += sys.argv\n        subprocess.check_call(args, env=env)\n        return \"parent\"\n    else:\n        install_mpi_excepthook()\n        return \"child\"",
    "doc": "Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children"
  },
  {
    "code": "def convert_episode_to_batch_major(episode):\n    \"\"\"Converts an episode to have the batch dimension in the major (first)\n    dimension.\n    \"\"\"\n    episode_batch = {}\n    for key in episode.keys():\n        val = np.array(episode[key]).copy()\n        # make inputs batch-major instead of time-major\n        episode_batch[key] = val.swapaxes(0, 1)\n\n    return episode_batch",
    "doc": "Converts an episode to have the batch dimension in the major (first)\n    dimension."
  },
  {
    "code": "def reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)",
    "doc": "Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI."
  },
  {
    "code": "def add_vtarg_and_adv(seg, gamma, lam):\n    \"\"\"\n    Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)\n    \"\"\"\n    new = np.append(seg[\"new\"], 0) # last element is only used for last vtarg, but we already zeroed it if last new = 1\n    vpred = np.append(seg[\"vpred\"], seg[\"nextvpred\"])\n    T = len(seg[\"rew\"])\n    seg[\"adv\"] = gaelam = np.empty(T, 'float32')\n    rew = seg[\"rew\"]\n    lastgaelam = 0\n    for t in reversed(range(T)):\n        nonterminal = 1-new[t+1]\n        delta = rew[t] + gamma * vpred[t+1] * nonterminal - vpred[t]\n        gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n    seg[\"tdlamret\"] = seg[\"adv\"] + seg[\"vpred\"]",
    "doc": "Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)"
  },
  {
    "code": "def switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation.\n    \"\"\"\n    x_shape = copy.copy(then_expression.get_shape())\n    x = tf.cond(tf.cast(condition, 'bool'),\n                lambda: then_expression,\n                lambda: else_expression)\n    x.set_shape(x_shape)\n    return x",
    "doc": "Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation."
  },
  {
    "code": "def huber_loss(x, delta=1.0):\n    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n    return tf.where(\n        tf.abs(x) < delta,\n        tf.square(x) * 0.5,\n        delta * (tf.abs(x) - 0.5 * delta)\n    )",
    "doc": "Reference: https://en.wikipedia.org/wiki/Huber_loss"
  },
  {
    "code": "def get_session(config=None):\n    \"\"\"Get default session or create one with a given config\"\"\"\n    sess = tf.get_default_session()\n    if sess is None:\n        sess = make_session(config=config, make_default=True)\n    return sess",
    "doc": "Get default session or create one with a given config"
  },
  {
    "code": "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n    if num_cpu is None:\n        num_cpu = int(os.getenv('RCALL_NUM_CPU', multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu)\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)",
    "doc": "Returns a session that will use <num_cpu> CPU's only"
  },
  {
    "code": "def initialize():\n    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    get_session().run(tf.variables_initializer(new_variables))\n    ALREADY_INITIALIZED.update(new_variables)",
    "doc": "Initialize all the uninitialized variables in the global scope."
  },
  {
    "code": "def function(inputs, outputs, updates=None, givens=None):\n    \"\"\"Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=\"x\")\n        y = tf.placeholder(tf.int32, (), name=\"y\")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    updates: [tf.Operation] or tf.Operation\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n\n    \"\"\"\n    if isinstance(outputs, list):\n        return _Function(inputs, outputs, updates, givens=givens)\n    elif isinstance(outputs, (dict, collections.OrderedDict)):\n        f = _Function(inputs, outputs.values(), updates, givens=givens)\n        return lambda *args, **kwargs: type(outputs)(zip(outputs.keys(), f(*args, **kwargs)))\n    else:\n        f = _Function(inputs, [outputs], updates, givens=givens)\n        return lambda *args, **kwargs: f(*args, **kwargs)[0]",
    "doc": "Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=\"x\")\n        y = tf.placeholder(tf.int32, (), name=\"y\")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    updates: [tf.Operation] or tf.Operation\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored."
  },
  {
    "code": "def adjust_shape(placeholder, data):\n    '''\n    adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters:\n        placeholder     tensorflow input placeholder\n\n        data            input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns:\n        reshaped data\n    '''\n\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\n        return data\n    if isinstance(data, list):\n        data = np.array(data)\n\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\n\n    assert _check_shape(placeholder_shape, data.shape), \\\n        'Shape of data {} is not compatible with shape of the placeholder {}'.format(data.shape, placeholder_shape)\n\n    return np.reshape(data, placeholder_shape)",
    "doc": "adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters:\n        placeholder     tensorflow input placeholder\n\n        data            input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns:\n        reshaped data"
  },
  {
    "code": "def _check_shape(placeholder_shape, data_shape):\n    ''' check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)'''\n\n    return True\n    squeezed_placeholder_shape = _squeeze_shape(placeholder_shape)\n    squeezed_data_shape = _squeeze_shape(data_shape)\n\n    for i, s_data in enumerate(squeezed_data_shape):\n        s_placeholder = squeezed_placeholder_shape[i]\n        if s_placeholder != -1 and s_data != s_placeholder:\n            return False\n\n    return True",
    "doc": "check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)"
  },
  {
    "code": "def profile(n):\n    \"\"\"\n    Usage:\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n        return func_wrapper\n    return decorator_with_name",
    "doc": "Usage:\n    @profile(\"my_func\")\n    def my_func(): code"
  },
  {
    "code": "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n    \"\"\"Configure environment for DeepMind-style Atari.\n    \"\"\"\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if 'FIRE' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env",
    "doc": "Configure environment for DeepMind-style Atari."
  },
  {
    "code": "def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs",
    "doc": "Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes."
  },
  {
    "code": "def sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None: comm = MPI.COMM_WORLD\n    import tensorflow as tf\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val)\n        for (var, val) in zip(variables, values)])",
    "doc": "Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's"
  },
  {
    "code": "def gpu_count():\n    \"\"\"\n    Count the GPUs on this machine.\n    \"\"\"\n    if shutil.which('nvidia-smi') is None:\n        return 0\n    output = subprocess.check_output(['nvidia-smi', '--query-gpu=gpu_name', '--format=csv'])\n    return max(0, len(output.split(b'\\n')) - 2)",
    "doc": "Count the GPUs on this machine."
  },
  {
    "code": "def setup_mpi_gpus():\n    \"\"\"\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\n    \"\"\"\n    if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n        if sys.platform == 'darwin': # This Assumes if you're on OSX you're just\n            ids = []                 # doing a smoke test and don't want GPUs\n        else:\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\n            ids = [lrank]\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))",
    "doc": "Set CUDA_VISIBLE_DEVICES to MPI rank if not already set"
  },
  {
    "code": "def get_local_rank_size(comm):\n    \"\"\"\n    Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine\n    \"\"\"\n    this_node = platform.node()\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\n    node2rankssofar = defaultdict(int)\n    local_rank = None\n    for (rank, node) in ranks_nodes:\n        if rank == comm.Get_rank():\n            local_rank = node2rankssofar[node]\n        node2rankssofar[node] += 1\n    assert local_rank is not None\n    return local_rank, node2rankssofar[this_node]",
    "doc": "Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine"
  },
  {
    "code": "def share_file(comm, path):\n    \"\"\"\n    Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines\n    \"\"\"\n    localrank, _ = get_local_rank_size(comm)\n    if comm.Get_rank() == 0:\n        with open(path, 'rb') as fh:\n            data = fh.read()\n        comm.bcast(data)\n    else:\n        data = comm.bcast(None)\n        if localrank == 0:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, 'wb') as fh:\n                fh.write(data)\n    comm.Barrier()",
    "doc": "Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines"
  },
  {
    "code": "def dict_gather(comm, d, op='mean', assert_all_have_data=True):\n    \"\"\"\n    Perform a reduction operation over dicts\n    \"\"\"\n    if comm is None: return d\n    alldicts = comm.allgather(d)\n    size = comm.size\n    k2li = defaultdict(list)\n    for d in alldicts:\n        for (k,v) in d.items():\n            k2li[k].append(v)\n    result = {}\n    for (k,li) in k2li.items():\n        if assert_all_have_data:\n            assert len(li)==size, \"only %i out of %i MPI workers have sent '%s'\" % (len(li), size, k)\n        if op=='mean':\n            result[k] = np.mean(li, axis=0)\n        elif op=='sum':\n            result[k] = np.sum(li, axis=0)\n        else:\n            assert 0, op\n    return result",
    "doc": "Perform a reduction operation over dicts"
  },
  {
    "code": "def mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn('WARNING: tried to compute mean on non-float {}={}'.format(name, val))\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name : name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}",
    "doc": "Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean"
  },
  {
    "code": "def learn(*,\n        network,\n        env,\n        total_timesteps,\n        timesteps_per_batch=1024, # what to train on\n        max_kl=0.001,\n        cg_iters=10,\n        gamma=0.99,\n        lam=1.0, # advantage estimation\n        seed=None,\n        ent_coef=0.0,\n        cg_damping=1e-2,\n        vf_stepsize=3e-4,\n        vf_iters =3,\n        max_episodes=0, max_iters=0,  # time constraint\n        callback=None,\n        load_path=None,\n        **network_kwargs\n        ):\n    '''\n    learn a policy function with TRPO algorithm\n\n    Parameters:\n    ----------\n\n    network                 neural network to learn. Can be either string ('mlp', 'cnn', 'lstm', 'lnlstm' for basic types)\n                            or function that takes input placeholder and returns tuple (output, None) for feedforward nets\n                            or (output, (state_placeholder, state_output, mask_placeholder)) for recurrent nets\n\n    env                     environment (one of the gym environments or wrapped via baselines.common.vec_env.VecEnv-type class\n\n    timesteps_per_batch     timesteps per gradient estimation batch\n\n    max_kl                  max KL divergence between old policy and new policy ( KL(pi_old || pi) )\n\n    ent_coef                coefficient of policy entropy term in the optimization objective\n\n    cg_iters                number of iterations of conjugate gradient algorithm\n\n    cg_damping              conjugate gradient damping\n\n    vf_stepsize             learning rate for adam optimizer used to optimie value function loss\n\n    vf_iters                number of iterations of value function optimization iterations per each policy optimization step\n\n    total_timesteps           max number of timesteps\n\n    max_episodes            max number of episodes\n\n    max_iters               maximum number of policy optimization iterations\n\n    callback                function to be called with (locals(), globals()) each policy optimization step\n\n    load_path               str, path to load the model from (default: None, i.e. no model is loaded)\n\n    **network_kwargs        keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n\n    Returns:\n    -------\n\n    learnt model\n\n    '''\n\n    if MPI is not None:\n        nworkers = MPI.COMM_WORLD.Get_size()\n        rank = MPI.COMM_WORLD.Get_rank()\n    else:\n        nworkers = 1\n        rank = 0\n\n    cpus_per_worker = 1\n    U.get_session(config=tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=cpus_per_worker,\n            intra_op_parallelism_threads=cpus_per_worker\n    ))\n\n\n    policy = build_policy(env, network, value_network='copy', **network_kwargs)\n    set_global_seeds(seed)\n\n    np.set_printoptions(precision=3)\n    # Setup losses and stuff\n    # ----------------------------------------\n    ob_space = env.observation_space\n    ac_space = env.action_space\n\n    ob = observation_placeholder(ob_space)\n    with tf.variable_scope(\"pi\"):\n        pi = policy(observ_placeholder=ob)\n    with tf.variable_scope(\"oldpi\"):\n        oldpi = policy(observ_placeholder=ob)\n\n    atarg = tf.placeholder(dtype=tf.float32, shape=[None]) # Target advantage function (if applicable)\n    ret = tf.placeholder(dtype=tf.float32, shape=[None]) # Empirical return\n\n    ac = pi.pdtype.sample_placeholder([None])\n\n    kloldnew = oldpi.pd.kl(pi.pd)\n    ent = pi.pd.entropy()\n    meankl = tf.reduce_mean(kloldnew)\n    meanent = tf.reduce_mean(ent)\n    entbonus = ent_coef * meanent\n\n    vferr = tf.reduce_mean(tf.square(pi.vf - ret))\n\n    ratio = tf.exp(pi.pd.logp(ac) - oldpi.pd.logp(ac)) # advantage * pnew / pold\n    surrgain = tf.reduce_mean(ratio * atarg)\n\n    optimgain = surrgain + entbonus\n    losses = [optimgain, meankl, entbonus, surrgain, meanent]\n    loss_names = [\"optimgain\", \"meankl\", \"entloss\", \"surrgain\", \"entropy\"]\n\n    dist = meankl\n\n    all_var_list = get_trainable_variables(\"pi\")\n    # var_list = [v for v in all_var_list if v.name.split(\"/\")[1].startswith(\"pol\")]\n    # vf_var_list = [v for v in all_var_list if v.name.split(\"/\")[1].startswith(\"vf\")]\n    var_list = get_pi_trainable_variables(\"pi\")\n    vf_var_list = get_vf_trainable_variables(\"pi\")\n\n    vfadam = MpiAdam(vf_var_list)\n\n    get_flat = U.GetFlat(var_list)\n    set_from_flat = U.SetFromFlat(var_list)\n    klgrads = tf.gradients(dist, var_list)\n    flat_tangent = tf.placeholder(dtype=tf.float32, shape=[None], name=\"flat_tan\")\n    shapes = [var.get_shape().as_list() for var in var_list]\n    start = 0\n    tangents = []\n    for shape in shapes:\n        sz = U.intprod(shape)\n        tangents.append(tf.reshape(flat_tangent[start:start+sz], shape))\n        start += sz\n    gvp = tf.add_n([tf.reduce_sum(g*tangent) for (g, tangent) in zipsame(klgrads, tangents)]) #pylint: disable=E1111\n    fvp = U.flatgrad(gvp, var_list)\n\n    assign_old_eq_new = U.function([],[], updates=[tf.assign(oldv, newv)\n        for (oldv, newv) in zipsame(get_variables(\"oldpi\"), get_variables(\"pi\"))])\n\n    compute_losses = U.function([ob, ac, atarg], losses)\n    compute_lossandgrad = U.function([ob, ac, atarg], losses + [U.flatgrad(optimgain, var_list)])\n    compute_fvp = U.function([flat_tangent, ob, ac, atarg], fvp)\n    compute_vflossandgrad = U.function([ob, ret], U.flatgrad(vferr, vf_var_list))\n\n    @contextmanager\n    def timed(msg):\n        if rank == 0:\n            print(colorize(msg, color='magenta'))\n            tstart = time.time()\n            yield\n            print(colorize(\"done in %.3f seconds\"%(time.time() - tstart), color='magenta'))\n        else:\n            yield\n\n    def allmean(x):\n        assert isinstance(x, np.ndarray)\n        if MPI is not None:\n            out = np.empty_like(x)\n            MPI.COMM_WORLD.Allreduce(x, out, op=MPI.SUM)\n            out /= nworkers\n        else:\n            out = np.copy(x)\n\n        return out\n\n    U.initialize()\n    if load_path is not None:\n        pi.load(load_path)\n\n    th_init = get_flat()\n    if MPI is not None:\n        MPI.COMM_WORLD.Bcast(th_init, root=0)\n\n    set_from_flat(th_init)\n    vfadam.sync()\n    print(\"Init param sum\", th_init.sum(), flush=True)\n\n    # Prepare for rollouts\n    # ----------------------------------------\n    seg_gen = traj_segment_generator(pi, env, timesteps_per_batch, stochastic=True)\n\n    episodes_so_far = 0\n    timesteps_so_far = 0\n    iters_so_far = 0\n    tstart = time.time()\n    lenbuffer = deque(maxlen=40) # rolling buffer for episode lengths\n    rewbuffer = deque(maxlen=40) # rolling buffer for episode rewards\n\n    if sum([max_iters>0, total_timesteps>0, max_episodes>0])==0:\n        # noththing to be done\n        return pi\n\n    assert sum([max_iters>0, total_timesteps>0, max_episodes>0]) < 2, \\\n        'out of max_iters, total_timesteps, and max_episodes only one should be specified'\n\n    while True:\n        if callback: callback(locals(), globals())\n        if total_timesteps and timesteps_so_far >= total_timesteps:\n            break\n        elif max_episodes and episodes_so_far >= max_episodes:\n            break\n        elif max_iters and iters_so_far >= max_iters:\n            break\n        logger.log(\"********** Iteration %i ************\"%iters_so_far)\n\n        with timed(\"sampling\"):\n            seg = seg_gen.__next__()\n        add_vtarg_and_adv(seg, gamma, lam)\n\n        # ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets))\n        ob, ac, atarg, tdlamret = seg[\"ob\"], seg[\"ac\"], seg[\"adv\"], seg[\"tdlamret\"]\n        vpredbefore = seg[\"vpred\"] # predicted value function before udpate\n        atarg = (atarg - atarg.mean()) / atarg.std() # standardized advantage function estimate\n\n        if hasattr(pi, \"ret_rms\"): pi.ret_rms.update(tdlamret)\n        if hasattr(pi, \"ob_rms\"): pi.ob_rms.update(ob) # update running mean/std for policy\n\n        args = seg[\"ob\"], seg[\"ac\"], atarg\n        fvpargs = [arr[::5] for arr in args]\n        def fisher_vector_product(p):\n            return allmean(compute_fvp(p, *fvpargs)) + cg_damping * p\n\n        assign_old_eq_new() # set old parameter values to new parameter values\n        with timed(\"computegrad\"):\n            *lossbefore, g = compute_lossandgrad(*args)\n        lossbefore = allmean(np.array(lossbefore))\n        g = allmean(g)\n        if np.allclose(g, 0):\n            logger.log(\"Got zero gradient. not updating\")\n        else:\n            with timed(\"cg\"):\n                stepdir = cg(fisher_vector_product, g, cg_iters=cg_iters, verbose=rank==0)\n            assert np.isfinite(stepdir).all()\n            shs = .5*stepdir.dot(fisher_vector_product(stepdir))\n            lm = np.sqrt(shs / max_kl)\n            # logger.log(\"lagrange multiplier:\", lm, \"gnorm:\", np.linalg.norm(g))\n            fullstep = stepdir / lm\n            expectedimprove = g.dot(fullstep)\n            surrbefore = lossbefore[0]\n            stepsize = 1.0\n            thbefore = get_flat()\n            for _ in range(10):\n                thnew = thbefore + fullstep * stepsize\n                set_from_flat(thnew)\n                meanlosses = surr, kl, *_ = allmean(np.array(compute_losses(*args)))\n                improve = surr - surrbefore\n                logger.log(\"Expected: %.3f Actual: %.3f\"%(expectedimprove, improve))\n                if not np.isfinite(meanlosses).all():\n                    logger.log(\"Got non-finite value of losses -- bad!\")\n                elif kl > max_kl * 1.5:\n                    logger.log(\"violated KL constraint. shrinking step.\")\n                elif improve < 0:\n                    logger.log(\"surrogate didn't improve. shrinking step.\")\n                else:\n                    logger.log(\"Stepsize OK!\")\n                    break\n                stepsize *= .5\n            else:\n                logger.log(\"couldn't compute a good step\")\n                set_from_flat(thbefore)\n            if nworkers > 1 and iters_so_far % 20 == 0:\n                paramsums = MPI.COMM_WORLD.allgather((thnew.sum(), vfadam.getflat().sum())) # list of tuples\n                assert all(np.allclose(ps, paramsums[0]) for ps in paramsums[1:])\n\n        for (lossname, lossval) in zip(loss_names, meanlosses):\n            logger.record_tabular(lossname, lossval)\n\n        with timed(\"vf\"):\n\n            for _ in range(vf_iters):\n                for (mbob, mbret) in dataset.iterbatches((seg[\"ob\"], seg[\"tdlamret\"]),\n                include_final_partial_batch=False, batch_size=64):\n                    g = allmean(compute_vflossandgrad(mbob, mbret))\n                    vfadam.update(g, vf_stepsize)\n\n        logger.record_tabular(\"ev_tdlam_before\", explained_variance(vpredbefore, tdlamret))\n\n        lrlocal = (seg[\"ep_lens\"], seg[\"ep_rets\"]) # local values\n        if MPI is not None:\n            listoflrpairs = MPI.COMM_WORLD.allgather(lrlocal) # list of tuples\n        else:\n            listoflrpairs = [lrlocal]\n\n        lens, rews = map(flatten_lists, zip(*listoflrpairs))\n        lenbuffer.extend(lens)\n        rewbuffer.extend(rews)\n\n        logger.record_tabular(\"EpLenMean\", np.mean(lenbuffer))\n        logger.record_tabular(\"EpRewMean\", np.mean(rewbuffer))\n        logger.record_tabular(\"EpThisIter\", len(lens))\n        episodes_so_far += len(lens)\n        timesteps_so_far += sum(lens)\n        iters_so_far += 1\n\n        logger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n        logger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n        logger.record_tabular(\"TimeElapsed\", time.time() - tstart)\n\n        if rank==0:\n            logger.dump_tabular()\n\n    return pi",
    "doc": "learn a policy function with TRPO algorithm\n\n    Parameters:\n    ----------\n\n    network                 neural network to learn. Can be either string ('mlp', 'cnn', 'lstm', 'lnlstm' for basic types)\n                            or function that takes input placeholder and returns tuple (output, None) for feedforward nets\n                            or (output, (state_placeholder, state_output, mask_placeholder)) for recurrent nets\n\n    env                     environment (one of the gym environments or wrapped via baselines.common.vec_env.VecEnv-type class\n\n    timesteps_per_batch     timesteps per gradient estimation batch\n\n    max_kl                  max KL divergence between old policy and new policy ( KL(pi_old || pi) )\n\n    ent_coef                coefficient of policy entropy term in the optimization objective\n\n    cg_iters                number of iterations of conjugate gradient algorithm\n\n    cg_damping              conjugate gradient damping\n\n    vf_stepsize             learning rate for adam optimizer used to optimie value function loss\n\n    vf_iters                number of iterations of value function optimization iterations per each policy optimization step\n\n    total_timesteps           max number of timesteps\n\n    max_episodes            max number of episodes\n\n    max_iters               maximum number of policy optimization iterations\n\n    callback                function to be called with (locals(), globals()) each policy optimization step\n\n    load_path               str, path to load the model from (default: None, i.e. no model is loaded)\n\n    **network_kwargs        keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n\n    Returns:\n    -------\n\n    learnt model"
  },
  {
    "code": "def discount(x, gamma):\n    \"\"\"\n    computes discounted sums along 0th dimension of x.\n\n    inputs\n    ------\n    x: ndarray\n    gamma: float\n\n    outputs\n    -------\n    y: ndarray with same shape as x, satisfying\n\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\n                where k = len(x) - t - 1\n\n    \"\"\"\n    assert x.ndim >= 1\n    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]",
    "doc": "computes discounted sums along 0th dimension of x.\n\n    inputs\n    ------\n    x: ndarray\n    gamma: float\n\n    outputs\n    -------\n    y: ndarray with same shape as x, satisfying\n\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\n                where k = len(x) - t - 1"
  },
  {
    "code": "def explained_variance(ypred,y):\n    \"\"\"\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    \"\"\"\n    assert y.ndim == 1 and ypred.ndim == 1\n    vary = np.var(y)\n    return np.nan if vary==0 else 1 - np.var(y-ypred)/vary",
    "doc": "Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero"
  },
  {
    "code": "def discount_with_boundaries(X, New, gamma):\n    \"\"\"\n    X: 2d array of floats, time x features\n    New: 2d array of bools, indicating when a new episode has started\n    \"\"\"\n    Y = np.zeros_like(X)\n    T = X.shape[0]\n    Y[T-1] = X[T-1]\n    for t in range(T-2, -1, -1):\n        Y[t] = X[t] + gamma * Y[t+1] * (1 - New[t+1])\n    return Y",
    "doc": "X: 2d array of floats, time x features\n    New: 2d array of bools, indicating when a new episode has started"
  },
  {
    "code": "def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"\n        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)",
    "doc": "Sample a batch of experiences.\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise."
  },
  {
    "code": "def add(self, *args, **kwargs):\n        \"\"\"See ReplayBuffer.store_effect\"\"\"\n        idx = self._next_idx\n        super().add(*args, **kwargs)\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha",
    "doc": "See ReplayBuffer.store_effect"
  },
  {
    "code": "def sample(self, batch_size, beta):\n        \"\"\"Sample a batch of experiences.\n\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"\n        assert beta > 0\n\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n        p_min = self._it_min.min() / self._it_sum.sum()\n        max_weight = (p_min * len(self._storage)) ** (-beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self._storage)) ** (-beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        encoded_sample = self._encode_sample(idxes)\n        return tuple(list(encoded_sample) + [weights, idxes])",
    "doc": "Sample a batch of experiences.\n\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences"
  },
  {
    "code": "def update_priorities(self, idxes, priorities):\n        \"\"\"Update priorities of sampled transitions.\n\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"\n        assert len(idxes) == len(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self._storage)\n            self._it_sum[idx] = priority ** self._alpha\n            self._it_min[idx] = priority ** self._alpha\n\n            self._max_priority = max(self._max_priority, priority)",
    "doc": "Update priorities of sampled transitions.\n\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`."
  },
  {
    "code": "def wrap_deepmind_retro(env, scale=True, frame_stack=4):\n    \"\"\"\n    Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind\n    \"\"\"\n    env = WarpFrame(env)\n    env = ClipRewardEnv(env)\n    if frame_stack > 1:\n        env = FrameStack(env, frame_stack)\n    if scale:\n        env = ScaledFloatFrame(env)\n    return env",
    "doc": "Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind"
  },
  {
    "code": "def scope_vars(scope, trainable_only=False):\n    \"\"\"\n    Get variables inside a scope\n    The scope can be specified as a string\n    Parameters\n    ----------\n    scope: str or VariableScope\n        scope in which the variables reside.\n    trainable_only: bool\n        whether or not to return only the variables that were marked as trainable.\n    Returns\n    -------\n    vars: [tf.Variable]\n        list of variables in `scope`.\n    \"\"\"\n    return tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES if trainable_only else tf.GraphKeys.GLOBAL_VARIABLES,\n        scope=scope if isinstance(scope, str) else scope.name\n    )",
    "doc": "Get variables inside a scope\n    The scope can be specified as a string\n    Parameters\n    ----------\n    scope: str or VariableScope\n        scope in which the variables reside.\n    trainable_only: bool\n        whether or not to return only the variables that were marked as trainable.\n    Returns\n    -------\n    vars: [tf.Variable]\n        list of variables in `scope`."
  },
  {
    "code": "def build_act(make_obs_ph, q_func, num_actions, scope=\"deepq\", reuse=None):\n    \"\"\"Creates the act function:\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that take a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details.\n    \"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        observations_ph = make_obs_ph(\"observation\")\n        stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n        update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n\n        eps = tf.get_variable(\"eps\", (), initializer=tf.constant_initializer(0))\n\n        q_values = q_func(observations_ph.get(), num_actions, scope=\"q_func\")\n        deterministic_actions = tf.argmax(q_values, axis=1)\n\n        batch_size = tf.shape(observations_ph.get())[0]\n        random_actions = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=num_actions, dtype=tf.int64)\n        chose_random = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < eps\n        stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)\n\n        output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)\n        update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))\n        _act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph],\n                         outputs=output_actions,\n                         givens={update_eps_ph: -1.0, stochastic_ph: True},\n                         updates=[update_eps_expr])\n        def act(ob, stochastic=True, update_eps=-1):\n            return _act(ob, stochastic, update_eps)\n        return act",
    "doc": "Creates the act function:\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that take a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details."
  },
  {
    "code": "def build_act_with_param_noise(make_obs_ph, q_func, num_actions, scope=\"deepq\", reuse=None, param_noise_filter_func=None):\n    \"\"\"Creates the act function with support for parameter space noise exploration (https://arxiv.org/abs/1706.01905):\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that take a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n    param_noise_filter_func: tf.Variable -> bool\n        function that decides whether or not a variable should be perturbed. Only applicable\n        if param_noise is True. If set to None, default_param_noise_filter is used by default.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float, bool, float, bool) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details.\n    \"\"\"\n    if param_noise_filter_func is None:\n        param_noise_filter_func = default_param_noise_filter\n\n    with tf.variable_scope(scope, reuse=reuse):\n        observations_ph = make_obs_ph(\"observation\")\n        stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n        update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n        update_param_noise_threshold_ph = tf.placeholder(tf.float32, (), name=\"update_param_noise_threshold\")\n        update_param_noise_scale_ph = tf.placeholder(tf.bool, (), name=\"update_param_noise_scale\")\n        reset_ph = tf.placeholder(tf.bool, (), name=\"reset\")\n\n        eps = tf.get_variable(\"eps\", (), initializer=tf.constant_initializer(0))\n        param_noise_scale = tf.get_variable(\"param_noise_scale\", (), initializer=tf.constant_initializer(0.01), trainable=False)\n        param_noise_threshold = tf.get_variable(\"param_noise_threshold\", (), initializer=tf.constant_initializer(0.05), trainable=False)\n\n        # Unmodified Q.\n        q_values = q_func(observations_ph.get(), num_actions, scope=\"q_func\")\n\n        # Perturbable Q used for the actual rollout.\n        q_values_perturbed = q_func(observations_ph.get(), num_actions, scope=\"perturbed_q_func\")\n        # We have to wrap this code into a function due to the way tf.cond() works. See\n        # https://stackoverflow.com/questions/37063952/confused-by-the-behavior-of-tf-cond for\n        # a more detailed discussion.\n        def perturb_vars(original_scope, perturbed_scope):\n            all_vars = scope_vars(absolute_scope_name(original_scope))\n            all_perturbed_vars = scope_vars(absolute_scope_name(perturbed_scope))\n            assert len(all_vars) == len(all_perturbed_vars)\n            perturb_ops = []\n            for var, perturbed_var in zip(all_vars, all_perturbed_vars):\n                if param_noise_filter_func(perturbed_var):\n                    # Perturb this variable.\n                    op = tf.assign(perturbed_var, var + tf.random_normal(shape=tf.shape(var), mean=0., stddev=param_noise_scale))\n                else:\n                    # Do not perturb, just assign.\n                    op = tf.assign(perturbed_var, var)\n                perturb_ops.append(op)\n            assert len(perturb_ops) == len(all_vars)\n            return tf.group(*perturb_ops)\n\n        # Set up functionality to re-compute `param_noise_scale`. This perturbs yet another copy\n        # of the network and measures the effect of that perturbation in action space. If the perturbation\n        # is too big, reduce scale of perturbation, otherwise increase.\n        q_values_adaptive = q_func(observations_ph.get(), num_actions, scope=\"adaptive_q_func\")\n        perturb_for_adaption = perturb_vars(original_scope=\"q_func\", perturbed_scope=\"adaptive_q_func\")\n        kl = tf.reduce_sum(tf.nn.softmax(q_values) * (tf.log(tf.nn.softmax(q_values)) - tf.log(tf.nn.softmax(q_values_adaptive))), axis=-1)\n        mean_kl = tf.reduce_mean(kl)\n        def update_scale():\n            with tf.control_dependencies([perturb_for_adaption]):\n                update_scale_expr = tf.cond(mean_kl < param_noise_threshold,\n                    lambda: param_noise_scale.assign(param_noise_scale * 1.01),\n                    lambda: param_noise_scale.assign(param_noise_scale / 1.01),\n                )\n            return update_scale_expr\n\n        # Functionality to update the threshold for parameter space noise.\n        update_param_noise_threshold_expr = param_noise_threshold.assign(tf.cond(update_param_noise_threshold_ph >= 0,\n            lambda: update_param_noise_threshold_ph, lambda: param_noise_threshold))\n\n        # Put everything together.\n        deterministic_actions = tf.argmax(q_values_perturbed, axis=1)\n        batch_size = tf.shape(observations_ph.get())[0]\n        random_actions = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=num_actions, dtype=tf.int64)\n        chose_random = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < eps\n        stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)\n\n        output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)\n        update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))\n        updates = [\n            update_eps_expr,\n            tf.cond(reset_ph, lambda: perturb_vars(original_scope=\"q_func\", perturbed_scope=\"perturbed_q_func\"), lambda: tf.group(*[])),\n            tf.cond(update_param_noise_scale_ph, lambda: update_scale(), lambda: tf.Variable(0., trainable=False)),\n            update_param_noise_threshold_expr,\n        ]\n        _act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph, reset_ph, update_param_noise_threshold_ph, update_param_noise_scale_ph],\n                         outputs=output_actions,\n                         givens={update_eps_ph: -1.0, stochastic_ph: True, reset_ph: False, update_param_noise_threshold_ph: False, update_param_noise_scale_ph: False},\n                         updates=updates)\n        def act(ob, reset=False, update_param_noise_threshold=False, update_param_noise_scale=False, stochastic=True, update_eps=-1):\n            return _act(ob, stochastic, update_eps, reset, update_param_noise_threshold, update_param_noise_scale)\n        return act",
    "doc": "Creates the act function with support for parameter space noise exploration (https://arxiv.org/abs/1706.01905):\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that take a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n    param_noise_filter_func: tf.Variable -> bool\n        function that decides whether or not a variable should be perturbed. Only applicable\n        if param_noise is True. If set to None, default_param_noise_filter is used by default.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float, bool, float, bool) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details."
  },
  {
    "code": "def build_train(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping=None, gamma=1.0,\n    double_q=True, scope=\"deepq\", reuse=None, param_noise=False, param_noise_filter_func=None):\n    \"\"\"Creates the train function:\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that takes a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions\n    reuse: bool\n        whether or not to reuse the graph variables\n    optimizer: tf.train.Optimizer\n        optimizer to use for the Q-learning objective.\n    grad_norm_clipping: float or None\n        clip gradient norms to this value. If None no clipping is performed.\n    gamma: float\n        discount rate.\n    double_q: bool\n        if true will use Double Q Learning (https://arxiv.org/abs/1509.06461).\n        In general it is a good idea to keep it enabled.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    param_noise_filter_func: tf.Variable -> bool\n        function that decides whether or not a variable should be perturbed. Only applicable\n        if param_noise is True. If set to None, default_param_noise_filter is used by default.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details.\n    train: (object, np.array, np.array, object, np.array, np.array) -> np.array\n        optimize the error in Bellman's equation.\n`       See the top of the file for details.\n    update_target: () -> ()\n        copy the parameters from optimized Q function to the target Q function.\n`       See the top of the file for details.\n    debug: {str: function}\n        a bunch of functions to print debug data like q_values.\n    \"\"\"\n    if param_noise:\n        act_f = build_act_with_param_noise(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse,\n            param_noise_filter_func=param_noise_filter_func)\n    else:\n        act_f = build_act(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse)\n\n    with tf.variable_scope(scope, reuse=reuse):\n        # set up placeholders\n        obs_t_input = make_obs_ph(\"obs_t\")\n        act_t_ph = tf.placeholder(tf.int32, [None], name=\"action\")\n        rew_t_ph = tf.placeholder(tf.float32, [None], name=\"reward\")\n        obs_tp1_input = make_obs_ph(\"obs_tp1\")\n        done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n        importance_weights_ph = tf.placeholder(tf.float32, [None], name=\"weight\")\n\n        # q network evaluation\n        q_t = q_func(obs_t_input.get(), num_actions, scope=\"q_func\", reuse=True)  # reuse parameters from act\n        q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + \"/q_func\")\n\n        # target q network evalution\n        q_tp1 = q_func(obs_tp1_input.get(), num_actions, scope=\"target_q_func\")\n        target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + \"/target_q_func\")\n\n        # q scores for actions which we know were selected in the given state.\n        q_t_selected = tf.reduce_sum(q_t * tf.one_hot(act_t_ph, num_actions), 1)\n\n        # compute estimate of best possible value starting from state at t + 1\n        if double_q:\n            q_tp1_using_online_net = q_func(obs_tp1_input.get(), num_actions, scope=\"q_func\", reuse=True)\n            q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)\n            q_tp1_best = tf.reduce_sum(q_tp1 * tf.one_hot(q_tp1_best_using_online_net, num_actions), 1)\n        else:\n            q_tp1_best = tf.reduce_max(q_tp1, 1)\n        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n\n        # compute RHS of bellman equation\n        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n\n        # compute the error (potentially clipped)\n        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n        errors = U.huber_loss(td_error)\n        weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n\n        # compute optimization op (potentially with gradient clipping)\n        if grad_norm_clipping is not None:\n            gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)\n            for i, (grad, var) in enumerate(gradients):\n                if grad is not None:\n                    gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)\n            optimize_expr = optimizer.apply_gradients(gradients)\n        else:\n            optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)\n\n        # update_target_fn will be called periodically to copy Q network to target Q network\n        update_target_expr = []\n        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),\n                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n            update_target_expr.append(var_target.assign(var))\n        update_target_expr = tf.group(*update_target_expr)\n\n        # Create callable functions\n        train = U.function(\n            inputs=[\n                obs_t_input,\n                act_t_ph,\n                rew_t_ph,\n                obs_tp1_input,\n                done_mask_ph,\n                importance_weights_ph\n            ],\n            outputs=td_error,\n            updates=[optimize_expr]\n        )\n        update_target = U.function([], [], updates=[update_target_expr])\n\n        q_values = U.function([obs_t_input], q_t)\n\n        return act_f, train, update_target, {'q_values': q_values}",
    "doc": "Creates the train function:\n\n    Parameters\n    ----------\n    make_obs_ph: str -> tf.placeholder or TfInput\n        a function that takes a name and creates a placeholder of input with that name\n    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n        the model that takes the following inputs:\n            observation_in: object\n                the output of observation placeholder\n            num_actions: int\n                number of actions\n            scope: str\n            reuse: bool\n                should be passed to outer variable scope\n        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n    num_actions: int\n        number of actions\n    reuse: bool\n        whether or not to reuse the graph variables\n    optimizer: tf.train.Optimizer\n        optimizer to use for the Q-learning objective.\n    grad_norm_clipping: float or None\n        clip gradient norms to this value. If None no clipping is performed.\n    gamma: float\n        discount rate.\n    double_q: bool\n        if true will use Double Q Learning (https://arxiv.org/abs/1509.06461).\n        In general it is a good idea to keep it enabled.\n    scope: str or VariableScope\n        optional scope for variable_scope.\n    reuse: bool or None\n        whether or not the variables should be reused. To be able to reuse the scope must be given.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    param_noise_filter_func: tf.Variable -> bool\n        function that decides whether or not a variable should be perturbed. Only applicable\n        if param_noise is True. If set to None, default_param_noise_filter is used by default.\n\n    Returns\n    -------\n    act: (tf.Variable, bool, float) -> tf.Variable\n        function to select and action given observation.\n`       See the top of the file for details.\n    train: (object, np.array, np.array, object, np.array, np.array) -> np.array\n        optimize the error in Bellman's equation.\n`       See the top of the file for details.\n    update_target: () -> ()\n        copy the parameters from optimized Q function to the target Q function.\n`       See the top of the file for details.\n    debug: {str: function}\n        a bunch of functions to print debug data like q_values."
  },
  {
    "code": "def profile_tf_runningmeanstd():\n    import time\n    from baselines.common import tf_util\n\n    tf_util.get_session( config=tf.ConfigProto(\n        inter_op_parallelism_threads=1,\n        intra_op_parallelism_threads=1,\n        allow_soft_placement=True\n    ))\n\n    x = np.random.random((376,))\n\n    n_trials = 10000\n    rms = RunningMeanStd()\n    tfrms = TfRunningMeanStd()\n\n    tic1 = time.time()\n    for _ in range(n_trials):\n        rms.update(x)\n\n    tic2 = time.time()\n    for _ in range(n_trials):\n        tfrms.update(x)\n\n    tic3 = time.time()\n\n    print('rms update time ({} trials): {} s'.format(n_trials, tic2 - tic1))\n    print('tfrms update time ({} trials): {} s'.format(n_trials, tic3 - tic2))\n\n\n    tic1 = time.time()\n    for _ in range(n_trials):\n        z1 = rms.mean\n\n    tic2 = time.time()\n    for _ in range(n_trials):\n        z2 = tfrms.mean\n\n    assert z1 == z2\n\n    tic3 = time.time()\n\n    print('rms get mean time ({} trials): {} s'.format(n_trials, tic2 - tic1))\n    print('tfrms get mean time ({} trials): {} s'.format(n_trials, tic3 - tic2))\n\n\n\n    '''\n    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101\n    run_metadata = tf.RunMetadata()\n    profile_opts = dict(options=options, run_metadata=run_metadata)\n\n\n\n    from tensorflow.python.client import timeline\n    fetched_timeline = timeline.Timeline(run_metadata.step_stats) #pylint: disable=E1101\n    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n    outfile = '/tmp/timeline.json'\n    with open(outfile, 'wt') as f:\n        f.write(chrome_trace)\n    print('Successfully saved profile to {}. Exiting.'.format(outfile))\n    exit(0)\n    '''",
    "doc": "options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101\n    run_metadata = tf.RunMetadata()\n    profile_opts = dict(options=options, run_metadata=run_metadata)\n\n\n\n    from tensorflow.python.client import timeline\n    fetched_timeline = timeline.Timeline(run_metadata.step_stats) #pylint: disable=E1101\n    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n    outfile = '/tmp/timeline.json'\n    with open(outfile, 'wt') as f:\n        f.write(chrome_trace)\n    print('Successfully saved profile to {}. Exiting.'.format(outfile))\n    exit(0)"
  },
  {
    "code": "def make_sample_her_transitions(replay_strategy, replay_k, reward_fun):\n    \"\"\"Creates a sample function that can be used for HER experience replay.\n\n    Args:\n        replay_strategy (in ['future', 'none']): the HER replay strategy; if set to 'none',\n            regular DDPG experience replay is used\n        replay_k (int): the ratio between HER replays and regular replays (e.g. k = 4 -> 4 times\n            as many HER replays as regular replays are used)\n        reward_fun (function): function to re-compute the reward with substituted goals\n    \"\"\"\n    if replay_strategy == 'future':\n        future_p = 1 - (1. / (1 + replay_k))\n    else:  # 'replay_strategy' == 'none'\n        future_p = 0\n\n    def _sample_her_transitions(episode_batch, batch_size_in_transitions):\n        \"\"\"episode_batch is {key: array(buffer_size x T x dim_key)}\n        \"\"\"\n        T = episode_batch['u'].shape[1]\n        rollout_batch_size = episode_batch['u'].shape[0]\n        batch_size = batch_size_in_transitions\n\n        # Select which episodes and time steps to use.\n        episode_idxs = np.random.randint(0, rollout_batch_size, batch_size)\n        t_samples = np.random.randint(T, size=batch_size)\n        transitions = {key: episode_batch[key][episode_idxs, t_samples].copy()\n                       for key in episode_batch.keys()}\n\n        # Select future time indexes proportional with probability future_p. These\n        # will be used for HER replay by substituting in future goals.\n        her_indexes = np.where(np.random.uniform(size=batch_size) < future_p)\n        future_offset = np.random.uniform(size=batch_size) * (T - t_samples)\n        future_offset = future_offset.astype(int)\n        future_t = (t_samples + 1 + future_offset)[her_indexes]\n\n        # Replace goal with achieved goal but only for the previously-selected\n        # HER transitions (as defined by her_indexes). For the other transitions,\n        # keep the original goal.\n        future_ag = episode_batch['ag'][episode_idxs[her_indexes], future_t]\n        transitions['g'][her_indexes] = future_ag\n\n        # Reconstruct info dictionary for reward  computation.\n        info = {}\n        for key, value in transitions.items():\n            if key.startswith('info_'):\n                info[key.replace('info_', '')] = value\n\n        # Re-compute reward since we may have substituted the goal.\n        reward_params = {k: transitions[k] for k in ['ag_2', 'g']}\n        reward_params['info'] = info\n        transitions['r'] = reward_fun(**reward_params)\n\n        transitions = {k: transitions[k].reshape(batch_size, *transitions[k].shape[1:])\n                       for k in transitions.keys()}\n\n        assert(transitions['u'].shape[0] == batch_size_in_transitions)\n\n        return transitions\n\n    return _sample_her_transitions",
    "doc": "Creates a sample function that can be used for HER experience replay.\n\n    Args:\n        replay_strategy (in ['future', 'none']): the HER replay strategy; if set to 'none',\n            regular DDPG experience replay is used\n        replay_k (int): the ratio between HER replays and regular replays (e.g. k = 4 -> 4 times\n            as many HER replays as regular replays are used)\n        reward_fun (function): function to re-compute the reward with substituted goals"
  },
  {
    "code": "def model(inpt, num_actions, scope, reuse=False):\n    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        out = inpt\n        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.tanh)\n        out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n        return out",
    "doc": "This model takes as input an observation and returns values of all actions."
  },
  {
    "code": "def sample(self, batch_size):\n        \"\"\"Returns a dict {key: array(batch_size x shapes[key])}\n        \"\"\"\n        buffers = {}\n\n        with self.lock:\n            assert self.current_size > 0\n            for key in self.buffers.keys():\n                buffers[key] = self.buffers[key][:self.current_size]\n\n        buffers['o_2'] = buffers['o'][:, 1:, :]\n        buffers['ag_2'] = buffers['ag'][:, 1:, :]\n\n        transitions = self.sample_transitions(buffers, batch_size)\n\n        for key in (['r', 'o_2', 'ag_2'] + list(self.buffers.keys())):\n            assert key in transitions, \"key %s missing from transitions\" % key\n\n        return transitions",
    "doc": "Returns a dict {key: array(batch_size x shapes[key])}"
  },
  {
    "code": "def store_episode(self, episode_batch):\n        \"\"\"episode_batch: array(batch_size x (T or T+1) x dim_key)\n        \"\"\"\n        batch_sizes = [len(episode_batch[key]) for key in episode_batch.keys()]\n        assert np.all(np.array(batch_sizes) == batch_sizes[0])\n        batch_size = batch_sizes[0]\n\n        with self.lock:\n            idxs = self._get_storage_idx(batch_size)\n\n            # load inputs into buffers\n            for key in self.buffers.keys():\n                self.buffers[key][idxs] = episode_batch[key]\n\n            self.n_transitions_stored += batch_size * self.T",
    "doc": "episode_batch: array(batch_size x (T or T+1) x dim_key)"
  },
  {
    "code": "def store_episode(self, episode_batch, update_stats=True):\n        \"\"\"\n        episode_batch: array of batch_size x (T or T+1) x dim_key\n                       'o' is of size T+1, others are of size T\n        \"\"\"\n\n        self.buffer.store_episode(episode_batch)\n\n        if update_stats:\n            # add transitions to normalizer\n            episode_batch['o_2'] = episode_batch['o'][:, 1:, :]\n            episode_batch['ag_2'] = episode_batch['ag'][:, 1:, :]\n            num_normalizing_transitions = transitions_in_episode_batch(episode_batch)\n            transitions = self.sample_transitions(episode_batch, num_normalizing_transitions)\n\n            o, g, ag = transitions['o'], transitions['g'], transitions['ag']\n            transitions['o'], transitions['g'] = self._preprocess_og(o, ag, g)\n            # No need to preprocess the o_2 and g_2 since this is only used for stats\n\n            self.o_stats.update(transitions['o'])\n            self.g_stats.update(transitions['g'])\n\n            self.o_stats.recompute_stats()\n            self.g_stats.recompute_stats()",
    "doc": "episode_batch: array of batch_size x (T or T+1) x dim_key\n                       'o' is of size T+1, others are of size T"
  },
  {
    "code": "def parse_cmdline_kwargs(args):\n    '''\n    convert a list of '='-spaced command-line arguments to a dictionary, evaluating python objects when possible\n    '''\n    def parse(v):\n\n        assert isinstance(v, str)\n        try:\n            return eval(v)\n        except (NameError, SyntaxError):\n            return v\n\n    return {k: parse(v) for k,v in parse_unknown_args(args).items()}",
    "doc": "convert a list of '='-spaced command-line arguments to a dictionary, evaluating python objects when possible"
  },
  {
    "code": "def cached_make_env(make_env):\n    \"\"\"\n    Only creates a new environment from the provided function if one has not yet already been\n    created. This is useful here because we need to infer certain properties of the env, e.g.\n    its observation and action spaces, without any intend of actually using it.\n    \"\"\"\n    if make_env not in CACHED_ENVS:\n        env = make_env()\n        CACHED_ENVS[make_env] = env\n    return CACHED_ENVS[make_env]",
    "doc": "Only creates a new environment from the provided function if one has not yet already been\n    created. This is useful here because we need to infer certain properties of the env, e.g.\n    its observation and action spaces, without any intend of actually using it."
  },
  {
    "code": "def compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros/r\n            y1 = max(0, 1-rinv)*T + min(1, rinv)*y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1",
    "doc": "Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate."
  },
  {
    "code": "def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])",
    "doc": "Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates."
  },
  {
    "code": "def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)",
    "doc": "Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates."
  },
  {
    "code": "def draw_on_image(self, image, color=(0, 255, 0), alpha=1.0, size=3,\n                      copy=True, raise_if_out_of_image=False):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),))\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size//2, 0)\n        x2 = min(x + 1 + size//2, width)\n        y1 = max(y - size//2, 0)\n        y2 = min(y + 1 + size//2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = (x1_clipped < 0 or x1_clipped >= width)\n        x2_clipped_ooi = (x2_clipped < 0 or x2_clipped >= width+1)\n        y1_clipped_ooi = (y1_clipped < 0 or y1_clipped >= height)\n        y2_clipped_ooi = (y2_clipped < 0 or y2_clipped >= height+1)\n        x_ooi = (x1_clipped_ooi and x2_clipped_ooi)\n        y_ooi = (y1_clipped_ooi and y2_clipped_ooi)\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                        (1 - alpha)\n                        * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped]\n                        + alpha_color\n                )\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape))\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image",
    "doc": "Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint."
  },
  {
    "code": "def generate_similar_points_manhattan(self, nb_steps, step_size, return_array=False):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros((nb_steps + 1 + nb_steps + 2*(nb_steps**2), 2), dtype=np.float32)\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(self.y - nb_steps * step_size, self.y + nb_steps * step_size, nb_steps + 1 + nb_steps)\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(self.x - (width-1)//2 * step_size, self.x + (width-1)//2 * step_size, width)\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [self.deepcopy(x=points[i, 0], y=points[i, 1]) for i in sm.xrange(points.shape[0])]",
    "doc": "Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included."
  },
  {
    "code": "def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)",
    "doc": "Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)",
    "doc": "Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy."
  },
  {
    "code": "def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)",
    "doc": "Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints."
  },
  {
    "code": "def draw_on_image(self, image, color=(0, 255, 0), alpha=1.0, size=3,\n                      copy=True, raise_if_out_of_image=False):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image, color=color, alpha=alpha, size=size, copy=False,\n                raise_if_out_of_image=raise_if_out_of_image)\n        return image",
    "doc": "Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints."
  },
  {
    "code": "def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)",
    "doc": "Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them."
  },
  {
    "code": "def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result",
    "doc": "Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form."
  },
  {
    "code": "def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)",
    "doc": "Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array."
  },
  {
    "code": "def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size-1)//2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width-1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height-1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image",
    "doc": "Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints."
  },
  {
    "code": "def from_keypoint_image(image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None): # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\" % (\n                type(if_not_found_coords),))\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = (image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold)\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)",
    "doc": "Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints."
  },
  {
    "code": "def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1/(distance_maps+1)\n        return distance_maps",
    "doc": "Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``."
  },
  {
    "code": "def from_distance_maps(distance_maps, inverted=False, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=None, # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n                           nb_channels=None):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\" % (\n                type(if_not_found_coords),))\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = (distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold)\n            elif inverted and threshold is not None:\n                found = (distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold)\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)",
    "doc": "Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints."
  },
  {
    "code": "def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result",
    "doc": "Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)",
    "doc": "Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy."
  },
  {
    "code": "def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2",
    "doc": "Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise."
  },
  {
    "code": "def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords([(self.x1, self.y1), (self.x2, self.y2)],\n                                     from_shape, to_shape)\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label)",
    "doc": "Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates."
  },
  {
    "code": "def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom\n        )",
    "doc": "Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box."
  },
  {
    "code": "def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)",
    "doc": "Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything."
  },
  {
    "code": "def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )",
    "doc": "Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes."
  },
  {
    "code": "def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0",
    "doc": "Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes."
  },
  {
    "code": "def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height",
    "doc": "Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise."
  },
  {
    "code": "def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width-eps, y1=0, y2=height-eps)\n        return self.intersection(img_bb) is not None",
    "doc": "Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise."
  },
  {
    "code": "def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully",
    "doc": "Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise."
  },
  {
    "code": "def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(\n            x1=x1,\n            y1=y1,\n            x2=x2,\n            y2=y2,\n            label=self.label\n        )",
    "doc": "Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1+left-right,\n            x2=self.x2+left-right,\n            y1=self.y1+top-bottom,\n            y2=self.y2+top-bottom\n        )",
    "doc": "Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box."
  },
  {
    "code": "def draw_on_image(self, image, color=(0, 255, 0), alpha=1.0, size=1,\n                      copy=True, raise_if_out_of_image=False, thickness=None):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\" % (\n                self.x1, self.y1, self.x2, self.y2, image.shape))\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0]-1)\n                y2 = np.clip(y2, 0, image.shape[0]-1)\n                x1 = np.clip(x1, 0, image.shape[1]-1)\n                x2 = np.clip(x2, 0, image.shape[1]-1)\n\n            y = [y1-i, y1-i, y2+i, y2+i]\n            x = [x1-i, x2+i, x2+i, x1-i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result",
    "doc": "Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it."
  },
  {
    "code": "def extract_from_image(self, image, pad=True, pad_max=None,\n                           prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height-1)\n            x1, x2 = np.clip([x1, x2], 0, width-1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max)\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0)\n                <= (x1, y1, x2, y2)\n                < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = (out_height > 0)\n            nonzero_width = (out_width > 0)\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]),\n                            dtype=image.dtype)",
    "doc": "Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``."
  },
  {
    "code": "def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2)\n        ]",
    "doc": "Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints."
  },
  {
    "code": "def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label\n        )",
    "doc": "Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)",
    "doc": "Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy."
  },
  {
    "code": "def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape)\n                          for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)",
    "doc": "Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes."
  },
  {
    "code": "def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(xyxy.shape[1] == 4, \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,))\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)",
    "doc": "Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates."
  },
  {
    "code": "def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)",
    "doc": "Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``."
  },
  {
    "code": "def draw_on_image(self, image, color=(0, 255, 0), alpha=1.0, size=1,\n                      copy=True, raise_if_out_of_image=False, thickness=None):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness\n            )\n\n        return image",
    "doc": "Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes."
  },
  {
    "code": "def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [bb for bb in self.bounding_boxes\n                     if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)",
    "doc": "Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed."
  },
  {
    "code": "def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [bb.clip_out_of_image(self.shape)\n                   for bb in self.bounding_boxes if bb.is_partly_within_image(self.shape)]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)",
    "doc": "Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [bb.shift(top=top, right=right, bottom=bottom, left=left) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)",
    "doc": "Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes."
  },
  {
    "code": "def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))",
    "doc": "Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy."
  },
  {
    "code": "def Emboss(alpha=0, strength=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that embosses images and overlays the result with the original\n    image.\n\n    The embossed version pronounces highlights and shadows,\n    letting the image look as if it was recreated on a metal plate (\"embossed\").\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    strength : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the strength of the embossing.\n        Sane values are somewhere in the range ``(0, 2)`` with 1 being the standard\n        embossing effect. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))\n\n    embosses an image with a variable strength in the range ``0.5 <= x <= 1.5``\n    and overlays the result with a variable alpha in the range ``0.0 <= a <= 1.0``\n    over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True,\n                                              list_to_choice=True)\n    strength_param = iap.handle_continuous_param(strength, \"strength\", value_range=(0, None), tuple_to_uniform=True,\n                                                 list_to_choice=True)\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        strength_sample = strength_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([\n            [0, 0, 0],\n            [0, 1, 0],\n            [0, 0, 0]\n        ], dtype=np.float32)\n        matrix_effect = np.array([\n            [-1-strength_sample, 0-strength_sample, 0],\n            [0-strength_sample, 1, 0+strength_sample],\n            [0, 0+strength_sample, 1+strength_sample]\n        ], dtype=np.float32)\n        matrix = (1-alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(create_matrices, name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter that embosses images and overlays the result with the original\n    image.\n\n    The embossed version pronounces highlights and shadows,\n    letting the image look as if it was recreated on a metal plate (\"embossed\").\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    strength : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the strength of the embossing.\n        Sane values are somewhere in the range ``(0, 2)`` with 1 being the standard\n        embossing effect. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))\n\n    embosses an image with a variable strength in the range ``0.5 <= x <= 1.5``\n    and overlays the result with a variable alpha in the range ``0.0 <= a <= 1.0``\n    over the old image."
  },
  {
    "code": "def EdgeDetect(alpha=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that detects all edges in images, marks them in\n    a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = EdgeDetect(alpha=(0.0, 1.0))\n\n    detects edges in an image  and overlays the result with a variable alpha\n    in the range ``0.0 <= a <= 1.0`` over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True,\n                                              list_to_choice=True)\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        matrix_nochange = np.array([\n            [0, 0, 0],\n            [0, 1, 0],\n            [0, 0, 0]\n        ], dtype=np.float32)\n        matrix_effect = np.array([\n            [0, 1, 0],\n            [1, -4, 1],\n            [0, 1, 0]\n        ], dtype=np.float32)\n        matrix = (1-alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(create_matrices, name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter that detects all edges in images, marks them in\n    a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = EdgeDetect(alpha=(0.0, 1.0))\n\n    detects edges in an image  and overlays the result with a variable alpha\n    in the range ``0.0 <= a <= 1.0`` over the old image."
  },
  {
    "code": "def DirectedEdgeDetect(alpha=0, direction=(0.0, 1.0), name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that detects edges that have certain directions and marks them\n    in a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of edges to pronounce, where 0 represents 0 degrees and 1.0\n        represents 360 degrees (both clockwise, starting at the top).\n        Default value is ``(0.0, 1.0)``, i.e. pick a random angle per image.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=0)\n\n    turns input images into edge images in which edges are detected from\n    top side of the image (i.e. the top sides of horizontal edges are\n    added to the output).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=90/360)\n\n    same as before, but detecting edges from the right (right side of each\n    vertical edge).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=(0.0, 1.0))\n\n    same as before, but detecting edges from a variable direction (anything\n    between 0 and 1.0, i.e. 0 degrees and 360 degrees, starting from the\n    top and moving clockwise).\n\n    >>> aug = DirectedEdgeDetect(alpha=(0.0, 0.3), direction=0)\n\n    generates edge images (edges detected from the top) and overlays them\n    with the input images by a variable amount between 0 and 30 percent\n    (e.g. for 0.3 then ``0.7*old_image + 0.3*edge_image``).\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True,\n                                              list_to_choice=True)\n    direction_param = iap.handle_continuous_param(direction, \"direction\", value_range=None, tuple_to_uniform=True,\n                                                  list_to_choice=True)\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        direction_sample = direction_param.draw_sample(random_state=random_state_func)\n\n        deg = int(direction_sample * 360) % 360\n        rad = np.deg2rad(deg)\n        x = np.cos(rad - 0.5*np.pi)\n        y = np.sin(rad - 0.5*np.pi)\n        direction_vector = np.array([x, y])\n\n        matrix_effect = np.array([\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0]\n        ], dtype=np.float32)\n        for x in [-1, 0, 1]:\n            for y in [-1, 0, 1]:\n                if (x, y) != (0, 0):\n                    cell_vector = np.array([x, y])\n                    distance_deg = np.rad2deg(ia.angle_between_vectors(cell_vector, direction_vector))\n                    distance = distance_deg / 180\n                    similarity = (1 - distance)**4\n                    matrix_effect[y+1, x+1] = similarity\n        matrix_effect = matrix_effect / np.sum(matrix_effect)\n        matrix_effect = matrix_effect * (-1)\n        matrix_effect[1, 1] = 1\n\n        matrix_nochange = np.array([\n            [0, 0, 0],\n            [0, 1, 0],\n            [0, 0, 0]\n        ], dtype=np.float32)\n\n        matrix = (1-alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(create_matrices, name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter that detects edges that have certain directions and marks them\n    in a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of edges to pronounce, where 0 represents 0 degrees and 1.0\n        represents 360 degrees (both clockwise, starting at the top).\n        Default value is ``(0.0, 1.0)``, i.e. pick a random angle per image.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=0)\n\n    turns input images into edge images in which edges are detected from\n    top side of the image (i.e. the top sides of horizontal edges are\n    added to the output).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=90/360)\n\n    same as before, but detecting edges from the right (right side of each\n    vertical edge).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=(0.0, 1.0))\n\n    same as before, but detecting edges from a variable direction (anything\n    between 0 and 1.0, i.e. 0 degrees and 360 degrees, starting from the\n    top and moving clockwise).\n\n    >>> aug = DirectedEdgeDetect(alpha=(0.0, 0.3), direction=0)\n\n    generates edge images (edges detected from the top) and overlays them\n    with the input images by a variable amount between 0 and 30 percent\n    (e.g. for 0.3 then ``0.7*old_image + 0.3*edge_image``)."
  },
  {
    "code": "def normalize_shape(shape):\n    \"\"\"\n    Normalize a shape tuple or array to a shape tuple.\n\n    Parameters\n    ----------\n    shape : tuple of int or ndarray\n        The input to normalize. May optionally be an array.\n\n    Returns\n    -------\n    tuple of int\n        Shape tuple.\n\n    \"\"\"\n    if isinstance(shape, tuple):\n        return shape\n    assert ia.is_np_array(shape), (\n        \"Expected tuple of ints or array, got %s.\" % (type(shape),))\n    return shape.shape",
    "doc": "Normalize a shape tuple or array to a shape tuple.\n\n    Parameters\n    ----------\n    shape : tuple of int or ndarray\n        The input to normalize. May optionally be an array.\n\n    Returns\n    -------\n    tuple of int\n        Shape tuple."
  },
  {
    "code": "def project_coords(coords, from_shape, to_shape):\n    \"\"\"\n    Project coordinates from one image shape to another.\n\n    This performs a relative projection, e.g. a point at 60% of the old\n    image width will be at 60% of the new image width after projection.\n\n    Parameters\n    ----------\n    coords : ndarray or tuple of number\n        Coordinates to project. Either a ``(N,2)`` numpy array or a tuple\n        of `(x,y)` coordinates.\n\n    from_shape : tuple of int or ndarray\n        Old image shape.\n\n    to_shape : tuple of int or ndarray\n        New image shape.\n\n    Returns\n    -------\n    ndarray\n        Projected coordinates as ``(N,2)`` ``float32`` numpy array.\n\n    \"\"\"\n    from_shape = normalize_shape(from_shape)\n    to_shape = normalize_shape(to_shape)\n    if from_shape[0:2] == to_shape[0:2]:\n        return coords\n\n    from_height, from_width = from_shape[0:2]\n    to_height, to_width = to_shape[0:2]\n    assert all([v > 0 for v in [from_height, from_width, to_height, to_width]])\n\n    # make sure to not just call np.float32(coords) here as the following lines\n    # perform in-place changes and np.float32(.) only copies if the input\n    # was *not* a float32 array\n    coords_proj = np.array(coords).astype(np.float32)\n    coords_proj[:, 0] = (coords_proj[:, 0] / from_width) * to_width\n    coords_proj[:, 1] = (coords_proj[:, 1] / from_height) * to_height\n    return coords_proj",
    "doc": "Project coordinates from one image shape to another.\n\n    This performs a relative projection, e.g. a point at 60% of the old\n    image width will be at 60% of the new image width after projection.\n\n    Parameters\n    ----------\n    coords : ndarray or tuple of number\n        Coordinates to project. Either a ``(N,2)`` numpy array or a tuple\n        of `(x,y)` coordinates.\n\n    from_shape : tuple of int or ndarray\n        Old image shape.\n\n    to_shape : tuple of int or ndarray\n        New image shape.\n\n    Returns\n    -------\n    ndarray\n        Projected coordinates as ``(N,2)`` ``float32`` numpy array."
  },
  {
    "code": "def AdditiveGaussianNoise(loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True)\n    scale2 = iap.handle_continuous_param(scale, \"scale\", value_range=(0, None), tuple_to_uniform=True,\n                                         list_to_choice=True)\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(iap.Normal(loc=loc2, scale=scale2), per_channel=per_channel, name=name,\n                          deterministic=deterministic, random_state=random_state)",
    "doc": "Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent)."
  },
  {
    "code": "def AdditivePoissonNoise(lam=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(lam, \"lam\", value_range=(0, None), tuple_to_uniform=True,\n                                       list_to_choice=True)\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(iap.RandomSign(iap.Poisson(lam=lam2)), per_channel=per_channel, name=name,\n                          deterministic=deterministic, random_state=random_state)",
    "doc": "Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent)."
  },
  {
    "code": "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),))\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(p2, per_channel=per_channel, name=name, deterministic=deterministic,\n                               random_state=random_state)",
    "doc": "Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images."
  },
  {
    "code": "def CoarseDropout(p=0, size_px=None, size_percent=None, per_channel=False, min_size=4, name=None, deterministic=False,\n                  random_state=None):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),))\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_percent=size_percent, min_size=min_size)\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(p3, per_channel=per_channel, name=name, deterministic=deterministic,\n                               random_state=random_state)",
    "doc": "Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images."
  },
  {
    "code": "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(p=p, per_channel=True, name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``."
  },
  {
    "code": "def SaltAndPepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper."
  },
  {
    "code": "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = iap.ForceSign(\n        iap.Beta(0.5, 0.5) - 0.5,\n        positive=False,\n        mode=\"invert\"\n    ) + 0.5\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper."
  },
  {
    "code": "def CoarsePepper(p=0, size_px=None, size_percent=None, per_channel=False, min_size=4, name=None, deterministic=False,\n                 random_state=None):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(p, \"p\", tuple_to_uniform=True, list_to_choice=True)\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(other_param=mask, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(other_param=mask, size_percent=size_percent, min_size=min_size)\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = iap.ForceSign(\n        iap.Beta(0.5, 0.5) - 0.5,\n        positive=False,\n        mode=\"invert\"\n    ) + 0.5\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced."
  },
  {
    "code": "def ContrastNormalization(alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n    return contrast_lib.LinearContrast(alpha=alpha, per_channel=per_channel, name=name, deterministic=deterministic,\n                                       random_state=random_state)",
    "doc": "Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels."
  },
  {
    "code": "def is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Real) and not is_single_integer(val) and not isinstance(val, bool)",
    "doc": "Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False."
  },
  {
    "code": "def is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)",
    "doc": "Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False."
  },
  {
    "code": "def is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)",
    "doc": "Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False."
  },
  {
    "code": "def is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, '__call__')\n    else:\n        return callable(val)",
    "doc": "Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False."
  },
  {
    "code": "def flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i",
    "doc": "Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`."
  },
  {
    "code": "def new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)",
    "doc": "Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state."
  },
  {
    "code": "def copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy",
    "doc": "Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state."
  },
  {
    "code": "def derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_+i) for i in sm.xrange(n)]",
    "doc": "Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states."
  },
  {
    "code": "def _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb",
    "doc": "Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image."
  },
  {
    "code": "def _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n                  \"Expected the first two entries in to_shape to be None or numbers, \"\n                  + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),))\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n                        + \"or single float, got %s.\" % (type(to_shape),))\n\n    return tuple(to_shape_computed)",
    "doc": "Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape."
  },
  {
    "code": "def quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img",
    "doc": "Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8."
  },
  {
    "code": "def quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))",
    "doc": "Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects)."
  },
  {
    "code": "def quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap",
    "doc": "Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object."
  },
  {
    "code": "def quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi",
    "doc": "Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image."
  },
  {
    "code": "def quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi",
    "doc": "Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image."
  },
  {
    "code": "def quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon([(point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]])\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi",
    "doc": "Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image."
  },
  {
    "code": "def angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))",
    "doc": "Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592..."
  },
  {
    "code": "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n    def _make_line(p1, p2):\n        A = (p1[1] - p2[1])\n        B = (p2[0] - p1[0])\n        C = (p1[0]*p2[1] - p2[0]*p1[1])\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False",
    "doc": "Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False."
  },
  {
    "code": "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np",
    "doc": "Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it."
  },
  {
    "code": "def imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\"Cannot resize images, because at least one image has a height and/or width of zero. \"\n         + \"Observed shapes were: %s.\") % (str([image.shape for image in images]),)\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\" % (sizes,))\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],) if is_single_integer(sizes[0]) else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],) if is_single_integer(sizes[1]) else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\" % (sizes_str,))\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(len(sizes) == 2, \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),))\n        do_assert(all([is_single_number(val) for val in sizes]),\n                  \"Expected tuple with two ints or floats, got types %s.\" % (str([type(val) for val in sizes]),))\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(imresize_many_images(np.array(images), sizes=sizes, interpolation=interpolation))\n        else:\n            return [imresize_many_images(image[np.newaxis, ...], sizes=sizes, interpolation=interpolation)[0, ...]\n                    for image in images]\n\n    shape = images.shape\n    do_assert(images.ndim in [3, 4], \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),))\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(images,\n                         allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"int32\", \"float16\", \"float32\", \"float64\"],\n                         disallowed=[\"uint32\", \"uint64\", \"uint128\", \"uint256\", \"int64\", \"int128\", \"int256\",\n                                     \"float96\", \"float128\", \"float256\"],\n                         augmenter=None)\n    else:\n        iadt.gate_dtypes(images,\n                         allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\", \"float64\"],\n                         disallowed=[\"uint32\", \"uint64\", \"uint128\", \"uint256\", \"int32\", \"int64\", \"int128\", \"int256\",\n                                     \"float96\", \"float128\", \"float256\"],\n                         augmenter=None)\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result",
    "doc": "Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64."
  },
  {
    "code": "def imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(image[np.newaxis, :, :, :], sizes, interpolation=interpolation)\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]",
    "doc": "Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image."
  },
  {
    "code": "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\"uint32\", \"uint64\", \"int64\", \"float16\", \"float128\", \"bool\"]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = float(cval) if arr.dtype.kind == \"f\" else int(cval)  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(arr, top=top, bottom=bottom, left=left, right=right,\n                                             borderType=mapping_mode_np_to_cv2[mode], value=cval)\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx:channel_start_idx+4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(arr_c, top=top, bottom=bottom, left=left, right=right,\n                                                   borderType=mapping_mode_np_to_cv2[mode], value=cval_c)\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)",
    "doc": "Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``."
  },
  {
    "code": "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1/aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left",
    "doc": "Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``."
  },
  {
    "code": "def pad_to_aspect_ratio(arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(arr, aspect_ratio)\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded",
    "doc": "Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned."
  },
  {
    "code": "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n    iadt.gate_dtypes(arr,\n                     allowed=[\"bool\", \"uint8\", \"uint16\", \"uint32\", \"int8\", \"int16\", \"int32\",\n                              \"float16\", \"float32\", \"float64\", \"float128\"],\n                     disallowed=[\"uint64\", \"uint128\", \"uint256\", \"int64\", \"int128\", \"int256\",\n                                 \"float256\"],\n                     augmenter=None)\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = is_iterable(block_size) and len(block_size) in [2, 3] \\\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced",
    "doc": "Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling."
  },
  {
    "code": "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)",
    "doc": "Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling."
  },
  {
    "code": "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)",
    "doc": "Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling."
  },
  {
    "code": "def draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3)\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(nb_dtypes == 1, (\"All images provided to draw_grid() must have the same dtype, \"\n                                   + \"found %d dtypes (%s)\") % (nb_dtypes, \", \".join(dts)))\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\" % (str(channels), len(channels))\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid",
    "doc": "Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid."
  },
  {
    "code": "def show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)",
    "doc": "Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`."
  },
  {
    "code": "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(backend in [\"matplotlib\", \"cv2\"], \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,))\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(w, 6)  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()",
    "doc": "Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues."
  },
  {
    "code": "def warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n    warnings.warn(msg,\n                  category=DeprecationWarning,\n                  stacklevel=stacklevel)",
    "doc": "Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2."
  },
  {
    "code": "def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)",
    "doc": "Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed."
  },
  {
    "code": "def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)",
    "doc": "Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not."
  },
  {
    "code": "def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)",
    "doc": "A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified."
  },
  {
    "code": "def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)",
    "doc": "A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified."
  },
  {
    "code": "def pool(self):\n        \"\"\"Return the multiprocessing.Pool instance or create it if not done yet.\n\n        Returns\n        -------\n        multiprocessing.Pool\n            The multiprocessing.Pool used internally by this imgaug.multicore.Pool.\n\n        \"\"\"\n        if self._pool is None:\n            processes = self.processes\n            if processes is not None and processes < 0:\n                try:\n                    # cpu count includes the hyperthreads, e.g. 8 for 4 cores + hyperthreading\n                    processes = multiprocessing.cpu_count() - abs(processes)\n                    processes = max(processes, 1)\n                except (ImportError, NotImplementedError):\n                    processes = None\n\n            self._pool = multiprocessing.Pool(processes,\n                                              initializer=_Pool_initialize_worker,\n                                              initargs=(self.augseq, self.seed),\n                                              maxtasksperchild=self.maxtasksperchild)\n        return self._pool",
    "doc": "Return the multiprocessing.Pool instance or create it if not done yet.\n\n        Returns\n        -------\n        multiprocessing.Pool\n            The multiprocessing.Pool used internally by this imgaug.multicore.Pool."
  },
  {
    "code": "def map_batches(self, batches, chunksize=None):\n        \"\"\"\n        Augment batches.\n\n        Parameters\n        ----------\n        batches : list of imgaug.augmentables.batches.Batch\n            The batches to augment.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Returns\n        -------\n        list of imgaug.augmentables.batches.Batch\n            Augmented batches.\n\n        \"\"\"\n        assert isinstance(batches, list), (\"Expected to get a list as 'batches', got type %s. \"\n                                           + \"Call imap_batches() if you use generators.\") % (type(batches),)\n        return self.pool.map(_Pool_starworker, self._handle_batch_ids(batches), chunksize=chunksize)",
    "doc": "Augment batches.\n\n        Parameters\n        ----------\n        batches : list of imgaug.augmentables.batches.Batch\n            The batches to augment.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Returns\n        -------\n        list of imgaug.augmentables.batches.Batch\n            Augmented batches."
  },
  {
    "code": "def map_batches_async(self, batches, chunksize=None, callback=None, error_callback=None):\n        \"\"\"\n        Augment batches asynchonously.\n\n        Parameters\n        ----------\n        batches : list of imgaug.augmentables.batches.Batch\n            The batches to augment.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        callback : None or callable, optional\n            Function to call upon finish. See `multiprocessing.Pool`.\n\n        error_callback : None or callable, optional\n            Function to call upon errors. See `multiprocessing.Pool`.\n\n        Returns\n        -------\n        multiprocessing.MapResult\n            Asynchonous result. See `multiprocessing.Pool`.\n\n        \"\"\"\n        assert isinstance(batches, list), (\"Expected to get a list as 'batches', got type %s. \"\n                                           + \"Call imap_batches() if you use generators.\") % (type(batches),)\n        return self.pool.map_async(_Pool_starworker, self._handle_batch_ids(batches),\n                                   chunksize=chunksize, callback=callback, error_callback=error_callback)",
    "doc": "Augment batches asynchonously.\n\n        Parameters\n        ----------\n        batches : list of imgaug.augmentables.batches.Batch\n            The batches to augment.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        callback : None or callable, optional\n            Function to call upon finish. See `multiprocessing.Pool`.\n\n        error_callback : None or callable, optional\n            Function to call upon errors. See `multiprocessing.Pool`.\n\n        Returns\n        -------\n        multiprocessing.MapResult\n            Asynchonous result. See `multiprocessing.Pool`."
  },
  {
    "code": "def imap_batches(self, batches, chunksize=1):\n        \"\"\"\n        Augment batches from a generator.\n\n        Parameters\n        ----------\n        batches : generator of imgaug.augmentables.batches.Batch\n            The batches to augment, provided as a generator. Each call to the generator should yield exactly one\n            batch.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Yields\n        ------\n        imgaug.augmentables.batches.Batch\n            Augmented batch.\n\n        \"\"\"\n        assert ia.is_generator(batches), (\"Expected to get a generator as 'batches', got type %s. \"\n                                          + \"Call map_batches() if you use lists.\") % (type(batches),)\n        # TODO change this to 'yield from' once switched to 3.3+\n        gen = self.pool.imap(_Pool_starworker, self._handle_batch_ids_gen(batches), chunksize=chunksize)\n        for batch in gen:\n            yield batch",
    "doc": "Augment batches from a generator.\n\n        Parameters\n        ----------\n        batches : generator of imgaug.augmentables.batches.Batch\n            The batches to augment, provided as a generator. Each call to the generator should yield exactly one\n            batch.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Yields\n        ------\n        imgaug.augmentables.batches.Batch\n            Augmented batch."
  },
  {
    "code": "def imap_batches_unordered(self, batches, chunksize=1):\n        \"\"\"\n        Augment batches from a generator in a way that does not guarantee to preserve order.\n\n        Parameters\n        ----------\n        batches : generator of imgaug.augmentables.batches.Batch\n            The batches to augment, provided as a generator. Each call to the generator should yield exactly one\n            batch.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Yields\n        ------\n        imgaug.augmentables.batches.Batch\n            Augmented batch.\n\n        \"\"\"\n        assert ia.is_generator(batches), (\"Expected to get a generator as 'batches', got type %s. \"\n                                          + \"Call map_batches() if you use lists.\") % (type(batches),)\n        # TODO change this to 'yield from' once switched to 3.3+\n        gen = self.pool.imap_unordered(_Pool_starworker, self._handle_batch_ids_gen(batches), chunksize=chunksize)\n        for batch in gen:\n            yield batch",
    "doc": "Augment batches from a generator in a way that does not guarantee to preserve order.\n\n        Parameters\n        ----------\n        batches : generator of imgaug.augmentables.batches.Batch\n            The batches to augment, provided as a generator. Each call to the generator should yield exactly one\n            batch.\n\n        chunksize : None or int, optional\n            Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n            performance.\n\n        Yields\n        ------\n        imgaug.augmentables.batches.Batch\n            Augmented batch."
  },
  {
    "code": "def terminate(self):\n        \"\"\"Terminate the pool immediately.\"\"\"\n        if self._pool is not None:\n            self._pool.terminate()\n            self._pool.join()\n            self._pool = None",
    "doc": "Terminate the pool immediately."
  },
  {
    "code": "def terminate(self):\n        \"\"\"Stop all workers.\"\"\"\n        if not self.join_signal.is_set():\n            self.join_signal.set()\n        # give minimal time to put generated batches in queue and gracefully shut down\n        time.sleep(0.01)\n\n        if self.main_worker_thread.is_alive():\n            self.main_worker_thread.join()\n\n        if self.threaded:\n            for worker in self.workers:\n                if worker.is_alive():\n                    worker.join()\n        else:\n            for worker in self.workers:\n                if worker.is_alive():\n                    worker.terminate()\n                    worker.join()\n\n            # wait until all workers are fully terminated\n            while not self.all_finished():\n                time.sleep(0.001)\n\n        # empty queue until at least one element can be added and place None as signal that BL finished\n        if self.queue.full():\n            self.queue.get()\n        self.queue.put(pickle.dumps(None, protocol=-1))\n        time.sleep(0.01)\n\n        # clean the queue, this reportedly prevents hanging threads\n        while True:\n            try:\n                self._queue_internal.get(timeout=0.005)\n            except QueueEmpty:\n                break\n\n        if not self._queue_internal._closed:\n            self._queue_internal.close()\n        if not self.queue._closed:\n            self.queue.close()\n        self._queue_internal.join_thread()\n        self.queue.join_thread()\n        time.sleep(0.025)",
    "doc": "Stop all workers."
  },
  {
    "code": "def get_batch(self):\n        \"\"\"\n        Returns a batch from the queue of augmented batches.\n\n        If workers are still running and there are no batches in the queue,\n        it will automatically wait for the next batch.\n\n        Returns\n        -------\n        out : None or imgaug.Batch\n            One batch or None if all workers have finished.\n\n        \"\"\"\n        if self.all_finished():\n            return None\n\n        batch_str = self.queue_result.get()\n        batch = pickle.loads(batch_str)\n        if batch is not None:\n            return batch\n        else:\n            self.nb_workers_finished += 1\n            if self.nb_workers_finished >= self.nb_workers:\n                try:\n                    self.queue_source.get(timeout=0.001)  # remove the None from the source queue\n                except QueueEmpty:\n                    pass\n                return None\n            else:\n                return self.get_batch()",
    "doc": "Returns a batch from the queue of augmented batches.\n\n        If workers are still running and there are no batches in the queue,\n        it will automatically wait for the next batch.\n\n        Returns\n        -------\n        out : None or imgaug.Batch\n            One batch or None if all workers have finished."
  },
  {
    "code": "def _augment_images_worker(self, augseq, queue_source, queue_result, seedval):\n        \"\"\"\n        Augment endlessly images in the source queue.\n\n        This is a worker function for that endlessly queries the source queue (input batches),\n        augments batches in it and sends the result to the output queue.\n\n        \"\"\"\n        np.random.seed(seedval)\n        random.seed(seedval)\n        augseq.reseed(seedval)\n        ia.seed(seedval)\n\n        loader_finished = False\n\n        while not loader_finished:\n            # wait for a new batch in the source queue and load it\n            try:\n                batch_str = queue_source.get(timeout=0.1)\n                batch = pickle.loads(batch_str)\n                if batch is None:\n                    loader_finished = True\n                    # put it back in so that other workers know that the loading queue is finished\n                    queue_source.put(pickle.dumps(None, protocol=-1))\n                else:\n                    batch_aug = augseq.augment_batch(batch)\n\n                    # send augmented batch to output queue\n                    batch_str = pickle.dumps(batch_aug, protocol=-1)\n                    queue_result.put(batch_str)\n            except QueueEmpty:\n                time.sleep(0.01)\n\n        queue_result.put(pickle.dumps(None, protocol=-1))\n        time.sleep(0.01)",
    "doc": "Augment endlessly images in the source queue.\n\n        This is a worker function for that endlessly queries the source queue (input batches),\n        augments batches in it and sends the result to the output queue."
  },
  {
    "code": "def terminate(self):\n        \"\"\"\n        Terminates all background processes immediately.\n\n        This will also free their RAM.\n\n        \"\"\"\n        for worker in self.workers:\n            if worker.is_alive():\n                worker.terminate()\n        self.nb_workers_finished = len(self.workers)\n\n        if not self.queue_result._closed:\n            self.queue_result.close()\n        time.sleep(0.01)",
    "doc": "Terminates all background processes immediately.\n\n        This will also free their RAM."
  },
  {
    "code": "def to_normalized_batch(self):\n        \"\"\"Convert this unnormalized batch to an instance of Batch.\n\n        As this method is intended to be called before augmentation, it\n        assumes that none of the ``*_aug`` attributes is yet set.\n        It will produce an AssertionError otherwise.\n\n        The newly created Batch's ``*_unaug`` attributes will match the ones\n        in this batch, just in normalized form.\n\n        Returns\n        -------\n        imgaug.augmentables.batches.Batch\n            The batch, with ``*_unaug`` attributes being normalized.\n\n        \"\"\"\n        assert all([\n            attr is None for attr_name, attr in self.__dict__.items()\n            if attr_name.endswith(\"_aug\")]), \\\n            \"Expected UnnormalizedBatch to not contain any augmented data \" \\\n            \"before normalization, but at least one '*_aug' attribute was \" \\\n            \"already set.\"\n\n        images_unaug = nlib.normalize_images(self.images_unaug)\n        shapes = None\n        if images_unaug is not None:\n            shapes = [image.shape for image in images_unaug]\n\n        return Batch(\n            images=images_unaug,\n            heatmaps=nlib.normalize_heatmaps(\n                self.heatmaps_unaug, shapes),\n            segmentation_maps=nlib.normalize_segmentation_maps(\n                self.segmentation_maps_unaug, shapes),\n            keypoints=nlib.normalize_keypoints(\n                self.keypoints_unaug, shapes),\n            bounding_boxes=nlib.normalize_bounding_boxes(\n                self.bounding_boxes_unaug, shapes),\n            polygons=nlib.normalize_polygons(\n                self.polygons_unaug, shapes),\n            line_strings=nlib.normalize_line_strings(\n                self.line_strings_unaug, shapes),\n            data=self.data\n        )",
    "doc": "Convert this unnormalized batch to an instance of Batch.\n\n        As this method is intended to be called before augmentation, it\n        assumes that none of the ``*_aug`` attributes is yet set.\n        It will produce an AssertionError otherwise.\n\n        The newly created Batch's ``*_unaug`` attributes will match the ones\n        in this batch, just in normalized form.\n\n        Returns\n        -------\n        imgaug.augmentables.batches.Batch\n            The batch, with ``*_unaug`` attributes being normalized."
  },
  {
    "code": "def fill_from_augmented_normalized_batch(self, batch_aug_norm):\n        \"\"\"\n        Fill this batch with (normalized) augmentation results.\n\n        This method receives a (normalized) Batch instance, takes all\n        ``*_aug`` attributes out if it and assigns them to this\n        batch *in unnormalized form*. Hence, the datatypes of all ``*_aug``\n        attributes will match the datatypes of the ``*_unaug`` attributes.\n\n        Parameters\n        ----------\n        batch_aug_norm: imgaug.augmentables.batches.Batch\n            Batch after normalization and augmentation.\n\n        Returns\n        -------\n        imgaug.augmentables.batches.UnnormalizedBatch\n            New UnnormalizedBatch instance. All ``*_unaug`` attributes are\n            taken from the old UnnormalizedBatch (without deepcopying them)\n            and all ``*_aug`` attributes are taken from `batch_normalized`\n            converted to unnormalized form.\n\n        \"\"\"\n        # we take here the .data from the normalized batch instead of from\n        # self for the rare case where one has decided to somehow change it\n        # during augmentation\n        batch = UnnormalizedBatch(\n            images=self.images_unaug,\n            heatmaps=self.heatmaps_unaug,\n            segmentation_maps=self.segmentation_maps_unaug,\n            keypoints=self.keypoints_unaug,\n            bounding_boxes=self.bounding_boxes_unaug,\n            polygons=self.polygons_unaug,\n            line_strings=self.line_strings_unaug,\n            data=batch_aug_norm.data\n        )\n\n        batch.images_aug = nlib.invert_normalize_images(\n            batch_aug_norm.images_aug, self.images_unaug)\n        batch.heatmaps_aug = nlib.invert_normalize_heatmaps(\n            batch_aug_norm.heatmaps_aug, self.heatmaps_unaug)\n        batch.segmentation_maps_aug = nlib.invert_normalize_segmentation_maps(\n            batch_aug_norm.segmentation_maps_aug, self.segmentation_maps_unaug)\n        batch.keypoints_aug = nlib.invert_normalize_keypoints(\n            batch_aug_norm.keypoints_aug, self.keypoints_unaug)\n        batch.bounding_boxes_aug = nlib.invert_normalize_bounding_boxes(\n            batch_aug_norm.bounding_boxes_aug, self.bounding_boxes_unaug)\n        batch.polygons_aug = nlib.invert_normalize_polygons(\n            batch_aug_norm.polygons_aug, self.polygons_unaug)\n        batch.line_strings_aug = nlib.invert_normalize_line_strings(\n            batch_aug_norm.line_strings_aug, self.line_strings_unaug)\n\n        return batch",
    "doc": "Fill this batch with (normalized) augmentation results.\n\n        This method receives a (normalized) Batch instance, takes all\n        ``*_aug`` attributes out if it and assigns them to this\n        batch *in unnormalized form*. Hence, the datatypes of all ``*_aug``\n        attributes will match the datatypes of the ``*_unaug`` attributes.\n\n        Parameters\n        ----------\n        batch_aug_norm: imgaug.augmentables.batches.Batch\n            Batch after normalization and augmentation.\n\n        Returns\n        -------\n        imgaug.augmentables.batches.UnnormalizedBatch\n            New UnnormalizedBatch instance. All ``*_unaug`` attributes are\n            taken from the old UnnormalizedBatch (without deepcopying them)\n            and all ``*_aug`` attributes are taken from `batch_normalized`\n            converted to unnormalized form."
  },
  {
    "code": "def Positive(other_param, mode=\"invert\", reroll_count_max=2):\n    \"\"\"\n    Converts another parameter's results to positive values.\n\n    Parameters\n    ----------\n    other_param : imgaug.parameters.StochasticParameter\n        Other parameter which's sampled values are to be\n        modified.\n\n    mode : {'invert', 'reroll'}, optional\n        How to change the signs. Valid values are ``invert`` and ``reroll``.\n        ``invert`` means that wrong signs are simply flipped.\n        ``reroll`` means that all samples with wrong signs are sampled again,\n        optionally many times, until they randomly end up having the correct\n        sign.\n\n    reroll_count_max : int, optional\n        If `mode` is set to ``reroll``, this determines how often values may\n        be rerolled before giving up and simply flipping the sign (as in\n        ``mode=\"invert\"``). This shouldn't be set too high, as rerolling is\n        expensive.\n\n    Examples\n    --------\n    >>> param = Positive(Normal(0, 1), mode=\"reroll\")\n\n    Generates a normal distribution that has only positive values.\n\n    \"\"\"\n    return ForceSign(\n        other_param=other_param,\n        positive=True,\n        mode=mode,\n        reroll_count_max=reroll_count_max\n    )",
    "doc": "Converts another parameter's results to positive values.\n\n    Parameters\n    ----------\n    other_param : imgaug.parameters.StochasticParameter\n        Other parameter which's sampled values are to be\n        modified.\n\n    mode : {'invert', 'reroll'}, optional\n        How to change the signs. Valid values are ``invert`` and ``reroll``.\n        ``invert`` means that wrong signs are simply flipped.\n        ``reroll`` means that all samples with wrong signs are sampled again,\n        optionally many times, until they randomly end up having the correct\n        sign.\n\n    reroll_count_max : int, optional\n        If `mode` is set to ``reroll``, this determines how often values may\n        be rerolled before giving up and simply flipping the sign (as in\n        ``mode=\"invert\"``). This shouldn't be set too high, as rerolling is\n        expensive.\n\n    Examples\n    --------\n    >>> param = Positive(Normal(0, 1), mode=\"reroll\")\n\n    Generates a normal distribution that has only positive values."
  },
  {
    "code": "def Negative(other_param, mode=\"invert\", reroll_count_max=2):\n    \"\"\"\n    Converts another parameter's results to negative values.\n\n    Parameters\n    ----------\n    other_param : imgaug.parameters.StochasticParameter\n        Other parameter which's sampled values are to be\n        modified.\n\n    mode : {'invert', 'reroll'}, optional\n        How to change the signs. Valid values are ``invert`` and ``reroll``.\n        ``invert`` means that wrong signs are simply flipped.\n        ``reroll`` means that all samples with wrong signs are sampled again,\n        optionally many times, until they randomly end up having the correct\n        sign.\n\n    reroll_count_max : int, optional\n        If `mode` is set to ``reroll``, this determines how often values may\n        be rerolled before giving up and simply flipping the sign (as in\n        ``mode=\"invert\"``). This shouldn't be set too high, as rerolling is\n        expensive.\n\n    Examples\n    --------\n    >>> param = Negative(Normal(0, 1), mode=\"reroll\")\n\n    Generates a normal distribution that has only negative values.\n\n    \"\"\"\n    return ForceSign(\n        other_param=other_param,\n        positive=False,\n        mode=mode,\n        reroll_count_max=reroll_count_max\n    )",
    "doc": "Converts another parameter's results to negative values.\n\n    Parameters\n    ----------\n    other_param : imgaug.parameters.StochasticParameter\n        Other parameter which's sampled values are to be\n        modified.\n\n    mode : {'invert', 'reroll'}, optional\n        How to change the signs. Valid values are ``invert`` and ``reroll``.\n        ``invert`` means that wrong signs are simply flipped.\n        ``reroll`` means that all samples with wrong signs are sampled again,\n        optionally many times, until they randomly end up having the correct\n        sign.\n\n    reroll_count_max : int, optional\n        If `mode` is set to ``reroll``, this determines how often values may\n        be rerolled before giving up and simply flipping the sign (as in\n        ``mode=\"invert\"``). This shouldn't be set too high, as rerolling is\n        expensive.\n\n    Examples\n    --------\n    >>> param = Negative(Normal(0, 1), mode=\"reroll\")\n\n    Generates a normal distribution that has only negative values."
  },
  {
    "code": "def create_for_noise(other_param, threshold=(-10, 10), activated=True):\n        \"\"\"\n        Creates a Sigmoid that is adjusted to be used with noise parameters,\n        i.e. with parameters which's output values are in the range [0.0, 1.0].\n\n        Parameters\n        ----------\n        other_param : imgaug.parameters.StochasticParameter\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        threshold : number or tuple of number or iterable of number or imgaug.parameters.StochasticParameter,\\\n                    optional\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        activated : bool or number, optional\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        Returns\n        -------\n        Sigmoid\n            A sigmoid adjusted to be used with noise.\n\n        \"\"\"\n        return Sigmoid(other_param, threshold, activated, mul=20, add=-10)",
    "doc": "Creates a Sigmoid that is adjusted to be used with noise parameters,\n        i.e. with parameters which's output values are in the range [0.0, 1.0].\n\n        Parameters\n        ----------\n        other_param : imgaug.parameters.StochasticParameter\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        threshold : number or tuple of number or iterable of number or imgaug.parameters.StochasticParameter,\\\n                    optional\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        activated : bool or number, optional\n            See :func:`imgaug.parameters.Sigmoid.__init__`.\n\n        Returns\n        -------\n        Sigmoid\n            A sigmoid adjusted to be used with noise."
  },
  {
    "code": "def area(self):\n        \"\"\"\n        Estimate the area of the polygon.\n\n        Returns\n        -------\n        number\n            Area of the polygon.\n\n        \"\"\"\n        if len(self.exterior) < 3:\n            raise Exception(\"Cannot compute the polygon's area because it contains less than three points.\")\n        poly = self.to_shapely_polygon()\n        return poly.area",
    "doc": "Estimate the area of the polygon.\n\n        Returns\n        -------\n        number\n            Area of the polygon."
  },
  {
    "code": "def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the polygon onto an image with different shape.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at (x=20, y=20) on an image (width=100, height=200) will be\n        projected on a new image (width=200, height=100) to (x=40, y=10).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Polygon\n            Polygon object with new coordinates.\n\n        \"\"\"\n        if from_shape[0:2] == to_shape[0:2]:\n            return self.copy()\n        ls_proj = self.to_line_string(closed=False).project(\n            from_shape, to_shape)\n        return self.copy(exterior=ls_proj.coords)",
    "doc": "Project the polygon onto an image with different shape.\n\n        The relative coordinates of all points remain the same.\n        E.g. a point at (x=20, y=20) on an image (width=100, height=200) will be\n        projected on a new image (width=200, height=100) to (x=40, y=10).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Polygon\n            Polygon object with new coordinates."
  },
  {
    "code": "def find_closest_point_index(self, x, y, return_distance=False):\n        \"\"\"\n        Find the index of the point within the exterior that is closest to the given coordinates.\n\n        \"Closeness\" is here defined based on euclidean distance.\n        This method will raise an AssertionError if the exterior contains no points.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate around which to search for close points.\n\n        y : number\n            Y-coordinate around which to search for close points.\n\n        return_distance : bool, optional\n            Whether to also return the distance of the closest point.\n\n        Returns\n        -------\n        int\n            Index of the closest point.\n\n        number\n            Euclidean distance to the closest point.\n            This value is only returned if `return_distance` was set to True.\n\n        \"\"\"\n        ia.do_assert(len(self.exterior) > 0)\n        distances = []\n        for x2, y2 in self.exterior:\n            d = (x2 - x) ** 2 + (y2 - y) ** 2\n            distances.append(d)\n        distances = np.sqrt(distances)\n        closest_idx = np.argmin(distances)\n        if return_distance:\n            return closest_idx, distances[closest_idx]\n        return closest_idx",
    "doc": "Find the index of the point within the exterior that is closest to the given coordinates.\n\n        \"Closeness\" is here defined based on euclidean distance.\n        This method will raise an AssertionError if the exterior contains no points.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate around which to search for close points.\n\n        y : number\n            Y-coordinate around which to search for close points.\n\n        return_distance : bool, optional\n            Whether to also return the distance of the closest point.\n\n        Returns\n        -------\n        int\n            Index of the closest point.\n\n        number\n            Euclidean distance to the closest point.\n            This value is only returned if `return_distance` was set to True."
  },
  {
    "code": "def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the polygon is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the polygon is fully inside the image area.\n            False otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=True)",
    "doc": "Estimate whether the polygon is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the polygon is fully inside the image area.\n            False otherwise."
  },
  {
    "code": "def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the polygon is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the polygon is at least partially inside the image area.\n            False otherwise.\n\n        \"\"\"\n        return not self.is_out_of_image(image, fully=True, partly=False)",
    "doc": "Estimate whether the polygon is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the polygon is at least partially inside the image area.\n            False otherwise."
  },
  {
    "code": "def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the polygon is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the polygon is fully outside of the image area.\n\n        partly : bool, optional\n            Whether to return True if the polygon is at least partially outside fo the image area.\n\n        Returns\n        -------\n        bool\n            True if the polygon is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        # TODO this is inconsistent with line strings, which return a default\n        #      value in these cases\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot determine whether the polygon is inside the image, because it contains no points.\")\n        ls = self.to_line_string()\n        return ls.is_out_of_image(image, fully=fully, partly=partly)",
    "doc": "Estimate whether the polygon is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the polygon is fully outside of the image area.\n\n        partly : bool, optional\n            Whether to return True if the polygon is at least partially outside fo the image area.\n\n        Returns\n        -------\n        bool\n            True if the polygon is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise."
  },
  {
    "code": "def clip_out_of_image(self, image):\n        \"\"\"\n        Cut off all parts of the polygon that are outside of the image.\n\n        This operation may lead to new points being created.\n        As a single polygon may be split into multiple new polygons, the result\n        is always a list, which may contain more than one output polygon.\n\n        This operation will return an empty list if the polygon is completely\n        outside of the image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the polygon.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must\n            contain at least two integers.\n\n        Returns\n        -------\n        list of imgaug.Polygon\n            Polygon, clipped to fall within the image dimensions.\n            Returned as a list, because the clipping can split the polygon into\n            multiple parts. The list may also be empty, if the polygon was\n            fully outside of the image plane.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        # if fully out of image, clip everything away, nothing remaining\n        if self.is_out_of_image(image, fully=True, partly=False):\n            return []\n\n        h, w = image.shape[0:2] if ia.is_np_array(image) else image[0:2]\n        poly_shapely = self.to_shapely_polygon()\n        poly_image = shapely.geometry.Polygon([(0, 0), (w, 0), (w, h), (0, h)])\n        multipoly_inter_shapely = poly_shapely.intersection(poly_image)\n        if not isinstance(multipoly_inter_shapely, shapely.geometry.MultiPolygon):\n            ia.do_assert(isinstance(multipoly_inter_shapely, shapely.geometry.Polygon))\n            multipoly_inter_shapely = shapely.geometry.MultiPolygon([multipoly_inter_shapely])\n\n        polygons = []\n        for poly_inter_shapely in multipoly_inter_shapely.geoms:\n            polygons.append(Polygon.from_shapely(poly_inter_shapely, label=self.label))\n\n        # shapely changes the order of points, we try here to preserve it as\n        # much as possible\n        polygons_reordered = []\n        for polygon in polygons:\n            found = False\n            for x, y in self.exterior:\n                closest_idx, dist = polygon.find_closest_point_index(x=x, y=y, return_distance=True)\n                if dist < 1e-6:\n                    polygon_reordered = polygon.change_first_point_by_index(closest_idx)\n                    polygons_reordered.append(polygon_reordered)\n                    found = True\n                    break\n            ia.do_assert(found)  # could only not find closest points if new polys are empty\n\n        return polygons_reordered",
    "doc": "Cut off all parts of the polygon that are outside of the image.\n\n        This operation may lead to new points being created.\n        As a single polygon may be split into multiple new polygons, the result\n        is always a list, which may contain more than one output polygon.\n\n        This operation will return an empty list if the polygon is completely\n        outside of the image plane.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the polygon.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must\n            contain at least two integers.\n\n        Returns\n        -------\n        list of imgaug.Polygon\n            Polygon, clipped to fall within the image dimensions.\n            Returned as a list, because the clipping can split the polygon into\n            multiple parts. The list may also be empty, if the polygon was\n            fully outside of the image plane."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the polygon from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the polygon from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the polygon from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the polygon from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the polygon from the left.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Shifted polygon.\n\n        \"\"\"\n        ls_shifted = self.to_line_string(closed=False).shift(\n            top=top, right=right, bottom=bottom, left=left)\n        return self.copy(exterior=ls_shifted.coords)",
    "doc": "Shift the polygon from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the polygon from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the polygon from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the polygon from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the polygon from the left.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Shifted polygon."
  },
  {
    "code": "def draw_on_image(self,\n                      image,\n                      color=(0, 255, 0), color_face=None,\n                      color_lines=None, color_points=None,\n                      alpha=1.0, alpha_face=None,\n                      alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      raise_if_out_of_image=False):\n        \"\"\"\n        Draw the polygon on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the polygon. Usually expected to be\n            of dtype ``uint8``, though other dtypes are also handled.\n\n        color : iterable of int, optional\n            The color to use for the whole polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon area (excluding perimeter).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the line (aka perimeter/border) of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygon, where ``1.0`` denotes a completely\n            visible polygon and ``0.0`` an invisible one.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner area (excluding the perimeter),\n            where ``1.0`` denotes a completely visible inner area and ``0.0``\n            an invisible one.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's line (aka perimeter/border),\n            where ``1.0`` denotes a completely visible line and ``0.0`` an\n            invisible one.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygon.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon's line (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the polygon is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray\n            Image with polygon drawn on it. Result dtype is the same as the input dtype.\n\n        \"\"\"\n        assert color is not None\n        assert alpha is not None\n        assert size is not None\n\n        color_face = color_face if color_face is not None else np.array(color)\n        color_lines = color_lines if color_lines is not None else np.array(color) * 0.5\n        color_points = color_points if color_points is not None else np.array(color) * 0.5\n\n        alpha_face = alpha_face if alpha_face is not None else alpha * 0.5\n        alpha_lines = alpha_lines if alpha_lines is not None else alpha\n        alpha_points = alpha_points if alpha_points is not None else alpha\n\n        size_lines = size_lines if size_lines is not None else size\n        size_points = size_points if size_points is not None else size * 3\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color_face), (\n                \"Got a 2D image. Expected then 'color_face' to be a single \"\n                \"number, but got %s.\" % (str(color_face),))\n            color_face = [color_face]\n        elif image.ndim == 3 and ia.is_single_number(color_face):\n            color_face = [color_face] * image.shape[-1]\n\n        if alpha_face < 0.01:\n            alpha_face = 0\n        elif alpha_face > 0.99:\n            alpha_face = 1\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\"Cannot draw polygon %s on image with shape %s.\" % (\n                str(self), image.shape\n            ))\n\n        # TODO np.clip to image plane if is_fully_within_image(), similar to how it is done for bounding boxes\n\n        # TODO improve efficiency by only drawing in rectangle that covers poly instead of drawing in the whole image\n        # TODO for a rectangular polygon, the face coordinates include the top/left boundary but not the right/bottom\n        # boundary. This may be unintuitive when not drawing the boundary. Maybe somehow remove the boundary\n        # coordinates from the face coordinates after generating both?\n        input_dtype = image.dtype\n        result = image.astype(np.float32)\n        rr, cc = skimage.draw.polygon(self.yy_int, self.xx_int, shape=image.shape)\n        if len(rr) > 0:\n            if alpha_face == 1:\n                result[rr, cc] = np.float32(color_face)\n            elif alpha_face == 0:\n                pass\n            else:\n                result[rr, cc] = (\n                        (1 - alpha_face) * result[rr, cc, :]\n                        + alpha_face * np.float32(color_face)\n                )\n\n        ls_open = self.to_line_string(closed=False)\n        ls_closed = self.to_line_string(closed=True)\n        result = ls_closed.draw_lines_on_image(\n            result, color=color_lines, alpha=alpha_lines,\n            size=size_lines, raise_if_out_of_image=raise_if_out_of_image)\n        result = ls_open.draw_points_on_image(\n            result, color=color_points, alpha=alpha_points,\n            size=size_points, raise_if_out_of_image=raise_if_out_of_image)\n\n        if input_dtype.type == np.uint8:\n            result = np.clip(np.round(result), 0, 255).astype(input_dtype)  # TODO make clipping more flexible\n        else:\n            result = result.astype(input_dtype)\n\n        return result",
    "doc": "Draw the polygon on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the polygon. Usually expected to be\n            of dtype ``uint8``, though other dtypes are also handled.\n\n        color : iterable of int, optional\n            The color to use for the whole polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon area (excluding perimeter).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the line (aka perimeter/border) of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygon.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygon, where ``1.0`` denotes a completely\n            visible polygon and ``0.0`` an invisible one.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner area (excluding the perimeter),\n            where ``1.0`` denotes a completely visible inner area and ``0.0``\n            an invisible one.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's line (aka perimeter/border),\n            where ``1.0`` denotes a completely visible line and ``0.0`` an\n            invisible one.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygon.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon's line (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the polygon is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray\n            Image with polygon drawn on it. Result dtype is the same as the input dtype."
  },
  {
    "code": "def extract_from_image(self, image):\n        \"\"\"\n        Extract the image pixels within the polygon.\n\n        This function will zero-pad the image if the polygon is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the polygon.\n\n        Returns\n        -------\n        result : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the polygon. Zero-padded if the polygon is partially/fully\n            outside of the image.\n\n        \"\"\"\n        ia.do_assert(image.ndim in [2, 3])\n        if len(self.exterior) <= 2:\n            raise Exception(\"Polygon must be made up of at least 3 points to extract its area from an image.\")\n\n        bb = self.to_bounding_box()\n        bb_area = bb.extract_from_image(image)\n        if self.is_out_of_image(image, fully=True, partly=False):\n            return bb_area\n\n        xx = self.xx_int\n        yy = self.yy_int\n        xx_mask = xx - np.min(xx)\n        yy_mask = yy - np.min(yy)\n        height_mask = np.max(yy_mask)\n        width_mask = np.max(xx_mask)\n\n        rr_face, cc_face = skimage.draw.polygon(yy_mask, xx_mask, shape=(height_mask, width_mask))\n\n        mask = np.zeros((height_mask, width_mask), dtype=np.bool)\n        mask[rr_face, cc_face] = True\n\n        if image.ndim == 3:\n            mask = np.tile(mask[:, :, np.newaxis], (1, 1, image.shape[2]))\n\n        return bb_area * mask",
    "doc": "Extract the image pixels within the polygon.\n\n        This function will zero-pad the image if the polygon is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the polygon.\n\n        Returns\n        -------\n        result : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the polygon. Zero-padded if the polygon is partially/fully\n            outside of the image."
  },
  {
    "code": "def change_first_point_by_coords(self, x, y, max_distance=1e-4,\n                                     raise_if_too_far_away=True):\n        \"\"\"\n        Set the first point of the exterior to the given point based on its coordinates.\n\n        If multiple points are found, the closest one will be picked.\n        If no matching points are found, an exception is raised.\n\n        Note: This method does *not* work in-place.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate of the point.\n\n        y : number\n            Y-coordinate of the point.\n\n        max_distance : None or number, optional\n            Maximum distance past which possible matches are ignored.\n            If ``None`` the distance limit is deactivated.\n\n        raise_if_too_far_away : bool, optional\n            Whether to raise an exception if the closest found point is too\n            far away (``True``) or simply return an unchanged copy if this\n            object (``False``).\n\n        Returns\n        -------\n        imgaug.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        if len(self.exterior) == 0:\n            raise Exception(\"Cannot reorder polygon points, because it contains no points.\")\n\n        closest_idx, closest_dist = self.find_closest_point_index(x=x, y=y, return_distance=True)\n        if max_distance is not None and closest_dist > max_distance:\n            if not raise_if_too_far_away:\n                return self.deepcopy()\n\n            closest_point = self.exterior[closest_idx, :]\n            raise Exception(\n                \"Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded\" % (\n                    closest_point[0], closest_point[1], closest_dist)\n            )\n        return self.change_first_point_by_index(closest_idx)",
    "doc": "Set the first point of the exterior to the given point based on its coordinates.\n\n        If multiple points are found, the closest one will be picked.\n        If no matching points are found, an exception is raised.\n\n        Note: This method does *not* work in-place.\n\n        Parameters\n        ----------\n        x : number\n            X-coordinate of the point.\n\n        y : number\n            Y-coordinate of the point.\n\n        max_distance : None or number, optional\n            Maximum distance past which possible matches are ignored.\n            If ``None`` the distance limit is deactivated.\n\n        raise_if_too_far_away : bool, optional\n            Whether to raise an exception if the closest found point is too\n            far away (``True``) or simply return an unchanged copy if this\n            object (``False``).\n\n        Returns\n        -------\n        imgaug.Polygon\n            Copy of this polygon with the new point order."
  },
  {
    "code": "def change_first_point_by_index(self, point_idx):\n        \"\"\"\n        Set the first point of the exterior to the given point based on its index.\n\n        Note: This method does *not* work in-place.\n\n        Parameters\n        ----------\n        point_idx : int\n            Index of the desired starting point.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Copy of this polygon with the new point order.\n\n        \"\"\"\n        ia.do_assert(0 <= point_idx < len(self.exterior))\n        if point_idx == 0:\n            return self.deepcopy()\n        exterior = np.concatenate(\n            (self.exterior[point_idx:, :], self.exterior[:point_idx, :]),\n            axis=0\n        )\n        return self.deepcopy(exterior=exterior)",
    "doc": "Set the first point of the exterior to the given point based on its index.\n\n        Note: This method does *not* work in-place.\n\n        Parameters\n        ----------\n        point_idx : int\n            Index of the desired starting point.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Copy of this polygon with the new point order."
  },
  {
    "code": "def to_shapely_polygon(self):\n        \"\"\"\n        Convert this polygon to a Shapely polygon.\n\n        Returns\n        -------\n        shapely.geometry.Polygon\n            The Shapely polygon matching this polygon's exterior.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        return shapely.geometry.Polygon([(point[0], point[1]) for point in self.exterior])",
    "doc": "Convert this polygon to a Shapely polygon.\n\n        Returns\n        -------\n        shapely.geometry.Polygon\n            The Shapely polygon matching this polygon's exterior."
  },
  {
    "code": "def to_shapely_line_string(self, closed=False, interpolate=0):\n        \"\"\"\n        Convert this polygon to a Shapely LineString object.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to return the line string with the last point being identical to the first point.\n\n        interpolate : int, optional\n            Number of points to interpolate between any pair of two consecutive points. These points are added\n            to the final line string.\n\n        Returns\n        -------\n        shapely.geometry.LineString\n            The Shapely LineString matching the polygon's exterior.\n\n        \"\"\"\n        return _convert_points_to_shapely_line_string(self.exterior, closed=closed, interpolate=interpolate)",
    "doc": "Convert this polygon to a Shapely LineString object.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to return the line string with the last point being identical to the first point.\n\n        interpolate : int, optional\n            Number of points to interpolate between any pair of two consecutive points. These points are added\n            to the final line string.\n\n        Returns\n        -------\n        shapely.geometry.LineString\n            The Shapely LineString matching the polygon's exterior."
  },
  {
    "code": "def to_bounding_box(self):\n        \"\"\"\n        Convert this polygon to a bounding box tightly containing the whole polygon.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Tight bounding box around the polygon.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.bbs import BoundingBox\n\n        xx = self.xx\n        yy = self.yy\n        return BoundingBox(x1=min(xx), x2=max(xx), y1=min(yy), y2=max(yy), label=self.label)",
    "doc": "Convert this polygon to a bounding box tightly containing the whole polygon.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Tight bounding box around the polygon."
  },
  {
    "code": "def to_keypoints(self):\n        \"\"\"\n        Convert this polygon's `exterior` to ``Keypoint`` instances.\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Exterior vertices as ``Keypoint`` instances.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [Keypoint(x=point[0], y=point[1]) for point in self.exterior]",
    "doc": "Convert this polygon's `exterior` to ``Keypoint`` instances.\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Exterior vertices as ``Keypoint`` instances."
  },
  {
    "code": "def to_line_string(self, closed=True):\n        \"\"\"\n        Convert this polygon's `exterior` to a ``LineString`` instance.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to close the line string, i.e. to add the first point of\n            the `exterior` also as the last point at the end of the line string.\n            This has no effect if the polygon has a single point or zero\n            points.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Exterior of the polygon as a line string.\n\n        \"\"\"\n        from imgaug.augmentables.lines import LineString\n        if not closed or len(self.exterior) <= 1:\n            return LineString(self.exterior, label=self.label)\n        return LineString(\n            np.concatenate([self.exterior, self.exterior[0:1, :]], axis=0),\n            label=self.label)",
    "doc": "Convert this polygon's `exterior` to a ``LineString`` instance.\n\n        Parameters\n        ----------\n        closed : bool, optional\n            Whether to close the line string, i.e. to add the first point of\n            the `exterior` also as the last point at the end of the line string.\n            This has no effect if the polygon has a single point or zero\n            points.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Exterior of the polygon as a line string."
  },
  {
    "code": "def from_shapely(polygon_shapely, label=None):\n        \"\"\"\n        Create a polygon from a Shapely polygon.\n\n        Note: This will remove any holes in the Shapely polygon.\n\n        Parameters\n        ----------\n        polygon_shapely : shapely.geometry.Polygon\n             The shapely polygon.\n\n        label : None or str, optional\n            The label of the new polygon.\n\n        Returns\n        -------\n        imgaug.Polygon\n            A polygon with the same exterior as the Shapely polygon.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        ia.do_assert(isinstance(polygon_shapely, shapely.geometry.Polygon))\n        # polygon_shapely.exterior can be None if the polygon was instantiated without points\n        if polygon_shapely.exterior is None or len(polygon_shapely.exterior.coords) == 0:\n            return Polygon([], label=label)\n        exterior = np.float32([[x, y] for (x, y) in polygon_shapely.exterior.coords])\n        return Polygon(exterior, label=label)",
    "doc": "Create a polygon from a Shapely polygon.\n\n        Note: This will remove any holes in the Shapely polygon.\n\n        Parameters\n        ----------\n        polygon_shapely : shapely.geometry.Polygon\n             The shapely polygon.\n\n        label : None or str, optional\n            The label of the new polygon.\n\n        Returns\n        -------\n        imgaug.Polygon\n            A polygon with the same exterior as the Shapely polygon."
  },
  {
    "code": "def exterior_almost_equals(self, other, max_distance=1e-6, points_per_edge=8):\n        \"\"\"\n        Estimate if this and other polygon's exterior are almost identical.\n\n        The two exteriors can have different numbers of points, but any point\n        randomly sampled on the exterior of one polygon should be close to the\n        closest point on the exterior of the other polygon.\n\n        Note that this method works approximately. One can come up with\n        polygons with fairly different shapes that will still be estimated as\n        equal by this method. In practice however this should be unlikely to be\n        the case. The probability for something like that goes down as the\n        interpolation parameter is increased.\n\n        Parameters\n        ----------\n        other : imgaug.Polygon or (N,2) ndarray or list of tuple\n            The other polygon with which to compare the exterior.\n            If this is an ndarray, it is assumed to represent an exterior.\n            It must then have dtype ``float32`` and shape ``(N,2)`` with the\n            second dimension denoting xy-coordinates.\n            If this is a list of tuples, it is assumed to represent an exterior.\n            Each tuple then must contain exactly two numbers, denoting\n            xy-coordinates.\n\n        max_distance : number, optional\n            The maximum euclidean distance between a point on one polygon and\n            the closest point on the other polygon. If the distance is exceeded\n            for any such pair, the two exteriors are not viewed as equal. The\n            points are other the points contained in the polygon's exterior\n            ndarray or interpolated points between these.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test).\n\n        \"\"\"\n        if isinstance(other, list):\n            other = Polygon(np.float32(other))\n        elif ia.is_np_array(other):\n            other = Polygon(other)\n        else:\n            assert isinstance(other, Polygon)\n            other = other\n\n        return self.to_line_string(closed=True).coords_almost_equals(\n            other.to_line_string(closed=True),\n            max_distance=max_distance,\n            points_per_edge=points_per_edge\n        )",
    "doc": "Estimate if this and other polygon's exterior are almost identical.\n\n        The two exteriors can have different numbers of points, but any point\n        randomly sampled on the exterior of one polygon should be close to the\n        closest point on the exterior of the other polygon.\n\n        Note that this method works approximately. One can come up with\n        polygons with fairly different shapes that will still be estimated as\n        equal by this method. In practice however this should be unlikely to be\n        the case. The probability for something like that goes down as the\n        interpolation parameter is increased.\n\n        Parameters\n        ----------\n        other : imgaug.Polygon or (N,2) ndarray or list of tuple\n            The other polygon with which to compare the exterior.\n            If this is an ndarray, it is assumed to represent an exterior.\n            It must then have dtype ``float32`` and shape ``(N,2)`` with the\n            second dimension denoting xy-coordinates.\n            If this is a list of tuples, it is assumed to represent an exterior.\n            Each tuple then must contain exactly two numbers, denoting\n            xy-coordinates.\n\n        max_distance : number, optional\n            The maximum euclidean distance between a point on one polygon and\n            the closest point on the other polygon. If the distance is exceeded\n            for any such pair, the two exteriors are not viewed as equal. The\n            points are other the points contained in the polygon's exterior\n            ndarray or interpolated points between these.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two polygon's exteriors can be viewed as equal\n            (approximate test)."
  },
  {
    "code": "def almost_equals(self, other, max_distance=1e-6, points_per_edge=8):\n        \"\"\"\n        Estimate if this polygon's and another's geometry/labels are similar.\n\n        This is the same as :func:`imgaug.Polygon.exterior_almost_equals` but\n        additionally compares the labels.\n\n        Parameters\n        ----------\n        other\n            The object to compare against. If not a Polygon, then False will\n            be returned.\n\n        max_distance : float, optional\n            See :func:`imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See :func:`imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            Whether the two polygons can be viewed as equal. In the case of\n            the exteriors this is an approximate test.\n\n        \"\"\"\n        if not isinstance(other, Polygon):\n            return False\n        if self.label is not None or other.label is not None:\n            if self.label is None:\n                return False\n            if other.label is None:\n                return False\n            if self.label != other.label:\n                return False\n        return self.exterior_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)",
    "doc": "Estimate if this polygon's and another's geometry/labels are similar.\n\n        This is the same as :func:`imgaug.Polygon.exterior_almost_equals` but\n        additionally compares the labels.\n\n        Parameters\n        ----------\n        other\n            The object to compare against. If not a Polygon, then False will\n            be returned.\n\n        max_distance : float, optional\n            See :func:`imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        points_per_edge : int, optional\n            See :func:`imgaug.augmentables.polys.Polygon.exterior_almost_equals`.\n\n        Returns\n        -------\n        bool\n            Whether the two polygons can be viewed as equal. In the case of\n            the exteriors this is an approximate test."
  },
  {
    "code": "def copy(self, exterior=None, label=None):\n        \"\"\"\n        Create a shallow copy of the Polygon object.\n\n        Parameters\n        ----------\n        exterior : list of imgaug.Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See :func:`imgaug.Polygon.__init__` for details.\n\n        label : None or str, optional\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(exterior=exterior, label=label)",
    "doc": "Create a shallow copy of the Polygon object.\n\n        Parameters\n        ----------\n        exterior : list of imgaug.Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See :func:`imgaug.Polygon.__init__` for details.\n\n        label : None or str, optional\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, exterior=None, label=None):\n        \"\"\"\n        Create a deep copy of the Polygon object.\n\n        Parameters\n        ----------\n        exterior : list of Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See `imgaug.Polygon.__init__` for details.\n\n        label : None or str\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Deep copy.\n\n        \"\"\"\n        return Polygon(\n            exterior=np.copy(self.exterior) if exterior is None else exterior,\n            label=self.label if label is None else label\n        )",
    "doc": "Create a deep copy of the Polygon object.\n\n        Parameters\n        ----------\n        exterior : list of Keypoint or list of tuple or (N,2) ndarray, optional\n            List of points defining the polygon. See `imgaug.Polygon.__init__` for details.\n\n        label : None or str\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.Polygon\n            Deep copy."
  },
  {
    "code": "def on(self, image):\n        \"\"\"\n        Project polygons from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the polygons are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Object containing all projected polygons.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        polygons = [poly.project(self.shape, shape) for poly in self.polygons]\n        # TODO use deepcopy() here\n        return PolygonsOnImage(polygons, shape)",
    "doc": "Project polygons from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the polygons are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Object containing all projected polygons."
  },
  {
    "code": "def draw_on_image(self,\n                      image,\n                      color=(0, 255, 0), color_face=None,\n                      color_lines=None, color_points=None,\n                      alpha=1.0, alpha_face=None,\n                      alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      raise_if_out_of_image=False):\n        \"\"\"\n        Draw all polygons onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as set in\n            ``PolygonsOnImage.shape``.\n\n        color : iterable of int, optional\n            The color to use for the whole polygons.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon areas (excluding perimeters).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the lines (aka perimeters/borders) of the\n            polygons. Must correspond to the channel layout of the image.\n            Usually RGB. If this is ``None``, it will be derived\n            from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygons.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygons, where ``1.0`` denotes\n            completely visible polygons and ``0.0`` invisible ones.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner areas (excluding the perimeters),\n            where ``1.0`` denotes completely visible inner areas and ``0.0``\n            invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's lines (aka perimeters/borders),\n            where ``1.0`` denotes completely visible perimeters and ``0.0``\n            invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            Currently this is an on/off choice, i.e. only ``0.0`` or ``1.0``\n            are allowed.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygons.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon lines (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            The size of all corner points. If set to ``C``, each corner point\n            will be drawn as a square of size ``C x C``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if any polygon is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        image : (H,W,C) ndarray\n            Image with drawn polygons.\n\n        \"\"\"\n        for poly in self.polygons:\n            image = poly.draw_on_image(\n                image,\n                color=color,\n                color_face=color_face,\n                color_lines=color_lines,\n                color_points=color_points,\n                alpha=alpha,\n                alpha_face=alpha_face,\n                alpha_lines=alpha_lines,\n                alpha_points=alpha_points,\n                size=size,\n                size_lines=size_lines,\n                size_points=size_points,\n                raise_if_out_of_image=raise_if_out_of_image\n            )\n        return image",
    "doc": "Draw all polygons onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as set in\n            ``PolygonsOnImage.shape``.\n\n        color : iterable of int, optional\n            The color to use for the whole polygons.\n            Must correspond to the channel layout of the image. Usually RGB.\n            The values for `color_face`, `color_lines` and `color_points`\n            will be derived from this color if they are set to ``None``.\n            This argument has no effect if `color_face`, `color_lines`\n            and `color_points` are all set anything other than ``None``.\n\n        color_face : None or iterable of int, optional\n            The color to use for the inner polygon areas (excluding perimeters).\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 1.0``.\n\n        color_lines : None or iterable of int, optional\n            The color to use for the lines (aka perimeters/borders) of the\n            polygons. Must correspond to the channel layout of the image.\n            Usually RGB. If this is ``None``, it will be derived\n            from ``color * 0.5``.\n\n        color_points : None or iterable of int, optional\n            The color to use for the corner points of the polygons.\n            Must correspond to the channel layout of the image. Usually RGB.\n            If this is ``None``, it will be derived from ``color * 0.5``.\n\n        alpha : float, optional\n            The opacity of the whole polygons, where ``1.0`` denotes\n            completely visible polygons and ``0.0`` invisible ones.\n            The values for `alpha_face`, `alpha_lines` and `alpha_points`\n            will be derived from this alpha value if they are set to ``None``.\n            This argument has no effect if `alpha_face`, `alpha_lines`\n            and `alpha_points` are all set anything other than ``None``.\n\n        alpha_face : None or number, optional\n            The opacity of the polygon's inner areas (excluding the perimeters),\n            where ``1.0`` denotes completely visible inner areas and ``0.0``\n            invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 0.5``.\n\n        alpha_lines : None or number, optional\n            The opacity of the polygon's lines (aka perimeters/borders),\n            where ``1.0`` denotes completely visible perimeters and ``0.0``\n            invisible ones.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        alpha_points : None or number, optional\n            The opacity of the polygon's corner points, where ``1.0`` denotes\n            completely visible corners and ``0.0`` invisible ones.\n            Currently this is an on/off choice, i.e. only ``0.0`` or ``1.0``\n            are allowed.\n            If this is ``None``, it will be derived from ``alpha * 1.0``.\n\n        size : int, optional\n            Size of the polygons.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the polygon lines (aka perimeter/border).\n            If ``None``, this value is derived from `size`.\n\n        size_points : int, optional\n            The size of all corner points. If set to ``C``, each corner point\n            will be drawn as a square of size ``C x C``.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if any polygon is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        image : (H,W,C) ndarray\n            Image with drawn polygons."
  },
  {
    "code": "def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all polygons that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove polygons that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove polygons that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Reduced set of polygons, with those that were fully/partially\n            outside of the image removed.\n\n        \"\"\"\n        polys_clean = [\n            poly for poly in self.polygons\n            if not poly.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        # TODO use deepcopy() here\n        return PolygonsOnImage(polys_clean, shape=self.shape)",
    "doc": "Remove all polygons that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove polygons that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove polygons that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Reduced set of polygons, with those that were fully/partially\n            outside of the image removed."
  },
  {
    "code": "def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all polygons that are outside of the image.\n\n        NOTE: The result can contain less polygons than the input did. That\n        happens when a polygon is fully outside of the image plane.\n\n        NOTE: The result can also contain *more* polygons than the input\n        did. That happens when distinct parts of a polygon are only\n        connected by areas that are outside of the image plane and hence will\n        be clipped off, resulting in two or more unconnected polygon parts that\n        are left in the image plane.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Polygons, clipped to fall within the image dimensions. Count of\n            output polygons may differ from the input count.\n\n        \"\"\"\n        polys_cut = [\n            poly.clip_out_of_image(self.shape)\n            for poly\n            in self.polygons\n            if poly.is_partly_within_image(self.shape)\n        ]\n        polys_cut_flat = [poly for poly_lst in polys_cut for poly in poly_lst]\n        # TODO use deepcopy() here\n        return PolygonsOnImage(polys_cut_flat, shape=self.shape)",
    "doc": "Clip off all parts from all polygons that are outside of the image.\n\n        NOTE: The result can contain less polygons than the input did. That\n        happens when a polygon is fully outside of the image plane.\n\n        NOTE: The result can also contain *more* polygons than the input\n        did. That happens when distinct parts of a polygon are only\n        connected by areas that are outside of the image plane and hence will\n        be clipped off, resulting in two or more unconnected polygon parts that\n        are left in the image plane.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Polygons, clipped to fall within the image dimensions. Count of\n            output polygons may differ from the input count."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all polygons from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all polygons from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all polygons from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all polygons from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all polygons from the left.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Shifted polygons.\n\n        \"\"\"\n        polys_new = [\n            poly.shift(top=top, right=right, bottom=bottom, left=left)\n            for poly\n            in self.polygons\n        ]\n        return PolygonsOnImage(polys_new, shape=self.shape)",
    "doc": "Shift all polygons from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all polygons from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all polygons from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all polygons from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all polygons from the left.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Shifted polygons."
  },
  {
    "code": "def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the PolygonsOnImage object.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for PolygonsOnImage,\n        # so use manual copy here too\n        polys = [poly.deepcopy() for poly in self.polygons]\n        return PolygonsOnImage(polys, tuple(self.shape))",
    "doc": "Create a deep copy of the PolygonsOnImage object.\n\n        Returns\n        -------\n        imgaug.PolygonsOnImage\n            Deep copy."
  },
  {
    "code": "def from_shapely(geometry, label=None):\n        \"\"\"\n        Create a MultiPolygon from a Shapely MultiPolygon, a Shapely Polygon or a Shapely GeometryCollection.\n\n        This also creates all necessary Polygons contained by this MultiPolygon.\n\n        Parameters\n        ----------\n        geometry : shapely.geometry.MultiPolygon or shapely.geometry.Polygon\\\n                   or shapely.geometry.collection.GeometryCollection\n            The object to convert to a MultiPolygon.\n\n        label : None or str, optional\n            A label assigned to all Polygons within the MultiPolygon.\n\n        Returns\n        -------\n        imgaug.MultiPolygon\n            The derived MultiPolygon.\n\n        \"\"\"\n        # load shapely lazily, which makes the dependency more optional\n        import shapely.geometry\n\n        if isinstance(geometry, shapely.geometry.MultiPolygon):\n            return MultiPolygon([Polygon.from_shapely(poly, label=label) for poly in geometry.geoms])\n        elif isinstance(geometry, shapely.geometry.Polygon):\n            return MultiPolygon([Polygon.from_shapely(geometry, label=label)])\n        elif isinstance(geometry, shapely.geometry.collection.GeometryCollection):\n            ia.do_assert(all([isinstance(poly, shapely.geometry.Polygon) for poly in geometry.geoms]))\n            return MultiPolygon([Polygon.from_shapely(poly, label=label) for poly in geometry.geoms])\n        else:\n            raise Exception(\"Unknown datatype '%s'. Expected shapely.geometry.Polygon or \"\n                            \"shapely.geometry.MultiPolygon or \"\n                            \"shapely.geometry.collections.GeometryCollection.\" % (type(geometry),))",
    "doc": "Create a MultiPolygon from a Shapely MultiPolygon, a Shapely Polygon or a Shapely GeometryCollection.\n\n        This also creates all necessary Polygons contained by this MultiPolygon.\n\n        Parameters\n        ----------\n        geometry : shapely.geometry.MultiPolygon or shapely.geometry.Polygon\\\n                   or shapely.geometry.collection.GeometryCollection\n            The object to convert to a MultiPolygon.\n\n        label : None or str, optional\n            A label assigned to all Polygons within the MultiPolygon.\n\n        Returns\n        -------\n        imgaug.MultiPolygon\n            The derived MultiPolygon."
  },
  {
    "code": "def Pad(px=None, percent=None, pad_mode=\"constant\", pad_cval=0, keep_size=True, sample_independently=True,\n        name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that pads images, i.e. adds columns/rows to them.\n\n    dtype support::\n\n        See ``imgaug.augmenters.size.CropAndPad``.\n\n    Parameters\n    ----------\n    px : None or int or imgaug.parameters.StochasticParameter or tuple, optional\n        The number of pixels to pad on each side of the image.\n        Either this or the parameter `percent` may be set, not both at the same\n        time.\n\n            * If None, then pixel-based padding will not be used.\n            * If int, then that exact number of pixels will always be padded.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two ints with values a and b, then each side will\n              be padded by a random amount in the range ``a <= x <= b``.\n              ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single integer (always pad by\n              exactly that value), a tuple of two ints ``a`` and ``b`` (pad by\n              an amount ``a <= x <= b``), a list of ints (pad by a random value\n              that is contained in the list) or a StochasticParameter (sample\n              the amount to pad from that parameter).\n\n    percent : None or int or float or imgaug.parameters.StochasticParameter \\\n              or tuple, optional\n        The number of pixels to pad on each side of the image given\n        *in percent* of the image height/width.\n        E.g. if this is set to 0.1, the augmenter will always add 10 percent\n        of the image's height to the top, 10 percent of the width to the right,\n        10 percent of the height at the bottom and 10 percent of the width to\n        the left. Either this or the parameter `px` may be set, not both at the\n        same time.\n\n            * If None, then percent-based padding will not be used.\n            * If int, then expected to be 0 (no padding).\n            * If float, then that percentage will always be padded.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two floats with values a and b, then each side will\n              be padded by a random percentage in the range ``a <= x <= b``.\n              ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single float (always pad by\n              exactly that percent value), a tuple of two floats ``a`` and ``b``\n              (pad by a percentage ``a <= x <= b``), a list of floats (pad by a\n              random value that is contained in the list) or a\n              StochasticParameter (sample the percentage to pad from that\n              parameter).\n\n    pad_mode : imgaug.ALL or str or list of str or \\\n               imgaug.parameters.StochasticParameter, optional\n        Padding mode to use. The available modes match the numpy padding modes,\n        i.e. ``constant``, ``edge``, ``linear_ramp``, ``maximum``, ``median``,\n        ``minimum``, ``reflect``, ``symmetric``, ``wrap``. The modes\n        ``constant`` and ``linear_ramp`` use extra values, which are provided\n        by ``pad_cval`` when necessary. See :func:`imgaug.imgaug.pad` for\n        more details.\n\n            * If ``imgaug.ALL``, then a random mode from all available modes\n              will be sampled per image.\n            * If a string, it will be used as the pad mode for all images.\n            * If a list of strings, a random one of these will be sampled per\n              image and used as the mode.\n            * If StochasticParameter, a random mode will be sampled from this\n              parameter per image.\n\n    pad_cval : number or tuple of number list of number or \\\n               imgaug.parameters.StochasticParameter, optional\n        The constant value to use if the pad mode is ``constant`` or the end\n        value to use if the mode is ``linear_ramp``.\n        See :func:`imgaug.imgaug.pad` for more details.\n\n            * If number, then that value will be used.\n            * If a tuple of two numbers and at least one of them is a float,\n              then a random number will be sampled from the continuous range\n              ``a <= x <= b`` and used as the value. If both numbers are\n              integers, the range is discrete.\n            * If a list of number, then a random value will be chosen from the\n              elements of the list and used as the value.\n            * If StochasticParameter, a random value will be sampled from that\n              parameter per image.\n\n    keep_size : bool, optional\n        After padding, the result image will usually have a different\n        height/width compared to the original input image. If this parameter is\n        set to True, then the padded image will be resized to the input image's\n        size, i.e. the augmenter's output shape is always identical to the\n        input shape.\n\n    sample_independently : bool, optional\n        If False AND the values for `px`/`percent` result in exactly one\n        probability distribution for the amount to pad, only one single value\n        will be sampled from that probability distribution and used for all\n        sides. I.e. the pad amount then is the same for all sides.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pad(px=(0, 10))\n\n    pads each side by a random value from the range 0px to 10px (the value\n    is sampled per side). The added rows/columns are filled with black pixels.\n\n    >>> aug = iaa.Pad(px=(0, 10), sample_independently=False)\n\n    samples one value v from the discrete range ``[0..10]`` and pads all sides\n    by ``v`` pixels.\n\n    >>> aug = iaa.Pad(px=(0, 10), keep_size=False)\n\n    pads each side by a random value from the range 0px to 10px (the value\n    is sampled per side). After padding, the images are NOT resized to their\n    original size (i.e. the images may end up having different heights/widths).\n\n    >>> aug = iaa.Pad(px=((0, 10), (0, 5), (0, 10), (0, 5)))\n\n    pads the top and bottom by a random value from the range 0px to 10px\n    and the left and right by a random value in the range 0px to 5px.\n\n    >>> aug = iaa.Pad(percent=(0, 0.1))\n\n    pads each side by a random value from the range 0 percent to\n    10 percent. (Percent with respect to the side's size, e.g. for the\n    top side it uses the image's height.)\n\n    >>> aug = iaa.Pad(percent=([0.05, 0.1], [0.05, 0.1], [0.05, 0.1], [0.05, 0.1]))\n\n    pads each side by either 5 percent or 10 percent.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=\"edge\")\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). The padding uses the ``edge`` mode from numpy's\n    pad function.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=[\"constant\", \"edge\"])\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). The padding uses randomly either the ``constant``\n    or ``edge`` mode from numpy's pad function.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=ia.ALL, pad_cval=(0, 255))\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). It uses a random mode for numpy's pad function.\n    If the mode is ``constant`` or ``linear_ramp``, it samples a random value\n    ``v`` from the range ``[0, 255]`` and uses that as the constant\n    value (``mode=constant``) or end value (``mode=linear_ramp``).\n\n    \"\"\"\n\n    def recursive_validate(v):\n        if v is None:\n            return v\n        elif ia.is_single_number(v):\n            ia.do_assert(v >= 0)\n            return v\n        elif isinstance(v, iap.StochasticParameter):\n            return v\n        elif isinstance(v, tuple):\n            return tuple([recursive_validate(v_) for v_ in v])\n        elif isinstance(v, list):\n            return [recursive_validate(v_) for v_ in v]\n        else:\n            raise Exception(\"Expected None or int or float or StochasticParameter or list or tuple, got %s.\" % (\n                type(v),))\n\n    px = recursive_validate(px)\n    percent = recursive_validate(percent)\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    aug = CropAndPad(\n        px=px, percent=percent,\n        pad_mode=pad_mode, pad_cval=pad_cval,\n        keep_size=keep_size, sample_independently=sample_independently,\n        name=name, deterministic=deterministic, random_state=random_state\n    )\n    return aug",
    "doc": "Augmenter that pads images, i.e. adds columns/rows to them.\n\n    dtype support::\n\n        See ``imgaug.augmenters.size.CropAndPad``.\n\n    Parameters\n    ----------\n    px : None or int or imgaug.parameters.StochasticParameter or tuple, optional\n        The number of pixels to pad on each side of the image.\n        Either this or the parameter `percent` may be set, not both at the same\n        time.\n\n            * If None, then pixel-based padding will not be used.\n            * If int, then that exact number of pixels will always be padded.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two ints with values a and b, then each side will\n              be padded by a random amount in the range ``a <= x <= b``.\n              ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single integer (always pad by\n              exactly that value), a tuple of two ints ``a`` and ``b`` (pad by\n              an amount ``a <= x <= b``), a list of ints (pad by a random value\n              that is contained in the list) or a StochasticParameter (sample\n              the amount to pad from that parameter).\n\n    percent : None or int or float or imgaug.parameters.StochasticParameter \\\n              or tuple, optional\n        The number of pixels to pad on each side of the image given\n        *in percent* of the image height/width.\n        E.g. if this is set to 0.1, the augmenter will always add 10 percent\n        of the image's height to the top, 10 percent of the width to the right,\n        10 percent of the height at the bottom and 10 percent of the width to\n        the left. Either this or the parameter `px` may be set, not both at the\n        same time.\n\n            * If None, then percent-based padding will not be used.\n            * If int, then expected to be 0 (no padding).\n            * If float, then that percentage will always be padded.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two floats with values a and b, then each side will\n              be padded by a random percentage in the range ``a <= x <= b``.\n              ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single float (always pad by\n              exactly that percent value), a tuple of two floats ``a`` and ``b``\n              (pad by a percentage ``a <= x <= b``), a list of floats (pad by a\n              random value that is contained in the list) or a\n              StochasticParameter (sample the percentage to pad from that\n              parameter).\n\n    pad_mode : imgaug.ALL or str or list of str or \\\n               imgaug.parameters.StochasticParameter, optional\n        Padding mode to use. The available modes match the numpy padding modes,\n        i.e. ``constant``, ``edge``, ``linear_ramp``, ``maximum``, ``median``,\n        ``minimum``, ``reflect``, ``symmetric``, ``wrap``. The modes\n        ``constant`` and ``linear_ramp`` use extra values, which are provided\n        by ``pad_cval`` when necessary. See :func:`imgaug.imgaug.pad` for\n        more details.\n\n            * If ``imgaug.ALL``, then a random mode from all available modes\n              will be sampled per image.\n            * If a string, it will be used as the pad mode for all images.\n            * If a list of strings, a random one of these will be sampled per\n              image and used as the mode.\n            * If StochasticParameter, a random mode will be sampled from this\n              parameter per image.\n\n    pad_cval : number or tuple of number list of number or \\\n               imgaug.parameters.StochasticParameter, optional\n        The constant value to use if the pad mode is ``constant`` or the end\n        value to use if the mode is ``linear_ramp``.\n        See :func:`imgaug.imgaug.pad` for more details.\n\n            * If number, then that value will be used.\n            * If a tuple of two numbers and at least one of them is a float,\n              then a random number will be sampled from the continuous range\n              ``a <= x <= b`` and used as the value. If both numbers are\n              integers, the range is discrete.\n            * If a list of number, then a random value will be chosen from the\n              elements of the list and used as the value.\n            * If StochasticParameter, a random value will be sampled from that\n              parameter per image.\n\n    keep_size : bool, optional\n        After padding, the result image will usually have a different\n        height/width compared to the original input image. If this parameter is\n        set to True, then the padded image will be resized to the input image's\n        size, i.e. the augmenter's output shape is always identical to the\n        input shape.\n\n    sample_independently : bool, optional\n        If False AND the values for `px`/`percent` result in exactly one\n        probability distribution for the amount to pad, only one single value\n        will be sampled from that probability distribution and used for all\n        sides. I.e. the pad amount then is the same for all sides.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pad(px=(0, 10))\n\n    pads each side by a random value from the range 0px to 10px (the value\n    is sampled per side). The added rows/columns are filled with black pixels.\n\n    >>> aug = iaa.Pad(px=(0, 10), sample_independently=False)\n\n    samples one value v from the discrete range ``[0..10]`` and pads all sides\n    by ``v`` pixels.\n\n    >>> aug = iaa.Pad(px=(0, 10), keep_size=False)\n\n    pads each side by a random value from the range 0px to 10px (the value\n    is sampled per side). After padding, the images are NOT resized to their\n    original size (i.e. the images may end up having different heights/widths).\n\n    >>> aug = iaa.Pad(px=((0, 10), (0, 5), (0, 10), (0, 5)))\n\n    pads the top and bottom by a random value from the range 0px to 10px\n    and the left and right by a random value in the range 0px to 5px.\n\n    >>> aug = iaa.Pad(percent=(0, 0.1))\n\n    pads each side by a random value from the range 0 percent to\n    10 percent. (Percent with respect to the side's size, e.g. for the\n    top side it uses the image's height.)\n\n    >>> aug = iaa.Pad(percent=([0.05, 0.1], [0.05, 0.1], [0.05, 0.1], [0.05, 0.1]))\n\n    pads each side by either 5 percent or 10 percent.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=\"edge\")\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). The padding uses the ``edge`` mode from numpy's\n    pad function.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=[\"constant\", \"edge\"])\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). The padding uses randomly either the ``constant``\n    or ``edge`` mode from numpy's pad function.\n\n    >>> aug = iaa.Pad(px=(0, 10), pad_mode=ia.ALL, pad_cval=(0, 255))\n\n    pads each side by a random value from the range 0px to 10px (the values\n    are sampled per side). It uses a random mode for numpy's pad function.\n    If the mode is ``constant`` or ``linear_ramp``, it samples a random value\n    ``v`` from the range ``[0, 255]`` and uses that as the constant\n    value (``mode=constant``) or end value (``mode=linear_ramp``)."
  },
  {
    "code": "def Crop(px=None, percent=None, keep_size=True, sample_independently=True,\n         name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that crops/cuts away pixels at the sides of the image.\n\n    That allows to cut out subimages from given (full) input images.\n    The number of pixels to cut off may be defined in absolute values or\n    percent of the image sizes.\n\n    dtype support::\n\n        See ``imgaug.augmenters.size.CropAndPad``.\n\n    Parameters\n    ----------\n    px : None or int or imgaug.parameters.StochasticParameter or tuple, optional\n        The number of pixels to crop away (cut off) on each side of the image.\n        Either this or the parameter `percent` may be set, not both at the same\n        time.\n\n            * If None, then pixel-based cropping will not be used.\n            * If int, then that exact number of pixels will always be cropped.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two ints with values ``a`` and ``b``, then each\n              side will be cropped by a random amount in the range\n              ``a <= x <= b``. ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single integer (always crop by\n              exactly that value), a tuple of two ints ``a`` and ``b`` (crop by\n              an amount ``a <= x <= b``), a list of ints (crop by a random\n              value that is contained in the list) or a StochasticParameter\n              (sample the amount to crop from that parameter).\n\n    percent : None or int or float or imgaug.parameters.StochasticParameter \\\n              or tuple, optional\n        The number of pixels to crop away (cut off) on each side of the image\n        given *in percent* of the image height/width.\n        E.g. if this is set to 0.1, the augmenter will always crop away\n        10 percent of the image's height at the top, 10 percent of the width\n        on the right, 10 percent of the height at the bottom and 10 percent\n        of the width on the left.\n        Either this or the parameter `px` may be set, not both at the same time.\n\n            * If None, then percent-based cropping will not be used.\n            * If int, then expected to be 0 (no cropping).\n            * If float, then that percentage will always be cropped away.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two floats with values ``a`` and ``b``, then each\n              side will be cropped by a random percentage in the range\n              ``a <= x <= b``. ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single float (always crop by\n              exactly that percent value), a tuple of two floats a and ``b``\n              (crop by a percentage ``a <= x <= b``), a list of floats (crop by\n              a random value that is contained in the list) or a\n              StochasticParameter (sample the percentage to crop from that\n              parameter).\n\n    keep_size : bool, optional\n        After cropping, the result image has a different height/width than\n        the input image. If this parameter is set to True, then the cropped\n        image will be resized to the input image's size, i.e. the image size\n        is then not changed by the augmenter.\n\n    sample_independently : bool, optional\n        If False AND the values for `px`/`percent` result in exactly one\n        probability distribution for the amount to crop, only one\n        single value will be sampled from that probability distribution\n        and used for all sides. I.e. the crop amount then is the same\n        for all sides.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Crop(px=(0, 10))\n\n    crops each side by a random value from the range 0px to 10px (the value\n    is sampled per side).\n\n    >>> aug = iaa.Crop(px=(0, 10), sample_independently=False)\n\n    samples one value ``v`` from the discrete range ``[0..10]`` and crops all\n    sides by ``v`` pixels.\n\n    >>> aug = iaa.Crop(px=(0, 10), keep_size=False)\n\n    crops each side by a random value from the range 0px to 10px (the value\n    is sampled per side). After cropping, the images are NOT resized to their\n    original size (i.e. the images may end up having different heights/widths).\n\n    >>> aug = iaa.Crop(px=((0, 10), (0, 5), (0, 10), (0, 5)))\n\n    crops the top and bottom by a random value from the range 0px to 10px\n    and the left and right by a random value in the range 0px to 5px.\n\n    >>> aug = iaa.Crop(percent=(0, 0.1))\n\n    crops each side by a random value from the range 0 percent to\n    10 percent. (Percent with respect to the side's size, e.g. for the\n    top side it uses the image's height.)\n\n    >>> aug = iaa.Crop(percent=([0.05, 0.1], [0.05, 0.1], [0.05, 0.1], [0.05, 0.1]))\n\n    crops each side by either 5 percent or 10 percent.\n\n    \"\"\"\n\n    def recursive_negate(v):\n        if v is None:\n            return v\n        elif ia.is_single_number(v):\n            ia.do_assert(v >= 0)\n            return -v\n        elif isinstance(v, iap.StochasticParameter):\n            return iap.Multiply(v, -1)\n        elif isinstance(v, tuple):\n            return tuple([recursive_negate(v_) for v_ in v])\n        elif isinstance(v, list):\n            return [recursive_negate(v_) for v_ in v]\n        else:\n            raise Exception(\"Expected None or int or float or StochasticParameter or list or tuple, got %s.\" % (\n                type(v),))\n\n    px = recursive_negate(px)\n    percent = recursive_negate(percent)\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    aug = CropAndPad(\n        px=px, percent=percent,\n        keep_size=keep_size, sample_independently=sample_independently,\n        name=name, deterministic=deterministic, random_state=random_state\n    )\n    return aug",
    "doc": "Augmenter that crops/cuts away pixels at the sides of the image.\n\n    That allows to cut out subimages from given (full) input images.\n    The number of pixels to cut off may be defined in absolute values or\n    percent of the image sizes.\n\n    dtype support::\n\n        See ``imgaug.augmenters.size.CropAndPad``.\n\n    Parameters\n    ----------\n    px : None or int or imgaug.parameters.StochasticParameter or tuple, optional\n        The number of pixels to crop away (cut off) on each side of the image.\n        Either this or the parameter `percent` may be set, not both at the same\n        time.\n\n            * If None, then pixel-based cropping will not be used.\n            * If int, then that exact number of pixels will always be cropped.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two ints with values ``a`` and ``b``, then each\n              side will be cropped by a random amount in the range\n              ``a <= x <= b``. ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single integer (always crop by\n              exactly that value), a tuple of two ints ``a`` and ``b`` (crop by\n              an amount ``a <= x <= b``), a list of ints (crop by a random\n              value that is contained in the list) or a StochasticParameter\n              (sample the amount to crop from that parameter).\n\n    percent : None or int or float or imgaug.parameters.StochasticParameter \\\n              or tuple, optional\n        The number of pixels to crop away (cut off) on each side of the image\n        given *in percent* of the image height/width.\n        E.g. if this is set to 0.1, the augmenter will always crop away\n        10 percent of the image's height at the top, 10 percent of the width\n        on the right, 10 percent of the height at the bottom and 10 percent\n        of the width on the left.\n        Either this or the parameter `px` may be set, not both at the same time.\n\n            * If None, then percent-based cropping will not be used.\n            * If int, then expected to be 0 (no cropping).\n            * If float, then that percentage will always be cropped away.\n            * If StochasticParameter, then that parameter will be used for each\n              image. Four samples will be drawn per image (top, right, bottom,\n              left).\n            * If a tuple of two floats with values ``a`` and ``b``, then each\n              side will be cropped by a random percentage in the range\n              ``a <= x <= b``. ``x`` is sampled per image side.\n            * If a tuple of four entries, then the entries represent top, right,\n              bottom, left. Each entry may be a single float (always crop by\n              exactly that percent value), a tuple of two floats a and ``b``\n              (crop by a percentage ``a <= x <= b``), a list of floats (crop by\n              a random value that is contained in the list) or a\n              StochasticParameter (sample the percentage to crop from that\n              parameter).\n\n    keep_size : bool, optional\n        After cropping, the result image has a different height/width than\n        the input image. If this parameter is set to True, then the cropped\n        image will be resized to the input image's size, i.e. the image size\n        is then not changed by the augmenter.\n\n    sample_independently : bool, optional\n        If False AND the values for `px`/`percent` result in exactly one\n        probability distribution for the amount to crop, only one\n        single value will be sampled from that probability distribution\n        and used for all sides. I.e. the crop amount then is the same\n        for all sides.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Crop(px=(0, 10))\n\n    crops each side by a random value from the range 0px to 10px (the value\n    is sampled per side).\n\n    >>> aug = iaa.Crop(px=(0, 10), sample_independently=False)\n\n    samples one value ``v`` from the discrete range ``[0..10]`` and crops all\n    sides by ``v`` pixels.\n\n    >>> aug = iaa.Crop(px=(0, 10), keep_size=False)\n\n    crops each side by a random value from the range 0px to 10px (the value\n    is sampled per side). After cropping, the images are NOT resized to their\n    original size (i.e. the images may end up having different heights/widths).\n\n    >>> aug = iaa.Crop(px=((0, 10), (0, 5), (0, 10), (0, 5)))\n\n    crops the top and bottom by a random value from the range 0px to 10px\n    and the left and right by a random value in the range 0px to 5px.\n\n    >>> aug = iaa.Crop(percent=(0, 0.1))\n\n    crops each side by a random value from the range 0 percent to\n    10 percent. (Percent with respect to the side's size, e.g. for the\n    top side it uses the image's height.)\n\n    >>> aug = iaa.Crop(percent=([0.05, 0.1], [0.05, 0.1], [0.05, 0.1], [0.05, 0.1]))\n\n    crops each side by either 5 percent or 10 percent."
  },
  {
    "code": "def isect_segments__naive(segments):\n    \"\"\"\n    Brute force O(n2) version of ``isect_segments`` for test validation.\n    \"\"\"\n    isect = []\n\n    # order points left -> right\n    if Real is float:\n        segments = [\n            (s[0], s[1]) if s[0][X] <= s[1][X] else\n            (s[1], s[0])\n            for s in segments]\n    else:\n        segments = [\n            (\n                (Real(s[0][0]), Real(s[0][1])),\n                (Real(s[1][0]), Real(s[1][1])),\n            ) if (s[0] <= s[1]) else\n            (\n                (Real(s[1][0]), Real(s[1][1])),\n                (Real(s[0][0]), Real(s[0][1])),\n            )\n            for s in segments]\n\n    n = len(segments)\n\n    for i in range(n):\n        a0, a1 = segments[i]\n        for j in range(i + 1, n):\n            b0, b1 = segments[j]\n            if a0 not in (b0, b1) and a1 not in (b0, b1):\n                ix = isect_seg_seg_v2_point(a0, a1, b0, b1)\n                if ix is not None:\n                    # USE_IGNORE_SEGMENT_ENDINGS handled already\n                    isect.append(ix)\n\n    return isect",
    "doc": "Brute force O(n2) version of ``isect_segments`` for test validation."
  },
  {
    "code": "def isect_polygon__naive(points):\n    \"\"\"\n    Brute force O(n2) version of ``isect_polygon`` for test validation.\n    \"\"\"\n    isect = []\n\n    n = len(points)\n\n    if Real is float:\n        pass\n    else:\n        points = [(Real(p[0]), Real(p[1])) for p in points]\n\n\n    for i in range(n):\n        a0, a1 = points[i], points[(i + 1) % n]\n        for j in range(i + 1, n):\n            b0, b1 = points[j], points[(j + 1) % n]\n            if a0 not in (b0, b1) and a1 not in (b0, b1):\n                ix = isect_seg_seg_v2_point(a0, a1, b0, b1)\n                if ix is not None:\n\n                    if USE_IGNORE_SEGMENT_ENDINGS:\n                        if ((len_squared_v2v2(ix, a0) < NUM_EPS_SQ or\n                             len_squared_v2v2(ix, a1) < NUM_EPS_SQ) and\n                            (len_squared_v2v2(ix, b0) < NUM_EPS_SQ or\n                             len_squared_v2v2(ix, b1) < NUM_EPS_SQ)):\n                            continue\n\n                    isect.append(ix)\n\n    return isect",
    "doc": "Brute force O(n2) version of ``isect_polygon`` for test validation."
  },
  {
    "code": "def get_intersections(self):\n        \"\"\"\n        Return a list of unordered intersection points.\n        \"\"\"\n        if Real is float:\n            return list(self.intersections.keys())\n        else:\n            return [(float(p[0]), float(p[1])) for p in self.intersections.keys()]",
    "doc": "Return a list of unordered intersection points."
  },
  {
    "code": "def get_intersections_with_segments(self):\n        \"\"\"\n        Return a list of unordered intersection '(point, segment)' pairs,\n        where segments may contain 2 or more values.\n        \"\"\"\n        if Real is float:\n            return [\n                (p, [event.segment for event in event_set])\n                for p, event_set in self.intersections.items()\n            ]\n        else:\n            return [\n                (\n                    (float(p[0]), float(p[1])),\n                    [((float(event.segment[0][0]), float(event.segment[0][1])),\n                      (float(event.segment[1][0]), float(event.segment[1][1])))\n                     for event in event_set],\n                )\n                for p, event_set in self.intersections.items()\n            ]",
    "doc": "Return a list of unordered intersection '(point, segment)' pairs,\n        where segments may contain 2 or more values."
  },
  {
    "code": "def poll(self):\n        \"\"\"\n        Get, and remove, the first (lowest) item from this queue.\n\n        :return: the first (lowest) item from this queue.\n        :rtype: Point, Event pair.\n        \"\"\"\n        assert(len(self.events_scan) != 0)\n        p, events_current = self.events_scan.pop_min()\n        return p, events_current",
    "doc": "Get, and remove, the first (lowest) item from this queue.\n\n        :return: the first (lowest) item from this queue.\n        :rtype: Point, Event pair."
  },
  {
    "code": "def clear(self):\n        \"\"\"T.clear() -> None.  Remove all items from T.\"\"\"\n        def _clear(node):\n            if node is not None:\n                _clear(node.left)\n                _clear(node.right)\n                node.free()\n        _clear(self._root)\n        self._count = 0\n        self._root = None",
    "doc": "T.clear() -> None.  Remove all items from T."
  },
  {
    "code": "def pop_item(self):\n        \"\"\"T.pop_item() -> (k, v), remove and return some (key, value) pair as a\n        2-tuple; but raise KeyError if T is empty.\n        \"\"\"\n        if self.is_empty():\n            raise KeyError(\"pop_item(): tree is empty\")\n        node = self._root\n        while True:\n            if node.left is not None:\n                node = node.left\n            elif node.right is not None:\n                node = node.right\n            else:\n                break\n        key = node.key\n        value = node.value\n        self.remove(key)\n        return key, value",
    "doc": "T.pop_item() -> (k, v), remove and return some (key, value) pair as a\n        2-tuple; but raise KeyError if T is empty."
  },
  {
    "code": "def min_item(self):\n        \"\"\"Get item with min key of tree, raises ValueError if tree is empty.\"\"\"\n        if self.is_empty():\n            raise ValueError(\"Tree is empty\")\n        node = self._root\n        while node.left is not None:\n            node = node.left\n        return node.key, node.value",
    "doc": "Get item with min key of tree, raises ValueError if tree is empty."
  },
  {
    "code": "def max_item(self):\n        \"\"\"Get item with max key of tree, raises ValueError if tree is empty.\"\"\"\n        if self.is_empty():\n            raise ValueError(\"Tree is empty\")\n        node = self._root\n        while node.right is not None:\n            node = node.right\n        return node.key, node.value",
    "doc": "Get item with max key of tree, raises ValueError if tree is empty."
  },
  {
    "code": "def succ_item(self, key, default=_sentinel):\n        \"\"\"Get successor (k,v) pair of key, raises KeyError if key is max key\n        or key does not exist. optimized for pypy.\n        \"\"\"\n        # removed graingets version, because it was little slower on CPython and much slower on pypy\n        # this version runs about 4x faster with pypy than the Cython version\n        # Note: Code sharing of succ_item() and ceiling_item() is possible, but has always a speed penalty.\n        node = self._root\n        succ_node = None\n        while node is not None:\n            cmp = self._cmp(self._cmp_data, key, node.key)\n            if cmp == 0:\n                break\n            elif cmp < 0:\n                if (succ_node is None) or self._cmp(self._cmp_data, node.key, succ_node.key) < 0:\n                    succ_node = node\n                node = node.left\n            else:\n                node = node.right\n\n        if node is None:  # stay at dead end\n            if default is _sentinel:\n                raise KeyError(str(key))\n            return default\n        # found node of key\n        if node.right is not None:\n            # find smallest node of right subtree\n            node = node.right\n            while node.left is not None:\n                node = node.left\n            if succ_node is None:\n                succ_node = node\n            elif self._cmp(self._cmp_data, node.key, succ_node.key) < 0:\n                succ_node = node\n        elif succ_node is None:  # given key is biggest in tree\n            if default is _sentinel:\n                raise KeyError(str(key))\n            return default\n        return succ_node.key, succ_node.value",
    "doc": "Get successor (k,v) pair of key, raises KeyError if key is max key\n        or key does not exist. optimized for pypy."
  },
  {
    "code": "def prev_item(self, key, default=_sentinel):\n        \"\"\"Get predecessor (k,v) pair of key, raises KeyError if key is min key\n        or key does not exist. optimized for pypy.\n        \"\"\"\n        # removed graingets version, because it was little slower on CPython and much slower on pypy\n        # this version runs about 4x faster with pypy than the Cython version\n        # Note: Code sharing of prev_item() and floor_item() is possible, but has always a speed penalty.\n        node = self._root\n        prev_node = None\n\n        while node is not None:\n            cmp = self._cmp(self._cmp_data, key, node.key)\n            if cmp == 0:\n                break\n            elif cmp < 0:\n                node = node.left\n            else:\n                if (prev_node is None) or self._cmp(self._cmp_data, prev_node.key, node.key) < 0:\n                    prev_node = node\n                node = node.right\n\n        if node is None:  # stay at dead end (None)\n            if default is _sentinel:\n                raise KeyError(str(key))\n            return default\n        # found node of key\n        if node.left is not None:\n            # find biggest node of left subtree\n            node = node.left\n            while node.right is not None:\n                node = node.right\n            if prev_node is None:\n                prev_node = node\n            elif self._cmp(self._cmp_data, prev_node.key, node.key) < 0:\n                prev_node = node\n        elif prev_node is None:  # given key is smallest in tree\n            if default is _sentinel:\n                raise KeyError(str(key))\n            return default\n        return prev_node.key, prev_node.value",
    "doc": "Get predecessor (k,v) pair of key, raises KeyError if key is min key\n        or key does not exist. optimized for pypy."
  },
  {
    "code": "def set_default(self, key, default=None):\n        \"\"\"T.set_default(k[,d]) -> T.get(k,d), also set T[k]=d if k not in T\"\"\"\n        try:\n            return self.get_value(key)\n        except KeyError:\n            self.insert(key, default)\n            return default",
    "doc": "T.set_default(k[,d]) -> T.get(k,d), also set T[k]=d if k not in T"
  },
  {
    "code": "def pop(self, key, *args):\n        \"\"\"T.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n        If key is not found, d is returned if given, otherwise KeyError is raised\n        \"\"\"\n        if len(args) > 1:\n            raise TypeError(\"pop expected at most 2 arguments, got %d\" % (1 + len(args)))\n        try:\n            value = self.get_value(key)\n            self.remove(key)\n            return value\n        except KeyError:\n            if len(args) == 0:\n                raise\n            else:\n                return args[0]",
    "doc": "T.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n        If key is not found, d is returned if given, otherwise KeyError is raised"
  },
  {
    "code": "def prev_key(self, key, default=_sentinel):\n        \"\"\"Get predecessor to key, raises KeyError if key is min key\n        or key does not exist.\n        \"\"\"\n        item = self.prev_item(key, default)\n        return default if item is default else item[0]",
    "doc": "Get predecessor to key, raises KeyError if key is min key\n        or key does not exist."
  },
  {
    "code": "def succ_key(self, key, default=_sentinel):\n        \"\"\"Get successor to key, raises KeyError if key is max key\n        or key does not exist.\n        \"\"\"\n        item = self.succ_item(key, default)\n        return default if item is default else item[0]",
    "doc": "Get successor to key, raises KeyError if key is max key\n        or key does not exist."
  },
  {
    "code": "def key_slice(self, start_key, end_key, reverse=False):\n        \"\"\"T.key_slice(start_key, end_key) -> key iterator:\n        start_key <= key < end_key.\n\n        Yields keys in ascending order if reverse is False else in descending order.\n        \"\"\"\n        return (k for k, v in self.iter_items(start_key, end_key, reverse=reverse))",
    "doc": "T.key_slice(start_key, end_key) -> key iterator:\n        start_key <= key < end_key.\n\n        Yields keys in ascending order if reverse is False else in descending order."
  },
  {
    "code": "def iter_items(self,  start_key=None, end_key=None, reverse=False):\n        \"\"\"Iterates over the (key, value) items of the associated tree,\n        in ascending order if reverse is True, iterate in descending order,\n        reverse defaults to False\"\"\"\n        # optimized iterator (reduced method calls) - faster on CPython but slower on pypy\n\n        if self.is_empty():\n            return []\n        if reverse:\n            return self._iter_items_backward(start_key, end_key)\n        else:\n            return self._iter_items_forward(start_key, end_key)",
    "doc": "Iterates over the (key, value) items of the associated tree,\n        in ascending order if reverse is True, iterate in descending order,\n        reverse defaults to False"
  },
  {
    "code": "def insert(self, key, value):\n        \"\"\"T.insert(key, value) <==> T[key] = value, insert key, value into tree.\"\"\"\n        if self._root is None:  # Empty tree case\n            self._root = self._new_node(key, value)\n            self._root.red = False  # make root black\n            return\n\n        head = Node()  # False tree root\n        grand_parent = None\n        grand_grand_parent = head\n        parent = None  # parent\n        direction = 0\n        last = 0\n\n        # Set up helpers\n        grand_grand_parent.right = self._root\n        node = grand_grand_parent.right\n        # Search down the tree\n        while True:\n            if node is None:  # Insert new node at the bottom\n                node = self._new_node(key, value)\n                parent[direction] = node\n            elif RBTree.is_red(node.left) and RBTree.is_red(node.right):  # Color flip\n                node.red = True\n                node.left.red = False\n                node.right.red = False\n\n            # Fix red violation\n            if RBTree.is_red(node) and RBTree.is_red(parent):\n                direction2 = 1 if grand_grand_parent.right is grand_parent else 0\n                if node is parent[last]:\n                    grand_grand_parent[direction2] = RBTree.jsw_single(grand_parent, 1 - last)\n                else:\n                    grand_grand_parent[direction2] = RBTree.jsw_double(grand_parent, 1 - last)\n\n            # Stop if found\n            if self._cmp(self._cmp_data, key, node.key) == 0:\n                node.value = value  # set new value for key\n                break\n\n            last = direction\n            direction = 0 if (self._cmp(self._cmp_data, key, node.key) < 0) else 1\n            # Update helpers\n            if grand_parent is not None:\n                grand_grand_parent = grand_parent\n            grand_parent = parent\n            parent = node\n            node = node[direction]\n\n        self._root = head.right  # Update root\n        self._root.red = False",
    "doc": "T.insert(key, value) <==> T[key] = value, insert key, value into tree."
  },
  {
    "code": "def remove(self, key):\n        \"\"\"T.remove(key) <==> del T[key], remove item <key> from tree.\"\"\"\n        if self._root is None:\n            raise KeyError(str(key))\n        head = Node()  # False tree root\n        node = head\n        node.right = self._root\n        parent = None\n        grand_parent = None\n        found = None  # Found item\n        direction = 1\n\n        # Search and push a red down\n        while node[direction] is not None:\n            last = direction\n\n            # Update helpers\n            grand_parent = parent\n            parent = node\n            node = node[direction]\n\n            direction = 1 if (self._cmp(self._cmp_data, node.key, key) < 0) else 0\n\n            # Save found node\n            if self._cmp(self._cmp_data, key, node.key) == 0:\n                found = node\n\n            # Push the red node down\n            if not RBTree.is_red(node) and not RBTree.is_red(node[direction]):\n                if RBTree.is_red(node[1 - direction]):\n                    parent[last] = RBTree.jsw_single(node, direction)\n                    parent = parent[last]\n                elif not RBTree.is_red(node[1 - direction]):\n                    sibling = parent[1 - last]\n                    if sibling is not None:\n                        if (not RBTree.is_red(sibling[1 - last])) and (not RBTree.is_red(sibling[last])):\n                            # Color flip\n                            parent.red = False\n                            sibling.red = True\n                            node.red = True\n                        else:\n                            direction2 = 1 if grand_parent.right is parent else 0\n                            if RBTree.is_red(sibling[last]):\n                                grand_parent[direction2] = RBTree.jsw_double(parent, last)\n                            elif RBTree.is_red(sibling[1-last]):\n                                grand_parent[direction2] = RBTree.jsw_single(parent, last)\n                            # Ensure correct coloring\n                            grand_parent[direction2].red = True\n                            node.red = True\n                            grand_parent[direction2].left.red = False\n                            grand_parent[direction2].right.red = False\n\n        # Replace and remove if found\n        if found is not None:\n            found.key = node.key\n            found.value = node.value\n            parent[int(parent.right is node)] = node[int(node.left is None)]\n            node.free()\n            self._count -= 1\n\n        # Update root and make it black\n        self._root = head.right\n        if self._root is not None:\n            self._root.red = False\n        if not found:\n            raise KeyError(str(key))",
    "doc": "T.remove(key) <==> del T[key], remove item <key> from tree."
  },
  {
    "code": "def noise2d(self, x, y):\n        \"\"\"\n        Generate 2D OpenSimplex noise from X,Y coordinates.\n        \"\"\"\n        # Place input coordinates onto grid.\n        stretch_offset = (x + y) * STRETCH_CONSTANT_2D\n        xs = x + stretch_offset\n        ys = y + stretch_offset\n\n        # Floor to get grid coordinates of rhombus (stretched square) super-cell origin.\n        xsb = floor(xs)\n        ysb = floor(ys)\n\n        # Skew out to get actual coordinates of rhombus origin. We'll need these later.\n        squish_offset = (xsb + ysb) * SQUISH_CONSTANT_2D\n        xb = xsb + squish_offset\n        yb = ysb + squish_offset\n\n        # Compute grid coordinates relative to rhombus origin.\n        xins = xs - xsb\n        yins = ys - ysb\n\n        # Sum those together to get a value that determines which region we're in.\n        in_sum = xins + yins\n\n        # Positions relative to origin point.\n        dx0 = x - xb\n        dy0 = y - yb\n\n        value = 0\n\n        # Contribution (1,0)\n        dx1 = dx0 - 1 - SQUISH_CONSTANT_2D\n        dy1 = dy0 - 0 - SQUISH_CONSTANT_2D\n        attn1 = 2 - dx1 * dx1 - dy1 * dy1\n        extrapolate = self._extrapolate2d\n        if attn1 > 0:\n            attn1 *= attn1\n            value += attn1 * attn1 * extrapolate(xsb + 1, ysb + 0, dx1, dy1)\n\n        # Contribution (0,1)\n        dx2 = dx0 - 0 - SQUISH_CONSTANT_2D\n        dy2 = dy0 - 1 - SQUISH_CONSTANT_2D\n        attn2 = 2 - dx2 * dx2 - dy2 * dy2\n        if attn2 > 0:\n            attn2 *= attn2\n            value += attn2 * attn2 * extrapolate(xsb + 0, ysb + 1, dx2, dy2)\n\n        if in_sum <= 1: # We're inside the triangle (2-Simplex) at (0,0)\n            zins = 1 - in_sum\n            if zins > xins or zins > yins: # (0,0) is one of the closest two triangular vertices\n                if xins > yins:\n                    xsv_ext = xsb + 1\n                    ysv_ext = ysb - 1\n                    dx_ext = dx0 - 1\n                    dy_ext = dy0 + 1\n                else:\n                    xsv_ext = xsb - 1\n                    ysv_ext = ysb + 1\n                    dx_ext = dx0 + 1\n                    dy_ext = dy0 - 1\n            else: # (1,0) and (0,1) are the closest two vertices.\n                xsv_ext = xsb + 1\n                ysv_ext = ysb + 1\n                dx_ext = dx0 - 1 - 2 * SQUISH_CONSTANT_2D\n                dy_ext = dy0 - 1 - 2 * SQUISH_CONSTANT_2D\n        else: # We're inside the triangle (2-Simplex) at (1,1)\n            zins = 2 - in_sum\n            if zins < xins or zins < yins: # (0,0) is one of the closest two triangular vertices\n                if xins > yins:\n                    xsv_ext = xsb + 2\n                    ysv_ext = ysb + 0\n                    dx_ext = dx0 - 2 - 2 * SQUISH_CONSTANT_2D\n                    dy_ext = dy0 + 0 - 2 * SQUISH_CONSTANT_2D\n                else:\n                    xsv_ext = xsb + 0\n                    ysv_ext = ysb + 2\n                    dx_ext = dx0 + 0 - 2 * SQUISH_CONSTANT_2D\n                    dy_ext = dy0 - 2 - 2 * SQUISH_CONSTANT_2D\n            else: # (1,0) and (0,1) are the closest two vertices.\n                dx_ext = dx0\n                dy_ext = dy0\n                xsv_ext = xsb\n                ysv_ext = ysb\n            xsb += 1\n            ysb += 1\n            dx0 = dx0 - 1 - 2 * SQUISH_CONSTANT_2D\n            dy0 = dy0 - 1 - 2 * SQUISH_CONSTANT_2D\n\n        # Contribution (0,0) or (1,1)\n        attn0 = 2 - dx0 * dx0 - dy0 * dy0\n        if attn0 > 0:\n            attn0 *= attn0\n            value += attn0 * attn0 * extrapolate(xsb, ysb, dx0, dy0)\n\n        # Extra Vertex\n        attn_ext = 2 - dx_ext * dx_ext - dy_ext * dy_ext\n        if attn_ext > 0:\n            attn_ext *= attn_ext\n            value += attn_ext * attn_ext * extrapolate(xsv_ext, ysv_ext, dx_ext, dy_ext)\n\n        return value / NORM_CONSTANT_2D",
    "doc": "Generate 2D OpenSimplex noise from X,Y coordinates."
  },
  {
    "code": "def noise3d(self, x, y, z):\n        \"\"\"\n        Generate 3D OpenSimplex noise from X,Y,Z coordinates.\n        \"\"\"\n        # Place input coordinates on simplectic honeycomb.\n        stretch_offset = (x + y + z) * STRETCH_CONSTANT_3D\n        xs = x + stretch_offset\n        ys = y + stretch_offset\n        zs = z + stretch_offset\n\n        # Floor to get simplectic honeycomb coordinates of rhombohedron (stretched cube) super-cell origin.\n        xsb = floor(xs)\n        ysb = floor(ys)\n        zsb = floor(zs)\n\n        # Skew out to get actual coordinates of rhombohedron origin. We'll need these later.\n        squish_offset = (xsb + ysb + zsb) * SQUISH_CONSTANT_3D\n        xb = xsb + squish_offset\n        yb = ysb + squish_offset\n        zb = zsb + squish_offset\n\n        # Compute simplectic honeycomb coordinates relative to rhombohedral origin.\n        xins = xs - xsb\n        yins = ys - ysb\n        zins = zs - zsb\n\n        # Sum those together to get a value that determines which region we're in.\n        in_sum = xins + yins + zins\n\n        # Positions relative to origin point.\n        dx0 = x - xb\n        dy0 = y - yb\n        dz0 = z - zb\n\n        value = 0\n        extrapolate = self._extrapolate3d\n        if in_sum <= 1: # We're inside the tetrahedron (3-Simplex) at (0,0,0)\n\n            # Determine which two of (0,0,1), (0,1,0), (1,0,0) are closest.\n            a_point = 0x01\n            a_score = xins\n            b_point = 0x02\n            b_score = yins\n            if a_score >= b_score and zins > b_score:\n                b_score = zins\n                b_point = 0x04\n            elif a_score < b_score and zins > a_score:\n                a_score = zins\n                a_point = 0x04\n\n            # Now we determine the two lattice points not part of the tetrahedron that may contribute.\n            # This depends on the closest two tetrahedral vertices, including (0,0,0)\n            wins = 1 - in_sum\n            if wins > a_score or wins > b_score: # (0,0,0) is one of the closest two tetrahedral vertices.\n                c = b_point if (b_score > a_score) else a_point # Our other closest vertex is the closest out of a and b.\n\n                if (c & 0x01) == 0:\n                    xsv_ext0 = xsb - 1\n                    xsv_ext1 = xsb\n                    dx_ext0 = dx0 + 1\n                    dx_ext1 = dx0\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb + 1\n                    dx_ext0 = dx_ext1 = dx0 - 1\n\n                if (c & 0x02) == 0:\n                    ysv_ext0 = ysv_ext1 = ysb\n                    dy_ext0 = dy_ext1 = dy0\n                    if (c & 0x01) == 0:\n                        ysv_ext1 -= 1\n                        dy_ext1 += 1\n                    else:\n                        ysv_ext0 -= 1\n                        dy_ext0 += 1\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy0 - 1\n\n                if (c & 0x04) == 0:\n                    zsv_ext0 = zsb\n                    zsv_ext1 = zsb - 1\n                    dz_ext0 = dz0\n                    dz_ext1 = dz0 + 1\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb + 1\n                    dz_ext0 = dz_ext1 = dz0 - 1\n            else: # (0,0,0) is not one of the closest two tetrahedral vertices.\n                c = (a_point | b_point) # Our two extra vertices are determined by the closest two.\n\n                if (c & 0x01) == 0:\n                    xsv_ext0 = xsb\n                    xsv_ext1 = xsb - 1\n                    dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_3D\n                    dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb + 1\n                    dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D\n                    dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D\n\n                if (c & 0x02) == 0:\n                    ysv_ext0 = ysb\n                    ysv_ext1 = ysb - 1\n                    dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_3D\n                    dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb + 1\n                    dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D\n                    dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D\n\n                if (c & 0x04) == 0:\n                    zsv_ext0 = zsb\n                    zsv_ext1 = zsb - 1\n                    dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_3D\n                    dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb + 1\n                    dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D\n                    dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D\n\n            # Contribution (0,0,0)\n            attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0\n            if attn0 > 0:\n                attn0 *= attn0\n                value += attn0 * attn0 * extrapolate(xsb + 0, ysb + 0, zsb + 0, dx0, dy0, dz0)\n\n            # Contribution (1,0,0)\n            dx1 = dx0 - 1 - SQUISH_CONSTANT_3D\n            dy1 = dy0 - 0 - SQUISH_CONSTANT_3D\n            dz1 = dz0 - 0 - SQUISH_CONSTANT_3D\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 1, ysb + 0, zsb + 0, dx1, dy1, dz1)\n\n            # Contribution (0,1,0)\n            dx2 = dx0 - 0 - SQUISH_CONSTANT_3D\n            dy2 = dy0 - 1 - SQUISH_CONSTANT_3D\n            dz2 = dz1\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 0, ysb + 1, zsb + 0, dx2, dy2, dz2)\n\n            # Contribution (0,0,1)\n            dx3 = dx2\n            dy3 = dy1\n            dz3 = dz0 - 1 - SQUISH_CONSTANT_3D\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 0, ysb + 0, zsb + 1, dx3, dy3, dz3)\n        elif in_sum >= 2: # We're inside the tetrahedron (3-Simplex) at (1,1,1)\n\n            # Determine which two tetrahedral vertices are the closest, out of (1,1,0), (1,0,1), (0,1,1) but not (1,1,1).\n            a_point = 0x06\n            a_score = xins\n            b_point = 0x05\n            b_score = yins\n            if a_score <= b_score and zins < b_score:\n                b_score = zins\n                b_point = 0x03\n            elif a_score > b_score and zins < a_score:\n                a_score = zins\n                a_point = 0x03\n\n            # Now we determine the two lattice points not part of the tetrahedron that may contribute.\n            # This depends on the closest two tetrahedral vertices, including (1,1,1)\n            wins = 3 - in_sum\n            if wins < a_score or wins < b_score: # (1,1,1) is one of the closest two tetrahedral vertices.\n                c = b_point if (b_score < a_score) else a_point # Our other closest vertex is the closest out of a and b.\n\n                if (c & 0x01) != 0:\n                    xsv_ext0 = xsb + 2\n                    xsv_ext1 = xsb + 1\n                    dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_3D\n                    dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb\n                    dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_3D\n\n                if (c & 0x02) != 0:\n                    ysv_ext0 = ysv_ext1 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D\n                    if (c & 0x01) != 0:\n                        ysv_ext1 += 1\n                        dy_ext1 -= 1\n                    else:\n                        ysv_ext0 += 1\n                        dy_ext0 -= 1\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb\n                    dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_3D\n\n                if (c & 0x04) != 0:\n                    zsv_ext0 = zsb + 1\n                    zsv_ext1 = zsb + 2\n                    dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D\n                    dz_ext1 = dz0 - 2 - 3 * SQUISH_CONSTANT_3D\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb\n                    dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_3D\n            else: # (1,1,1) is not one of the closest two tetrahedral vertices.\n                c = (a_point & b_point) # Our two extra vertices are determined by the closest two.\n\n                if (c & 0x01) != 0:\n                    xsv_ext0 = xsb + 1\n                    xsv_ext1 = xsb + 2\n                    dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D\n                    dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb\n                    dx_ext0 = dx0 - SQUISH_CONSTANT_3D\n                    dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D\n\n                if (c & 0x02) != 0:\n                    ysv_ext0 = ysb + 1\n                    ysv_ext1 = ysb + 2\n                    dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D\n                    dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb\n                    dy_ext0 = dy0 - SQUISH_CONSTANT_3D\n                    dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D\n\n                if (c & 0x04) != 0:\n                    zsv_ext0 = zsb + 1\n                    zsv_ext1 = zsb + 2\n                    dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D\n                    dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb\n                    dz_ext0 = dz0 - SQUISH_CONSTANT_3D\n                    dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D\n\n            # Contribution (1,1,0)\n            dx3 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D\n            dy3 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D\n            dz3 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 1, ysb + 1, zsb + 0, dx3, dy3, dz3)\n\n            # Contribution (1,0,1)\n            dx2 = dx3\n            dy2 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D\n            dz2 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 1, ysb + 0, zsb + 1, dx2, dy2, dz2)\n\n            # Contribution (0,1,1)\n            dx1 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D\n            dy1 = dy3\n            dz1 = dz2\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 0, ysb + 1, zsb + 1, dx1, dy1, dz1)\n\n            # Contribution (1,1,1)\n            dx0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D\n            dy0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D\n            dz0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D\n            attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0\n            if attn0 > 0:\n                attn0 *= attn0\n                value += attn0 * attn0 * extrapolate(xsb + 1, ysb + 1, zsb + 1, dx0, dy0, dz0)\n        else: # We're inside the octahedron (Rectified 3-Simplex) in between.\n            # Decide between point (0,0,1) and (1,1,0) as closest\n            p1 = xins + yins\n            if p1 > 1:\n                a_score = p1 - 1\n                a_point = 0x03\n                a_is_further_side = True\n            else:\n                a_score = 1 - p1\n                a_point = 0x04\n                a_is_further_side = False\n\n            # Decide between point (0,1,0) and (1,0,1) as closest\n            p2 = xins + zins\n            if p2 > 1:\n                b_score = p2 - 1\n                b_point = 0x05\n                b_is_further_side = True\n            else:\n                b_score = 1 - p2\n                b_point = 0x02\n                b_is_further_side = False\n\n            # The closest out of the two (1,0,0) and (0,1,1) will replace the furthest out of the two decided above, if closer.\n            p3 = yins + zins\n            if p3 > 1:\n                score = p3 - 1\n                if a_score <= b_score and a_score < score:\n                    a_point = 0x06\n                    a_is_further_side = True\n                elif a_score > b_score and b_score < score:\n                    b_point = 0x06\n                    b_is_further_side = True\n            else:\n                score = 1 - p3\n                if a_score <= b_score and a_score < score:\n                    a_point = 0x01\n                    a_is_further_side = False\n                elif a_score > b_score and b_score < score:\n                    b_point = 0x01\n                    b_is_further_side = False\n\n            # Where each of the two closest points are determines how the extra two vertices are calculated.\n            if a_is_further_side == b_is_further_side:\n                if a_is_further_side: # Both closest points on (1,1,1) side\n\n                    # One of the two extra points is (1,1,1)\n                    dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D\n                    dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D\n                    dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D\n                    xsv_ext0 = xsb + 1\n                    ysv_ext0 = ysb + 1\n                    zsv_ext0 = zsb + 1\n\n                    # Other extra point is based on the shared axis.\n                    c = (a_point & b_point)\n                    if (c & 0x01) != 0:\n                        dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb + 2\n                        ysv_ext1 = ysb\n                        zsv_ext1 = zsb\n                    elif (c & 0x02) != 0:\n                        dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb\n                        ysv_ext1 = ysb + 2\n                        zsv_ext1 = zsb\n                    else:\n                        dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb\n                        ysv_ext1 = ysb\n                        zsv_ext1 = zsb + 2\n                else:# Both closest points on (0,0,0) side\n\n                    # One of the two extra points is (0,0,0)\n                    dx_ext0 = dx0\n                    dy_ext0 = dy0\n                    dz_ext0 = dz0\n                    xsv_ext0 = xsb\n                    ysv_ext0 = ysb\n                    zsv_ext0 = zsb\n\n                    # Other extra point is based on the omitted axis.\n                    c = (a_point | b_point)\n                    if (c & 0x01) == 0:\n                        dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb - 1\n                        ysv_ext1 = ysb + 1\n                        zsv_ext1 = zsb + 1\n                    elif (c & 0x02) == 0:\n                        dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb + 1\n                        ysv_ext1 = ysb - 1\n                        zsv_ext1 = zsb + 1\n                    else:\n                        dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D\n                        dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D\n                        dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D\n                        xsv_ext1 = xsb + 1\n                        ysv_ext1 = ysb + 1\n                        zsv_ext1 = zsb - 1\n            else: # One point on (0,0,0) side, one point on (1,1,1) side\n                if a_is_further_side:\n                    c1 = a_point\n                    c2 = b_point\n                else:\n                    c1 = b_point\n                    c2 = a_point\n\n                # One contribution is a _permutation of (1,1,-1)\n                if (c1 & 0x01) == 0:\n                    dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_3D\n                    dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D\n                    dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D\n                    xsv_ext0 = xsb - 1\n                    ysv_ext0 = ysb + 1\n                    zsv_ext0 = zsb + 1\n                elif (c1 & 0x02) == 0:\n                    dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D\n                    dy_ext0 = dy0 + 1 - SQUISH_CONSTANT_3D\n                    dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D\n                    xsv_ext0 = xsb + 1\n                    ysv_ext0 = ysb - 1\n                    zsv_ext0 = zsb + 1\n                else:\n                    dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D\n                    dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D\n                    dz_ext0 = dz0 + 1 - SQUISH_CONSTANT_3D\n                    xsv_ext0 = xsb + 1\n                    ysv_ext0 = ysb + 1\n                    zsv_ext0 = zsb - 1\n\n                # One contribution is a _permutation of (0,0,2)\n                dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D\n                dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D\n                dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D\n                xsv_ext1 = xsb\n                ysv_ext1 = ysb\n                zsv_ext1 = zsb\n                if (c2 & 0x01) != 0:\n                    dx_ext1 -= 2\n                    xsv_ext1 += 2\n                elif (c2 & 0x02) != 0:\n                    dy_ext1 -= 2\n                    ysv_ext1 += 2\n                else:\n                    dz_ext1 -= 2\n                    zsv_ext1 += 2\n\n            # Contribution (1,0,0)\n            dx1 = dx0 - 1 - SQUISH_CONSTANT_3D\n            dy1 = dy0 - 0 - SQUISH_CONSTANT_3D\n            dz1 = dz0 - 0 - SQUISH_CONSTANT_3D\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 1, ysb + 0, zsb + 0, dx1, dy1, dz1)\n\n            # Contribution (0,1,0)\n            dx2 = dx0 - 0 - SQUISH_CONSTANT_3D\n            dy2 = dy0 - 1 - SQUISH_CONSTANT_3D\n            dz2 = dz1\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 0, ysb + 1, zsb + 0, dx2, dy2, dz2)\n\n            # Contribution (0,0,1)\n            dx3 = dx2\n            dy3 = dy1\n            dz3 = dz0 - 1 - SQUISH_CONSTANT_3D\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 0, ysb + 0, zsb + 1, dx3, dy3, dz3)\n\n            # Contribution (1,1,0)\n            dx4 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D\n            dy4 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D\n            dz4 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D\n            attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4\n            if attn4 > 0:\n                attn4 *= attn4\n                value += attn4 * attn4 * extrapolate(xsb + 1, ysb + 1, zsb + 0, dx4, dy4, dz4)\n\n            # Contribution (1,0,1)\n            dx5 = dx4\n            dy5 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D\n            dz5 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D\n            attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5\n            if attn5 > 0:\n                attn5 *= attn5\n                value += attn5 * attn5 * extrapolate(xsb + 1, ysb + 0, zsb + 1, dx5, dy5, dz5)\n\n            # Contribution (0,1,1)\n            dx6 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D\n            dy6 = dy4\n            dz6 = dz5\n            attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6\n            if attn6 > 0:\n                attn6 *= attn6\n                value += attn6 * attn6 * extrapolate(xsb + 0, ysb + 1, zsb + 1, dx6, dy6, dz6)\n\n        # First extra vertex\n        attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0\n        if attn_ext0 > 0:\n            attn_ext0 *= attn_ext0\n            value += attn_ext0 * attn_ext0 * extrapolate(xsv_ext0, ysv_ext0, zsv_ext0, dx_ext0, dy_ext0, dz_ext0)\n\n        # Second extra vertex\n        attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1\n        if attn_ext1 > 0:\n            attn_ext1 *= attn_ext1\n            value += attn_ext1 * attn_ext1 * extrapolate(xsv_ext1, ysv_ext1, zsv_ext1, dx_ext1, dy_ext1, dz_ext1)\n\n        return value / NORM_CONSTANT_3D",
    "doc": "Generate 3D OpenSimplex noise from X,Y,Z coordinates."
  },
  {
    "code": "def noise4d(self, x, y, z, w):\n        \"\"\"\n        Generate 4D OpenSimplex noise from X,Y,Z,W coordinates.\n        \"\"\"\n        # Place input coordinates on simplectic honeycomb.\n        stretch_offset = (x + y + z + w) * STRETCH_CONSTANT_4D\n        xs = x + stretch_offset\n        ys = y + stretch_offset\n        zs = z + stretch_offset\n        ws = w + stretch_offset\n\n        # Floor to get simplectic honeycomb coordinates of rhombo-hypercube super-cell origin.\n        xsb = floor(xs)\n        ysb = floor(ys)\n        zsb = floor(zs)\n        wsb = floor(ws)\n\n        # Skew out to get actual coordinates of stretched rhombo-hypercube origin. We'll need these later.\n        squish_offset = (xsb + ysb + zsb + wsb) * SQUISH_CONSTANT_4D\n        xb = xsb + squish_offset\n        yb = ysb + squish_offset\n        zb = zsb + squish_offset\n        wb = wsb + squish_offset\n\n        # Compute simplectic honeycomb coordinates relative to rhombo-hypercube origin.\n        xins = xs - xsb\n        yins = ys - ysb\n        zins = zs - zsb\n        wins = ws - wsb\n\n        # Sum those together to get a value that determines which region we're in.\n        in_sum = xins + yins + zins + wins\n\n        # Positions relative to origin po.\n        dx0 = x - xb\n        dy0 = y - yb\n        dz0 = z - zb\n        dw0 = w - wb\n\n        value = 0\n        extrapolate = self._extrapolate4d\n        if in_sum <= 1: # We're inside the pentachoron (4-Simplex) at (0,0,0,0)\n\n            # Determine which two of (0,0,0,1), (0,0,1,0), (0,1,0,0), (1,0,0,0) are closest.\n            a_po = 0x01\n            a_score = xins\n            b_po = 0x02\n            b_score = yins\n            if a_score >= b_score and zins > b_score:\n                b_score = zins\n                b_po = 0x04\n            elif a_score < b_score and zins > a_score:\n                a_score = zins\n                a_po = 0x04\n\n            if a_score >= b_score and wins > b_score:\n                b_score = wins\n                b_po = 0x08\n            elif a_score < b_score and wins > a_score:\n                a_score = wins\n                a_po = 0x08\n\n            # Now we determine the three lattice pos not part of the pentachoron that may contribute.\n            # This depends on the closest two pentachoron vertices, including (0,0,0,0)\n            uins = 1 - in_sum\n            if uins > a_score or uins > b_score: # (0,0,0,0) is one of the closest two pentachoron vertices.\n                c = b_po if (b_score > a_score) else a_po # Our other closest vertex is the closest out of a and b.\n                if (c & 0x01) == 0:\n                    xsv_ext0 = xsb - 1\n                    xsv_ext1 = xsv_ext2 = xsb\n                    dx_ext0 = dx0 + 1\n                    dx_ext1 = dx_ext2 = dx0\n                else:\n                    xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1\n                    dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 1\n\n                if (c & 0x02) == 0:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb\n                    dy_ext0 = dy_ext1 = dy_ext2 = dy0\n                    if (c & 0x01) == 0x01:\n                        ysv_ext0 -= 1\n                        dy_ext0 += 1\n                    else:\n                        ysv_ext1 -= 1\n                        dy_ext1 += 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1\n\n                if (c & 0x04) == 0:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb\n                    dz_ext0 = dz_ext1 = dz_ext2 = dz0\n                    if (c & 0x03) != 0:\n                        if (c & 0x03) == 0x03:\n                            zsv_ext0 -= 1\n                            dz_ext0 += 1\n                        else:\n                            zsv_ext1 -= 1\n                            dz_ext1 += 1\n\n                    else:\n                        zsv_ext2 -= 1\n                        dz_ext2 += 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1\n                    dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1\n\n\n                if (c & 0x08) == 0:\n                    wsv_ext0 = wsv_ext1 = wsb\n                    wsv_ext2 = wsb - 1\n                    dw_ext0 = dw_ext1 = dw0\n                    dw_ext2 = dw0 + 1\n                else:\n                    wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1\n                    dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 1\n\n            else: # (0,0,0,0) is not one of the closest two pentachoron vertices.\n                c = (a_po | b_po) # Our three extra vertices are determined by the closest two.\n\n                if (c & 0x01) == 0:\n                    xsv_ext0 = xsv_ext2 = xsb\n                    xsv_ext1 = xsb - 1\n                    dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_4D\n                    dx_ext2 = dx0 - SQUISH_CONSTANT_4D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1\n                    dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx_ext2 = dx0 - 1 - SQUISH_CONSTANT_4D\n\n                if (c & 0x02) == 0:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb\n                    dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext1 = dy_ext2 = dy0 - SQUISH_CONSTANT_4D\n                    if (c & 0x01) == 0x01:\n                        ysv_ext1 -= 1\n                        dy_ext1 += 1\n                    else:\n                        ysv_ext2 -= 1\n                        dy_ext2 += 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1\n                    dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext1 = dy_ext2 = dy0 - 1 - SQUISH_CONSTANT_4D\n\n                if (c & 0x04) == 0:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb\n                    dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext1 = dz_ext2 = dz0 - SQUISH_CONSTANT_4D\n                    if (c & 0x03) == 0x03:\n                        zsv_ext1 -= 1\n                        dz_ext1 += 1\n                    else:\n                        zsv_ext2 -= 1\n                        dz_ext2 += 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1\n                    dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext1 = dz_ext2 = dz0 - 1 - SQUISH_CONSTANT_4D\n\n\n                if (c & 0x08) == 0:\n                    wsv_ext0 = wsv_ext1 = wsb\n                    wsv_ext2 = wsb - 1\n                    dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw0 - SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 + 1 - SQUISH_CONSTANT_4D\n                else:\n                    wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1\n                    dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw_ext2 = dw0 - 1 - SQUISH_CONSTANT_4D\n\n            # Contribution (0,0,0,0)\n            attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0\n            if attn0 > 0:\n                attn0 *= attn0\n                value += attn0 * attn0 * extrapolate(xsb + 0, ysb + 0, zsb + 0, wsb + 0, dx0, dy0, dz0, dw0)\n\n            # Contribution (1,0,0,0)\n            dx1 = dx0 - 1 - SQUISH_CONSTANT_4D\n            dy1 = dy0 - 0 - SQUISH_CONSTANT_4D\n            dz1 = dz0 - 0 - SQUISH_CONSTANT_4D\n            dw1 = dw0 - 0 - SQUISH_CONSTANT_4D\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 1, ysb + 0, zsb + 0, wsb + 0, dx1, dy1, dz1, dw1)\n\n            # Contribution (0,1,0,0)\n            dx2 = dx0 - 0 - SQUISH_CONSTANT_4D\n            dy2 = dy0 - 1 - SQUISH_CONSTANT_4D\n            dz2 = dz1\n            dw2 = dw1\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 0, ysb + 1, zsb + 0, wsb + 0, dx2, dy2, dz2, dw2)\n\n            # Contribution (0,0,1,0)\n            dx3 = dx2\n            dy3 = dy1\n            dz3 = dz0 - 1 - SQUISH_CONSTANT_4D\n            dw3 = dw1\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 0, ysb + 0, zsb + 1, wsb + 0, dx3, dy3, dz3, dw3)\n\n            # Contribution (0,0,0,1)\n            dx4 = dx2\n            dy4 = dy1\n            dz4 = dz1\n            dw4 = dw0 - 1 - SQUISH_CONSTANT_4D\n            attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4\n            if attn4 > 0:\n                attn4 *= attn4\n                value += attn4 * attn4 * extrapolate(xsb + 0, ysb + 0, zsb + 0, wsb + 1, dx4, dy4, dz4, dw4)\n\n        elif in_sum >= 3: # We're inside the pentachoron (4-Simplex) at (1,1,1,1)\n            # Determine which two of (1,1,1,0), (1,1,0,1), (1,0,1,1), (0,1,1,1) are closest.\n            a_po = 0x0E\n            a_score = xins\n            b_po = 0x0D\n            b_score = yins\n            if a_score <= b_score and zins < b_score:\n                b_score = zins\n                b_po = 0x0B\n            elif a_score > b_score and zins < a_score:\n                a_score = zins\n                a_po = 0x0B\n\n            if a_score <= b_score and wins < b_score:\n                b_score = wins\n                b_po = 0x07\n            elif a_score > b_score and wins < a_score:\n                a_score = wins\n                a_po = 0x07\n\n            # Now we determine the three lattice pos not part of the pentachoron that may contribute.\n            # This depends on the closest two pentachoron vertices, including (0,0,0,0)\n            uins = 4 - in_sum\n            if uins < a_score or uins < b_score: # (1,1,1,1) is one of the closest two pentachoron vertices.\n                c = b_po if (b_score < a_score) else a_po # Our other closest vertex is the closest out of a and b.\n\n                if (c & 0x01) != 0:\n                    xsv_ext0 = xsb + 2\n                    xsv_ext1 = xsv_ext2 = xsb + 1\n                    dx_ext0 = dx0 - 2 - 4 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb\n                    dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 4 * SQUISH_CONSTANT_4D\n\n                if (c & 0x02) != 0:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    if (c & 0x01) != 0:\n                        ysv_ext1 += 1\n                        dy_ext1 -= 1\n                    else:\n                        ysv_ext0 += 1\n                        dy_ext0 -= 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb\n                    dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 4 * SQUISH_CONSTANT_4D\n\n                if (c & 0x04) != 0:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1\n                    dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    if (c & 0x03) != 0x03:\n                        if (c & 0x03) == 0:\n                            zsv_ext0 += 1\n                            dz_ext0 -= 1\n                        else:\n                            zsv_ext1 += 1\n                            dz_ext1 -= 1\n\n                    else:\n                        zsv_ext2 += 1\n                        dz_ext2 -= 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb\n                    dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 4 * SQUISH_CONSTANT_4D\n\n                if (c & 0x08) != 0:\n                    wsv_ext0 = wsv_ext1 = wsb + 1\n                    wsv_ext2 = wsb + 2\n                    dw_ext0 = dw_ext1 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 - 2 - 4 * SQUISH_CONSTANT_4D\n                else:\n                    wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb\n                    dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 4 * SQUISH_CONSTANT_4D\n\n            else: # (1,1,1,1) is not one of the closest two pentachoron vertices.\n                c = (a_po & b_po) # Our three extra vertices are determined by the closest two.\n\n                if (c & 0x01) != 0:\n                    xsv_ext0 = xsv_ext2 = xsb + 1\n                    xsv_ext1 = xsb + 2\n                    dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D\n                    dx_ext2 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb\n                    dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx_ext2 = dx0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c & 0x02) != 0:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1\n                    dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext1 = dy_ext2 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    if (c & 0x01) != 0:\n                        ysv_ext2 += 1\n                        dy_ext2 -= 1\n                    else:\n                        ysv_ext1 += 1\n                        dy_ext1 -= 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb\n                    dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext1 = dy_ext2 = dy0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c & 0x04) != 0:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1\n                    dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext1 = dz_ext2 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    if (c & 0x03) != 0:\n                        zsv_ext2 += 1\n                        dz_ext2 -= 1\n                    else:\n                        zsv_ext1 += 1\n                        dz_ext1 -= 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb\n                    dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext1 = dz_ext2 = dz0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c & 0x08) != 0:\n                    wsv_ext0 = wsv_ext1 = wsb + 1\n                    wsv_ext2 = wsb + 2\n                    dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D\n                else:\n                    wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb\n                    dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw_ext2 = dw0 - 3 * SQUISH_CONSTANT_4D\n\n            # Contribution (1,1,1,0)\n            dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dw4 = dw0 - 3 * SQUISH_CONSTANT_4D\n            attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4\n            if attn4 > 0:\n                attn4 *= attn4\n                value += attn4 * attn4 * extrapolate(xsb + 1, ysb + 1, zsb + 1, wsb + 0, dx4, dy4, dz4, dw4)\n\n            # Contribution (1,1,0,1)\n            dx3 = dx4\n            dy3 = dy4\n            dz3 = dz0 - 3 * SQUISH_CONSTANT_4D\n            dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 1, ysb + 1, zsb + 0, wsb + 1, dx3, dy3, dz3, dw3)\n\n            # Contribution (1,0,1,1)\n            dx2 = dx4\n            dy2 = dy0 - 3 * SQUISH_CONSTANT_4D\n            dz2 = dz4\n            dw2 = dw3\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 1, ysb + 0, zsb + 1, wsb + 1, dx2, dy2, dz2, dw2)\n\n            # Contribution (0,1,1,1)\n            dx1 = dx0 - 3 * SQUISH_CONSTANT_4D\n            dz1 = dz4\n            dy1 = dy4\n            dw1 = dw3\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 0, ysb + 1, zsb + 1, wsb + 1, dx1, dy1, dz1, dw1)\n\n            # Contribution (1,1,1,1)\n            dx0 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D\n            dy0 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D\n            dz0 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D\n            dw0 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D\n            attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0\n            if attn0 > 0:\n                attn0 *= attn0\n                value += attn0 * attn0 * extrapolate(xsb + 1, ysb + 1, zsb + 1, wsb + 1, dx0, dy0, dz0, dw0)\n\n        elif in_sum <= 2: # We're inside the first dispentachoron (Rectified 4-Simplex)\n            a_is_bigger_side = True\n            b_is_bigger_side = True\n\n            # Decide between (1,1,0,0) and (0,0,1,1)\n            if xins + yins > zins + wins:\n                a_score = xins + yins\n                a_po = 0x03\n            else:\n                a_score = zins + wins\n                a_po = 0x0C\n\n            # Decide between (1,0,1,0) and (0,1,0,1)\n            if xins + zins > yins + wins:\n                b_score = xins + zins\n                b_po = 0x05\n            else:\n                b_score = yins + wins\n                b_po = 0x0A\n\n            # Closer between (1,0,0,1) and (0,1,1,0) will replace the further of a and b, if closer.\n            if xins + wins > yins + zins:\n                score = xins + wins\n                if a_score >= b_score and score > b_score:\n                    b_score = score\n                    b_po = 0x09\n                elif a_score < b_score and score > a_score:\n                    a_score = score\n                    a_po = 0x09\n\n            else:\n                score = yins + zins\n                if a_score >= b_score and score > b_score:\n                    b_score = score\n                    b_po = 0x06\n                elif a_score < b_score and score > a_score:\n                    a_score = score\n                    a_po = 0x06\n\n            # Decide if (1,0,0,0) is closer.\n            p1 = 2 - in_sum + xins\n            if a_score >= b_score and p1 > b_score:\n                b_score = p1\n                b_po = 0x01\n                b_is_bigger_side = False\n            elif a_score < b_score and p1 > a_score:\n                a_score = p1\n                a_po = 0x01\n                a_is_bigger_side = False\n\n            # Decide if (0,1,0,0) is closer.\n            p2 = 2 - in_sum + yins\n            if a_score >= b_score and p2 > b_score:\n                b_score = p2\n                b_po = 0x02\n                b_is_bigger_side = False\n            elif a_score < b_score and p2 > a_score:\n                a_score = p2\n                a_po = 0x02\n                a_is_bigger_side = False\n\n            # Decide if (0,0,1,0) is closer.\n            p3 = 2 - in_sum + zins\n            if a_score >= b_score and p3 > b_score:\n                b_score = p3\n                b_po = 0x04\n                b_is_bigger_side = False\n            elif a_score < b_score and p3 > a_score:\n                a_score = p3\n                a_po = 0x04\n                a_is_bigger_side = False\n\n            # Decide if (0,0,0,1) is closer.\n            p4 = 2 - in_sum + wins\n            if a_score >= b_score and p4 > b_score:\n                b_po = 0x08\n                b_is_bigger_side = False\n            elif a_score < b_score and p4 > a_score:\n                a_po = 0x08\n                a_is_bigger_side = False\n\n            # Where each of the two closest pos are determines how the extra three vertices are calculated.\n            if a_is_bigger_side == b_is_bigger_side:\n                if a_is_bigger_side: # Both closest pos on the bigger side\n                    c1 = (a_po | b_po)\n                    c2 = (a_po & b_po)\n                    if (c1 & 0x01) == 0:\n                        xsv_ext0 = xsb\n                        xsv_ext1 = xsb - 1\n                        dx_ext0 = dx0 - 3 * SQUISH_CONSTANT_4D\n                        dx_ext1 = dx0 + 1 - 2 * SQUISH_CONSTANT_4D\n                    else:\n                        xsv_ext0 = xsv_ext1 = xsb + 1\n                        dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        dx_ext1 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n\n                    if (c1 & 0x02) == 0:\n                        ysv_ext0 = ysb\n                        ysv_ext1 = ysb - 1\n                        dy_ext0 = dy0 - 3 * SQUISH_CONSTANT_4D\n                        dy_ext1 = dy0 + 1 - 2 * SQUISH_CONSTANT_4D\n                    else:\n                        ysv_ext0 = ysv_ext1 = ysb + 1\n                        dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        dy_ext1 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n\n                    if (c1 & 0x04) == 0:\n                        zsv_ext0 = zsb\n                        zsv_ext1 = zsb - 1\n                        dz_ext0 = dz0 - 3 * SQUISH_CONSTANT_4D\n                        dz_ext1 = dz0 + 1 - 2 * SQUISH_CONSTANT_4D\n                    else:\n                        zsv_ext0 = zsv_ext1 = zsb + 1\n                        dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        dz_ext1 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n\n                    if (c1 & 0x08) == 0:\n                        wsv_ext0 = wsb\n                        wsv_ext1 = wsb - 1\n                        dw_ext0 = dw0 - 3 * SQUISH_CONSTANT_4D\n                        dw_ext1 = dw0 + 1 - 2 * SQUISH_CONSTANT_4D\n                    else:\n                        wsv_ext0 = wsv_ext1 = wsb + 1\n                        dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        dw_ext1 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n\n                    # One combination is a _permutation of (0,0,0,2) based on c2\n                    xsv_ext2 = xsb\n                    ysv_ext2 = ysb\n                    zsv_ext2 = zsb\n                    wsv_ext2 = wsb\n                    dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D\n                    if (c2 & 0x01) != 0:\n                        xsv_ext2 += 2\n                        dx_ext2 -= 2\n                    elif (c2 & 0x02) != 0:\n                        ysv_ext2 += 2\n                        dy_ext2 -= 2\n                    elif (c2 & 0x04) != 0:\n                        zsv_ext2 += 2\n                        dz_ext2 -= 2\n                    else:\n                        wsv_ext2 += 2\n                        dw_ext2 -= 2\n\n                else: # Both closest pos on the smaller side\n                    # One of the two extra pos is (0,0,0,0)\n                    xsv_ext2 = xsb\n                    ysv_ext2 = ysb\n                    zsv_ext2 = zsb\n                    wsv_ext2 = wsb\n                    dx_ext2 = dx0\n                    dy_ext2 = dy0\n                    dz_ext2 = dz0\n                    dw_ext2 = dw0\n\n                    # Other two pos are based on the omitted axes.\n                    c = (a_po | b_po)\n\n                    if (c & 0x01) == 0:\n                        xsv_ext0 = xsb - 1\n                        xsv_ext1 = xsb\n                        dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D\n                        dx_ext1 = dx0 - SQUISH_CONSTANT_4D\n                    else:\n                        xsv_ext0 = xsv_ext1 = xsb + 1\n                        dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D\n\n                    if (c & 0x02) == 0:\n                        ysv_ext0 = ysv_ext1 = ysb\n                        dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D\n                        if (c & 0x01) == 0x01:\n                            ysv_ext0 -= 1\n                            dy_ext0 += 1\n                        else:\n                            ysv_ext1 -= 1\n                            dy_ext1 += 1\n\n                    else:\n                        ysv_ext0 = ysv_ext1 = ysb + 1\n                        dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D\n\n                    if (c & 0x04) == 0:\n                        zsv_ext0 = zsv_ext1 = zsb\n                        dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D\n                        if (c & 0x03) == 0x03:\n                            zsv_ext0 -= 1\n                            dz_ext0 += 1\n                        else:\n                            zsv_ext1 -= 1\n                            dz_ext1 += 1\n\n                    else:\n                        zsv_ext0 = zsv_ext1 = zsb + 1\n                        dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D\n\n\n                    if (c & 0x08) == 0:\n                        wsv_ext0 = wsb\n                        wsv_ext1 = wsb - 1\n                        dw_ext0 = dw0 - SQUISH_CONSTANT_4D\n                        dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D\n                    else:\n                        wsv_ext0 = wsv_ext1 = wsb + 1\n                        dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D\n\n            else: # One po on each \"side\"\n                if a_is_bigger_side:\n                    c1 = a_po\n                    c2 = b_po\n                else:\n                    c1 = b_po\n                    c2 = a_po\n\n                # Two contributions are the bigger-sided po with each 0 replaced with -1.\n                if (c1 & 0x01) == 0:\n                    xsv_ext0 = xsb - 1\n                    xsv_ext1 = xsb\n                    dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D\n                    dx_ext1 = dx0 - SQUISH_CONSTANT_4D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb + 1\n                    dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D\n\n                if (c1 & 0x02) == 0:\n                    ysv_ext0 = ysv_ext1 = ysb\n                    dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D\n                    if (c1 & 0x01) == 0x01:\n                        ysv_ext0 -= 1\n                        dy_ext0 += 1\n                    else:\n                        ysv_ext1 -= 1\n                        dy_ext1 += 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D\n\n                if (c1 & 0x04) == 0:\n                    zsv_ext0 = zsv_ext1 = zsb\n                    dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D\n                    if (c1 & 0x03) == 0x03:\n                        zsv_ext0 -= 1\n                        dz_ext0 += 1\n                    else:\n                        zsv_ext1 -= 1\n                        dz_ext1 += 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb + 1\n                    dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D\n\n                if (c1 & 0x08) == 0:\n                    wsv_ext0 = wsb\n                    wsv_ext1 = wsb - 1\n                    dw_ext0 = dw0 - SQUISH_CONSTANT_4D\n                    dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D\n                else:\n                    wsv_ext0 = wsv_ext1 = wsb + 1\n                    dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D\n\n                # One contribution is a _permutation of (0,0,0,2) based on the smaller-sided po\n                xsv_ext2 = xsb\n                ysv_ext2 = ysb\n                zsv_ext2 = zsb\n                wsv_ext2 = wsb\n                dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D\n                dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D\n                dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D\n                dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D\n                if (c2 & 0x01) != 0:\n                    xsv_ext2 += 2\n                    dx_ext2 -= 2\n                elif (c2 & 0x02) != 0:\n                    ysv_ext2 += 2\n                    dy_ext2 -= 2\n                elif (c2 & 0x04) != 0:\n                    zsv_ext2 += 2\n                    dz_ext2 -= 2\n                else:\n                    wsv_ext2 += 2\n                    dw_ext2 -= 2\n\n            # Contribution (1,0,0,0)\n            dx1 = dx0 - 1 - SQUISH_CONSTANT_4D\n            dy1 = dy0 - 0 - SQUISH_CONSTANT_4D\n            dz1 = dz0 - 0 - SQUISH_CONSTANT_4D\n            dw1 = dw0 - 0 - SQUISH_CONSTANT_4D\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 1, ysb + 0, zsb + 0, wsb + 0, dx1, dy1, dz1, dw1)\n\n            # Contribution (0,1,0,0)\n            dx2 = dx0 - 0 - SQUISH_CONSTANT_4D\n            dy2 = dy0 - 1 - SQUISH_CONSTANT_4D\n            dz2 = dz1\n            dw2 = dw1\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 0, ysb + 1, zsb + 0, wsb + 0, dx2, dy2, dz2, dw2)\n\n            # Contribution (0,0,1,0)\n            dx3 = dx2\n            dy3 = dy1\n            dz3 = dz0 - 1 - SQUISH_CONSTANT_4D\n            dw3 = dw1\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 0, ysb + 0, zsb + 1, wsb + 0, dx3, dy3, dz3, dw3)\n\n            # Contribution (0,0,0,1)\n            dx4 = dx2\n            dy4 = dy1\n            dz4 = dz1\n            dw4 = dw0 - 1 - SQUISH_CONSTANT_4D\n            attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4\n            if attn4 > 0:\n                attn4 *= attn4\n                value += attn4 * attn4 * extrapolate(xsb + 0, ysb + 0, zsb + 0, wsb + 1, dx4, dy4, dz4, dw4)\n\n            # Contribution (1,1,0,0)\n            dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5\n            if attn5 > 0:\n                attn5 *= attn5\n                value += attn5 * attn5 * extrapolate(xsb + 1, ysb + 1, zsb + 0, wsb + 0, dx5, dy5, dz5, dw5)\n\n            # Contribution (1,0,1,0)\n            dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6\n            if attn6 > 0:\n                attn6 *= attn6\n                value += attn6 * attn6 * extrapolate(xsb + 1, ysb + 0, zsb + 1, wsb + 0, dx6, dy6, dz6, dw6)\n\n            # Contribution (1,0,0,1)\n            dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7\n            if attn7 > 0:\n                attn7 *= attn7\n                value += attn7 * attn7 * extrapolate(xsb + 1, ysb + 0, zsb + 0, wsb + 1, dx7, dy7, dz7, dw7)\n\n            # Contribution (0,1,1,0)\n            dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8\n            if attn8 > 0:\n                attn8 *= attn8\n                value += attn8 * attn8 * extrapolate(xsb + 0, ysb + 1, zsb + 1, wsb + 0, dx8, dy8, dz8, dw8)\n\n            # Contribution (0,1,0,1)\n            dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9\n            if attn9 > 0:\n                attn9 *= attn9\n                value += attn9 * attn9 * extrapolate(xsb + 0, ysb + 1, zsb + 0, wsb + 1, dx9, dy9, dz9, dw9)\n\n            # Contribution (0,0,1,1)\n            dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10\n            if attn10 > 0:\n                attn10 *= attn10\n                value += attn10 * attn10 * extrapolate(xsb + 0, ysb + 0, zsb + 1, wsb + 1, dx10, dy10, dz10, dw10)\n\n        else: # We're inside the second dispentachoron (Rectified 4-Simplex)\n            a_is_bigger_side = True\n            b_is_bigger_side = True\n\n            # Decide between (0,0,1,1) and (1,1,0,0)\n            if xins + yins < zins + wins:\n                a_score = xins + yins\n                a_po = 0x0C\n            else:\n                a_score = zins + wins\n                a_po = 0x03\n\n            # Decide between (0,1,0,1) and (1,0,1,0)\n            if xins + zins < yins + wins:\n                b_score = xins + zins\n                b_po = 0x0A\n            else:\n                b_score = yins + wins\n                b_po = 0x05\n\n            # Closer between (0,1,1,0) and (1,0,0,1) will replace the further of a and b, if closer.\n            if xins + wins < yins + zins:\n                score = xins + wins\n                if a_score <= b_score and score < b_score:\n                    b_score = score\n                    b_po = 0x06\n                elif a_score > b_score and score < a_score:\n                    a_score = score\n                    a_po = 0x06\n\n            else:\n                score = yins + zins\n                if a_score <= b_score and score < b_score:\n                    b_score = score\n                    b_po = 0x09\n                elif a_score > b_score and score < a_score:\n                    a_score = score\n                    a_po = 0x09\n\n            # Decide if (0,1,1,1) is closer.\n            p1 = 3 - in_sum + xins\n            if a_score <= b_score and p1 < b_score:\n                b_score = p1\n                b_po = 0x0E\n                b_is_bigger_side = False\n            elif a_score > b_score and p1 < a_score:\n                a_score = p1\n                a_po = 0x0E\n                a_is_bigger_side = False\n\n            # Decide if (1,0,1,1) is closer.\n            p2 = 3 - in_sum + yins\n            if a_score <= b_score and p2 < b_score:\n                b_score = p2\n                b_po = 0x0D\n                b_is_bigger_side = False\n            elif a_score > b_score and p2 < a_score:\n                a_score = p2\n                a_po = 0x0D\n                a_is_bigger_side = False\n\n            # Decide if (1,1,0,1) is closer.\n            p3 = 3 - in_sum + zins\n            if a_score <= b_score and p3 < b_score:\n                b_score = p3\n                b_po = 0x0B\n                b_is_bigger_side = False\n            elif a_score > b_score and p3 < a_score:\n                a_score = p3\n                a_po = 0x0B\n                a_is_bigger_side = False\n\n            # Decide if (1,1,1,0) is closer.\n            p4 = 3 - in_sum + wins\n            if a_score <= b_score and p4 < b_score:\n                b_po = 0x07\n                b_is_bigger_side = False\n            elif a_score > b_score and p4 < a_score:\n                a_po = 0x07\n                a_is_bigger_side = False\n\n            # Where each of the two closest pos are determines how the extra three vertices are calculated.\n            if a_is_bigger_side == b_is_bigger_side:\n                if a_is_bigger_side: # Both closest pos on the bigger side\n                    c1 = (a_po & b_po)\n                    c2 = (a_po | b_po)\n\n                    # Two contributions are _permutations of (0,0,0,1) and (0,0,0,2) based on c1\n                    xsv_ext0 = xsv_ext1 = xsb\n                    ysv_ext0 = ysv_ext1 = ysb\n                    zsv_ext0 = zsv_ext1 = zsb\n                    wsv_ext0 = wsv_ext1 = wsb\n                    dx_ext0 = dx0 - SQUISH_CONSTANT_4D\n                    dy_ext0 = dy0 - SQUISH_CONSTANT_4D\n                    dz_ext0 = dz0 - SQUISH_CONSTANT_4D\n                    dw_ext0 = dw0 - SQUISH_CONSTANT_4D\n                    dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw0 - 2 * SQUISH_CONSTANT_4D\n                    if (c1 & 0x01) != 0:\n                        xsv_ext0 += 1\n                        dx_ext0 -= 1\n                        xsv_ext1 += 2\n                        dx_ext1 -= 2\n                    elif (c1 & 0x02) != 0:\n                        ysv_ext0 += 1\n                        dy_ext0 -= 1\n                        ysv_ext1 += 2\n                        dy_ext1 -= 2\n                    elif (c1 & 0x04) != 0:\n                        zsv_ext0 += 1\n                        dz_ext0 -= 1\n                        zsv_ext1 += 2\n                        dz_ext1 -= 2\n                    else:\n                        wsv_ext0 += 1\n                        dw_ext0 -= 1\n                        wsv_ext1 += 2\n                        dw_ext1 -= 2\n\n                    # One contribution is a _permutation of (1,1,1,-1) based on c2\n                    xsv_ext2 = xsb + 1\n                    ysv_ext2 = ysb + 1\n                    zsv_ext2 = zsb + 1\n                    wsv_ext2 = wsb + 1\n                    dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n                    if (c2 & 0x01) == 0:\n                        xsv_ext2 -= 2\n                        dx_ext2 += 2\n                    elif (c2 & 0x02) == 0:\n                        ysv_ext2 -= 2\n                        dy_ext2 += 2\n                    elif (c2 & 0x04) == 0:\n                        zsv_ext2 -= 2\n                        dz_ext2 += 2\n                    else:\n                        wsv_ext2 -= 2\n                        dw_ext2 += 2\n\n                else: # Both closest pos on the smaller side\n                    # One of the two extra pos is (1,1,1,1)\n                    xsv_ext2 = xsb + 1\n                    ysv_ext2 = ysb + 1\n                    zsv_ext2 = zsb + 1\n                    wsv_ext2 = wsb + 1\n                    dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D\n                    dw_ext2 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D\n\n                    # Other two pos are based on the shared axes.\n                    c = (a_po & b_po)\n                    if (c & 0x01) != 0:\n                        xsv_ext0 = xsb + 2\n                        xsv_ext1 = xsb + 1\n                        dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D\n                        dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    else:\n                        xsv_ext0 = xsv_ext1 = xsb\n                        dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D\n\n                    if (c & 0x02) != 0:\n                        ysv_ext0 = ysv_ext1 = ysb + 1\n                        dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        if (c & 0x01) == 0:\n                            ysv_ext0 += 1\n                            dy_ext0 -= 1\n                        else:\n                            ysv_ext1 += 1\n                            dy_ext1 -= 1\n\n                    else:\n                        ysv_ext0 = ysv_ext1 = ysb\n                        dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D\n\n                    if (c & 0x04) != 0:\n                        zsv_ext0 = zsv_ext1 = zsb + 1\n                        dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        if (c & 0x03) == 0:\n                            zsv_ext0 += 1\n                            dz_ext0 -= 1\n                        else:\n                            zsv_ext1 += 1\n                            dz_ext1 -= 1\n\n                    else:\n                        zsv_ext0 = zsv_ext1 = zsb\n                        dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D\n\n\n                    if (c & 0x08) != 0:\n                        wsv_ext0 = wsb + 1\n                        wsv_ext1 = wsb + 2\n                        dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n                        dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D\n                    else:\n                        wsv_ext0 = wsv_ext1 = wsb\n                        dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D\n\n            else: # One po on each \"side\"\n                if a_is_bigger_side:\n                    c1 = a_po\n                    c2 = b_po\n                else:\n                    c1 = b_po\n                    c2 = a_po\n\n                # Two contributions are the bigger-sided po with each 1 replaced with 2.\n                if (c1 & 0x01) != 0:\n                    xsv_ext0 = xsb + 2\n                    xsv_ext1 = xsb + 1\n                    dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D\n                    dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n                else:\n                    xsv_ext0 = xsv_ext1 = xsb\n                    dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c1 & 0x02) != 0:\n                    ysv_ext0 = ysv_ext1 = ysb + 1\n                    dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    if (c1 & 0x01) == 0:\n                        ysv_ext0 += 1\n                        dy_ext0 -= 1\n                    else:\n                        ysv_ext1 += 1\n                        dy_ext1 -= 1\n\n                else:\n                    ysv_ext0 = ysv_ext1 = ysb\n                    dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c1 & 0x04) != 0:\n                    zsv_ext0 = zsv_ext1 = zsb + 1\n                    dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    if (c1 & 0x03) == 0:\n                        zsv_ext0 += 1\n                        dz_ext0 -= 1\n                    else:\n                        zsv_ext1 += 1\n                        dz_ext1 -= 1\n\n                else:\n                    zsv_ext0 = zsv_ext1 = zsb\n                    dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D\n\n                if (c1 & 0x08) != 0:\n                    wsv_ext0 = wsb + 1\n                    wsv_ext1 = wsb + 2\n                    dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n                    dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D\n                else:\n                    wsv_ext0 = wsv_ext1 = wsb\n                    dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D\n\n                # One contribution is a _permutation of (1,1,1,-1) based on the smaller-sided po\n                xsv_ext2 = xsb + 1\n                ysv_ext2 = ysb + 1\n                zsv_ext2 = zsb + 1\n                wsv_ext2 = wsb + 1\n                dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n                dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n                dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n                dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n                if (c2 & 0x01) == 0:\n                    xsv_ext2 -= 2\n                    dx_ext2 += 2\n                elif (c2 & 0x02) == 0:\n                    ysv_ext2 -= 2\n                    dy_ext2 += 2\n                elif (c2 & 0x04) == 0:\n                    zsv_ext2 -= 2\n                    dz_ext2 += 2\n                else:\n                    wsv_ext2 -= 2\n                    dw_ext2 += 2\n\n            # Contribution (1,1,1,0)\n            dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D\n            dw4 = dw0 - 3 * SQUISH_CONSTANT_4D\n            attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4\n            if attn4 > 0:\n                attn4 *= attn4\n                value += attn4 * attn4 * extrapolate(xsb + 1, ysb + 1, zsb + 1, wsb + 0, dx4, dy4, dz4, dw4)\n\n            # Contribution (1,1,0,1)\n            dx3 = dx4\n            dy3 = dy4\n            dz3 = dz0 - 3 * SQUISH_CONSTANT_4D\n            dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D\n            attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3\n            if attn3 > 0:\n                attn3 *= attn3\n                value += attn3 * attn3 * extrapolate(xsb + 1, ysb + 1, zsb + 0, wsb + 1, dx3, dy3, dz3, dw3)\n\n            # Contribution (1,0,1,1)\n            dx2 = dx4\n            dy2 = dy0 - 3 * SQUISH_CONSTANT_4D\n            dz2 = dz4\n            dw2 = dw3\n            attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2\n            if attn2 > 0:\n                attn2 *= attn2\n                value += attn2 * attn2 * extrapolate(xsb + 1, ysb + 0, zsb + 1, wsb + 1, dx2, dy2, dz2, dw2)\n\n            # Contribution (0,1,1,1)\n            dx1 = dx0 - 3 * SQUISH_CONSTANT_4D\n            dz1 = dz4\n            dy1 = dy4\n            dw1 = dw3\n            attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1\n            if attn1 > 0:\n                attn1 *= attn1\n                value += attn1 * attn1 * extrapolate(xsb + 0, ysb + 1, zsb + 1, wsb + 1, dx1, dy1, dz1, dw1)\n\n            # Contribution (1,1,0,0)\n            dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5\n            if attn5 > 0:\n                attn5 *= attn5\n                value += attn5 * attn5 * extrapolate(xsb + 1, ysb + 1, zsb + 0, wsb + 0, dx5, dy5, dz5, dw5)\n\n            # Contribution (1,0,1,0)\n            dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6\n            if attn6 > 0:\n                attn6 *= attn6\n                value += attn6 * attn6 * extrapolate(xsb + 1, ysb + 0, zsb + 1, wsb + 0, dx6, dy6, dz6, dw6)\n\n            # Contribution (1,0,0,1)\n            dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7\n            if attn7 > 0:\n                attn7 *= attn7\n                value += attn7 * attn7 * extrapolate(xsb + 1, ysb + 0, zsb + 0, wsb + 1, dx7, dy7, dz7, dw7)\n\n            # Contribution (0,1,1,0)\n            dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D\n            attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8\n            if attn8 > 0:\n                attn8 *= attn8\n                value += attn8 * attn8 * extrapolate(xsb + 0, ysb + 1, zsb + 1, wsb + 0, dx8, dy8, dz8, dw8)\n\n            # Contribution (0,1,0,1)\n            dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9\n            if attn9 > 0:\n                attn9 *= attn9\n                value += attn9 * attn9 * extrapolate(xsb + 0, ysb + 1, zsb + 0, wsb + 1, dx9, dy9, dz9, dw9)\n\n            # Contribution (0,0,1,1)\n            dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D\n            dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D\n            dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D\n            attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10\n            if attn10 > 0:\n                attn10 *= attn10\n                value += attn10 * attn10 * extrapolate(xsb + 0, ysb + 0, zsb + 1, wsb + 1, dx10, dy10, dz10, dw10)\n\n        # First extra vertex\n        attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0 - dw_ext0 * dw_ext0\n        if attn_ext0 > 0:\n            attn_ext0 *= attn_ext0\n            value += attn_ext0 * attn_ext0 * extrapolate(xsv_ext0, ysv_ext0, zsv_ext0, wsv_ext0, dx_ext0, dy_ext0, dz_ext0, dw_ext0)\n\n        # Second extra vertex\n        attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1 - dw_ext1 * dw_ext1\n        if attn_ext1 > 0:\n            attn_ext1 *= attn_ext1\n            value += attn_ext1 * attn_ext1 * extrapolate(xsv_ext1, ysv_ext1, zsv_ext1, wsv_ext1, dx_ext1, dy_ext1, dz_ext1, dw_ext1)\n\n        # Third extra vertex\n        attn_ext2 = 2 - dx_ext2 * dx_ext2 - dy_ext2 * dy_ext2 - dz_ext2 * dz_ext2 - dw_ext2 * dw_ext2\n        if attn_ext2 > 0:\n            attn_ext2 *= attn_ext2\n            value += attn_ext2 * attn_ext2 * extrapolate(xsv_ext2, ysv_ext2, zsv_ext2, wsv_ext2, dx_ext2, dy_ext2, dz_ext2, dw_ext2)\n\n        return value / NORM_CONSTANT_4D",
    "doc": "Generate 4D OpenSimplex noise from X,Y,Z,W coordinates."
  },
  {
    "code": "def adjust_contrast_gamma(arr, gamma):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * ((I_ij/255)**gamma)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gamma : number\n        Exponent for the contrast adjustment. Higher values darken the image.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast.\n\n    \"\"\"\n    # int8 is also possible according to docs\n    # https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#cv2.LUT , but here it seemed\n    # like `d` was 0 for CV_8S, causing that to fail\n    if arr.dtype.name == \"uint8\":\n        min_value, _center_value, max_value = iadt.get_value_range_of_dtype(arr.dtype)\n        dynamic_range = max_value - min_value\n\n        value_range = np.linspace(0, 1.0, num=dynamic_range+1, dtype=np.float32)\n        # 255 * ((I_ij/255)**gamma)\n        # using np.float32(.) here still works when the input is a numpy array of size 1\n        table = (min_value + (value_range ** np.float32(gamma)) * dynamic_range)\n        arr_aug = cv2.LUT(arr, np.clip(table, min_value, max_value).astype(arr.dtype))\n        if arr.ndim == 3 and arr_aug.ndim == 2:\n            return arr_aug[..., np.newaxis]\n        return arr_aug\n    else:\n        return ski_exposure.adjust_gamma(arr, gamma)",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * ((I_ij/255)**gamma)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gamma : number\n        Exponent for the contrast adjustment. Higher values darken the image.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast."
  },
  {
    "code": "def adjust_contrast_sigmoid(arr, gain, cutoff):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gain : number\n        Multiplier for the sigmoid function's output.\n        Higher values lead to quicker changes from dark to light pixels.\n\n    cutoff : number\n        Cutoff that shifts the sigmoid function in horizontal direction.\n        Higher values mean that the switch from dark to light pixels happens later, i.e.\n        the pixels will remain darker.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast.\n\n    \"\"\"\n    # int8 is also possible according to docs\n    # https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#cv2.LUT , but here it seemed\n    # like `d` was 0 for CV_8S, causing that to fail\n    if arr.dtype.name == \"uint8\":\n        min_value, _center_value, max_value = iadt.get_value_range_of_dtype(arr.dtype)\n        dynamic_range = max_value - min_value\n\n        value_range = np.linspace(0, 1.0, num=dynamic_range+1, dtype=np.float32)\n        # 255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))\n        # using np.float32(.) here still works when the input is a numpy array of size 1\n        gain = np.float32(gain)\n        cutoff = np.float32(cutoff)\n        table = min_value + dynamic_range * 1/(1 + np.exp(gain * (cutoff - value_range)))\n        arr_aug = cv2.LUT(arr, np.clip(table, min_value, max_value).astype(arr.dtype))\n        if arr.ndim == 3 and arr_aug.ndim == 2:\n            return arr_aug[..., np.newaxis]\n        return arr_aug\n    else:\n        return ski_exposure.adjust_sigmoid(arr, cutoff=cutoff, gain=gain)",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gain : number\n        Multiplier for the sigmoid function's output.\n        Higher values lead to quicker changes from dark to light pixels.\n\n    cutoff : number\n        Cutoff that shifts the sigmoid function in horizontal direction.\n        Higher values mean that the switch from dark to light pixels happens later, i.e.\n        the pixels will remain darker.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast."
  },
  {
    "code": "def adjust_contrast_log(arr, gain):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * gain * log_2(1 + I_ij/255)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gain : number\n        Multiplier for the logarithm result. Values around 1.0 lead to a contrast-adjusted\n        images. Values above 1.0 quickly lead to partially broken images due to exceeding the\n        datatype's value range.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast.\n\n    \"\"\"\n    # int8 is also possible according to docs\n    # https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#cv2.LUT , but here it seemed\n    # like `d` was 0 for CV_8S, causing that to fail\n    if arr.dtype.name == \"uint8\":\n        min_value, _center_value, max_value = iadt.get_value_range_of_dtype(arr.dtype)\n        dynamic_range = max_value - min_value\n\n        value_range = np.linspace(0, 1.0, num=dynamic_range+1, dtype=np.float32)\n        # 255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))\n        # using np.float32(.) here still works when the input is a numpy array of size 1\n        gain = np.float32(gain)\n        table = min_value + dynamic_range * gain * np.log2(1 + value_range)\n        arr_aug = cv2.LUT(arr, np.clip(table, min_value, max_value).astype(arr.dtype))\n        if arr.ndim == 3 and arr_aug.ndim == 2:\n            return arr_aug[..., np.newaxis]\n        return arr_aug\n    else:\n        return ski_exposure.adjust_log(arr, gain=gain)",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * gain * log_2(1 + I_ij/255)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2) (3)\n        * ``uint16``: yes; tested (2) (3)\n        * ``uint32``: yes; tested (2) (3)\n        * ``uint64``: yes; tested (2) (3) (4)\n        * ``int8``: limited; tested (2) (3) (5)\n        * ``int16``: limited; tested (2) (3) (5)\n        * ``int32``: limited; tested (2) (3) (5)\n        * ``int64``: limited; tested (2) (3) (4) (5)\n        * ``float16``: limited; tested (5)\n        * ``float32``: limited; tested (5)\n        * ``float64``: limited; tested (5)\n        * ``float128``: no (6)\n        * ``bool``: no (7)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by ``skimage``.\n        - (2) Normalization is done as ``I_ij/max``, where ``max`` is the maximum value of the\n              dtype, e.g. 255 for ``uint8``. The normalization is reversed afterwards,\n              e.g. ``result*255`` for ``uint8``.\n        - (3) Integer-like values are not rounded after applying the contrast adjustment equation\n              (before inverting the normalization to 0.0-1.0 space), i.e. projection from continuous\n              space to discrete happens according to floor function.\n        - (4) Note that scikit-image doc says that integers are converted to ``float64`` values before\n              applying the contrast normalization method. This might lead to inaccuracies for large\n              64bit integer values. Tests showed no indication of that happening though.\n        - (5) Must not contain negative values. Values >=0 are fully supported.\n        - (6) Leads to error in scikit-image.\n        - (7) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    gain : number\n        Multiplier for the logarithm result. Values around 1.0 lead to a contrast-adjusted\n        images. Values above 1.0 quickly lead to partially broken images due to exceeding the\n        datatype's value range.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast."
  },
  {
    "code": "def adjust_contrast_linear(arr, alpha):\n    \"\"\"Adjust contrast by scaling each pixel value to ``127 + alpha*(I_ij-127)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2)\n        * ``uint16``: yes; tested (2)\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (3)\n        * ``int8``: yes; tested (2)\n        * ``int16``: yes; tested (2)\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (2)\n        * ``float32``: yes; tested (2)\n        * ``float64``: yes; tested (2)\n        * ``float128``: no (2)\n        * ``bool``: no (4)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by raw ``numpy``.\n        - (2) Only tested for reasonable alphas with up to a value of around 100.\n        - (3) Conversion to ``float64`` is done during augmentation, hence ``uint64``, ``int64``,\n              and ``float128`` support cannot be guaranteed.\n        - (4) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    alpha : number\n        Multiplier to linearly pronounce (>1.0), dampen (0.0 to 1.0) or invert (<0.0) the\n        difference between each pixel value and the center value, e.g. ``127`` for ``uint8``.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast.\n\n    \"\"\"\n    # int8 is also possible according to docs\n    # https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#cv2.LUT , but here it seemed\n    # like `d` was 0 for CV_8S, causing that to fail\n    if arr.dtype.name == \"uint8\":\n        min_value, center_value, max_value = iadt.get_value_range_of_dtype(arr.dtype)\n\n        value_range = np.arange(0, 256, dtype=np.float32)\n        # 127 + alpha*(I_ij-127)\n        # using np.float32(.) here still works when the input is a numpy array of size 1\n        alpha = np.float32(alpha)\n        table = center_value + alpha * (value_range - center_value)\n        arr_aug = cv2.LUT(arr, np.clip(table, min_value, max_value).astype(arr.dtype))\n        if arr.ndim == 3 and arr_aug.ndim == 2:\n            return arr_aug[..., np.newaxis]\n        return arr_aug\n    else:\n        input_dtype = arr.dtype\n        _min_value, center_value, _max_value = iadt.get_value_range_of_dtype(input_dtype)\n        if input_dtype.kind in [\"u\", \"i\"]:\n            center_value = int(center_value)\n        image_aug = center_value + alpha * (arr.astype(np.float64)-center_value)\n        image_aug = iadt.restore_dtypes_(image_aug, input_dtype)\n        return image_aug",
    "doc": "Adjust contrast by scaling each pixel value to ``127 + alpha*(I_ij-127)``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1) (2)\n        * ``uint16``: yes; tested (2)\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (3)\n        * ``int8``: yes; tested (2)\n        * ``int16``: yes; tested (2)\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (2)\n        * ``float32``: yes; tested (2)\n        * ``float64``: yes; tested (2)\n        * ``float128``: no (2)\n        * ``bool``: no (4)\n\n        - (1) Handled by ``cv2``. Other dtypes are handled by raw ``numpy``.\n        - (2) Only tested for reasonable alphas with up to a value of around 100.\n        - (3) Conversion to ``float64`` is done during augmentation, hence ``uint64``, ``int64``,\n              and ``float128`` support cannot be guaranteed.\n        - (4) Does not make sense for contrast adjustments.\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Array for which to adjust the contrast. Dtype ``uint8`` is fastest.\n\n    alpha : number\n        Multiplier to linearly pronounce (>1.0), dampen (0.0 to 1.0) or invert (<0.0) the\n        difference between each pixel value and the center value, e.g. ``127`` for ``uint8``.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array with adjusted contrast."
  },
  {
    "code": "def GammaContrast(gamma=1, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * ((I_ij/255)**gamma)``.\n\n    Values in the range ``gamma=(0.5, 2.0)`` seem to be sensible.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_gamma`.\n\n    Parameters\n    ----------\n    gamma : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Exponent for the contrast adjustment. Higher values darken the image.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform gamma contrast adjustment.\n\n    \"\"\"\n    params1d = [iap.handle_continuous_param(gamma, \"gamma\", value_range=None, tuple_to_uniform=True,\n                                            list_to_choice=True)]\n    func = adjust_contrast_gamma\n    return _ContrastFuncWrapper(\n        func, params1d, per_channel,\n        dtypes_allowed=[\"uint8\", \"uint16\", \"uint32\", \"uint64\",\n                        \"int8\", \"int16\", \"int32\", \"int64\",\n                        \"float16\", \"float32\", \"float64\"],\n        dtypes_disallowed=[\"float96\", \"float128\", \"float256\", \"bool\"],\n        name=name if name is not None else ia.caller_name(),\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * ((I_ij/255)**gamma)``.\n\n    Values in the range ``gamma=(0.5, 2.0)`` seem to be sensible.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_gamma`.\n\n    Parameters\n    ----------\n    gamma : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Exponent for the contrast adjustment. Higher values darken the image.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform gamma contrast adjustment."
  },
  {
    "code": "def SigmoidContrast(gain=10, cutoff=0.5, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))``.\n\n    Values in the range ``gain=(5, 20)`` and ``cutoff=(0.25, 0.75)`` seem to be sensible.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_sigmoid`.\n\n    Parameters\n    ----------\n    gain : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier for the sigmoid function's output.\n        Higher values lead to quicker changes from dark to light pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    cutoff : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Cutoff that shifts the sigmoid function in horizontal direction.\n        Higher values mean that the switch from dark to light pixels happens later, i.e.\n        the pixels will remain darker.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform sigmoid contrast adjustment.\n\n    \"\"\"\n    # TODO add inv parameter?\n    params1d = [\n        iap.handle_continuous_param(gain, \"gain\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True),\n        iap.handle_continuous_param(cutoff, \"cutoff\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True)\n    ]\n    func = adjust_contrast_sigmoid\n    return _ContrastFuncWrapper(\n        func, params1d, per_channel,\n        dtypes_allowed=[\"uint8\", \"uint16\", \"uint32\", \"uint64\",\n                        \"int8\", \"int16\", \"int32\", \"int64\",\n                        \"float16\", \"float32\", \"float64\"],\n        dtypes_disallowed=[\"float96\", \"float128\", \"float256\", \"bool\"],\n        name=name if name is not None else ia.caller_name(),\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * 1/(1 + exp(gain*(cutoff - I_ij/255)))``.\n\n    Values in the range ``gain=(5, 20)`` and ``cutoff=(0.25, 0.75)`` seem to be sensible.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_sigmoid`.\n\n    Parameters\n    ----------\n    gain : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier for the sigmoid function's output.\n        Higher values lead to quicker changes from dark to light pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    cutoff : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Cutoff that shifts the sigmoid function in horizontal direction.\n        Higher values mean that the switch from dark to light pixels happens later, i.e.\n        the pixels will remain darker.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform sigmoid contrast adjustment."
  },
  {
    "code": "def LogContrast(gain=1, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adjust contrast by scaling each pixel value to ``255 * gain * log_2(1 + I_ij/255)``.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_log`.\n\n    Parameters\n    ----------\n    gain : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier for the logarithm result. Values around 1.0 lead to a contrast-adjusted\n        images. Values above 1.0 quickly lead to partially broken images due to exceeding the\n        datatype's value range.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform logarithmic contrast adjustment.\n\n    \"\"\"\n    # TODO add inv parameter?\n    params1d = [iap.handle_continuous_param(gain, \"gain\", value_range=(0, None), tuple_to_uniform=True,\n                                            list_to_choice=True)]\n    func = adjust_contrast_log\n    return _ContrastFuncWrapper(\n        func, params1d, per_channel,\n        dtypes_allowed=[\"uint8\", \"uint16\", \"uint32\", \"uint64\",\n                        \"int8\", \"int16\", \"int32\", \"int64\",\n                        \"float16\", \"float32\", \"float64\"],\n        dtypes_disallowed=[\"float96\", \"float128\", \"float256\", \"bool\"],\n        name=name if name is not None else ia.caller_name(),\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adjust contrast by scaling each pixel value to ``255 * gain * log_2(1 + I_ij/255)``.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_log`.\n\n    Parameters\n    ----------\n    gain : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier for the logarithm result. Values around 1.0 lead to a contrast-adjusted\n        images. Values above 1.0 quickly lead to partially broken images due to exceeding the\n        datatype's value range.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform logarithmic contrast adjustment."
  },
  {
    "code": "def LinearContrast(alpha=1, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"Adjust contrast by scaling each pixel value to ``127 + alpha*(I_ij-127)``.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_linear`.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier to linearly pronounce (>1.0), dampen (0.0 to 1.0) or invert (<0.0) the\n        difference between each pixel value and the center value, e.g. ``127`` for ``uint8``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform contrast adjustment by linearly scaling the distance to 128.\n\n    \"\"\"\n    params1d = [\n        iap.handle_continuous_param(alpha, \"alpha\", value_range=None, tuple_to_uniform=True, list_to_choice=True)\n    ]\n    func = adjust_contrast_linear\n    return _ContrastFuncWrapper(\n        func, params1d, per_channel,\n        dtypes_allowed=[\"uint8\", \"uint16\", \"uint32\",\n                        \"int8\", \"int16\", \"int32\",\n                        \"float16\", \"float32\", \"float64\"],\n        dtypes_disallowed=[\"uint64\", \"int64\", \"float96\", \"float128\", \"float256\", \"bool\"],\n        name=name if name is not None else ia.caller_name(),\n        deterministic=deterministic,\n        random_state=random_state\n    )",
    "doc": "Adjust contrast by scaling each pixel value to ``127 + alpha*(I_ij-127)``.\n\n    dtype support::\n\n        See :func:`imgaug.augmenters.contrast.adjust_contrast_linear`.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Multiplier to linearly pronounce (>1.0), dampen (0.0 to 1.0) or invert (<0.0) the\n        difference between each pixel value and the center value, e.g. ``127`` for ``uint8``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the range ``[a, b]`` will be used per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    per_channel :  bool or float, optional\n        Whether to use the same value for all channels (False) or to sample a new value for each\n        channel (True). If this value is a float ``p``, then for ``p`` percent of all images `per_channel`\n        will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Returns\n    -------\n    _ContrastFuncWrapper\n        Augmenter to perform contrast adjustment by linearly scaling the distance to 128."
  },
  {
    "code": "def InColorspace(to_colorspace, from_colorspace=\"RGB\", children=None, name=None, deterministic=False,\n                 random_state=None):\n    \"\"\"Convert images to another colorspace.\"\"\"\n    return WithColorspace(to_colorspace, from_colorspace, children, name, deterministic, random_state)",
    "doc": "Convert images to another colorspace."
  },
  {
    "code": "def Grayscale(alpha=0, from_colorspace=\"RGB\", name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to convert images to their grayscale versions.\n\n    NOTE: Number of output channels is still 3, i.e. this augmenter just \"removes\" color.\n\n    TODO check dtype support\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The alpha value of the grayscale image when overlayed over the\n        old image. A value close to 1.0 means, that mostly the new grayscale\n        image is visible. A value close to 0.0 means, that mostly the\n        old image is visible.\n\n            * If a number, exactly that value will always be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    from_colorspace : str, optional\n        The source colorspace (of the input images).\n        Allowed strings are: ``RGB``, ``BGR``, ``GRAY``, ``CIE``, ``YCrCb``, ``HSV``, ``HLS``, ``Lab``, ``Luv``.\n        See :func:`imgaug.augmenters.color.ChangeColorspace.__init__`.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Grayscale(alpha=1.0)\n\n    creates an augmenter that turns images to their grayscale versions.\n\n    >>> aug = iaa.Grayscale(alpha=(0.0, 1.0))\n\n    creates an augmenter that turns images to their grayscale versions with\n    an alpha value in the range ``0 <= alpha <= 1``. An alpha value of 0.5 would\n    mean, that the output image is 50 percent of the input image and 50\n    percent of the grayscale image (i.e. 50 percent of color removed).\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ChangeColorspace(to_colorspace=ChangeColorspace.GRAY, alpha=alpha, from_colorspace=from_colorspace,\n                            name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter to convert images to their grayscale versions.\n\n    NOTE: Number of output channels is still 3, i.e. this augmenter just \"removes\" color.\n\n    TODO check dtype support\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The alpha value of the grayscale image when overlayed over the\n        old image. A value close to 1.0 means, that mostly the new grayscale\n        image is visible. A value close to 0.0 means, that mostly the\n        old image is visible.\n\n            * If a number, exactly that value will always be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    from_colorspace : str, optional\n        The source colorspace (of the input images).\n        Allowed strings are: ``RGB``, ``BGR``, ``GRAY``, ``CIE``, ``YCrCb``, ``HSV``, ``HLS``, ``Lab``, ``Luv``.\n        See :func:`imgaug.augmenters.color.ChangeColorspace.__init__`.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Grayscale(alpha=1.0)\n\n    creates an augmenter that turns images to their grayscale versions.\n\n    >>> aug = iaa.Grayscale(alpha=(0.0, 1.0))\n\n    creates an augmenter that turns images to their grayscale versions with\n    an alpha value in the range ``0 <= alpha <= 1``. An alpha value of 0.5 would\n    mean, that the output image is 50 percent of the input image and 50\n    percent of the grayscale image (i.e. 50 percent of color removed)."
  },
  {
    "code": "def height(self):\n        \"\"\"Get the height of a bounding box encapsulating the line.\"\"\"\n        if len(self.coords) <= 1:\n            return 0\n        return np.max(self.yy) - np.min(self.yy)",
    "doc": "Get the height of a bounding box encapsulating the line."
  },
  {
    "code": "def width(self):\n        \"\"\"Get the width of a bounding box encapsulating the line.\"\"\"\n        if len(self.coords) <= 1:\n            return 0\n        return np.max(self.xx) - np.min(self.xx)",
    "doc": "Get the width of a bounding box encapsulating the line."
  },
  {
    "code": "def get_pointwise_inside_image_mask(self, image):\n        \"\"\"\n        Get for each point whether it is inside of the given image plane.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        ndarray\n            Boolean array with one value per point indicating whether it is\n            inside of the provided image plane (``True``) or not (``False``).\n\n        \"\"\"\n        if len(self.coords) == 0:\n            return np.zeros((0,), dtype=bool)\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        x_within = np.logical_and(0 <= self.xx, self.xx < width)\n        y_within = np.logical_and(0 <= self.yy, self.yy < height)\n        return np.logical_and(x_within, y_within)",
    "doc": "Get for each point whether it is inside of the given image plane.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        ndarray\n            Boolean array with one value per point indicating whether it is\n            inside of the provided image plane (``True``) or not (``False``)."
  },
  {
    "code": "def compute_neighbour_distances(self):\n        \"\"\"\n        Get the euclidean distance between each two consecutive points.\n\n        Returns\n        -------\n        ndarray\n            Euclidean distances between point pairs.\n            Same order as in `coords`. For ``N`` points, ``N-1`` distances\n            are returned.\n\n        \"\"\"\n        if len(self.coords) <= 1:\n            return np.zeros((0,), dtype=np.float32)\n        return np.sqrt(\n            np.sum(\n                (self.coords[:-1, :] - self.coords[1:, :]) ** 2,\n                axis=1\n            )\n        )",
    "doc": "Get the euclidean distance between each two consecutive points.\n\n        Returns\n        -------\n        ndarray\n            Euclidean distances between point pairs.\n            Same order as in `coords`. For ``N`` points, ``N-1`` distances\n            are returned."
  },
  {
    "code": "def compute_pointwise_distances(self, other, default=None):\n        \"\"\"\n        Compute the minimal distance between each point on self and other.\n\n        Parameters\n        ----------\n        other : tuple of number \\\n                or imgaug.augmentables.kps.Keypoint \\\n                or imgaug.augmentables.LineString\n            Other object to which to compute the distances.\n\n        default\n            Value to return if `other` contains no points.\n\n        Returns\n        -------\n        list of float\n            Distances to `other` or `default` if not distance could be computed.\n\n        \"\"\"\n        import shapely.geometry\n        from .kps import Keypoint\n\n        if isinstance(other, Keypoint):\n            other = shapely.geometry.Point((other.x, other.y))\n        elif isinstance(other, LineString):\n            if len(other.coords) == 0:\n                return default\n            elif len(other.coords) == 1:\n                other = shapely.geometry.Point(other.coords[0, :])\n            else:\n                other = shapely.geometry.LineString(other.coords)\n        elif isinstance(other, tuple):\n            assert len(other) == 2\n            other = shapely.geometry.Point(other)\n        else:\n            raise ValueError(\n                (\"Expected Keypoint or LineString or tuple (x,y), \"\n                 + \"got type %s.\") % (type(other),))\n\n        return [shapely.geometry.Point(point).distance(other)\n                for point in self.coords]",
    "doc": "Compute the minimal distance between each point on self and other.\n\n        Parameters\n        ----------\n        other : tuple of number \\\n                or imgaug.augmentables.kps.Keypoint \\\n                or imgaug.augmentables.LineString\n            Other object to which to compute the distances.\n\n        default\n            Value to return if `other` contains no points.\n\n        Returns\n        -------\n        list of float\n            Distances to `other` or `default` if not distance could be computed."
  },
  {
    "code": "def compute_distance(self, other, default=None):\n        \"\"\"\n        Compute the minimal distance between the line string and `other`.\n\n        Parameters\n        ----------\n        other : tuple of number \\\n                or imgaug.augmentables.kps.Keypoint \\\n                or imgaug.augmentables.LineString\n            Other object to which to compute the distance.\n\n        default\n            Value to return if this line string or `other` contain no points.\n\n        Returns\n        -------\n        float\n            Distance to `other` or `default` if not distance could be computed.\n\n        \"\"\"\n        # FIXME this computes distance pointwise, does not have to be identical\n        #       with the actual min distance (e.g. edge center to other's point)\n        distances = self.compute_pointwise_distances(other, default=[])\n        if len(distances) == 0:\n            return default\n        return min(distances)",
    "doc": "Compute the minimal distance between the line string and `other`.\n\n        Parameters\n        ----------\n        other : tuple of number \\\n                or imgaug.augmentables.kps.Keypoint \\\n                or imgaug.augmentables.LineString\n            Other object to which to compute the distance.\n\n        default\n            Value to return if this line string or `other` contain no points.\n\n        Returns\n        -------\n        float\n            Distance to `other` or `default` if not distance could be computed."
  },
  {
    "code": "def contains(self, other, max_distance=1e-4):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.augmentables.kps.Keypoint\n            Point to check for.\n\n        max_distance : float\n            Maximum allowed euclidean distance between the point and the\n            closest point on the line. If the threshold is exceeded, the point\n            is not considered to be contained in the line.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the line string, False otherwise.\n            It is contained if its distance to the line or any of its points\n            is below a threshold.\n\n        \"\"\"\n        return self.compute_distance(other, default=np.inf) < max_distance",
    "doc": "Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.augmentables.kps.Keypoint\n            Point to check for.\n\n        max_distance : float\n            Maximum allowed euclidean distance between the point and the\n            closest point on the line. If the threshold is exceeded, the point\n            is not considered to be contained in the line.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the line string, False otherwise.\n            It is contained if its distance to the line or any of its points\n            is below a threshold."
  },
  {
    "code": "def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the line string onto a differently shaped image.\n\n        E.g. if a point of the line string is on its original image at\n        ``x=(10 of 100 pixels)`` and ``y=(20 of 100 pixels)`` and is projected\n        onto a new image with size ``(width=200, height=200)``, its new\n        position will be ``(x=20, y=40)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.augmentables.lines.LineString\n            Line string with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(self.coords, from_shape, to_shape)\n        return self.copy(coords=coords_proj)",
    "doc": "Project the line string onto a differently shaped image.\n\n        E.g. if a point of the line string is on its original image at\n        ``x=(10 of 100 pixels)`` and ``y=(20 of 100 pixels)`` and is projected\n        onto a new image with size ``(width=200, height=200)``, its new\n        position will be ``(x=20, y=40)``.\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.augmentables.lines.LineString\n            Line string with new coordinates."
  },
  {
    "code": "def is_fully_within_image(self, image, default=False):\n        \"\"\"\n        Estimate whether the line string is fully inside the image area.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            True if the line string is fully inside the image area.\n            False otherwise.\n\n        \"\"\"\n        if len(self.coords) == 0:\n            return default\n        return np.all(self.get_pointwise_inside_image_mask(image))",
    "doc": "Estimate whether the line string is fully inside the image area.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            True if the line string is fully inside the image area.\n            False otherwise."
  },
  {
    "code": "def is_partly_within_image(self, image, default=False):\n        \"\"\"\n        Estimate whether the line string is at least partially inside the image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            True if the line string is at least partially inside the image area.\n            False otherwise.\n\n        \"\"\"\n        if len(self.coords) == 0:\n            return default\n        # check mask first to avoid costly computation of intersection points\n        # whenever possible\n        mask = self.get_pointwise_inside_image_mask(image)\n        if np.any(mask):\n            return True\n        return len(self.clip_out_of_image(image)) > 0",
    "doc": "Estimate whether the line string is at least partially inside the image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            True if the line string is at least partially inside the image area.\n            False otherwise."
  },
  {
    "code": "def is_out_of_image(self, image, fully=True, partly=False, default=True):\n        \"\"\"\n        Estimate whether the line is partially/fully outside of the image area.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the\n            image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially\n            outside fo the image area.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            `default` if the line string has no points.\n            True if the line string is partially/fully outside of the image\n            area, depending on defined parameters.\n            False otherwise.\n\n        \"\"\"\n        if len(self.coords) == 0:\n            return default\n\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully",
    "doc": "Estimate whether the line is partially/fully outside of the image area.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the\n            image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially\n            outside fo the image area.\n\n        default\n            Default value to return if the line string contains no points.\n\n        Returns\n        -------\n        bool\n            `default` if the line string has no points.\n            True if the line string is partially/fully outside of the image\n            area, depending on defined parameters.\n            False otherwise."
  },
  {
    "code": "def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the line_string that are outside of the image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        list of imgaug.augmentables.lines.LineString\n            Line strings, clipped to the image shape.\n            The result may contain any number of line strins, including zero.\n\n        \"\"\"\n        if len(self.coords) == 0:\n            return []\n\n        inside_image_mask = self.get_pointwise_inside_image_mask(image)\n        ooi_mask = ~inside_image_mask\n\n        if len(self.coords) == 1:\n            if not np.any(inside_image_mask):\n                return []\n            return [self.copy()]\n\n        if np.all(inside_image_mask):\n            return [self.copy()]\n\n        # top, right, bottom, left image edges\n        # we subtract eps here, because intersection() works inclusively,\n        # i.e. not subtracting eps would be equivalent to 0<=x<=C for C being\n        # height or width\n        # don't set the eps too low, otherwise points at height/width seem\n        # to get rounded to height/width by shapely, which can cause problems\n        # when first clipping and then calling is_fully_within_image()\n        # returning false\n        height, width = normalize_shape(image)[0:2]\n        eps = 1e-3\n        edges = [\n            LineString([(0.0, 0.0), (width - eps, 0.0)]),\n            LineString([(width - eps, 0.0), (width - eps, height - eps)]),\n            LineString([(width - eps, height - eps), (0.0, height - eps)]),\n            LineString([(0.0, height - eps), (0.0, 0.0)])\n        ]\n        intersections = self.find_intersections_with(edges)\n\n        points = []\n        gen = enumerate(zip(self.coords[:-1], self.coords[1:],\n                            ooi_mask[:-1], ooi_mask[1:],\n                            intersections))\n        for i, (line_start, line_end, ooi_start, ooi_end, inter_line) in gen:\n            points.append((line_start, False, ooi_start))\n            for p_inter in inter_line:\n                points.append((p_inter, True, False))\n\n            is_last = (i == len(self.coords) - 2)\n            if is_last and not ooi_end:\n                points.append((line_end, False, ooi_end))\n\n        lines = []\n        line = []\n        for i, (coord, was_added, ooi) in enumerate(points):\n            # remove any point that is outside of the image,\n            # also start a new line once such a point is detected\n            if ooi:\n                if len(line) > 0:\n                    lines.append(line)\n                    line = []\n                continue\n\n            if not was_added:\n                # add all points that were part of the original line string\n                # AND that are inside the image plane\n                line.append(coord)\n            else:\n                is_last_point = (i == len(points)-1)\n                # ooi is a numpy.bool_, hence the bool(.)\n                is_next_ooi = (not is_last_point\n                               and bool(points[i+1][2]) is True)\n\n                # Add all points that were new (i.e. intersections), so\n                # long that they aren't essentially identical to other point.\n                # This prevents adding overlapping intersections multiple times.\n                # (E.g. when a line intersects with a corner of the image plane\n                # and also with one of its edges.)\n                p_prev = line[-1] if len(line) > 0 else None\n                # ignore next point if end reached or next point is out of image\n                p_next = None\n                if not is_last_point and not is_next_ooi:\n                    p_next = points[i+1][0]\n                dist_prev = None\n                dist_next = None\n                if p_prev is not None:\n                    dist_prev = np.linalg.norm(\n                        np.float32(coord) - np.float32(p_prev))\n                if p_next is not None:\n                    dist_next = np.linalg.norm(\n                        np.float32(coord) - np.float32(p_next))\n\n                dist_prev_ok = (dist_prev is None or dist_prev > 1e-2)\n                dist_next_ok = (dist_next is None or dist_next > 1e-2)\n                if dist_prev_ok and dist_next_ok:\n                    line.append(coord)\n\n        if len(line) > 0:\n            lines.append(line)\n\n        lines = [line for line in lines if len(line) > 0]\n        return [self.deepcopy(coords=line) for line in lines]",
    "doc": "Clip off all parts of the line_string that are outside of the image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        list of imgaug.augmentables.lines.LineString\n            Line strings, clipped to the image shape.\n            The result may contain any number of line strins, including zero."
  },
  {
    "code": "def find_intersections_with(self, other):\n        \"\"\"\n        Find all intersection points between the line string and `other`.\n\n        Parameters\n        ----------\n        other : tuple of number or list of tuple of number or \\\n                list of LineString or LineString\n            The other geometry to use during intersection tests.\n\n        Returns\n        -------\n        list of list of tuple of number\n            All intersection points. One list per pair of consecutive start\n            and end point, i.e. `N-1` lists of `N` points. Each list may\n            be empty or may contain multiple points.\n\n        \"\"\"\n        import shapely.geometry\n\n        geom = _convert_var_to_shapely_geometry(other)\n\n        result = []\n        for p_start, p_end in zip(self.coords[:-1], self.coords[1:]):\n            ls = shapely.geometry.LineString([p_start, p_end])\n            intersections = ls.intersection(geom)\n            intersections = list(_flatten_shapely_collection(intersections))\n\n            intersections_points = []\n            for inter in intersections:\n                if isinstance(inter, shapely.geometry.linestring.LineString):\n                    inter_start = (inter.coords[0][0], inter.coords[0][1])\n                    inter_end = (inter.coords[-1][0], inter.coords[-1][1])\n                    intersections_points.extend([inter_start, inter_end])\n                else:\n                    assert isinstance(inter, shapely.geometry.point.Point), (\n                        \"Expected to find shapely.geometry.point.Point or \"\n                        \"shapely.geometry.linestring.LineString intersection, \"\n                        \"actually found %s.\" % (type(inter),))\n                    intersections_points.append((inter.x, inter.y))\n\n            # sort by distance to start point, this makes it later on easier\n            # to remove duplicate points\n            inter_sorted = sorted(\n                intersections_points,\n                key=lambda p: np.linalg.norm(np.float32(p) - p_start)\n            )\n\n            result.append(inter_sorted)\n        return result",
    "doc": "Find all intersection points between the line string and `other`.\n\n        Parameters\n        ----------\n        other : tuple of number or list of tuple of number or \\\n                list of LineString or LineString\n            The other geometry to use during intersection tests.\n\n        Returns\n        -------\n        list of list of tuple of number\n            All intersection points. One list per pair of consecutive start\n            and end point, i.e. `N-1` lists of `N` points. Each list may\n            be empty or may contain multiple points."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift/move the line string from one or more image sides.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            left.\n\n        Returns\n        -------\n        result : imgaug.augmentables.lines.LineString\n            Shifted line string.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        coords = np.copy(self.coords)\n        coords[:, 0] += left - right\n        coords[:, 1] += top - bottom\n        return self.copy(coords=coords)",
    "doc": "Shift/move the line string from one or more image sides.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the\n            left.\n\n        Returns\n        -------\n        result : imgaug.augmentables.lines.LineString\n            Shifted line string."
  },
  {
    "code": "def draw_mask(self, image_shape, size_lines=1, size_points=0,\n                  raise_if_out_of_image=False):\n        \"\"\"\n        Draw this line segment as a binary image mask.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line segments.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Boolean line mask of shape `image_shape` (no channel axis).\n\n        \"\"\"\n        heatmap = self.draw_heatmap_array(\n            image_shape,\n            alpha_lines=1.0, alpha_points=1.0,\n            size_lines=size_lines, size_points=size_points,\n            antialiased=False,\n            raise_if_out_of_image=raise_if_out_of_image)\n        return heatmap > 0.5",
    "doc": "Draw this line segment as a binary image mask.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line segments.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Boolean line mask of shape `image_shape` (no channel axis)."
  },
  {
    "code": "def draw_lines_heatmap_array(self, image_shape, alpha=1.0,\n                                 size=1, antialiased=True,\n                                 raise_if_out_of_image=False):\n        \"\"\"\n        Draw the line segments of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        size : int, optional\n            Thickness of the line segments.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string. All values are in the interval ``[0.0, 1.0]``.\n\n        \"\"\"\n        assert len(image_shape) == 2 or (\n            len(image_shape) == 3 and image_shape[-1] == 1), (\n            \"Expected (H,W) or (H,W,1) as image_shape, got %s.\" % (\n                image_shape,))\n\n        arr = self.draw_lines_on_image(\n            np.zeros(image_shape, dtype=np.uint8),\n            color=255, alpha=alpha, size=size,\n            antialiased=antialiased,\n            raise_if_out_of_image=raise_if_out_of_image\n        )\n        return arr.astype(np.float32) / 255.0",
    "doc": "Draw the line segments of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        size : int, optional\n            Thickness of the line segments.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string. All values are in the interval ``[0.0, 1.0]``."
  },
  {
    "code": "def draw_points_heatmap_array(self, image_shape, alpha=1.0,\n                                  size=1, raise_if_out_of_image=False):\n        \"\"\"\n        Draw the points of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the point mask.\n\n        alpha : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string points. All values are in the interval ``[0.0, 1.0]``.\n\n        \"\"\"\n        assert len(image_shape) == 2 or (\n            len(image_shape) == 3 and image_shape[-1] == 1), (\n            \"Expected (H,W) or (H,W,1) as image_shape, got %s.\" % (\n                image_shape,))\n\n        arr = self.draw_points_on_image(\n            np.zeros(image_shape, dtype=np.uint8),\n            color=255, alpha=alpha, size=size,\n            raise_if_out_of_image=raise_if_out_of_image\n        )\n        return arr.astype(np.float32) / 255.0",
    "doc": "Draw the points of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the point mask.\n\n        alpha : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string points. All values are in the interval ``[0.0, 1.0]``."
  },
  {
    "code": "def draw_heatmap_array(self, image_shape, alpha_lines=1.0, alpha_points=1.0,\n                           size_lines=1, size_points=0, antialiased=True,\n                           raise_if_out_of_image=False):\n        \"\"\"\n        Draw the line segments and points of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        alpha_lines : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        alpha_points : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size_lines : int, optional\n            Thickness of the line segments.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line segments and points. All values are in the\n            interval ``[0.0, 1.0]``.\n\n        \"\"\"\n        heatmap_lines = self.draw_lines_heatmap_array(\n            image_shape,\n            alpha=alpha_lines,\n            size=size_lines,\n            antialiased=antialiased,\n            raise_if_out_of_image=raise_if_out_of_image)\n        if size_points <= 0:\n            return heatmap_lines\n\n        heatmap_points = self.draw_points_heatmap_array(\n            image_shape,\n            alpha=alpha_points,\n            size=size_points,\n            raise_if_out_of_image=raise_if_out_of_image)\n\n        heatmap = np.dstack([heatmap_lines, heatmap_points])\n        return np.max(heatmap, axis=2)",
    "doc": "Draw the line segments and points of the line string as a heatmap array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        alpha_lines : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        alpha_points : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size_lines : int, optional\n            Thickness of the line segments.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line segments and points. All values are in the\n            interval ``[0.0, 1.0]``."
  },
  {
    "code": "def draw_lines_on_image(self, image, color=(0, 255, 0),\n                            alpha=1.0, size=3,\n                            antialiased=True,\n                            raise_if_out_of_image=False):\n        \"\"\"\n        Draw the line segments of the line string on a given image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The image onto which to draw.\n            Expected to be ``uint8`` and of shape ``(H, W, C)`` with ``C``\n            usually being ``3`` (other values are not tested).\n            If a tuple, expected to be ``(H, W, C)`` and will lead to a new\n            ``uint8`` array of zeros being created.\n\n        color : int or iterable of int\n            Color to use as RGB, i.e. three values.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        size : int, optional\n            Thickness of the line segments.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            `image` with line drawn on it.\n\n        \"\"\"\n        from .. import dtypes as iadt\n        from ..augmenters import blend as blendlib\n\n        image_was_empty = False\n        if isinstance(image, tuple):\n            image_was_empty = True\n            image = np.zeros(image, dtype=np.uint8)\n        assert image.ndim in [2, 3], (\n            (\"Expected image or shape of form (H,W) or (H,W,C), \"\n             + \"got shape %s.\") % (image.shape,))\n\n        if len(self.coords) <= 1 or alpha < 0 + 1e-4 or size < 1:\n            return np.copy(image)\n\n        if raise_if_out_of_image \\\n                and self.is_out_of_image(image, partly=False, fully=True):\n            raise Exception(\n                \"Cannot draw line string '%s' on image with shape %s, because \"\n                \"it would be out of bounds.\" % (\n                    self.__str__(), image.shape))\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),))\n            color = [color]\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        image = image.astype(np.float32)\n        height, width = image.shape[0:2]\n\n        # We can't trivially exclude lines outside of the image here, because\n        # even if start and end point are outside, there can still be parts of\n        # the line inside the image.\n        # TODO Do this with edge-wise intersection tests\n        lines = []\n        for line_start, line_end in zip(self.coords[:-1], self.coords[1:]):\n            # note that line() expects order (y1, x1, y2, x2), hence ([1], [0])\n            lines.append((line_start[1], line_start[0],\n                          line_end[1], line_end[0]))\n\n        # skimage.draw.line can only handle integers\n        lines = np.round(np.float32(lines)).astype(np.int32)\n\n        # size == 0 is already covered above\n        # Note here that we have to be careful not to draw lines two times\n        # at their intersection points, e.g. for (p0, p1), (p1, 2) we could\n        # end up drawing at p1 twice, leading to higher values if alpha is used.\n        color = np.float32(color)\n        heatmap = np.zeros(image.shape[0:2], dtype=np.float32)\n        for line in lines:\n            if antialiased:\n                rr, cc, val = skimage.draw.line_aa(*line)\n            else:\n                rr, cc = skimage.draw.line(*line)\n                val = 1.0\n\n            # mask check here, because line() can generate coordinates\n            # outside of the image plane\n            rr_mask = np.logical_and(0 <= rr, rr < height)\n            cc_mask = np.logical_and(0 <= cc, cc < width)\n            mask = np.logical_and(rr_mask, cc_mask)\n\n            if np.any(mask):\n                rr = rr[mask]\n                cc = cc[mask]\n                val = val[mask] if not ia.is_single_number(val) else val\n                heatmap[rr, cc] = val * alpha\n\n        if size > 1:\n            kernel = np.ones((size, size), dtype=np.uint8)\n            heatmap = cv2.dilate(heatmap, kernel)\n\n        if image_was_empty:\n            image_blend = image + heatmap * color\n        else:\n            image_color_shape = image.shape[0:2]\n            if image.ndim == 3:\n                image_color_shape = image_color_shape + (1,)\n            image_color = np.tile(color, image_color_shape)\n            image_blend = blendlib.blend_alpha(image_color, image, heatmap)\n\n        image_blend = iadt.restore_dtypes_(image_blend, np.uint8)\n        return image_blend",
    "doc": "Draw the line segments of the line string on a given image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The image onto which to draw.\n            Expected to be ``uint8`` and of shape ``(H, W, C)`` with ``C``\n            usually being ``3`` (other values are not tested).\n            If a tuple, expected to be ``(H, W, C)`` and will lead to a new\n            ``uint8`` array of zeros being created.\n\n        color : int or iterable of int\n            Color to use as RGB, i.e. three values.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote a more visible\n            line string.\n\n        size : int, optional\n            Thickness of the line segments.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            `image` with line drawn on it."
  },
  {
    "code": "def draw_points_on_image(self, image, color=(0, 128, 0),\n                             alpha=1.0, size=3,\n                             copy=True, raise_if_out_of_image=False):\n        \"\"\"\n        Draw the points of the line string on a given image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The image onto which to draw.\n            Expected to be ``uint8`` and of shape ``(H, W, C)`` with ``C``\n            usually being ``3`` (other values are not tested).\n            If a tuple, expected to be ``(H, W, C)`` and will lead to a new\n            ``uint8`` array of zeros being created.\n\n        color : iterable of int\n            Color to use as RGB, i.e. three values.\n\n        alpha : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size : int, optional\n            Size of the points in pixels.\n\n        copy : bool, optional\n            Whether it is allowed to draw directly in the input\n            array (``False``) or it has to be copied (``True``).\n            The routine may still have to copy, even if ``copy=False`` was\n            used. Always use the return value.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string points. All values are in the interval ``[0.0, 1.0]``.\n\n        \"\"\"\n        from .kps import KeypointsOnImage\n        kpsoi = KeypointsOnImage.from_xy_array(self.coords, shape=image.shape)\n        image = kpsoi.draw_on_image(\n            image, color=color, alpha=alpha,\n            size=size, copy=copy,\n            raise_if_out_of_image=raise_if_out_of_image)\n\n        return image",
    "doc": "Draw the points of the line string on a given image.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The image onto which to draw.\n            Expected to be ``uint8`` and of shape ``(H, W, C)`` with ``C``\n            usually being ``3`` (other values are not tested).\n            If a tuple, expected to be ``(H, W, C)`` and will lead to a new\n            ``uint8`` array of zeros being created.\n\n        color : iterable of int\n            Color to use as RGB, i.e. three values.\n\n        alpha : float, optional\n            Opacity of the line string points. Higher values denote a more\n            visible points.\n\n        size : int, optional\n            Size of the points in pixels.\n\n        copy : bool, optional\n            Whether it is allowed to draw directly in the input\n            array (``False``) or it has to be copied (``True``).\n            The routine may still have to copy, even if ``copy=False`` was\n            used. Always use the return value.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Float array of shape `image_shape` (no channel axis) with drawn\n            line string points. All values are in the interval ``[0.0, 1.0]``."
  },
  {
    "code": "def draw_on_image(self, image,\n                      color=(0, 255, 0), color_lines=None, color_points=None,\n                      alpha=1.0, alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      antialiased=True,\n                      raise_if_out_of_image=False):\n        \"\"\"\n        Draw the line string on an image.\n\n        Parameters\n        ----------\n        image : ndarray\n            The `(H,W,C)` `uint8` image onto which to draw the line string.\n\n        color : iterable of int, optional\n            Color to use as RGB, i.e. three values.\n            The color of the line and points are derived from this value,\n            unless they are set.\n\n        color_lines : None or iterable of int\n            Color to use for the line segments as RGB, i.e. three values.\n            If ``None``, this value is derived from `color`.\n\n        color_points : None or iterable of int\n            Color to use for the points as RGB, i.e. three values.\n            If ``None``, this value is derived from ``0.5 * color``.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote more visible\n            points.\n            The alphas of the line and points are derived from this value,\n            unless they are set.\n\n        alpha_lines : None or float, optional\n            Opacity of the line string. Higher values denote more visible\n            line string.\n            If ``None``, this value is derived from `alpha`.\n\n        alpha_points : None or float, optional\n            Opacity of the line string points. Higher values denote more\n            visible points.\n            If ``None``, this value is derived from `alpha`.\n\n        size : int, optional\n            Size of the line string.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the line segments.\n            If ``None``, this value is derived from `size`.\n\n        size_points : None or int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n            This does currently not affect the point drawing.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Image with line string drawn on it.\n\n        \"\"\"\n        assert color is not None\n        assert alpha is not None\n        assert size is not None\n\n        color_lines = color_lines if color_lines is not None \\\n            else np.float32(color)\n        color_points = color_points if color_points is not None \\\n            else np.float32(color) * 0.5\n\n        alpha_lines = alpha_lines if alpha_lines is not None \\\n            else np.float32(alpha)\n        alpha_points = alpha_points if alpha_points is not None \\\n            else np.float32(alpha)\n\n        size_lines = size_lines if size_lines is not None else size\n        size_points = size_points if size_points is not None else size * 3\n\n        image = self.draw_lines_on_image(\n            image, color=np.array(color_lines).astype(np.uint8),\n            alpha=alpha_lines, size=size_lines,\n            antialiased=antialiased,\n            raise_if_out_of_image=raise_if_out_of_image)\n\n        image = self.draw_points_on_image(\n            image, color=np.array(color_points).astype(np.uint8),\n            alpha=alpha_points, size=size_points,\n            copy=False,\n            raise_if_out_of_image=raise_if_out_of_image)\n\n        return image",
    "doc": "Draw the line string on an image.\n\n        Parameters\n        ----------\n        image : ndarray\n            The `(H,W,C)` `uint8` image onto which to draw the line string.\n\n        color : iterable of int, optional\n            Color to use as RGB, i.e. three values.\n            The color of the line and points are derived from this value,\n            unless they are set.\n\n        color_lines : None or iterable of int\n            Color to use for the line segments as RGB, i.e. three values.\n            If ``None``, this value is derived from `color`.\n\n        color_points : None or iterable of int\n            Color to use for the points as RGB, i.e. three values.\n            If ``None``, this value is derived from ``0.5 * color``.\n\n        alpha : float, optional\n            Opacity of the line string. Higher values denote more visible\n            points.\n            The alphas of the line and points are derived from this value,\n            unless they are set.\n\n        alpha_lines : None or float, optional\n            Opacity of the line string. Higher values denote more visible\n            line string.\n            If ``None``, this value is derived from `alpha`.\n\n        alpha_points : None or float, optional\n            Opacity of the line string points. Higher values denote more\n            visible points.\n            If ``None``, this value is derived from `alpha`.\n\n        size : int, optional\n            Size of the line string.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the line segments.\n            If ``None``, this value is derived from `size`.\n\n        size_points : None or int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n            This does currently not affect the point drawing.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Image with line string drawn on it."
  },
  {
    "code": "def extract_from_image(self, image, size=1, pad=True, pad_max=None,\n                           antialiased=True, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels covered by the line string.\n\n        It will only extract pixels overlapped by the line string.\n\n        This function will by default zero-pad the image if the line string is\n        partially/fully outside of the image. This is for consistency with\n        the same implementations for bounding boxes and polygons.\n\n        Parameters\n        ----------\n        image : ndarray\n            The image of shape `(H,W,[C])` from which to extract the pixels\n            within the line string.\n\n        size : int, optional\n            Thickness of the line.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        antialiased : bool, optional\n            Whether to apply anti-aliasing to the line string.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from\n            becoming zero. If this is set to True and height or width of the\n            line string is below 1, the height/width will be increased to 1.\n            This can be useful to prevent problems, e.g. with image saving or\n            plotting. If it is set to False, images will be returned as\n            ``(H', W')`` or ``(H', W', 3)`` with ``H`` or ``W`` potentially\n            being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels overlapping with the line string. Zero-padded if the\n            line string is partially/fully outside of the image and\n            ``pad=True``. If `prevent_zero_size` is activated, it is\n            guarantueed that ``H'>0`` and ``W'>0``, otherwise only\n            ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        from .bbs import BoundingBox\n\n        assert image.ndim in [2, 3], (\n            \"Expected image of shape (H,W,[C]), \"\n            \"got shape %s.\" % (image.shape,))\n\n        if len(self.coords) == 0 or size <= 0:\n            if prevent_zero_size:\n                return np.zeros((1, 1) + image.shape[2:], dtype=image.dtype)\n            return np.zeros((0, 0) + image.shape[2:], dtype=image.dtype)\n\n        xx = self.xx_int\n        yy = self.yy_int\n\n        # this would probably work if drawing was subpixel-accurate\n        # x1 = np.min(self.coords[:, 0]) - (size / 2)\n        # y1 = np.min(self.coords[:, 1]) - (size / 2)\n        # x2 = np.max(self.coords[:, 0]) + (size / 2)\n        # y2 = np.max(self.coords[:, 1]) + (size / 2)\n\n        # this works currently with non-subpixel-accurate drawing\n        sizeh = (size - 1) / 2\n        x1 = np.min(xx) - sizeh\n        y1 = np.min(yy) - sizeh\n        x2 = np.max(xx) + 1 + sizeh\n        y2 = np.max(yy) + 1 + sizeh\n        bb = BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2)\n\n        if len(self.coords) == 1:\n            return bb.extract_from_image(image, pad=pad, pad_max=pad_max,\n                                         prevent_zero_size=prevent_zero_size)\n\n        heatmap = self.draw_lines_heatmap_array(\n            image.shape[0:2], alpha=1.0, size=size, antialiased=antialiased)\n        if image.ndim == 3:\n            heatmap = np.atleast_3d(heatmap)\n        image_masked = image.astype(np.float32) * heatmap\n        extract = bb.extract_from_image(image_masked, pad=pad, pad_max=pad_max,\n                                        prevent_zero_size=prevent_zero_size)\n        return np.clip(np.round(extract), 0, 255).astype(np.uint8)",
    "doc": "Extract the image pixels covered by the line string.\n\n        It will only extract pixels overlapped by the line string.\n\n        This function will by default zero-pad the image if the line string is\n        partially/fully outside of the image. This is for consistency with\n        the same implementations for bounding boxes and polygons.\n\n        Parameters\n        ----------\n        image : ndarray\n            The image of shape `(H,W,[C])` from which to extract the pixels\n            within the line string.\n\n        size : int, optional\n            Thickness of the line.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        antialiased : bool, optional\n            Whether to apply anti-aliasing to the line string.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from\n            becoming zero. If this is set to True and height or width of the\n            line string is below 1, the height/width will be increased to 1.\n            This can be useful to prevent problems, e.g. with image saving or\n            plotting. If it is set to False, images will be returned as\n            ``(H', W')`` or ``(H', W', 3)`` with ``H`` or ``W`` potentially\n            being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels overlapping with the line string. Zero-padded if the\n            line string is partially/fully outside of the image and\n            ``pad=True``. If `prevent_zero_size` is activated, it is\n            guarantueed that ``H'>0`` and ``W'>0``, otherwise only\n            ``H'>=0`` and ``W'>=0``."
  },
  {
    "code": "def concatenate(self, other):\n        \"\"\"\n        Concatenate this line string with another one.\n\n        This will add a line segment between the end point of this line string\n        and the start point of `other`.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.lines.LineString or ndarray \\\n                or iterable of tuple of number\n            The points to add to this line string.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            New line string with concatenated points.\n            The `label` of this line string will be kept.\n\n        \"\"\"\n        if not isinstance(other, LineString):\n            other = LineString(other)\n        return self.deepcopy(\n            coords=np.concatenate([self.coords, other.coords], axis=0))",
    "doc": "Concatenate this line string with another one.\n\n        This will add a line segment between the end point of this line string\n        and the start point of `other`.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.lines.LineString or ndarray \\\n                or iterable of tuple of number\n            The points to add to this line string.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            New line string with concatenated points.\n            The `label` of this line string will be kept."
  },
  {
    "code": "def subdivide(self, points_per_edge):\n        \"\"\"\n        Adds ``N`` interpolated points with uniform spacing to each edge.\n\n        For each edge between points ``A`` and ``B`` this adds points\n        at ``A + (i/(1+N)) * (B - A)``, where ``i`` is the index of the added\n        point and ``N`` is the number of points to add per edge.\n\n        Calling this method two times will split each edge at its center\n        and then again split each newly created edge at their center.\n        It is equivalent to calling `subdivide(3)`.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        LineString\n            Line string with subdivided edges.\n\n        \"\"\"\n        if len(self.coords) <= 1 or points_per_edge < 1:\n            return self.deepcopy()\n        coords = interpolate_points(self.coords, nb_steps=points_per_edge,\n                                    closed=False)\n        return self.deepcopy(coords=coords)",
    "doc": "Adds ``N`` interpolated points with uniform spacing to each edge.\n\n        For each edge between points ``A`` and ``B`` this adds points\n        at ``A + (i/(1+N)) * (B - A)``, where ``i`` is the index of the added\n        point and ``N`` is the number of points to add per edge.\n\n        Calling this method two times will split each edge at its center\n        and then again split each newly created edge at their center.\n        It is equivalent to calling `subdivide(3)`.\n\n        Parameters\n        ----------\n        points_per_edge : int\n            Number of points to interpolate on each edge.\n\n        Returns\n        -------\n        LineString\n            Line string with subdivided edges."
  },
  {
    "code": "def to_keypoints(self):\n        \"\"\"\n        Convert the line string points to keypoints.\n\n        Returns\n        -------\n        list of imgaug.augmentables.kps.Keypoint\n            Points of the line string as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n        return [Keypoint(x=x, y=y) for (x, y) in self.coords]",
    "doc": "Convert the line string points to keypoints.\n\n        Returns\n        -------\n        list of imgaug.augmentables.kps.Keypoint\n            Points of the line string as keypoints."
  },
  {
    "code": "def to_bounding_box(self):\n        \"\"\"\n        Generate a bounding box encapsulating the line string.\n\n        Returns\n        -------\n        None or imgaug.augmentables.bbs.BoundingBox\n            Bounding box encapsulating the line string.\n            ``None`` if the line string contained no points.\n\n        \"\"\"\n        from .bbs import BoundingBox\n        # we don't have to mind the case of len(.) == 1 here, because\n        # zero-sized BBs are considered valid\n        if len(self.coords) == 0:\n            return None\n        return BoundingBox(x1=np.min(self.xx), y1=np.min(self.yy),\n                           x2=np.max(self.xx), y2=np.max(self.yy),\n                           label=self.label)",
    "doc": "Generate a bounding box encapsulating the line string.\n\n        Returns\n        -------\n        None or imgaug.augmentables.bbs.BoundingBox\n            Bounding box encapsulating the line string.\n            ``None`` if the line string contained no points."
  },
  {
    "code": "def to_polygon(self):\n        \"\"\"\n        Generate a polygon from the line string points.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with the same corner points as the line string.\n            Note that the polygon might be invalid, e.g. contain less than 3\n            points or have self-intersections.\n\n        \"\"\"\n        from .polys import Polygon\n        return Polygon(self.coords, label=self.label)",
    "doc": "Generate a polygon from the line string points.\n\n        Returns\n        -------\n        imgaug.augmentables.polys.Polygon\n            Polygon with the same corner points as the line string.\n            Note that the polygon might be invalid, e.g. contain less than 3\n            points or have self-intersections."
  },
  {
    "code": "def to_heatmap(self, image_shape, size_lines=1, size_points=0,\n                   antialiased=True, raise_if_out_of_image=False):\n        \"\"\"\n        Generate a heatmap object from the line string.\n\n        This is similar to\n        :func:`imgaug.augmentables.lines.LineString.draw_lines_heatmap_array`\n        executed with ``alpha=1.0``. The result is wrapped in a\n        ``HeatmapsOnImage`` object instead of just an array.\n        No points are drawn.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        imgaug.augmentables.heatmaps.HeatmapOnImage\n            Heatmap object containing drawn line string.\n\n        \"\"\"\n        from .heatmaps import HeatmapsOnImage\n        return HeatmapsOnImage(\n            self.draw_heatmap_array(\n                image_shape, size_lines=size_lines, size_points=size_points,\n                antialiased=antialiased,\n                raise_if_out_of_image=raise_if_out_of_image),\n            shape=image_shape\n        )",
    "doc": "Generate a heatmap object from the line string.\n\n        This is similar to\n        :func:`imgaug.augmentables.lines.LineString.draw_lines_heatmap_array`\n        executed with ``alpha=1.0``. The result is wrapped in a\n        ``HeatmapsOnImage`` object instead of just an array.\n        No points are drawn.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        antialiased : bool, optional\n            Whether to draw the line with anti-aliasing activated.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        imgaug.augmentables.heatmaps.HeatmapOnImage\n            Heatmap object containing drawn line string."
  },
  {
    "code": "def to_segmentation_map(self, image_shape, size_lines=1, size_points=0,\n                            raise_if_out_of_image=False):\n        \"\"\"\n        Generate a segmentation map object from the line string.\n\n        This is similar to\n        :func:`imgaug.augmentables.lines.LineString.draw_mask`.\n        The result is wrapped in a ``SegmentationMapOnImage`` object\n        instead of just an array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        imgaug.augmentables.segmaps.SegmentationMapOnImage\n            Segmentation map object containing drawn line string.\n\n        \"\"\"\n        from .segmaps import SegmentationMapOnImage\n        return SegmentationMapOnImage(\n            self.draw_mask(\n                image_shape, size_lines=size_lines, size_points=size_points,\n                raise_if_out_of_image=raise_if_out_of_image),\n            shape=image_shape\n        )",
    "doc": "Generate a segmentation map object from the line string.\n\n        This is similar to\n        :func:`imgaug.augmentables.lines.LineString.draw_mask`.\n        The result is wrapped in a ``SegmentationMapOnImage`` object\n        instead of just an array.\n\n        Parameters\n        ----------\n        image_shape : tuple of int\n            The shape of the image onto which to draw the line mask.\n\n        size_lines : int, optional\n            Thickness of the line.\n\n        size_points : int, optional\n            Size of the points in pixels.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        imgaug.augmentables.segmaps.SegmentationMapOnImage\n            Segmentation map object containing drawn line string."
  },
  {
    "code": "def coords_almost_equals(self, other, max_distance=1e-6, points_per_edge=8):\n        \"\"\"\n        Compare this and another LineString's coordinates.\n\n        This is an approximate method based on pointwise distances and can\n        in rare corner cases produce wrong outputs.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.lines.LineString \\\n                or tuple of number \\\n                or ndarray \\\n                or list of ndarray \\\n                or list of tuple of number\n            The other line string or its coordinates.\n\n        max_distance : float\n            Max distance of any point from the other line string before\n            the two line strings are evaluated to be unequal.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two LineString's coordinates are almost identical,\n            i.e. the max distance is below the threshold.\n            If both have no coordinates, ``True`` is returned.\n            If only one has no coordinates, ``False`` is returned.\n            Beyond that, the number of points is not evaluated.\n\n        \"\"\"\n        if isinstance(other, LineString):\n            pass\n        elif isinstance(other, tuple):\n            other = LineString([other])\n        else:\n            other = LineString(other)\n\n        if len(self.coords) == 0 and len(other.coords) == 0:\n            return True\n        elif 0 in [len(self.coords), len(other.coords)]:\n            # only one of the two line strings has no coords\n            return False\n\n        self_subd = self.subdivide(points_per_edge)\n        other_subd = other.subdivide(points_per_edge)\n\n        dist_self2other = self_subd.compute_pointwise_distances(other_subd)\n        dist_other2self = other_subd.compute_pointwise_distances(self_subd)\n        dist = max(np.max(dist_self2other), np.max(dist_other2self))\n        return  dist < max_distance",
    "doc": "Compare this and another LineString's coordinates.\n\n        This is an approximate method based on pointwise distances and can\n        in rare corner cases produce wrong outputs.\n\n        Parameters\n        ----------\n        other : imgaug.augmentables.lines.LineString \\\n                or tuple of number \\\n                or ndarray \\\n                or list of ndarray \\\n                or list of tuple of number\n            The other line string or its coordinates.\n\n        max_distance : float\n            Max distance of any point from the other line string before\n            the two line strings are evaluated to be unequal.\n\n        points_per_edge : int, optional\n            How many points to interpolate on each edge.\n\n        Returns\n        -------\n        bool\n            Whether the two LineString's coordinates are almost identical,\n            i.e. the max distance is below the threshold.\n            If both have no coordinates, ``True`` is returned.\n            If only one has no coordinates, ``False`` is returned.\n            Beyond that, the number of points is not evaluated."
  },
  {
    "code": "def almost_equals(self, other, max_distance=1e-4, points_per_edge=8):\n        \"\"\"\n        Compare this and another LineString.\n\n        Parameters\n        ----------\n        other: imgaug.augmentables.lines.LineString\n            The other line string. Must be a LineString instance, not just\n            its coordinates.\n\n        max_distance : float, optional\n            See :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`.\n\n        points_per_edge : int, optional\n            See :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`.\n\n        Returns\n        -------\n        bool\n            ``True`` if the coordinates are almost equal according to\n            :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`\n            and additionally the labels are identical. Otherwise ``False``.\n\n        \"\"\"\n        if self.label != other.label:\n            return False\n        return self.coords_almost_equals(\n            other, max_distance=max_distance, points_per_edge=points_per_edge)",
    "doc": "Compare this and another LineString.\n\n        Parameters\n        ----------\n        other: imgaug.augmentables.lines.LineString\n            The other line string. Must be a LineString instance, not just\n            its coordinates.\n\n        max_distance : float, optional\n            See :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`.\n\n        points_per_edge : int, optional\n            See :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`.\n\n        Returns\n        -------\n        bool\n            ``True`` if the coordinates are almost equal according to\n            :func:`imgaug.augmentables.lines.LineString.coords_almost_equals`\n            and additionally the labels are identical. Otherwise ``False``."
  },
  {
    "code": "def copy(self, coords=None, label=None):\n        \"\"\"\n        Create a shallow copy of the LineString object.\n\n        Parameters\n        ----------\n        coords : None or iterable of tuple of number or ndarray\n            If not ``None``, then the coords of the copied object will be set\n            to this value.\n\n        label : None or str\n            If not ``None``, then the label of the copied object will be set to\n            this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Shallow copy.\n\n        \"\"\"\n        return LineString(coords=self.coords if coords is None else coords,\n                          label=self.label if label is None else label)",
    "doc": "Create a shallow copy of the LineString object.\n\n        Parameters\n        ----------\n        coords : None or iterable of tuple of number or ndarray\n            If not ``None``, then the coords of the copied object will be set\n            to this value.\n\n        label : None or str\n            If not ``None``, then the label of the copied object will be set to\n            this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, coords=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        coords : None or iterable of tuple of number or ndarray\n            If not ``None``, then the coords of the copied object will be set\n            to this value.\n\n        label : None or str\n            If not ``None``, then the label of the copied object will be set to\n            this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Deep copy.\n\n        \"\"\"\n        return LineString(\n            coords=np.copy(self.coords) if coords is None else coords,\n            label=copylib.deepcopy(self.label) if label is None else label)",
    "doc": "Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        coords : None or iterable of tuple of number or ndarray\n            If not ``None``, then the coords of the copied object will be set\n            to this value.\n\n        label : None or str\n            If not ``None``, then the label of the copied object will be set to\n            this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineString\n            Deep copy."
  },
  {
    "code": "def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The new image onto which to project.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        line_strings : imgaug.augmentables.lines.LineStrings\n            Object containing all projected line strings.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        line_strings = [ls.project(self.shape, shape)\n                        for ls in self.line_strings]\n        return self.deepcopy(line_strings=line_strings, shape=shape)",
    "doc": "Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            The new image onto which to project.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n\n        Returns\n        -------\n        line_strings : imgaug.augmentables.lines.LineStrings\n            Object containing all projected line strings."
  },
  {
    "code": "def from_xy_arrays(cls, xy, shape):\n        \"\"\"\n        Convert an `(N,M,2)` ndarray to a LineStringsOnImage object.\n\n        This is the inverse of\n        :func:`imgaug.augmentables.lines.LineStringsOnImage.to_xy_array`.\n\n        Parameters\n        ----------\n        xy : (N,M,2) ndarray or iterable of (M,2) ndarray\n            Array containing the point coordinates ``N`` line strings\n            with each ``M`` points given as ``(x,y)`` coordinates.\n            ``M`` may differ if an iterable of arrays is used.\n            Each array should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            ``(H,W,[C])`` shape of the image on which the line strings are\n            placed.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Object containing a list of ``LineString`` objects following the\n            provided point coordinates.\n\n        \"\"\"\n        lss = []\n        for xy_ls in xy:\n            lss.append(LineString(xy_ls))\n        return cls(lss, shape)",
    "doc": "Convert an `(N,M,2)` ndarray to a LineStringsOnImage object.\n\n        This is the inverse of\n        :func:`imgaug.augmentables.lines.LineStringsOnImage.to_xy_array`.\n\n        Parameters\n        ----------\n        xy : (N,M,2) ndarray or iterable of (M,2) ndarray\n            Array containing the point coordinates ``N`` line strings\n            with each ``M`` points given as ``(x,y)`` coordinates.\n            ``M`` may differ if an iterable of arrays is used.\n            Each array should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            ``(H,W,[C])`` shape of the image on which the line strings are\n            placed.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Object containing a list of ``LineString`` objects following the\n            provided point coordinates."
  },
  {
    "code": "def to_xy_arrays(self, dtype=np.float32):\n        \"\"\"\n        Convert this object to an iterable of ``(M,2)`` arrays of points.\n\n        This is the inverse of\n        :func:`imgaug.augmentables.lines.LineStringsOnImage.from_xy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        list of ndarray\n            The arrays of point coordinates, each given as ``(M,2)``.\n\n        \"\"\"\n        from .. import dtypes as iadt\n        return [iadt.restore_dtypes_(np.copy(ls.coords), dtype)\n                for ls in self.line_strings]",
    "doc": "Convert this object to an iterable of ``(M,2)`` arrays of points.\n\n        This is the inverse of\n        :func:`imgaug.augmentables.lines.LineStringsOnImage.from_xy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        list of ndarray\n            The arrays of point coordinates, each given as ``(M,2)``."
  },
  {
    "code": "def draw_on_image(self, image,\n                      color=(0, 255, 0), color_lines=None, color_points=None,\n                      alpha=1.0, alpha_lines=None, alpha_points=None,\n                      size=1, size_lines=None, size_points=None,\n                      antialiased=True,\n                      raise_if_out_of_image=False):\n        \"\"\"\n        Draw all line strings onto a given image.\n\n        Parameters\n        ----------\n        image : ndarray\n            The `(H,W,C)` `uint8` image onto which to draw the line strings.\n\n        color : iterable of int, optional\n            Color to use as RGB, i.e. three values.\n            The color of the lines and points are derived from this value,\n            unless they are set.\n\n        color_lines : None or iterable of int\n            Color to use for the line segments as RGB, i.e. three values.\n            If ``None``, this value is derived from `color`.\n\n        color_points : None or iterable of int\n            Color to use for the points as RGB, i.e. three values.\n            If ``None``, this value is derived from ``0.5 * color``.\n\n        alpha : float, optional\n            Opacity of the line strings. Higher values denote more visible\n            points.\n            The alphas of the line and points are derived from this value,\n            unless they are set.\n\n        alpha_lines : None or float, optional\n            Opacity of the line strings. Higher values denote more visible\n            line string.\n            If ``None``, this value is derived from `alpha`.\n\n        alpha_points : None or float, optional\n            Opacity of the line string points. Higher values denote more\n            visible points.\n            If ``None``, this value is derived from `alpha`.\n\n        size : int, optional\n            Size of the line strings.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the line segments.\n            If ``None``, this value is derived from `size`.\n\n        size_points : None or int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        antialiased : bool, optional\n            Whether to draw the lines with anti-aliasing activated.\n            This does currently not affect the point drawing.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if a line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Image with line strings drawn on it.\n\n        \"\"\"\n        # TODO improve efficiency here by copying only once\n        for ls in self.line_strings:\n            image = ls.draw_on_image(\n                image,\n                color=color, color_lines=color_lines, color_points=color_points,\n                alpha=alpha, alpha_lines=alpha_lines, alpha_points=alpha_points,\n                size=size, size_lines=size_lines, size_points=size_points,\n                antialiased=antialiased,\n                raise_if_out_of_image=raise_if_out_of_image\n            )\n\n        return image",
    "doc": "Draw all line strings onto a given image.\n\n        Parameters\n        ----------\n        image : ndarray\n            The `(H,W,C)` `uint8` image onto which to draw the line strings.\n\n        color : iterable of int, optional\n            Color to use as RGB, i.e. three values.\n            The color of the lines and points are derived from this value,\n            unless they are set.\n\n        color_lines : None or iterable of int\n            Color to use for the line segments as RGB, i.e. three values.\n            If ``None``, this value is derived from `color`.\n\n        color_points : None or iterable of int\n            Color to use for the points as RGB, i.e. three values.\n            If ``None``, this value is derived from ``0.5 * color``.\n\n        alpha : float, optional\n            Opacity of the line strings. Higher values denote more visible\n            points.\n            The alphas of the line and points are derived from this value,\n            unless they are set.\n\n        alpha_lines : None or float, optional\n            Opacity of the line strings. Higher values denote more visible\n            line string.\n            If ``None``, this value is derived from `alpha`.\n\n        alpha_points : None or float, optional\n            Opacity of the line string points. Higher values denote more\n            visible points.\n            If ``None``, this value is derived from `alpha`.\n\n        size : int, optional\n            Size of the line strings.\n            The sizes of the line and points are derived from this value,\n            unless they are set.\n\n        size_lines : None or int, optional\n            Thickness of the line segments.\n            If ``None``, this value is derived from `size`.\n\n        size_points : None or int, optional\n            Size of the points in pixels.\n            If ``None``, this value is derived from ``3 * size``.\n\n        antialiased : bool, optional\n            Whether to draw the lines with anti-aliasing activated.\n            This does currently not affect the point drawing.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if a line string is fully\n            outside of the image. If set to False, no error will be raised and\n            only the parts inside the image will be drawn.\n\n        Returns\n        -------\n        ndarray\n            Image with line strings drawn on it."
  },
  {
    "code": "def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all line strings that are fully/partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove line strings that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove line strings that are partially outside of the\n            image.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Reduced set of line strings, with those that were fully/partially\n            outside of the image removed.\n\n        \"\"\"\n        lss_clean = [ls for ls in self.line_strings\n                     if not ls.is_out_of_image(\n                         self.shape, fully=fully, partly=partly)]\n        return LineStringsOnImage(lss_clean, shape=self.shape)",
    "doc": "Remove all line strings that are fully/partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove line strings that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove line strings that are partially outside of the\n            image.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Reduced set of line strings, with those that were fully/partially\n            outside of the image removed."
  },
  {
    "code": "def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts of the line strings that are outside of the image.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Line strings, clipped to fall within the image dimensions.\n\n        \"\"\"\n        lss_cut = [ls_clipped\n                   for ls in self.line_strings\n                   for ls_clipped in ls.clip_out_of_image(self.shape)]\n        return LineStringsOnImage(lss_cut, shape=self.shape)",
    "doc": "Clip off all parts of the line strings that are outside of the image.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Line strings, clipped to fall within the image dimensions."
  },
  {
    "code": "def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift/move the line strings from one or more image sides.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            left.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Shifted line strings.\n\n        \"\"\"\n        lss_new = [ls.shift(top=top, right=right, bottom=bottom, left=left)\n                   for ls in self.line_strings]\n        return LineStringsOnImage(lss_new, shape=self.shape)",
    "doc": "Shift/move the line strings from one or more image sides.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the\n            left.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Shifted line strings."
  },
  {
    "code": "def copy(self, line_strings=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the LineStringsOnImage object.\n\n        Parameters\n        ----------\n        line_strings : None \\\n                       or list of imgaug.augmentables.lines.LineString, optional\n            List of line strings on the image.\n            If not ``None``, then the ``line_strings`` attribute of the copied\n            object will be set to this value.\n\n        shape : None or tuple of int or ndarray, optional\n            The shape of the image on which the objects are placed.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n            If not ``None``, then the ``shape`` attribute of the copied object\n            will be set to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Shallow copy.\n\n        \"\"\"\n        lss = self.line_strings if line_strings is None else line_strings\n        shape = self.shape if shape is None else shape\n        return LineStringsOnImage(line_strings=lss, shape=shape)",
    "doc": "Create a shallow copy of the LineStringsOnImage object.\n\n        Parameters\n        ----------\n        line_strings : None \\\n                       or list of imgaug.augmentables.lines.LineString, optional\n            List of line strings on the image.\n            If not ``None``, then the ``line_strings`` attribute of the copied\n            object will be set to this value.\n\n        shape : None or tuple of int or ndarray, optional\n            The shape of the image on which the objects are placed.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n            If not ``None``, then the ``shape`` attribute of the copied object\n            will be set to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Shallow copy."
  },
  {
    "code": "def deepcopy(self, line_strings=None, shape=None):\n        \"\"\"\n        Create a deep copy of the LineStringsOnImage object.\n\n        Parameters\n        ----------\n        line_strings : None \\\n                       or list of imgaug.augmentables.lines.LineString, optional\n            List of line strings on the image.\n            If not ``None``, then the ``line_strings`` attribute of the copied\n            object will be set to this value.\n\n        shape : None or tuple of int or ndarray, optional\n            The shape of the image on which the objects are placed.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n            If not ``None``, then the ``shape`` attribute of the copied object\n            will be set to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Deep copy.\n\n        \"\"\"\n        lss = self.line_strings if line_strings is None else line_strings\n        shape = self.shape if shape is None else shape\n        return LineStringsOnImage(\n            line_strings=[ls.deepcopy() for ls in lss],\n            shape=tuple(shape))",
    "doc": "Create a deep copy of the LineStringsOnImage object.\n\n        Parameters\n        ----------\n        line_strings : None \\\n                       or list of imgaug.augmentables.lines.LineString, optional\n            List of line strings on the image.\n            If not ``None``, then the ``line_strings`` attribute of the copied\n            object will be set to this value.\n\n        shape : None or tuple of int or ndarray, optional\n            The shape of the image on which the objects are placed.\n            Either an image with shape ``(H,W,[C])`` or a tuple denoting\n            such an image shape.\n            If not ``None``, then the ``shape`` attribute of the copied object\n            will be set to this value.\n\n        Returns\n        -------\n        imgaug.augmentables.lines.LineStringsOnImage\n            Deep copy."
  },
  {
    "code": "def blend_alpha(image_fg, image_bg, alpha, eps=1e-2):\n    \"\"\"\n    Blend two images using an alpha blending.\n\n    In an alpha blending, the two images are naively mixed. Let ``A`` be the foreground image\n    and ``B`` the background image and ``a`` is the alpha value. Each pixel intensity is then\n    computed as ``a * A_ij + (1-a) * B_ij``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested (1)\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested (1)\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: no (2)\n        * ``bool``: yes; fully tested (2)\n\n        - (1) Tests show that these dtypes work, but a conversion to float128 happens, which only\n              has 96 bits of size instead of true 128 bits and hence not twice as much resolution.\n              It is possible that these dtypes result in inaccuracies, though the tests did not\n              indicate that.\n        - (2) Not available due to the input dtype having to be increased to an equivalent float\n              dtype with two times the input resolution.\n        - (3) Mapped internally to ``float16``.\n\n    Parameters\n    ----------\n    image_fg : (H,W,[C]) ndarray\n        Foreground image. Shape and dtype kind must match the one of the\n        background image.\n\n    image_bg : (H,W,[C]) ndarray\n        Background image. Shape and dtype kind must match the one of the\n        foreground image.\n\n    alpha : number or iterable of number or ndarray\n        The blending factor, between 0.0 and 1.0. Can be interpreted as the opacity of the\n        foreground image. Values around 1.0 result in only the foreground image being visible.\n        Values around 0.0 result in only the background image being visible.\n        Multiple alphas may be provided. In these cases, there must be exactly one alpha per\n        channel in the foreground/background image. Alternatively, for ``(H,W,C)`` images,\n        either one ``(H,W)`` array or an ``(H,W,C)`` array of alphas may be provided,\n        denoting the elementwise alpha value.\n\n    eps : number, optional\n        Controls when an alpha is to be interpreted as exactly 1.0 or exactly 0.0, resulting\n        in only the foreground/background being visible and skipping the actual computation.\n\n    Returns\n    -------\n    image_blend : (H,W,C) ndarray\n        Blend of foreground and background image.\n\n    \"\"\"\n    assert image_fg.shape == image_bg.shape\n    assert image_fg.dtype.kind == image_bg.dtype.kind\n    # TODO switch to gate_dtypes()\n    assert image_fg.dtype.name not in [\"float128\"]\n    assert image_bg.dtype.name not in [\"float128\"]\n\n    # TODO add test for this\n    input_was_2d = (len(image_fg.shape) == 2)\n    if input_was_2d:\n        image_fg = np.atleast_3d(image_fg)\n        image_bg = np.atleast_3d(image_bg)\n\n    input_was_bool = False\n    if image_fg.dtype.kind == \"b\":\n        input_was_bool = True\n        # use float32 instead of float16 here because it seems to be faster\n        image_fg = image_fg.astype(np.float32)\n        image_bg = image_bg.astype(np.float32)\n\n    alpha = np.array(alpha, dtype=np.float64)\n    if alpha.size == 1:\n        pass\n    else:\n        if alpha.ndim == 2:\n            assert alpha.shape == image_fg.shape[0:2]\n            alpha = alpha.reshape((alpha.shape[0], alpha.shape[1], 1))\n        elif alpha.ndim == 3:\n            assert alpha.shape == image_fg.shape or alpha.shape == image_fg.shape[0:2] + (1,)\n        else:\n            alpha = alpha.reshape((1, 1, -1))\n        if alpha.shape[2] != image_fg.shape[2]:\n            alpha = np.tile(alpha, (1, 1, image_fg.shape[2]))\n\n    if not input_was_bool:\n        if np.all(alpha >= 1.0 - eps):\n            return np.copy(image_fg)\n        elif np.all(alpha <= eps):\n            return np.copy(image_bg)\n\n    # for efficiency reaons, only test one value of alpha here, even if alpha is much larger\n    assert 0 <= alpha.item(0) <= 1.0\n\n    dt_images = iadt.get_minimal_dtype([image_fg, image_bg])\n\n    # doing this only for non-float images led to inaccuracies for large floats values\n    isize = dt_images.itemsize * 2\n    isize = max(isize, 4)  # at least 4 bytes (=float32), tends to be faster than float16\n    dt_blend = np.dtype(\"f%d\" % (isize,))\n\n    if alpha.dtype != dt_blend:\n        alpha = alpha.astype(dt_blend)\n    if image_fg.dtype != dt_blend:\n        image_fg = image_fg.astype(dt_blend)\n    if image_bg.dtype != dt_blend:\n        image_bg = image_bg.astype(dt_blend)\n\n    # the following is equivalent to\n    #     image_blend = alpha * image_fg + (1 - alpha) * image_bg\n    # but supposedly faster\n    image_blend = image_bg + alpha * (image_fg - image_bg)\n\n    if input_was_bool:\n        image_blend = image_blend > 0.5\n    else:\n        # skip clip, because alpha is expected to be in range [0.0, 1.0] and both images must have same dtype\n        # dont skip round, because otherwise it is very unlikely to hit the image's max possible value\n        image_blend = iadt.restore_dtypes_(image_blend, dt_images, clip=False, round=True)\n\n    if input_was_2d:\n        return image_blend[:, :, 0]\n    return image_blend",
    "doc": "Blend two images using an alpha blending.\n\n    In an alpha blending, the two images are naively mixed. Let ``A`` be the foreground image\n    and ``B`` the background image and ``a`` is the alpha value. Each pixel intensity is then\n    computed as ``a * A_ij + (1-a) * B_ij``.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested (1)\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested (1)\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: no (2)\n        * ``bool``: yes; fully tested (2)\n\n        - (1) Tests show that these dtypes work, but a conversion to float128 happens, which only\n              has 96 bits of size instead of true 128 bits and hence not twice as much resolution.\n              It is possible that these dtypes result in inaccuracies, though the tests did not\n              indicate that.\n        - (2) Not available due to the input dtype having to be increased to an equivalent float\n              dtype with two times the input resolution.\n        - (3) Mapped internally to ``float16``.\n\n    Parameters\n    ----------\n    image_fg : (H,W,[C]) ndarray\n        Foreground image. Shape and dtype kind must match the one of the\n        background image.\n\n    image_bg : (H,W,[C]) ndarray\n        Background image. Shape and dtype kind must match the one of the\n        foreground image.\n\n    alpha : number or iterable of number or ndarray\n        The blending factor, between 0.0 and 1.0. Can be interpreted as the opacity of the\n        foreground image. Values around 1.0 result in only the foreground image being visible.\n        Values around 0.0 result in only the background image being visible.\n        Multiple alphas may be provided. In these cases, there must be exactly one alpha per\n        channel in the foreground/background image. Alternatively, for ``(H,W,C)`` images,\n        either one ``(H,W)`` array or an ``(H,W,C)`` array of alphas may be provided,\n        denoting the elementwise alpha value.\n\n    eps : number, optional\n        Controls when an alpha is to be interpreted as exactly 1.0 or exactly 0.0, resulting\n        in only the foreground/background being visible and skipping the actual computation.\n\n    Returns\n    -------\n    image_blend : (H,W,C) ndarray\n        Blend of foreground and background image."
  },
  {
    "code": "def SimplexNoiseAlpha(first=None, second=None, per_channel=False, size_px_max=(2, 16), upscale_method=None,\n                      iterations=(1, 3), aggregation_method=\"max\", sigmoid=True, sigmoid_thresh=None,\n                      name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to alpha-blend two image sources using simplex noise alpha masks.\n\n    The alpha masks are sampled using a simplex noise method, roughly creating\n    connected blobs of 1s surrounded by 0s. If nearest neighbour upsampling\n    is used, these blobs can be rectangular with sharp edges.\n\n    dtype support::\n\n        See ``imgaug.augmenters.blend.AlphaElementwise``.\n\n    Parameters\n    ----------\n    first : None or imgaug.augmenters.meta.Augmenter or iterable of imgaug.augmenters.meta.Augmenter, optional\n        Augmenter(s) that make up the first of the two branches.\n\n            * If None, then the input images will be reused as the output\n              of the first branch.\n            * If Augmenter, then that augmenter will be used as the branch.\n            * If iterable of Augmenter, then that iterable will be converted\n              into a Sequential and used as the augmenter.\n\n    second : None or imgaug.augmenters.meta.Augmenter or iterable of imgaug.augmenters.meta.Augmenter, optional\n        Augmenter(s) that make up the second of the two branches.\n\n            * If None, then the input images will be reused as the output\n              of the second branch.\n            * If Augmenter, then that augmenter will be used as the branch.\n            * If iterable of Augmenter, then that iterable will be converted\n              into a Sequential and used as the augmenter.\n\n    per_channel : bool or float, optional\n        Whether to use the same factor for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    size_px_max : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        The simplex noise is always generated in a low resolution environment.\n        This parameter defines the maximum size of that environment (in\n        pixels). The environment is initialized at the same size as the input\n        image and then downscaled, so that no side exceeds `size_px_max`\n        (aspect ratio is kept).\n\n            * If int, then that number will be used as the size for all\n              iterations.\n            * If tuple of two ints ``(a, b)``, then a value will be sampled\n              per iteration from the discrete range ``[a..b]``.\n            * If a list of ints, then a value will be picked per iteration at\n              random from that list.\n            * If a StochasticParameter, then a value will be sampled from\n              that parameter per iteration.\n\n    upscale_method : None or imgaug.ALL or str or list of str or imgaug.parameters.StochasticParameter, optional\n        After generating the noise maps in low resolution environments, they\n        have to be upscaled to the input image size. This parameter controls\n        the upscaling method.\n\n            * If None, then either ``nearest`` or ``linear`` or ``cubic`` is picked.\n              Most weight is put on linear, followed by cubic.\n            * If ia.ALL, then either ``nearest`` or ``linear`` or ``area`` or ``cubic``\n              is picked per iteration (all same probability).\n            * If string, then that value will be used as the method (must be\n              'nearest' or ``linear`` or ``area`` or ``cubic``).\n            * If list of string, then a random value will be picked from that\n              list per iteration.\n            * If StochasticParameter, then a random value will be sampled\n              from that parameter per iteration.\n\n    iterations : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        How often to repeat the simplex noise generation process per image.\n\n            * If int, then that number will be used as the iterations for all\n              images.\n            * If tuple of two ints ``(a, b)``, then a value will be sampled\n              per image from the discrete range ``[a..b]``.\n            * If a list of ints, then a value will be picked per image at\n              random from that list.\n            * If a StochasticParameter, then a value will be sampled from\n              that parameter per image.\n\n    aggregation_method : imgaug.ALL or str or list of str or imgaug.parameters.StochasticParameter, optional\n        The noise maps (from each iteration) are combined to one noise map\n        using an aggregation process. This parameter defines the method used\n        for that process. Valid methods are ``min``, ``max`` or ``avg``,\n        where ``min`` combines the noise maps by taking the (elementwise) minimum\n        over all iteration's results, ``max`` the (elementwise) maximum and\n        ``avg`` the (elementwise) average.\n\n            * If imgaug.ALL, then a random value will be picked per image from the\n              valid ones.\n            * If a string, then that value will always be used as the method.\n            * If a list of string, then a random value will be picked from\n              that list per image.\n            * If a StochasticParameter, then a random value will be sampled\n              from that paramter per image.\n\n    sigmoid : bool or number, optional\n        Whether to apply a sigmoid function to the final noise maps, resulting\n        in maps that have more extreme values (close to 0.0 or 1.0).\n\n            * If bool, then a sigmoid will always (True) or never (False) be\n              applied.\n            * If a number ``p`` with ``0<=p<=1``, then a sigmoid will be applied to\n              ``p`` percent of all final noise maps.\n\n    sigmoid_thresh : None or number or tuple of number or imgaug.parameters.StochasticParameter, optional\n        Threshold of the sigmoid, when applied. Thresholds above zero\n        (e.g. 5.0) will move the saddle point towards the right, leading to\n        more values close to 0.0.\n\n            * If None, then ``Normal(0, 5.0)`` will be used.\n            * If number, then that threshold will be used for all images.\n            * If tuple of two numbers ``(a, b)``, then a random value will\n              be sampled per image from the range ``[a, b]``.\n            * If StochasticParameter, then a random value will be sampled from\n              that parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0))\n\n    Detects per image all edges, marks them in a black and white image and\n    then alpha-blends the result with the original image using simplex noise\n    masks.\n\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0), upscale_method=\"linear\")\n\n    Same as the first example, but uses only (smooth) linear upscaling to\n    scale the simplex noise masks to the final image sizes, i.e. no nearest\n    neighbour upsampling is used, which would result in rectangles with hard\n    edges.\n\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0), sigmoid_thresh=iap.Normal(10.0, 5.0))\n\n    Same as the first example, but uses a threshold for the sigmoid function\n    that is further to the right. This is more conservative, i.e. the generated\n    noise masks will be mostly black (values around 0.0), which means that\n    most of the original images (parameter/branch `second`) will be kept,\n    rather than using the results of the augmentation (parameter/branch\n    `first`).\n\n    \"\"\"\n    upscale_method_default = iap.Choice([\"nearest\", \"linear\", \"cubic\"], p=[0.05, 0.6, 0.35])\n    sigmoid_thresh_default = iap.Normal(0.0, 5.0)\n\n    noise = iap.SimplexNoise(\n        size_px_max=size_px_max,\n        upscale_method=upscale_method if upscale_method is not None else upscale_method_default\n    )\n\n    if iterations != 1:\n        noise = iap.IterativeNoiseAggregator(\n            noise,\n            iterations=iterations,\n            aggregation_method=aggregation_method\n        )\n\n    if sigmoid is False or (ia.is_single_number(sigmoid) and sigmoid <= 0.01):\n        noise = iap.Sigmoid.create_for_noise(\n            noise,\n            threshold=sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default,\n            activated=sigmoid\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AlphaElementwise(\n        factor=noise, first=first, second=second, per_channel=per_channel,\n        name=name, deterministic=deterministic, random_state=random_state\n    )",
    "doc": "Augmenter to alpha-blend two image sources using simplex noise alpha masks.\n\n    The alpha masks are sampled using a simplex noise method, roughly creating\n    connected blobs of 1s surrounded by 0s. If nearest neighbour upsampling\n    is used, these blobs can be rectangular with sharp edges.\n\n    dtype support::\n\n        See ``imgaug.augmenters.blend.AlphaElementwise``.\n\n    Parameters\n    ----------\n    first : None or imgaug.augmenters.meta.Augmenter or iterable of imgaug.augmenters.meta.Augmenter, optional\n        Augmenter(s) that make up the first of the two branches.\n\n            * If None, then the input images will be reused as the output\n              of the first branch.\n            * If Augmenter, then that augmenter will be used as the branch.\n            * If iterable of Augmenter, then that iterable will be converted\n              into a Sequential and used as the augmenter.\n\n    second : None or imgaug.augmenters.meta.Augmenter or iterable of imgaug.augmenters.meta.Augmenter, optional\n        Augmenter(s) that make up the second of the two branches.\n\n            * If None, then the input images will be reused as the output\n              of the second branch.\n            * If Augmenter, then that augmenter will be used as the branch.\n            * If iterable of Augmenter, then that iterable will be converted\n              into a Sequential and used as the augmenter.\n\n    per_channel : bool or float, optional\n        Whether to use the same factor for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    size_px_max : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        The simplex noise is always generated in a low resolution environment.\n        This parameter defines the maximum size of that environment (in\n        pixels). The environment is initialized at the same size as the input\n        image and then downscaled, so that no side exceeds `size_px_max`\n        (aspect ratio is kept).\n\n            * If int, then that number will be used as the size for all\n              iterations.\n            * If tuple of two ints ``(a, b)``, then a value will be sampled\n              per iteration from the discrete range ``[a..b]``.\n            * If a list of ints, then a value will be picked per iteration at\n              random from that list.\n            * If a StochasticParameter, then a value will be sampled from\n              that parameter per iteration.\n\n    upscale_method : None or imgaug.ALL or str or list of str or imgaug.parameters.StochasticParameter, optional\n        After generating the noise maps in low resolution environments, they\n        have to be upscaled to the input image size. This parameter controls\n        the upscaling method.\n\n            * If None, then either ``nearest`` or ``linear`` or ``cubic`` is picked.\n              Most weight is put on linear, followed by cubic.\n            * If ia.ALL, then either ``nearest`` or ``linear`` or ``area`` or ``cubic``\n              is picked per iteration (all same probability).\n            * If string, then that value will be used as the method (must be\n              'nearest' or ``linear`` or ``area`` or ``cubic``).\n            * If list of string, then a random value will be picked from that\n              list per iteration.\n            * If StochasticParameter, then a random value will be sampled\n              from that parameter per iteration.\n\n    iterations : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        How often to repeat the simplex noise generation process per image.\n\n            * If int, then that number will be used as the iterations for all\n              images.\n            * If tuple of two ints ``(a, b)``, then a value will be sampled\n              per image from the discrete range ``[a..b]``.\n            * If a list of ints, then a value will be picked per image at\n              random from that list.\n            * If a StochasticParameter, then a value will be sampled from\n              that parameter per image.\n\n    aggregation_method : imgaug.ALL or str or list of str or imgaug.parameters.StochasticParameter, optional\n        The noise maps (from each iteration) are combined to one noise map\n        using an aggregation process. This parameter defines the method used\n        for that process. Valid methods are ``min``, ``max`` or ``avg``,\n        where ``min`` combines the noise maps by taking the (elementwise) minimum\n        over all iteration's results, ``max`` the (elementwise) maximum and\n        ``avg`` the (elementwise) average.\n\n            * If imgaug.ALL, then a random value will be picked per image from the\n              valid ones.\n            * If a string, then that value will always be used as the method.\n            * If a list of string, then a random value will be picked from\n              that list per image.\n            * If a StochasticParameter, then a random value will be sampled\n              from that paramter per image.\n\n    sigmoid : bool or number, optional\n        Whether to apply a sigmoid function to the final noise maps, resulting\n        in maps that have more extreme values (close to 0.0 or 1.0).\n\n            * If bool, then a sigmoid will always (True) or never (False) be\n              applied.\n            * If a number ``p`` with ``0<=p<=1``, then a sigmoid will be applied to\n              ``p`` percent of all final noise maps.\n\n    sigmoid_thresh : None or number or tuple of number or imgaug.parameters.StochasticParameter, optional\n        Threshold of the sigmoid, when applied. Thresholds above zero\n        (e.g. 5.0) will move the saddle point towards the right, leading to\n        more values close to 0.0.\n\n            * If None, then ``Normal(0, 5.0)`` will be used.\n            * If number, then that threshold will be used for all images.\n            * If tuple of two numbers ``(a, b)``, then a random value will\n              be sampled per image from the range ``[a, b]``.\n            * If StochasticParameter, then a random value will be sampled from\n              that parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0))\n\n    Detects per image all edges, marks them in a black and white image and\n    then alpha-blends the result with the original image using simplex noise\n    masks.\n\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0), upscale_method=\"linear\")\n\n    Same as the first example, but uses only (smooth) linear upscaling to\n    scale the simplex noise masks to the final image sizes, i.e. no nearest\n    neighbour upsampling is used, which would result in rectangles with hard\n    edges.\n\n    >>> aug = iaa.SimplexNoiseAlpha(iaa.EdgeDetect(1.0), sigmoid_thresh=iap.Normal(10.0, 5.0))\n\n    Same as the first example, but uses a threshold for the sigmoid function\n    that is further to the right. This is more conservative, i.e. the generated\n    noise masks will be mostly black (values around 0.0), which means that\n    most of the original images (parameter/branch `second`) will be kept,\n    rather than using the results of the augmentation (parameter/branch\n    `first`)."
  },
  {
    "code": "def OneOf(children, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that always executes exactly one of its children.\n\n    dtype support::\n\n        See ``imgaug.augmenters.meta.SomeOf``.\n\n    Parameters\n    ----------\n    children : list of imgaug.augmenters.meta.Augmenter\n        The choices of augmenters to apply.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> imgs = [np.ones((10, 10))]\n    >>> seq = iaa.OneOf([\n    >>>     iaa.Fliplr(1.0),\n    >>>     iaa.Flipud(1.0)\n    >>> ])\n    >>> imgs_aug = seq.augment_images(imgs)\n\n    flips each image either horizontally or vertically.\n\n\n    >>> seq = iaa.OneOf([\n    >>>     iaa.Fliplr(1.0),\n    >>>     iaa.Sequential([\n    >>>         iaa.GaussianBlur(1.0),\n    >>>         iaa.Dropout(0.05),\n    >>>         iaa.AdditiveGaussianNoise(0.1*255)\n    >>>     ]),\n    >>>     iaa.Noop()\n    >>> ])\n    >>> imgs_aug = seq.augment_images(imgs)\n\n    either flips each image horizontally, or adds blur+dropout+noise or does\n    nothing.\n\n    \"\"\"\n    return SomeOf(n=1, children=children, random_order=False, name=name, deterministic=deterministic,\n                  random_state=random_state)",
    "doc": "Augmenter that always executes exactly one of its children.\n\n    dtype support::\n\n        See ``imgaug.augmenters.meta.SomeOf``.\n\n    Parameters\n    ----------\n    children : list of imgaug.augmenters.meta.Augmenter\n        The choices of augmenters to apply.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> imgs = [np.ones((10, 10))]\n    >>> seq = iaa.OneOf([\n    >>>     iaa.Fliplr(1.0),\n    >>>     iaa.Flipud(1.0)\n    >>> ])\n    >>> imgs_aug = seq.augment_images(imgs)\n\n    flips each image either horizontally or vertically.\n\n\n    >>> seq = iaa.OneOf([\n    >>>     iaa.Fliplr(1.0),\n    >>>     iaa.Sequential([\n    >>>         iaa.GaussianBlur(1.0),\n    >>>         iaa.Dropout(0.05),\n    >>>         iaa.AdditiveGaussianNoise(0.1*255)\n    >>>     ]),\n    >>>     iaa.Noop()\n    >>> ])\n    >>> imgs_aug = seq.augment_images(imgs)\n\n    either flips each image horizontally, or adds blur+dropout+noise or does\n    nothing."
  },
  {
    "code": "def AssertLambda(func_images=None, func_heatmaps=None, func_keypoints=None,\n                 func_polygons=None, name=None, deterministic=False,\n                 random_state=None):\n    \"\"\"\n    Augmenter that runs an assert on each batch of input images\n    using a lambda function as condition.\n\n    This is useful to make generic assumption about the input images and error\n    out early if they aren't met.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: yes; tested\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    func_images : None or callable, optional\n        The function to call for each batch of images.\n        It must follow the form ``function(images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_images`.\n\n    func_heatmaps : None or callable, optional\n        The function to call for each batch of heatmaps.\n        It must follow the form ``function(heatmaps, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_heatmaps`.\n\n    func_keypoints : None or callable, optional\n        The function to call for each batch of keypoints.\n        It must follow the form ``function(keypoints_on_images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_keypoints`.\n\n    func_polygons : None or callable, optional\n        The function to call for each batch of polygons.\n        It must follow the form ``function(polygons_on_images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_polygons`.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    \"\"\"\n    def func_images_assert(images, random_state, parents, hooks):\n        ia.do_assert(func_images(images, random_state, parents, hooks),\n                     \"Input images did not fulfill user-defined assertion in AssertLambda.\")\n        return images\n\n    def func_heatmaps_assert(heatmaps, random_state, parents, hooks):\n        ia.do_assert(func_heatmaps(heatmaps, random_state, parents, hooks),\n                     \"Input heatmaps did not fulfill user-defined assertion in AssertLambda.\")\n        return heatmaps\n\n    def func_keypoints_assert(keypoints_on_images, random_state, parents, hooks):\n        ia.do_assert(func_keypoints(keypoints_on_images, random_state, parents, hooks),\n                     \"Input keypoints did not fulfill user-defined assertion in AssertLambda.\")\n        return keypoints_on_images\n\n    def func_polygons_assert(polygons_on_images, random_state, parents, hooks):\n        ia.do_assert(func_polygons(polygons_on_images, random_state, parents, hooks),\n                     \"Input polygons did not fulfill user-defined assertion in AssertLambda.\")\n        return polygons_on_images\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n    return Lambda(func_images_assert if func_images is not None else None,\n                  func_heatmaps_assert if func_heatmaps is not None else None,\n                  func_keypoints_assert if func_keypoints is not None else None,\n                  func_polygons_assert if func_polygons is not None else None,\n                  name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter that runs an assert on each batch of input images\n    using a lambda function as condition.\n\n    This is useful to make generic assumption about the input images and error\n    out early if they aren't met.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: yes; tested\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    func_images : None or callable, optional\n        The function to call for each batch of images.\n        It must follow the form ``function(images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_images`.\n\n    func_heatmaps : None or callable, optional\n        The function to call for each batch of heatmaps.\n        It must follow the form ``function(heatmaps, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_heatmaps`.\n\n    func_keypoints : None or callable, optional\n        The function to call for each batch of keypoints.\n        It must follow the form ``function(keypoints_on_images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_keypoints`.\n\n    func_polygons : None or callable, optional\n        The function to call for each batch of polygons.\n        It must follow the form ``function(polygons_on_images, random_state, parents, hooks)``\n        and return either True (valid input) or False (invalid input).\n        It essentially reuses the interface of\n        :func:`imgaug.augmenters.meta.Augmenter._augment_polygons`.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`."
  },
  {
    "code": "def AssertShape(shape, check_images=True, check_heatmaps=True,\n                check_keypoints=True, check_polygons=True,\n                name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to make assumptions about the shape of input image(s), heatmaps and keypoints.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: yes; tested\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    shape : tuple\n        The expected shape, given as a tuple. The number of entries in the tuple must match the\n        number of dimensions, i.e. it must contain four entries for ``(N, H, W, C)``. If only a\n        single image is augmented via ``augment_image()``, then ``N`` is viewed as 1 by this\n        augmenter. If the input image(s) don't have a channel axis, then ``C`` is viewed as 1\n        by this augmenter.\n        Each of the four entries may be None or a tuple of two ints or a list of ints.\n\n            * If an entry is None, any value for that dimensions is accepted.\n            * If an entry is int, exactly that integer value will be accepted\n              or no other value.\n            * If an entry is a tuple of two ints with values ``a`` and ``b``, only a\n              value ``x`` with ``a <= x < b`` will be accepted for the dimension.\n            * If an entry is a list of ints, only a value for the dimension\n              will be accepted which is contained in the list.\n\n    check_images : bool, optional\n        Whether to validate input images via the given shape.\n\n    check_heatmaps : bool, optional\n        Whether to validate input heatmaps via the given shape.\n        The number of heatmaps will be checked and for each Heatmaps\n        instance its array's height and width, but not the channel\n        count as the channel number denotes the expected number of channels\n        in images.\n\n    check_keypoints : bool, optional\n        Whether to validate input keypoints via the given shape.\n        This will check (a) the number of keypoints and (b) for each\n        KeypointsOnImage instance the ``.shape``, i.e. the shape of the\n        corresponding image.\n\n    check_polygons : bool, optional\n        Whether to validate input keypoints via the given shape.\n        This will check (a) the number of polygons and (b) for each\n        PolygonsOnImage instance the ``.shape``, i.e. the shape of the\n        corresponding image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.AssertShape((None, 32, 32, 3)),\n    >>>     iaa.Fliplr(0.5)\n    >>> ])\n\n    will first check for each image batch, if it contains a variable number of\n    ``32x32`` images with 3 channels each. Only if that check succeeds, the\n    horizontal flip will be executed (otherwise an assertion error will be\n    thrown).\n\n    >>> seq = iaa.Sequential([\n    >>>     iaa.AssertShape((None, (32, 64), 32, [1, 3])),\n    >>>     iaa.Fliplr(0.5)\n    >>> ])\n\n    like above, but now the height may be in the range ``32 <= H < 64`` and\n    the number of channels may be either 1 or 3.\n\n    \"\"\"\n    ia.do_assert(len(shape) == 4, \"Expected shape to have length 4, got %d with shape: %s.\" % (len(shape), str(shape)))\n\n    def compare(observed, expected, dimension, image_index):\n        if expected is not None:\n            if ia.is_single_integer(expected):\n                ia.do_assert(observed == expected,\n                             \"Expected dim %d (entry index: %s) to have value %d, got %d.\" % (\n                                 dimension, image_index, expected, observed))\n            elif isinstance(expected, tuple):\n                ia.do_assert(len(expected) == 2)\n                ia.do_assert(expected[0] <= observed < expected[1],\n                             \"Expected dim %d (entry index: %s) to have value in range [%d, %d), got %d.\" % (\n                                 dimension, image_index, expected[0], expected[1], observed))\n            elif isinstance(expected, list):\n                ia.do_assert(any([observed == val for val in expected]),\n                             \"Expected dim %d (entry index: %s) to have any value of %s, got %d.\" % (\n                                 dimension, image_index, str(expected), observed))\n            else:\n                raise Exception((\"Invalid datatype for shape entry %d, expected each entry to be an integer, \"\n                                + \"a tuple (with two entries) or a list, got %s.\") % (dimension, type(expected),))\n\n    def func_images(images, _random_state, _parents, _hooks):\n        if check_images:\n            if isinstance(images, list):\n                if shape[0] is not None:\n                    compare(len(images), shape[0], 0, \"ALL\")\n\n                for i in sm.xrange(len(images)):\n                    image = images[i]\n                    ia.do_assert(len(image.shape) == 3,\n                                 \"Expected image number %d to have a shape of length 3, got %d (shape: %s).\" % (\n                                     i, len(image.shape), str(image.shape)))\n                    for j in sm.xrange(len(shape)-1):\n                        expected = shape[j+1]\n                        observed = image.shape[j]\n                        compare(observed, expected, j, i)\n            else:\n                ia.do_assert(len(images.shape) == 4,\n                             \"Expected image's shape to have length 4, got %d (shape: %s).\" % (\n                                 len(images.shape), str(images.shape)))\n                for i in range(4):\n                    expected = shape[i]\n                    observed = images.shape[i]\n                    compare(observed, expected, i, \"ALL\")\n        return images\n\n    def func_heatmaps(heatmaps, _random_state, _parents, _hooks):\n        if check_heatmaps:\n            if shape[0] is not None:\n                compare(len(heatmaps), shape[0], 0, \"ALL\")\n\n            for i in sm.xrange(len(heatmaps)):\n                heatmaps_i = heatmaps[i]\n                for j in sm.xrange(len(shape[0:2])):\n                    expected = shape[j+1]\n                    observed = heatmaps_i.arr_0to1.shape[j]\n                    compare(observed, expected, j, i)\n        return heatmaps\n\n    def func_keypoints(keypoints_on_images, _random_state, _parents, _hooks):\n        if check_keypoints:\n            if shape[0] is not None:\n                compare(len(keypoints_on_images), shape[0], 0, \"ALL\")\n\n            for i in sm.xrange(len(keypoints_on_images)):\n                keypoints_on_image = keypoints_on_images[i]\n                for j in sm.xrange(len(shape[0:2])):\n                    expected = shape[j+1]\n                    observed = keypoints_on_image.shape[j]\n                    compare(observed, expected, j, i)\n        return keypoints_on_images\n\n    def func_polygons(polygons_on_images, _random_state, _parents, _hooks):\n        if check_polygons:\n            if shape[0] is not None:\n                compare(len(polygons_on_images), shape[0], 0, \"ALL\")\n\n            for i in sm.xrange(len(polygons_on_images)):\n                polygons_on_image = polygons_on_images[i]\n                for j in sm.xrange(len(shape[0:2])):\n                    expected = shape[j+1]\n                    observed = polygons_on_image.shape[j]\n                    compare(observed, expected, j, i)\n        return polygons_on_images\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Lambda(func_images, func_heatmaps, func_keypoints, func_polygons,\n                  name=name, deterministic=deterministic,\n                  random_state=random_state)",
    "doc": "Augmenter to make assumptions about the shape of input image(s), heatmaps and keypoints.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: yes; tested\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    shape : tuple\n        The expected shape, given as a tuple. The number of entries in the tuple must match the\n        number of dimensions, i.e. it must contain four entries for ``(N, H, W, C)``. If only a\n        single image is augmented via ``augment_image()``, then ``N`` is viewed as 1 by this\n        augmenter. If the input image(s) don't have a channel axis, then ``C`` is viewed as 1\n        by this augmenter.\n        Each of the four entries may be None or a tuple of two ints or a list of ints.\n\n            * If an entry is None, any value for that dimensions is accepted.\n            * If an entry is int, exactly that integer value will be accepted\n              or no other value.\n            * If an entry is a tuple of two ints with values ``a`` and ``b``, only a\n              value ``x`` with ``a <= x < b`` will be accepted for the dimension.\n            * If an entry is a list of ints, only a value for the dimension\n              will be accepted which is contained in the list.\n\n    check_images : bool, optional\n        Whether to validate input images via the given shape.\n\n    check_heatmaps : bool, optional\n        Whether to validate input heatmaps via the given shape.\n        The number of heatmaps will be checked and for each Heatmaps\n        instance its array's height and width, but not the channel\n        count as the channel number denotes the expected number of channels\n        in images.\n\n    check_keypoints : bool, optional\n        Whether to validate input keypoints via the given shape.\n        This will check (a) the number of keypoints and (b) for each\n        KeypointsOnImage instance the ``.shape``, i.e. the shape of the\n        corresponding image.\n\n    check_polygons : bool, optional\n        Whether to validate input keypoints via the given shape.\n        This will check (a) the number of polygons and (b) for each\n        PolygonsOnImage instance the ``.shape``, i.e. the shape of the\n        corresponding image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.AssertShape((None, 32, 32, 3)),\n    >>>     iaa.Fliplr(0.5)\n    >>> ])\n\n    will first check for each image batch, if it contains a variable number of\n    ``32x32`` images with 3 channels each. Only if that check succeeds, the\n    horizontal flip will be executed (otherwise an assertion error will be\n    thrown).\n\n    >>> seq = iaa.Sequential([\n    >>>     iaa.AssertShape((None, (32, 64), 32, [1, 3])),\n    >>>     iaa.Fliplr(0.5)\n    >>> ])\n\n    like above, but now the height may be in the range ``32 <= H < 64`` and\n    the number of channels may be either 1 or 3."
  },
  {
    "code": "def shuffle_channels(image, random_state, channels=None):\n    \"\"\"\n    Randomize the order of (color) channels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; indirectly tested (1)\n        * ``uint32``: yes; indirectly tested (1)\n        * ``uint64``: yes; indirectly tested (1)\n        * ``int8``: yes; indirectly tested (1)\n        * ``int16``: yes; indirectly tested (1)\n        * ``int32``: yes; indirectly tested (1)\n        * ``int64``: yes; indirectly tested (1)\n        * ``float16``: yes; indirectly tested (1)\n        * ``float32``: yes; indirectly tested (1)\n        * ``float64``: yes; indirectly tested (1)\n        * ``float128``: yes; indirectly tested (1)\n        * ``bool``: yes; indirectly tested (1)\n\n        - (1) Indirectly tested via ``ChannelShuffle``.\n\n    Parameters\n    ----------\n    image : (H,W,[C]) ndarray\n        Image of any dtype for which to shuffle the channels.\n\n    random_state : numpy.random.RandomState\n        The random state to use for this shuffling operation.\n\n    channels : None or imgaug.ALL or list of int, optional\n        Which channels are allowed to be shuffled with each other.\n        If this is ``None`` or ``imgaug.ALL``, then all channels may be shuffled. If it is a list of integers,\n        then only the channels with indices in that list may be shuffled. (Values start at 0. All channel indices in\n        the list must exist in each image.)\n\n    Returns\n    -------\n    ndarray\n        The input image with shuffled channels.\n\n    \"\"\"\n    if image.ndim < 3 or image.shape[2] == 1:\n        return image\n    nb_channels = image.shape[2]\n    all_channels = np.arange(nb_channels)\n    is_all_channels = (\n        channels is None\n        or channels == ia.ALL\n        or len(set(all_channels).difference(set(channels))) == 0\n    )\n    if is_all_channels:\n        # note that if this is the case, then 'channels' may be None or imgaug.ALL, so don't simply move the\n        # assignment outside of the if/else\n        channels_perm = random_state.permutation(all_channels)\n        return image[..., channels_perm]\n    else:\n        channels_perm = random_state.permutation(channels)\n        channels_perm_full = all_channels\n        for channel_source, channel_target in zip(channels, channels_perm):\n            channels_perm_full[channel_source] = channel_target\n        return image[..., channels_perm_full]",
    "doc": "Randomize the order of (color) channels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; indirectly tested (1)\n        * ``uint32``: yes; indirectly tested (1)\n        * ``uint64``: yes; indirectly tested (1)\n        * ``int8``: yes; indirectly tested (1)\n        * ``int16``: yes; indirectly tested (1)\n        * ``int32``: yes; indirectly tested (1)\n        * ``int64``: yes; indirectly tested (1)\n        * ``float16``: yes; indirectly tested (1)\n        * ``float32``: yes; indirectly tested (1)\n        * ``float64``: yes; indirectly tested (1)\n        * ``float128``: yes; indirectly tested (1)\n        * ``bool``: yes; indirectly tested (1)\n\n        - (1) Indirectly tested via ``ChannelShuffle``.\n\n    Parameters\n    ----------\n    image : (H,W,[C]) ndarray\n        Image of any dtype for which to shuffle the channels.\n\n    random_state : numpy.random.RandomState\n        The random state to use for this shuffling operation.\n\n    channels : None or imgaug.ALL or list of int, optional\n        Which channels are allowed to be shuffled with each other.\n        If this is ``None`` or ``imgaug.ALL``, then all channels may be shuffled. If it is a list of integers,\n        then only the channels with indices in that list may be shuffled. (Values start at 0. All channel indices in\n        the list must exist in each image.)\n\n    Returns\n    -------\n    ndarray\n        The input image with shuffled channels."
  },
  {
    "code": "def blur_gaussian_(image, sigma, ksize=None, backend=\"auto\", eps=1e-3):\n    \"\"\"\n    Blur an image using gaussian blurring.\n\n    This operation might change the input image in-place.\n\n    dtype support::\n\n        if (backend=\"auto\")::\n\n            * ``uint8``: yes; fully tested (1)\n            * ``uint16``: yes; tested (1)\n            * ``uint32``: yes; tested (2)\n            * ``uint64``: yes; tested (2)\n            * ``int8``: yes; tested (1)\n            * ``int16``: yes; tested (1)\n            * ``int32``: yes; tested (1)\n            * ``int64``: yes; tested (2)\n            * ``float16``: yes; tested (1)\n            * ``float32``: yes; tested (1)\n            * ``float64``: yes; tested (1)\n            * ``float128``: no\n            * ``bool``: yes; tested (1)\n\n            - (1) Handled by ``cv2``. See ``backend=\"cv2\"``.\n            - (2) Handled by ``scipy``. See ``backend=\"scipy\"``.\n\n        if (backend=\"cv2\")::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: no (2)\n            * ``uint64``: no (3)\n            * ``int8``: yes; tested (4)\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested (5)\n            * ``int64``: no (6)\n            * ``float16``: yes; tested (7)\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: no (8)\n            * ``bool``: yes; tested (1)\n\n            - (1) Mapped internally to ``float32``. Otherwise causes ``TypeError: src data type = 0 is not supported``.\n            - (2) Causes ``TypeError: src data type = 6 is not supported``.\n            - (3) Causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957: error: (-213:The function/feature is not\n                  implemented) Unsupported combination of source format (=4), and buffer format (=5) in function\n                  'getLinearRowFilter'``.\n            - (4) Mapped internally to ``int16``. Otherwise causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957:\n                  error: (-213:The function/feature is not implemented) Unsupported combination of source format (=1),\n                  and buffer format (=5) in function 'getLinearRowFilter'``.\n            - (5) Mapped internally to ``float64``. Otherwise causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957:\n                  error: (-213:The function/feature is not implemented) Unsupported combination of source format (=4),\n                  and buffer format (=5) in function 'getLinearRowFilter'``.\n            - (6) Causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957: error: (-213:The function/feature is not\n                  implemented) Unsupported combination of source format (=4), and buffer format (=5) in function\n                  'getLinearRowFilter'``.\n            - (7) Mapped internally to ``float32``. Otherwise causes ``TypeError: src data type = 23 is not supported``.\n            - (8) Causes ``TypeError: src data type = 13 is not supported``.\n\n\n        if (backend=\"scipy\")::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested (1)\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: no (2)\n            * ``bool``: yes; tested (3)\n\n            - (1) Mapped internally to ``float32``. Otherwise causes ``RuntimeError: array type dtype('float16')\n                  not supported``.\n            - (2) Causes ``RuntimeError: array type dtype('float128') not supported``.\n            - (3) Mapped internally to ``float32``. Otherwise too inaccurate.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        The image to blur. Expected to be of shape ``(H, W)`` or ``(H, W, C)``.\n\n    sigma : number\n        Standard deviation of the gaussian blur. Larger numbers result in more large-scale blurring, which is overall\n        slower than small-scale blurring.\n\n    ksize : None or int, optional\n        Size in height/width of the gaussian kernel. This argument is only understood by the ``cv2`` backend.\n        If it is set to None, an appropriate value for `ksize` will automatically be derived from `sigma`.\n        The value is chosen tighter for larger sigmas to avoid as much as possible very large kernel sizes\n        and therey improve performance.\n\n    backend : {'auto', 'cv2', 'scipy'}, optional\n        Backend library to use. If ``auto``, then the likely best library will be automatically picked per image. That\n        is usually equivalent to ``cv2`` (OpenCV) and it will fall back to ``scipy`` for datatypes not supported by\n        OpenCV.\n\n    eps : number, optional\n        A threshold used to decide whether `sigma` can be considered zero.\n\n    Returns\n    -------\n    image : numpy.ndarray\n        The blurred image. Same shape and dtype as the input.\n\n    \"\"\"\n    if sigma > 0 + eps:\n        dtype = image.dtype\n\n        iadt.gate_dtypes(image,\n                         allowed=[\"bool\",\n                                  \"uint8\", \"uint16\", \"uint32\",\n                                  \"int8\", \"int16\", \"int32\", \"int64\", \"uint64\",\n                                  \"float16\", \"float32\", \"float64\"],\n                         disallowed=[\"uint128\", \"uint256\",\n                                     \"int128\", \"int256\",\n                                     \"float96\", \"float128\", \"float256\"],\n                         augmenter=None)\n\n        dts_not_supported_by_cv2 = [\"uint32\", \"uint64\", \"int64\", \"float128\"]\n        backend_to_use = backend\n        if backend == \"auto\":\n            backend_to_use = \"cv2\" if image.dtype.name not in dts_not_supported_by_cv2 else \"scipy\"\n        elif backend == \"cv2\":\n            assert image.dtype.name not in dts_not_supported_by_cv2,\\\n                (\"Requested 'cv2' backend, but provided %s input image, which \"\n                 + \"cannot be handled by that backend. Choose a different backend or \"\n                 + \"set backend to 'auto' or use a different datatype.\") % (image.dtype.name,)\n        elif backend == \"scipy\":\n            # can handle all dtypes that were allowed in gate_dtypes()\n            pass\n\n        if backend_to_use == \"scipy\":\n            if dtype.name == \"bool\":\n                # We convert bool to float32 here, because gaussian_filter() seems to only return True when\n                # the underlying value is approximately 1.0, not when it is above 0.5. So we do that here manually.\n                # cv2 does not support bool for gaussian blur\n                image = image.astype(np.float32, copy=False)\n            elif dtype.name == \"float16\":\n                image = image.astype(np.float32, copy=False)\n\n            # gaussian_filter() has no ksize argument\n            # TODO it does have a truncate argument that truncates at x standard deviations -- maybe can be used\n            #      similarly to ksize\n            if ksize is not None:\n                warnings.warn(\"Requested 'scipy' backend or picked it automatically by backend='auto' \"\n                              \"in blur_gaussian_(), but also provided 'ksize' argument, which is not understood by \"\n                              \"that backend and will be ignored.\")\n\n            # Note that while gaussian_filter can be applied to all channels at the same time, that should not\n            # be done here, because then the blurring would also happen across channels (e.g. red values might\n            # be mixed with blue values in RGB)\n            if image.ndim == 2:\n                image[:, :] = ndimage.gaussian_filter(image[:, :], sigma, mode=\"mirror\")\n            else:\n                nb_channels = image.shape[2]\n                for channel in sm.xrange(nb_channels):\n                    image[:, :, channel] = ndimage.gaussian_filter(image[:, :, channel], sigma, mode=\"mirror\")\n        else:\n            if dtype.name == \"bool\":\n                image = image.astype(np.float32, copy=False)\n            elif dtype.name == \"float16\":\n                image = image.astype(np.float32, copy=False)\n            elif dtype.name == \"int8\":\n                image = image.astype(np.int16, copy=False)\n            elif dtype.name == \"int32\":\n                image = image.astype(np.float64, copy=False)\n\n            # ksize here is derived from the equation to compute sigma based on ksize,\n            # see https://docs.opencv.org/3.1.0/d4/d86/group__imgproc__filter.html -> cv::getGaussianKernel()\n            # example values:\n            #   sig = 0.1 -> ksize = -1.666\n            #   sig = 0.5 -> ksize = 0.9999\n            #   sig = 1.0 -> ksize = 1.0\n            #   sig = 2.0 -> ksize = 11.0\n            #   sig = 3.0 -> ksize = 17.666\n            # ksize = ((sig - 0.8)/0.3 + 1)/0.5 + 1\n\n            if ksize is None:\n                if sigma < 3.0:\n                    ksize = 3.3 * sigma  # 99% of weight\n                elif sigma < 5.0:\n                    ksize = 2.9 * sigma  # 97% of weight\n                else:\n                    ksize = 2.6 * sigma  # 95% of weight\n\n                # we use 5x5 here as the minimum size as that simplifies comparisons with gaussian_filter() in the tests\n                # TODO reduce this to 3x3\n                ksize = int(max(ksize, 5))\n            else:\n                assert ia.is_single_integer(ksize), \"Expected 'ksize' argument to be a number, got %s.\" % (type(ksize),)\n\n            ksize = ksize + 1 if ksize % 2 == 0 else ksize\n\n            if ksize > 0:\n                image_warped = cv2.GaussianBlur(image, (ksize, ksize), sigmaX=sigma, sigmaY=sigma,\n                                                borderType=cv2.BORDER_REFLECT_101)\n\n                # re-add channel axis removed by cv2 if input was (H, W, 1)\n                image = image_warped[..., np.newaxis] if image.ndim == 3 and image_warped.ndim == 2 else image_warped\n\n        if dtype.name == \"bool\":\n            image = image > 0.5\n        elif dtype.name != image.dtype.name:\n            image = iadt.restore_dtypes_(image, dtype)\n\n    return image",
    "doc": "Blur an image using gaussian blurring.\n\n    This operation might change the input image in-place.\n\n    dtype support::\n\n        if (backend=\"auto\")::\n\n            * ``uint8``: yes; fully tested (1)\n            * ``uint16``: yes; tested (1)\n            * ``uint32``: yes; tested (2)\n            * ``uint64``: yes; tested (2)\n            * ``int8``: yes; tested (1)\n            * ``int16``: yes; tested (1)\n            * ``int32``: yes; tested (1)\n            * ``int64``: yes; tested (2)\n            * ``float16``: yes; tested (1)\n            * ``float32``: yes; tested (1)\n            * ``float64``: yes; tested (1)\n            * ``float128``: no\n            * ``bool``: yes; tested (1)\n\n            - (1) Handled by ``cv2``. See ``backend=\"cv2\"``.\n            - (2) Handled by ``scipy``. See ``backend=\"scipy\"``.\n\n        if (backend=\"cv2\")::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: no (2)\n            * ``uint64``: no (3)\n            * ``int8``: yes; tested (4)\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested (5)\n            * ``int64``: no (6)\n            * ``float16``: yes; tested (7)\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: no (8)\n            * ``bool``: yes; tested (1)\n\n            - (1) Mapped internally to ``float32``. Otherwise causes ``TypeError: src data type = 0 is not supported``.\n            - (2) Causes ``TypeError: src data type = 6 is not supported``.\n            - (3) Causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957: error: (-213:The function/feature is not\n                  implemented) Unsupported combination of source format (=4), and buffer format (=5) in function\n                  'getLinearRowFilter'``.\n            - (4) Mapped internally to ``int16``. Otherwise causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957:\n                  error: (-213:The function/feature is not implemented) Unsupported combination of source format (=1),\n                  and buffer format (=5) in function 'getLinearRowFilter'``.\n            - (5) Mapped internally to ``float64``. Otherwise causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957:\n                  error: (-213:The function/feature is not implemented) Unsupported combination of source format (=4),\n                  and buffer format (=5) in function 'getLinearRowFilter'``.\n            - (6) Causes ``cv2.error: OpenCV(3.4.5) (...)/filter.cpp:2957: error: (-213:The function/feature is not\n                  implemented) Unsupported combination of source format (=4), and buffer format (=5) in function\n                  'getLinearRowFilter'``.\n            - (7) Mapped internally to ``float32``. Otherwise causes ``TypeError: src data type = 23 is not supported``.\n            - (8) Causes ``TypeError: src data type = 13 is not supported``.\n\n\n        if (backend=\"scipy\")::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested (1)\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: no (2)\n            * ``bool``: yes; tested (3)\n\n            - (1) Mapped internally to ``float32``. Otherwise causes ``RuntimeError: array type dtype('float16')\n                  not supported``.\n            - (2) Causes ``RuntimeError: array type dtype('float128') not supported``.\n            - (3) Mapped internally to ``float32``. Otherwise too inaccurate.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        The image to blur. Expected to be of shape ``(H, W)`` or ``(H, W, C)``.\n\n    sigma : number\n        Standard deviation of the gaussian blur. Larger numbers result in more large-scale blurring, which is overall\n        slower than small-scale blurring.\n\n    ksize : None or int, optional\n        Size in height/width of the gaussian kernel. This argument is only understood by the ``cv2`` backend.\n        If it is set to None, an appropriate value for `ksize` will automatically be derived from `sigma`.\n        The value is chosen tighter for larger sigmas to avoid as much as possible very large kernel sizes\n        and therey improve performance.\n\n    backend : {'auto', 'cv2', 'scipy'}, optional\n        Backend library to use. If ``auto``, then the likely best library will be automatically picked per image. That\n        is usually equivalent to ``cv2`` (OpenCV) and it will fall back to ``scipy`` for datatypes not supported by\n        OpenCV.\n\n    eps : number, optional\n        A threshold used to decide whether `sigma` can be considered zero.\n\n    Returns\n    -------\n    image : numpy.ndarray\n        The blurred image. Same shape and dtype as the input."
  },
  {
    "code": "def MotionBlur(k=5, angle=(0, 360), direction=(-1.0, 1.0), order=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sharpens images and overlays the result with the original image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    k : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Kernel size to use.\n\n            * If a single int, then that value will be used for the height\n              and width of the kernel.\n            * If a tuple of two ints ``(a, b)``, then the kernel size will be\n              sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the kernel\n              size for the nth image.\n\n    angle : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of the motion blur in degrees (clockwise, relative to top center direction).\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Forward/backward direction of the motion blur. Lower values towards -1.0 will point the motion blur towards\n        the back (with angle provided via `angle`). Higher values towards 1.0 will point the motion blur forward.\n        A value of 0.0 leads to a uniformly (but still angled) motion blur.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    order : int or iterable of int or imgaug.ALL or imgaug.parameters.StochasticParameter, optional\n        Interpolation order to use when rotating the kernel according to `angle`.\n        See :func:`imgaug.augmenters.geometric.Affine.__init__`.\n        Recommended to be ``0`` or ``1``, with ``0`` being faster, but less continuous/smooth as `angle` is changed,\n        particularly around multiple of 45 degrees.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.MotionBlur(k=15)\n\n    Create a motion blur augmenter with kernel size of 15x15.\n\n    >>> aug = iaa.MotionBlur(k=15, angle=[-45, 45])\n\n    Create a motion blur augmenter with kernel size of 15x15 and a blur angle of either -45 or 45 degrees (randomly\n    picked per image).\n\n    \"\"\"\n    # TODO allow (1, None) and set to identity matrix if k == 1\n    k_param = iap.handle_discrete_param(k, \"k\", value_range=(3, None), tuple_to_uniform=True, list_to_choice=True,\n                                        allow_floats=False)\n    angle_param = iap.handle_continuous_param(angle, \"angle\", value_range=None, tuple_to_uniform=True,\n                                              list_to_choice=True)\n    direction_param = iap.handle_continuous_param(direction, \"direction\", value_range=(-1.0-1e-6, 1.0+1e-6),\n                                                  tuple_to_uniform=True, list_to_choice=True)\n\n    def create_matrices(image, nb_channels, random_state_func):\n        # avoid cyclic import between blur and geometric\n        from . import geometric as iaa_geometric\n\n        # force discrete for k_sample via int() in case of stochastic parameter\n        k_sample = int(k_param.draw_sample(random_state=random_state_func))\n        angle_sample = angle_param.draw_sample(random_state=random_state_func)\n        direction_sample = direction_param.draw_sample(random_state=random_state_func)\n\n        k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1\n        direction_sample = np.clip(direction_sample, -1.0, 1.0)\n        direction_sample = (direction_sample + 1.0) / 2.0\n\n        matrix = np.zeros((k_sample, k_sample), dtype=np.float32)\n        matrix[:, k_sample//2] = np.linspace(float(direction_sample), 1.0 - float(direction_sample), num=k_sample)\n        rot = iaa_geometric.Affine(rotate=angle_sample, order=order)\n        matrix = (rot.augment_image((matrix * 255).astype(np.uint8)) / 255.0).astype(np.float32)\n\n        return [matrix/np.sum(matrix)] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return iaa_convolutional.Convolve(create_matrices, name=name, deterministic=deterministic,\n                                      random_state=random_state)",
    "doc": "Augmenter that sharpens images and overlays the result with the original image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    k : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Kernel size to use.\n\n            * If a single int, then that value will be used for the height\n              and width of the kernel.\n            * If a tuple of two ints ``(a, b)``, then the kernel size will be\n              sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the kernel\n              size for the nth image.\n\n    angle : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of the motion blur in degrees (clockwise, relative to top center direction).\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Forward/backward direction of the motion blur. Lower values towards -1.0 will point the motion blur towards\n        the back (with angle provided via `angle`). Higher values towards 1.0 will point the motion blur forward.\n        A value of 0.0 leads to a uniformly (but still angled) motion blur.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    order : int or iterable of int or imgaug.ALL or imgaug.parameters.StochasticParameter, optional\n        Interpolation order to use when rotating the kernel according to `angle`.\n        See :func:`imgaug.augmenters.geometric.Affine.__init__`.\n        Recommended to be ``0`` or ``1``, with ``0`` being faster, but less continuous/smooth as `angle` is changed,\n        particularly around multiple of 45 degrees.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.MotionBlur(k=15)\n\n    Create a motion blur augmenter with kernel size of 15x15.\n\n    >>> aug = iaa.MotionBlur(k=15, angle=[-45, 45])\n\n    Create a motion blur augmenter with kernel size of 15x15 and a blur angle of either -45 or 45 degrees (randomly\n    picked per image)."
  },
  {
    "code": "def Clouds(name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to draw clouds in images.\n\n    This is a wrapper around ``CloudLayer``. It executes 1 to 2 layers per image, leading to varying densities\n    and frequency patterns of clouds.\n\n    This augmenter seems to be fairly robust w.r.t. the image size. Tested with ``96x128``, ``192x256``\n    and ``960x1280``.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Clouds()\n\n    Creates an augmenter that adds clouds to images.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return meta.SomeOf((1, 2), children=[\n        CloudLayer(\n            intensity_mean=(196, 255), intensity_freq_exponent=(-2.5, -2.0), intensity_coarse_scale=10,\n            alpha_min=0, alpha_multiplier=(0.25, 0.75), alpha_size_px_max=(2, 8), alpha_freq_exponent=(-2.5, -2.0),\n            sparsity=(0.8, 1.0), density_multiplier=(0.5, 1.0)\n        ),\n        CloudLayer(\n            intensity_mean=(196, 255), intensity_freq_exponent=(-2.0, -1.0), intensity_coarse_scale=10,\n            alpha_min=0, alpha_multiplier=(0.5, 1.0), alpha_size_px_max=(64, 128), alpha_freq_exponent=(-2.0, -1.0),\n            sparsity=(1.0, 1.4), density_multiplier=(0.8, 1.5)\n        )\n    ], random_order=False, name=name, deterministic=deterministic, random_state=random_state)",
    "doc": "Augmenter to draw clouds in images.\n\n    This is a wrapper around ``CloudLayer``. It executes 1 to 2 layers per image, leading to varying densities\n    and frequency patterns of clouds.\n\n    This augmenter seems to be fairly robust w.r.t. the image size. Tested with ``96x128``, ``192x256``\n    and ``960x1280``.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Clouds()\n\n    Creates an augmenter that adds clouds to images."
  },
  {
    "code": "def Fog(name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to draw fog in images.\n\n    This is a wrapper around ``CloudLayer``. It executes a single layer per image with a configuration leading\n    to fairly dense clouds with low-frequency patterns.\n\n    This augmenter seems to be fairly robust w.r.t. the image size. Tested with ``96x128``, ``192x256``\n    and ``960x1280``.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Fog()\n\n    Creates an augmenter that adds fog to images.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return CloudLayer(\n        intensity_mean=(220, 255), intensity_freq_exponent=(-2.0, -1.5), intensity_coarse_scale=2,\n        alpha_min=(0.7, 0.9), alpha_multiplier=0.3, alpha_size_px_max=(2, 8), alpha_freq_exponent=(-4.0, -2.0),\n        sparsity=0.9, density_multiplier=(0.4, 0.9),\n        name=name, deterministic=deterministic, random_state=random_state\n    )",
    "doc": "Augmenter to draw fog in images.\n\n    This is a wrapper around ``CloudLayer``. It executes a single layer per image with a configuration leading\n    to fairly dense clouds with low-frequency patterns.\n\n    This augmenter seems to be fairly robust w.r.t. the image size. Tested with ``96x128``, ``192x256``\n    and ``960x1280``.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Fog()\n\n    Creates an augmenter that adds fog to images."
  },
  {
    "code": "def Snowflakes(density=(0.005, 0.075), density_uniformity=(0.3, 0.9), flake_size=(0.2, 0.7),\n               flake_size_uniformity=(0.4, 0.8), angle=(-30, 30), speed=(0.007, 0.03),\n               name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter to add falling snowflakes to images.\n\n    This is a wrapper around ``SnowflakesLayer``. It executes 1 to 3 layers per image.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    density : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Density of the snowflake layer, as a probability of each pixel in low resolution space to be a snowflake.\n        Valid value range is ``(0.0, 1.0)``. Recommended to be around ``(0.01, 0.075)``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    density_uniformity : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Size uniformity of the snowflakes. Higher values denote more similarly sized snowflakes.\n        Valid value range is ``(0.0, 1.0)``. Recommended to be around ``0.5``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    flake_size : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Size of the snowflakes. This parameter controls the resolution at which snowflakes are sampled.\n        Higher values mean that the resolution is closer to the input image's resolution and hence each sampled\n        snowflake will be smaller (because of the smaller pixel size).\n\n        Valid value range is ``[0.0, 1.0)``. Recommended values:\n\n            * On ``96x128`` a value of ``(0.1, 0.4)`` worked well.\n            * On ``192x256`` a value of ``(0.2, 0.7)`` worked well.\n            * On ``960x1280`` a value of ``(0.7, 0.95)`` worked well.\n\n        Allowed datatypes:\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    flake_size_uniformity : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Controls the size uniformity of the snowflakes. Higher values mean that the snowflakes are more similarly\n        sized. Valid value range is ``(0.0, 1.0)``. Recommended to be around ``0.5``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    angle : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Angle in degrees of motion blur applied to the snowflakes, where ``0.0`` is motion blur that points straight\n        upwards. Recommended to be around ``(-30, 30)``.\n        See also :func:`imgaug.augmenters.blur.MotionBlur.__init__`.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    speed : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Perceived falling speed of the snowflakes. This parameter controls the motion blur's kernel size.\n        It follows roughly the form ``kernel_size = image_size * speed``. Hence,\n        Values around ``1.0`` denote that the motion blur should \"stretch\" each snowflake over the whole image.\n\n        Valid value range is ``(0.0, 1.0)``. Recommended values:\n\n            * On ``96x128`` a value of ``(0.01, 0.05)`` worked well.\n            * On ``192x256`` a value of ``(0.007, 0.03)`` worked well.\n            * On ``960x1280`` a value of ``(0.001, 0.03)`` worked well.\n\n\n        Allowed datatypes:\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Snowflakes(flake_size=(0.1, 0.4), speed=(0.01, 0.05))\n\n    Adds snowflakes to small images (around ``96x128``).\n\n    >>> aug = iaa.Snowflakes(flake_size=(0.2, 0.7), speed=(0.007, 0.03))\n\n    Adds snowflakes to medium-sized images (around ``192x256``).\n\n    >>> aug = iaa.Snowflakes(flake_size=(0.7, 0.95), speed=(0.001, 0.03))\n\n    Adds snowflakes to large images (around ``960x1280``).\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    layer = SnowflakesLayer(\n        density=density, density_uniformity=density_uniformity,\n        flake_size=flake_size, flake_size_uniformity=flake_size_uniformity,\n        angle=angle, speed=speed,\n        blur_sigma_fraction=(0.0001, 0.001)\n    )\n\n    return meta.SomeOf(\n        (1, 3), children=[layer.deepcopy() for _ in range(3)],\n        random_order=False, name=name, deterministic=deterministic, random_state=random_state\n    )",
    "doc": "Augmenter to add falling snowflakes to images.\n\n    This is a wrapper around ``SnowflakesLayer``. It executes 1 to 3 layers per image.\n\n    dtype support::\n\n        * ``uint8``: yes; tested\n        * ``uint16``: no (1)\n        * ``uint32``: no (1)\n        * ``uint64``: no (1)\n        * ``int8``: no (1)\n        * ``int16``: no (1)\n        * ``int32``: no (1)\n        * ``int64``: no (1)\n        * ``float16``: no (1)\n        * ``float32``: no (1)\n        * ``float64``: no (1)\n        * ``float128``: no (1)\n        * ``bool``: no (1)\n\n        - (1) Parameters of this augmenter are optimized for the value range of uint8.\n              While other dtypes may be accepted, they will lead to images augmented in\n              ways inappropriate for the respective dtype.\n\n    Parameters\n    ----------\n    density : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Density of the snowflake layer, as a probability of each pixel in low resolution space to be a snowflake.\n        Valid value range is ``(0.0, 1.0)``. Recommended to be around ``(0.01, 0.075)``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    density_uniformity : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Size uniformity of the snowflakes. Higher values denote more similarly sized snowflakes.\n        Valid value range is ``(0.0, 1.0)``. Recommended to be around ``0.5``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    flake_size : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Size of the snowflakes. This parameter controls the resolution at which snowflakes are sampled.\n        Higher values mean that the resolution is closer to the input image's resolution and hence each sampled\n        snowflake will be smaller (because of the smaller pixel size).\n\n        Valid value range is ``[0.0, 1.0)``. Recommended values:\n\n            * On ``96x128`` a value of ``(0.1, 0.4)`` worked well.\n            * On ``192x256`` a value of ``(0.2, 0.7)`` worked well.\n            * On ``960x1280`` a value of ``(0.7, 0.95)`` worked well.\n\n        Allowed datatypes:\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    flake_size_uniformity : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Controls the size uniformity of the snowflakes. Higher values mean that the snowflakes are more similarly\n        sized. Valid value range is ``(0.0, 1.0)``. Recommended to be around ``0.5``.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    angle : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Angle in degrees of motion blur applied to the snowflakes, where ``0.0`` is motion blur that points straight\n        upwards. Recommended to be around ``(-30, 30)``.\n        See also :func:`imgaug.augmenters.blur.MotionBlur.__init__`.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    speed : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        Perceived falling speed of the snowflakes. This parameter controls the motion blur's kernel size.\n        It follows roughly the form ``kernel_size = image_size * speed``. Hence,\n        Values around ``1.0`` denote that the motion blur should \"stretch\" each snowflake over the whole image.\n\n        Valid value range is ``(0.0, 1.0)``. Recommended values:\n\n            * On ``96x128`` a value of ``(0.01, 0.05)`` worked well.\n            * On ``192x256`` a value of ``(0.007, 0.03)`` worked well.\n            * On ``960x1280`` a value of ``(0.001, 0.03)`` worked well.\n\n\n        Allowed datatypes:\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the continuous range ``[a, b]`` will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image from that parameter.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Snowflakes(flake_size=(0.1, 0.4), speed=(0.01, 0.05))\n\n    Adds snowflakes to small images (around ``96x128``).\n\n    >>> aug = iaa.Snowflakes(flake_size=(0.2, 0.7), speed=(0.007, 0.03))\n\n    Adds snowflakes to medium-sized images (around ``192x256``).\n\n    >>> aug = iaa.Snowflakes(flake_size=(0.7, 0.95), speed=(0.001, 0.03))\n\n    Adds snowflakes to large images (around ``960x1280``)."
  },
  {
    "code": "def get_arr_int(self, background_threshold=0.01, background_class_id=None):\n        \"\"\"\n        Get the segmentation map array as an integer array of shape (H, W).\n\n        Each pixel in that array contains an integer value representing the pixel's class.\n        If multiple classes overlap, the one with the highest local float value is picked.\n        If that highest local value is below `background_threshold`, the method instead uses\n        the background class id as the pixel's class value.\n        By default, class id 0 is the background class. This may only be changed if the original\n        input to the segmentation map object was an integer map.\n\n        Parameters\n        ----------\n        background_threshold : float, optional\n            At each pixel, each class-heatmap has a value between 0.0 and 1.0. If none of the\n            class-heatmaps has a value above this threshold, the method uses the background class\n            id instead.\n\n        background_class_id : None or int, optional\n            Class id to fall back to if no class-heatmap passes the threshold at a spatial\n            location. May only be provided if the original input was an integer mask and in these\n            cases defaults to 0. If the input were float or boolean masks, the background class id\n            may not be set as it is assumed that the background is implicitly defined\n            as 'any spatial location that has zero-like values in all masks'.\n\n        Returns\n        -------\n        result : (H,W) ndarray\n            Segmentation map array (int32).\n            If the original input consisted of boolean or float masks, then the highest possible\n            class id is ``1+C``, where ``C`` is the number of provided float/boolean masks. The value\n            ``0`` in the integer mask then denotes the background class.\n\n        \"\"\"\n        if self.input_was[0] in [\"bool\", \"float\"]:\n            ia.do_assert(background_class_id is None,\n                         \"The background class id may only be changed if the original input to SegmentationMapOnImage \"\n                         + \"was an *integer* based segmentation map.\")\n\n        if background_class_id is None:\n            background_class_id = 0\n\n        channelwise_max_idx = np.argmax(self.arr, axis=2)\n        # for bool and float input masks, we assume that the background is implicitly given,\n        # i.e. anything where all masks/channels have zero-like values\n        # for int, we assume that the background class is explicitly given and has the index 0\n        if self.input_was[0] in [\"bool\", \"float\"]:\n            result = 1 + channelwise_max_idx\n        else:  # integer mask was provided\n            result = channelwise_max_idx\n        if background_threshold is not None and background_threshold > 0:\n            probs = np.amax(self.arr, axis=2)\n            result[probs < background_threshold] = background_class_id\n\n        return result.astype(np.int32)",
    "doc": "Get the segmentation map array as an integer array of shape (H, W).\n\n        Each pixel in that array contains an integer value representing the pixel's class.\n        If multiple classes overlap, the one with the highest local float value is picked.\n        If that highest local value is below `background_threshold`, the method instead uses\n        the background class id as the pixel's class value.\n        By default, class id 0 is the background class. This may only be changed if the original\n        input to the segmentation map object was an integer map.\n\n        Parameters\n        ----------\n        background_threshold : float, optional\n            At each pixel, each class-heatmap has a value between 0.0 and 1.0. If none of the\n            class-heatmaps has a value above this threshold, the method uses the background class\n            id instead.\n\n        background_class_id : None or int, optional\n            Class id to fall back to if no class-heatmap passes the threshold at a spatial\n            location. May only be provided if the original input was an integer mask and in these\n            cases defaults to 0. If the input were float or boolean masks, the background class id\n            may not be set as it is assumed that the background is implicitly defined\n            as 'any spatial location that has zero-like values in all masks'.\n\n        Returns\n        -------\n        result : (H,W) ndarray\n            Segmentation map array (int32).\n            If the original input consisted of boolean or float masks, then the highest possible\n            class id is ``1+C``, where ``C`` is the number of provided float/boolean masks. The value\n            ``0`` in the integer mask then denotes the background class."
  },
  {
    "code": "def draw(self, size=None, background_threshold=0.01, background_class_id=None, colors=None,\n             return_foreground_mask=False):\n        \"\"\"\n        Render the segmentation map as an RGB image.\n\n        Parameters\n        ----------\n        size : None or float or iterable of int or iterable of float, optional\n            Size of the rendered RGB image as ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            If set to None, no resizing is performed and the size of the segmentation map array is used.\n\n        background_threshold : float, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        background_class_id : None or int, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        colors : None or list of tuple of int, optional\n            Colors to use. One for each class to draw. If None, then default colors will be used.\n\n        return_foreground_mask : bool, optional\n            Whether to return a mask of the same size as the drawn segmentation map, containing\n            True at any spatial location that is not the background class and False everywhere else.\n\n        Returns\n        -------\n        segmap_drawn : (H,W,3) ndarray\n            Rendered segmentation map (dtype is uint8).\n\n        foreground_mask : (H,W) ndarray\n            Mask indicating the locations of foreground classes (dtype is bool).\n            This value is only returned if `return_foreground_mask` is True.\n\n        \"\"\"\n        arr = self.get_arr_int(background_threshold=background_threshold, background_class_id=background_class_id)\n        nb_classes = 1 + np.max(arr)\n        segmap_drawn = np.zeros((arr.shape[0], arr.shape[1], 3), dtype=np.uint8)\n        if colors is None:\n            colors = SegmentationMapOnImage.DEFAULT_SEGMENT_COLORS\n        ia.do_assert(nb_classes <= len(colors),\n                     \"Can't draw all %d classes as it would exceed the maximum number of %d available colors.\" % (\n                         nb_classes, len(colors),))\n\n        ids_in_map = np.unique(arr)\n        for c, color in zip(sm.xrange(nb_classes), colors):\n            if c in ids_in_map:\n                class_mask = (arr == c)\n                segmap_drawn[class_mask] = color\n\n        if return_foreground_mask:\n            background_class_id = 0 if background_class_id is None else background_class_id\n            foreground_mask = (arr != background_class_id)\n        else:\n            foreground_mask = None\n\n        if size is not None:\n            segmap_drawn = ia.imresize_single_image(segmap_drawn, size, interpolation=\"nearest\")\n            if foreground_mask is not None:\n                foreground_mask = ia.imresize_single_image(\n                    foreground_mask.astype(np.uint8), size, interpolation=\"nearest\") > 0\n\n        if foreground_mask is not None:\n            return segmap_drawn, foreground_mask\n        return segmap_drawn",
    "doc": "Render the segmentation map as an RGB image.\n\n        Parameters\n        ----------\n        size : None or float or iterable of int or iterable of float, optional\n            Size of the rendered RGB image as ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            If set to None, no resizing is performed and the size of the segmentation map array is used.\n\n        background_threshold : float, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        background_class_id : None or int, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        colors : None or list of tuple of int, optional\n            Colors to use. One for each class to draw. If None, then default colors will be used.\n\n        return_foreground_mask : bool, optional\n            Whether to return a mask of the same size as the drawn segmentation map, containing\n            True at any spatial location that is not the background class and False everywhere else.\n\n        Returns\n        -------\n        segmap_drawn : (H,W,3) ndarray\n            Rendered segmentation map (dtype is uint8).\n\n        foreground_mask : (H,W) ndarray\n            Mask indicating the locations of foreground classes (dtype is bool).\n            This value is only returned if `return_foreground_mask` is True."
  },
  {
    "code": "def draw_on_image(self, image, alpha=0.75, resize=\"segmentation_map\", background_threshold=0.01,\n                      background_class_id=None, colors=None, draw_background=False):\n        \"\"\"\n        Draw the segmentation map as an overlay over an image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            Image onto which to draw the segmentation map. Dtype is expected to be uint8.\n\n        alpha : float, optional\n            Alpha/opacity value to use for the mixing of image and segmentation map.\n            Higher values mean that the segmentation map will be more visible and the image less visible.\n\n        resize : {'segmentation_map', 'image'}, optional\n            In case of size differences between the image and segmentation map, either the image or\n            the segmentation map can be resized. This parameter controls which of the two will be\n            resized to the other's size.\n\n        background_threshold : float, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        background_class_id : None or int, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        colors : None or list of tuple of int, optional\n            Colors to use. One for each class to draw. If None, then default colors will be used.\n\n        draw_background : bool, optional\n            If True, the background will be drawn like any other class.\n            If False, the background will not be drawn, i.e. the respective background pixels\n            will be identical with the image's RGB color at the corresponding spatial location\n            and no color overlay will be applied.\n\n        Returns\n        -------\n        mix : (H,W,3) ndarray\n            Rendered overlays (dtype is uint8).\n\n        \"\"\"\n        # assert RGB image\n        ia.do_assert(image.ndim == 3)\n        ia.do_assert(image.shape[2] == 3)\n        ia.do_assert(image.dtype.type == np.uint8)\n\n        ia.do_assert(0 - 1e-8 <= alpha <= 1.0 + 1e-8)\n        ia.do_assert(resize in [\"segmentation_map\", \"image\"])\n\n        if resize == \"image\":\n            image = ia.imresize_single_image(image, self.arr.shape[0:2], interpolation=\"cubic\")\n\n        segmap_drawn, foreground_mask = self.draw(\n            background_threshold=background_threshold,\n            background_class_id=background_class_id,\n            size=image.shape[0:2] if resize == \"segmentation_map\" else None,\n            colors=colors,\n            return_foreground_mask=True\n        )\n\n        if draw_background:\n            mix = np.clip(\n                (1-alpha) * image + alpha * segmap_drawn,\n                0,\n                255\n            ).astype(np.uint8)\n        else:\n            foreground_mask = foreground_mask[..., np.newaxis]\n            mix = np.zeros_like(image)\n            mix += (~foreground_mask).astype(np.uint8) * image\n            mix += foreground_mask.astype(np.uint8) * np.clip(\n                (1-alpha) * image + alpha * segmap_drawn,\n                0,\n                255\n            ).astype(np.uint8)\n        return mix",
    "doc": "Draw the segmentation map as an overlay over an image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            Image onto which to draw the segmentation map. Dtype is expected to be uint8.\n\n        alpha : float, optional\n            Alpha/opacity value to use for the mixing of image and segmentation map.\n            Higher values mean that the segmentation map will be more visible and the image less visible.\n\n        resize : {'segmentation_map', 'image'}, optional\n            In case of size differences between the image and segmentation map, either the image or\n            the segmentation map can be resized. This parameter controls which of the two will be\n            resized to the other's size.\n\n        background_threshold : float, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        background_class_id : None or int, optional\n            See :func:`imgaug.SegmentationMapOnImage.get_arr_int`.\n\n        colors : None or list of tuple of int, optional\n            Colors to use. One for each class to draw. If None, then default colors will be used.\n\n        draw_background : bool, optional\n            If True, the background will be drawn like any other class.\n            If False, the background will not be drawn, i.e. the respective background pixels\n            will be identical with the image's RGB color at the corresponding spatial location\n            and no color overlay will be applied.\n\n        Returns\n        -------\n        mix : (H,W,3) ndarray\n            Rendered overlays (dtype is uint8)."
  },
  {
    "code": "def pad(self, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0.0):\n        \"\"\"\n        Pad the segmentation map on its top/right/bottom/left side.\n\n        Parameters\n        ----------\n        top : int, optional\n            Amount of pixels to add at the top side of the segmentation map. Must be 0 or greater.\n\n        right : int, optional\n            Amount of pixels to add at the right side of the segmentation map. Must be 0 or greater.\n\n        bottom : int, optional\n            Amount of pixels to add at the bottom side of the segmentation map. Must be 0 or greater.\n\n        left : int, optional\n            Amount of pixels to add at the left side of the segmentation map. Must be 0 or greater.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Padded segmentation map of height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n        \"\"\"\n        arr_padded = ia.pad(self.arr, top=top, right=right, bottom=bottom, left=left, mode=mode, cval=cval)\n        segmap = SegmentationMapOnImage(arr_padded, shape=self.shape)\n        segmap.input_was = self.input_was\n        return segmap",
    "doc": "Pad the segmentation map on its top/right/bottom/left side.\n\n        Parameters\n        ----------\n        top : int, optional\n            Amount of pixels to add at the top side of the segmentation map. Must be 0 or greater.\n\n        right : int, optional\n            Amount of pixels to add at the right side of the segmentation map. Must be 0 or greater.\n\n        bottom : int, optional\n            Amount of pixels to add at the bottom side of the segmentation map. Must be 0 or greater.\n\n        left : int, optional\n            Amount of pixels to add at the left side of the segmentation map. Must be 0 or greater.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Padded segmentation map of height ``H'=H+top+bottom`` and width ``W'=W+left+right``."
  },
  {
    "code": "def pad_to_aspect_ratio(self, aspect_ratio, mode=\"constant\", cval=0.0, return_pad_amounts=False):\n        \"\"\"\n        Pad the segmentation map on its sides so that its matches a target aspect ratio.\n\n        Depending on which dimension is smaller (height or width), only the corresponding\n        sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n        be padded equally.\n\n        Parameters\n        ----------\n        aspect_ratio : float\n            Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n            as much width as height.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        return_pad_amounts : bool, optional\n            If False, then only the padded image will be returned. If True, a tuple with two\n            entries will be returned, where the first entry is the padded image and the second\n            entry are the amounts by which each image side was padded. These amounts are again a\n            tuple of the form (top, right, bottom, left), with each value being an integer.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Padded segmentation map as SegmentationMapOnImage object.\n\n        pad_amounts : tuple of int\n            Amounts by which the segmentation map was padded on each side, given as a\n            tuple ``(top, right, bottom, left)``.\n            This tuple is only returned if `return_pad_amounts` was set to True.\n\n        \"\"\"\n        arr_padded, pad_amounts = ia.pad_to_aspect_ratio(self.arr, aspect_ratio=aspect_ratio, mode=mode, cval=cval,\n                                                         return_pad_amounts=True)\n        segmap = SegmentationMapOnImage(arr_padded, shape=self.shape)\n        segmap.input_was = self.input_was\n        if return_pad_amounts:\n            return segmap, pad_amounts\n        else:\n            return segmap",
    "doc": "Pad the segmentation map on its sides so that its matches a target aspect ratio.\n\n        Depending on which dimension is smaller (height or width), only the corresponding\n        sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n        be padded equally.\n\n        Parameters\n        ----------\n        aspect_ratio : float\n            Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n            as much width as height.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        return_pad_amounts : bool, optional\n            If False, then only the padded image will be returned. If True, a tuple with two\n            entries will be returned, where the first entry is the padded image and the second\n            entry are the amounts by which each image side was padded. These amounts are again a\n            tuple of the form (top, right, bottom, left), with each value being an integer.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Padded segmentation map as SegmentationMapOnImage object.\n\n        pad_amounts : tuple of int\n            Amounts by which the segmentation map was padded on each side, given as a\n            tuple ``(top, right, bottom, left)``.\n            This tuple is only returned if `return_pad_amounts` was set to True."
  },
  {
    "code": "def resize(self, sizes, interpolation=\"cubic\"):\n        \"\"\"\n        Resize the segmentation map array to the provided size given the provided interpolation.\n\n        Parameters\n        ----------\n        sizes : float or iterable of int or iterable of float\n            New size of the array in ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        interpolation : None or str or int, optional\n            The interpolation to use during resize.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            Note: The segmentation map is internally stored as multiple float-based heatmaps,\n            making smooth interpolations potentially more reasonable than nearest neighbour\n            interpolation.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Resized segmentation map object.\n\n        \"\"\"\n        arr_resized = ia.imresize_single_image(self.arr, sizes, interpolation=interpolation)\n\n        # cubic interpolation can lead to values outside of [0.0, 1.0],\n        # see https://github.com/opencv/opencv/issues/7195\n        # TODO area interpolation too?\n        arr_resized = np.clip(arr_resized, 0.0, 1.0)\n        segmap = SegmentationMapOnImage(arr_resized, shape=self.shape)\n        segmap.input_was = self.input_was\n        return segmap",
    "doc": "Resize the segmentation map array to the provided size given the provided interpolation.\n\n        Parameters\n        ----------\n        sizes : float or iterable of int or iterable of float\n            New size of the array in ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        interpolation : None or str or int, optional\n            The interpolation to use during resize.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            Note: The segmentation map is internally stored as multiple float-based heatmaps,\n            making smooth interpolations potentially more reasonable than nearest neighbour\n            interpolation.\n\n        Returns\n        -------\n        segmap : imgaug.SegmentationMapOnImage\n            Resized segmentation map object."
  },
  {
    "code": "def to_heatmaps(self, only_nonempty=False, not_none_if_no_nonempty=False):\n        \"\"\"\n        Convert segmentation map to heatmaps object.\n\n        Each segmentation map class will be represented as a single heatmap channel.\n\n        Parameters\n        ----------\n        only_nonempty : bool, optional\n            If True, then only heatmaps for classes that appear in the segmentation map will be\n            generated. Additionally, a list of these class ids will be returned.\n\n        not_none_if_no_nonempty : bool, optional\n            If `only_nonempty` is True and for a segmentation map no channel was non-empty,\n            this function usually returns None as the heatmaps object. If however this parameter\n            is set to True, a heatmaps object with one channel (representing class 0)\n            will be returned as a fallback in these cases.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage or None\n            Segmentation map as a heatmaps object.\n            If `only_nonempty` was set to True and no class appeared in the segmentation map,\n            then this is None.\n\n        class_indices : list of int\n            Class ids (0 to C-1) of the classes that were actually added to the heatmaps.\n            Only returned if `only_nonempty` was set to True.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n        if not only_nonempty:\n            return HeatmapsOnImage.from_0to1(self.arr, self.shape, min_value=0.0, max_value=1.0)\n        else:\n            nonempty_mask = np.sum(self.arr, axis=(0, 1)) > 0 + 1e-4\n            if np.sum(nonempty_mask) == 0:\n                if not_none_if_no_nonempty:\n                    nonempty_mask[0] = True\n                else:\n                    return None, []\n\n            class_indices = np.arange(self.arr.shape[2])[nonempty_mask]\n            channels = self.arr[..., class_indices]\n            return HeatmapsOnImage(channels, self.shape, min_value=0.0, max_value=1.0), class_indices",
    "doc": "Convert segmentation map to heatmaps object.\n\n        Each segmentation map class will be represented as a single heatmap channel.\n\n        Parameters\n        ----------\n        only_nonempty : bool, optional\n            If True, then only heatmaps for classes that appear in the segmentation map will be\n            generated. Additionally, a list of these class ids will be returned.\n\n        not_none_if_no_nonempty : bool, optional\n            If `only_nonempty` is True and for a segmentation map no channel was non-empty,\n            this function usually returns None as the heatmaps object. If however this parameter\n            is set to True, a heatmaps object with one channel (representing class 0)\n            will be returned as a fallback in these cases.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage or None\n            Segmentation map as a heatmaps object.\n            If `only_nonempty` was set to True and no class appeared in the segmentation map,\n            then this is None.\n\n        class_indices : list of int\n            Class ids (0 to C-1) of the classes that were actually added to the heatmaps.\n            Only returned if `only_nonempty` was set to True."
  },
  {
    "code": "def from_heatmaps(heatmaps, class_indices=None, nb_classes=None):\n        \"\"\"\n        Convert heatmaps to segmentation map.\n\n        Assumes that each class is represented as a single heatmap channel.\n\n        Parameters\n        ----------\n        heatmaps : imgaug.HeatmapsOnImage\n            Heatmaps to convert.\n\n        class_indices : None or list of int, optional\n            List of class indices represented by each heatmap channel. See also the\n            secondary output of :func:`imgaug.SegmentationMapOnImage.to_heatmap`.\n            If this is provided, it must have the same length as the number of heatmap channels.\n\n        nb_classes : None or int, optional\n            Number of classes. Must be provided if class_indices is set.\n\n        Returns\n        -------\n        imgaug.SegmentationMapOnImage\n            Segmentation map derived from heatmaps.\n\n        \"\"\"\n        if class_indices is None:\n            return SegmentationMapOnImage(heatmaps.arr_0to1, shape=heatmaps.shape)\n        else:\n            ia.do_assert(nb_classes is not None)\n            ia.do_assert(min(class_indices) >= 0)\n            ia.do_assert(max(class_indices) < nb_classes)\n            ia.do_assert(len(class_indices) == heatmaps.arr_0to1.shape[2])\n            arr_0to1 = heatmaps.arr_0to1\n            arr_0to1_full = np.zeros((arr_0to1.shape[0], arr_0to1.shape[1], nb_classes), dtype=np.float32)\n            for heatmap_channel, mapped_channel in enumerate(class_indices):\n                arr_0to1_full[:, :, mapped_channel] = arr_0to1[:, :, heatmap_channel]\n            return SegmentationMapOnImage(arr_0to1_full, shape=heatmaps.shape)",
    "doc": "Convert heatmaps to segmentation map.\n\n        Assumes that each class is represented as a single heatmap channel.\n\n        Parameters\n        ----------\n        heatmaps : imgaug.HeatmapsOnImage\n            Heatmaps to convert.\n\n        class_indices : None or list of int, optional\n            List of class indices represented by each heatmap channel. See also the\n            secondary output of :func:`imgaug.SegmentationMapOnImage.to_heatmap`.\n            If this is provided, it must have the same length as the number of heatmap channels.\n\n        nb_classes : None or int, optional\n            Number of classes. Must be provided if class_indices is set.\n\n        Returns\n        -------\n        imgaug.SegmentationMapOnImage\n            Segmentation map derived from heatmaps."
  },
  {
    "code": "def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the segmentation map object.\n\n        Returns\n        -------\n        imgaug.SegmentationMapOnImage\n            Deep copy.\n\n        \"\"\"\n        segmap = SegmentationMapOnImage(self.arr, shape=self.shape, nb_classes=self.nb_classes)\n        segmap.input_was = self.input_was\n        return segmap",
    "doc": "Create a deep copy of the segmentation map object.\n\n        Returns\n        -------\n        imgaug.SegmentationMapOnImage\n            Deep copy."
  },
  {
    "code": "def offer(self, p, e: Event):\n        \"\"\"\n        Offer a new event ``s`` at point ``p`` in this queue.\n        \"\"\"\n        existing = self.events_scan.setdefault(\n                p, ([], [], [], []) if USE_VERTICAL else\n                   ([], [], []))\n        # Can use double linked-list for easy insertion at beginning/end\n        '''\n        if e.type == Event.Type.END:\n            existing.insert(0, e)\n        else:\n            existing.append(e)\n        '''\n\n        existing[e.type].append(e)",
    "doc": "Offer a new event ``s`` at point ``p`` in this queue."
  },
  {
    "code": "def get_arr(self):\n        \"\"\"\n        Get the heatmap's array within the value range originally provided in ``__init__()``.\n\n        The HeatmapsOnImage object saves heatmaps internally in the value range ``(min=0.0, max=1.0)``.\n        This function converts the internal representation to ``(min=min_value, max=max_value)``,\n        where ``min_value`` and ``max_value`` are provided upon instantiation of the object.\n\n        Returns\n        -------\n        result : (H,W) ndarray or (H,W,C) ndarray\n            Heatmap array. Dtype is float32.\n\n        \"\"\"\n        if self.arr_was_2d and self.arr_0to1.shape[2] == 1:\n            arr = self.arr_0to1[:, :, 0]\n        else:\n            arr = self.arr_0to1\n\n        eps = np.finfo(np.float32).eps\n        min_is_zero = 0.0 - eps < self.min_value < 0.0 + eps\n        max_is_one = 1.0 - eps < self.max_value < 1.0 + eps\n        if min_is_zero and max_is_one:\n            return np.copy(arr)\n        else:\n            diff = self.max_value - self.min_value\n            return self.min_value + diff * arr",
    "doc": "Get the heatmap's array within the value range originally provided in ``__init__()``.\n\n        The HeatmapsOnImage object saves heatmaps internally in the value range ``(min=0.0, max=1.0)``.\n        This function converts the internal representation to ``(min=min_value, max=max_value)``,\n        where ``min_value`` and ``max_value`` are provided upon instantiation of the object.\n\n        Returns\n        -------\n        result : (H,W) ndarray or (H,W,C) ndarray\n            Heatmap array. Dtype is float32."
  },
  {
    "code": "def draw(self, size=None, cmap=\"jet\"):\n        \"\"\"\n        Render the heatmaps as RGB images.\n\n        Parameters\n        ----------\n        size : None or float or iterable of int or iterable of float, optional\n            Size of the rendered RGB image as ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            If set to None, no resizing is performed and the size of the heatmaps array is used.\n\n        cmap : str or None, optional\n            Color map of ``matplotlib`` to use in order to convert the heatmaps to RGB images.\n            If set to None, no color map will be used and the heatmaps will be converted\n            to simple intensity maps.\n\n        Returns\n        -------\n        heatmaps_drawn : list of (H,W,3) ndarray\n            Rendered heatmaps. One per heatmap array channel. Dtype is uint8.\n\n        \"\"\"\n        heatmaps_uint8 = self.to_uint8()\n        heatmaps_drawn = []\n\n        for c in sm.xrange(heatmaps_uint8.shape[2]):\n            # c:c+1 here, because the additional axis is needed by imresize_single_image\n            heatmap_c = heatmaps_uint8[..., c:c+1]\n\n            if size is not None:\n                heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=\"nearest\")\n            else:\n                heatmap_c_rs = heatmap_c\n            heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0\n\n            if cmap is not None:\n                # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n                import matplotlib.pyplot as plt\n\n                cmap_func = plt.get_cmap(cmap)\n                heatmap_cmapped = cmap_func(heatmap_c_rs)\n                heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)\n            else:\n                heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3))\n\n            heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)\n\n            heatmaps_drawn.append(heatmap_cmapped)\n        return heatmaps_drawn",
    "doc": "Render the heatmaps as RGB images.\n\n        Parameters\n        ----------\n        size : None or float or iterable of int or iterable of float, optional\n            Size of the rendered RGB image as ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n            If set to None, no resizing is performed and the size of the heatmaps array is used.\n\n        cmap : str or None, optional\n            Color map of ``matplotlib`` to use in order to convert the heatmaps to RGB images.\n            If set to None, no color map will be used and the heatmaps will be converted\n            to simple intensity maps.\n\n        Returns\n        -------\n        heatmaps_drawn : list of (H,W,3) ndarray\n            Rendered heatmaps. One per heatmap array channel. Dtype is uint8."
  },
  {
    "code": "def draw_on_image(self, image, alpha=0.75, cmap=\"jet\", resize=\"heatmaps\"):\n        \"\"\"\n        Draw the heatmaps as overlays over an image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            Image onto which to draw the heatmaps. Expected to be of dtype uint8.\n\n        alpha : float, optional\n            Alpha/opacity value to use for the mixing of image and heatmaps.\n            Higher values mean that the heatmaps will be more visible and the image less visible.\n\n        cmap : str or None, optional\n            Color map to use. See :func:`imgaug.HeatmapsOnImage.draw` for details.\n\n        resize : {'heatmaps', 'image'}, optional\n            In case of size differences between the image and heatmaps, either the image or\n            the heatmaps can be resized. This parameter controls which of the two will be resized\n            to the other's size.\n\n        Returns\n        -------\n        mix : list of (H,W,3) ndarray\n            Rendered overlays. One per heatmap array channel. Dtype is uint8.\n\n        \"\"\"\n        # assert RGB image\n        ia.do_assert(image.ndim == 3)\n        ia.do_assert(image.shape[2] == 3)\n        ia.do_assert(image.dtype.type == np.uint8)\n\n        ia.do_assert(0 - 1e-8 <= alpha <= 1.0 + 1e-8)\n        ia.do_assert(resize in [\"heatmaps\", \"image\"])\n\n        if resize == \"image\":\n            image = ia.imresize_single_image(image, self.arr_0to1.shape[0:2], interpolation=\"cubic\")\n\n        heatmaps_drawn = self.draw(\n            size=image.shape[0:2] if resize == \"heatmaps\" else None,\n            cmap=cmap\n        )\n\n        mix = [\n            np.clip((1-alpha) * image + alpha * heatmap_i, 0, 255).astype(np.uint8)\n            for heatmap_i\n            in heatmaps_drawn\n        ]\n\n        return mix",
    "doc": "Draw the heatmaps as overlays over an image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            Image onto which to draw the heatmaps. Expected to be of dtype uint8.\n\n        alpha : float, optional\n            Alpha/opacity value to use for the mixing of image and heatmaps.\n            Higher values mean that the heatmaps will be more visible and the image less visible.\n\n        cmap : str or None, optional\n            Color map to use. See :func:`imgaug.HeatmapsOnImage.draw` for details.\n\n        resize : {'heatmaps', 'image'}, optional\n            In case of size differences between the image and heatmaps, either the image or\n            the heatmaps can be resized. This parameter controls which of the two will be resized\n            to the other's size.\n\n        Returns\n        -------\n        mix : list of (H,W,3) ndarray\n            Rendered overlays. One per heatmap array channel. Dtype is uint8."
  },
  {
    "code": "def invert(self):\n        \"\"\"\n        Inverts each value in the heatmap, shifting low towards high values and vice versa.\n\n        This changes each value to::\n\n            v' = max - (v - min)\n\n        where ``v`` is the value at some spatial location, ``min`` is the minimum value in the heatmap\n        and ``max`` is the maximum value.\n        As the heatmap uses internally a 0.0 to 1.0 representation, this simply becomes ``v' = 1.0 - v``.\n\n        Note that the attributes ``min_value`` and ``max_value`` are not switched. They both keep their values.\n\n        This function can be useful e.g. when working with depth maps, where algorithms might have\n        an easier time representing the furthest away points with zeros, requiring an inverted\n        depth map.\n\n        Returns\n        -------\n        arr_inv : imgaug.HeatmapsOnImage\n            Inverted heatmap.\n\n        \"\"\"\n        arr_inv = HeatmapsOnImage.from_0to1(1 - self.arr_0to1, shape=self.shape, min_value=self.min_value,\n                                            max_value=self.max_value)\n        arr_inv.arr_was_2d = self.arr_was_2d\n        return arr_inv",
    "doc": "Inverts each value in the heatmap, shifting low towards high values and vice versa.\n\n        This changes each value to::\n\n            v' = max - (v - min)\n\n        where ``v`` is the value at some spatial location, ``min`` is the minimum value in the heatmap\n        and ``max`` is the maximum value.\n        As the heatmap uses internally a 0.0 to 1.0 representation, this simply becomes ``v' = 1.0 - v``.\n\n        Note that the attributes ``min_value`` and ``max_value`` are not switched. They both keep their values.\n\n        This function can be useful e.g. when working with depth maps, where algorithms might have\n        an easier time representing the furthest away points with zeros, requiring an inverted\n        depth map.\n\n        Returns\n        -------\n        arr_inv : imgaug.HeatmapsOnImage\n            Inverted heatmap."
  },
  {
    "code": "def pad(self, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0.0):\n        \"\"\"\n        Pad the heatmaps on their top/right/bottom/left side.\n\n        Parameters\n        ----------\n        top : int, optional\n            Amount of pixels to add at the top side of the heatmaps. Must be 0 or greater.\n\n        right : int, optional\n            Amount of pixels to add at the right side of the heatmaps. Must be 0 or greater.\n\n        bottom : int, optional\n            Amount of pixels to add at the bottom side of the heatmaps. Must be 0 or greater.\n\n        left : int, optional\n            Amount of pixels to add at the left side of the heatmaps. Must be 0 or greater.\n\n        mode : string, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Padded heatmaps of height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n        \"\"\"\n        arr_0to1_padded = ia.pad(self.arr_0to1, top=top, right=right, bottom=bottom, left=left, mode=mode, cval=cval)\n        return HeatmapsOnImage.from_0to1(arr_0to1_padded, shape=self.shape, min_value=self.min_value,\n                                         max_value=self.max_value)",
    "doc": "Pad the heatmaps on their top/right/bottom/left side.\n\n        Parameters\n        ----------\n        top : int, optional\n            Amount of pixels to add at the top side of the heatmaps. Must be 0 or greater.\n\n        right : int, optional\n            Amount of pixels to add at the right side of the heatmaps. Must be 0 or greater.\n\n        bottom : int, optional\n            Amount of pixels to add at the bottom side of the heatmaps. Must be 0 or greater.\n\n        left : int, optional\n            Amount of pixels to add at the left side of the heatmaps. Must be 0 or greater.\n\n        mode : string, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Padded heatmaps of height ``H'=H+top+bottom`` and width ``W'=W+left+right``."
  },
  {
    "code": "def pad_to_aspect_ratio(self, aspect_ratio, mode=\"constant\", cval=0.0, return_pad_amounts=False):\n        \"\"\"\n        Pad the heatmaps on their sides so that they match a target aspect ratio.\n\n        Depending on which dimension is smaller (height or width), only the corresponding\n        sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n        be padded equally.\n\n        Parameters\n        ----------\n        aspect_ratio : float\n            Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n            as much width as height.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        return_pad_amounts : bool, optional\n            If False, then only the padded image will be returned. If True, a tuple with two\n            entries will be returned, where the first entry is the padded image and the second\n            entry are the amounts by which each image side was padded. These amounts are again a\n            tuple of the form (top, right, bottom, left), with each value being an integer.\n\n        Returns\n        -------\n        heatmaps : imgaug.HeatmapsOnImage\n            Padded heatmaps as HeatmapsOnImage object.\n\n        pad_amounts : tuple of int\n            Amounts by which the heatmaps were padded on each side, given as a tuple ``(top, right, bottom, left)``.\n            This tuple is only returned if `return_pad_amounts` was set to True.\n\n        \"\"\"\n        arr_0to1_padded, pad_amounts = ia.pad_to_aspect_ratio(self.arr_0to1, aspect_ratio=aspect_ratio, mode=mode,\n                                                              cval=cval, return_pad_amounts=True)\n        heatmaps = HeatmapsOnImage.from_0to1(arr_0to1_padded, shape=self.shape, min_value=self.min_value,\n                                             max_value=self.max_value)\n        if return_pad_amounts:\n            return heatmaps, pad_amounts\n        else:\n            return heatmaps",
    "doc": "Pad the heatmaps on their sides so that they match a target aspect ratio.\n\n        Depending on which dimension is smaller (height or width), only the corresponding\n        sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n        be padded equally.\n\n        Parameters\n        ----------\n        aspect_ratio : float\n            Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n            as much width as height.\n\n        mode : str, optional\n            Padding mode to use. See :func:`numpy.pad` for details.\n\n        cval : number, optional\n            Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n        return_pad_amounts : bool, optional\n            If False, then only the padded image will be returned. If True, a tuple with two\n            entries will be returned, where the first entry is the padded image and the second\n            entry are the amounts by which each image side was padded. These amounts are again a\n            tuple of the form (top, right, bottom, left), with each value being an integer.\n\n        Returns\n        -------\n        heatmaps : imgaug.HeatmapsOnImage\n            Padded heatmaps as HeatmapsOnImage object.\n\n        pad_amounts : tuple of int\n            Amounts by which the heatmaps were padded on each side, given as a tuple ``(top, right, bottom, left)``.\n            This tuple is only returned if `return_pad_amounts` was set to True."
  },
  {
    "code": "def avg_pool(self, block_size):\n        \"\"\"\n        Resize the heatmap(s) array using average pooling of a given block/kernel size.\n\n        Parameters\n        ----------\n        block_size : int or tuple of int\n            Size of each block of values to pool, aka kernel size. See :func:`imgaug.pool` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps after average pooling.\n\n        \"\"\"\n        arr_0to1_reduced = ia.avg_pool(self.arr_0to1, block_size, cval=0.0)\n        return HeatmapsOnImage.from_0to1(arr_0to1_reduced, shape=self.shape, min_value=self.min_value,\n                                         max_value=self.max_value)",
    "doc": "Resize the heatmap(s) array using average pooling of a given block/kernel size.\n\n        Parameters\n        ----------\n        block_size : int or tuple of int\n            Size of each block of values to pool, aka kernel size. See :func:`imgaug.pool` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps after average pooling."
  },
  {
    "code": "def max_pool(self, block_size):\n        \"\"\"\n        Resize the heatmap(s) array using max-pooling of a given block/kernel size.\n\n        Parameters\n        ----------\n        block_size : int or tuple of int\n            Size of each block of values to pool, aka kernel size. See :func:`imgaug.pool` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps after max-pooling.\n\n        \"\"\"\n        arr_0to1_reduced = ia.max_pool(self.arr_0to1, block_size)\n        return HeatmapsOnImage.from_0to1(arr_0to1_reduced, shape=self.shape, min_value=self.min_value,\n                                         max_value=self.max_value)",
    "doc": "Resize the heatmap(s) array using max-pooling of a given block/kernel size.\n\n        Parameters\n        ----------\n        block_size : int or tuple of int\n            Size of each block of values to pool, aka kernel size. See :func:`imgaug.pool` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps after max-pooling."
  },
  {
    "code": "def resize(self, sizes, interpolation=\"cubic\"):\n        \"\"\"\n        Resize the heatmap(s) array to the provided size given the provided interpolation.\n\n        Parameters\n        ----------\n        sizes : float or iterable of int or iterable of float\n            New size of the array in ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        interpolation : None or str or int, optional\n            The interpolation to use during resize.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Resized heatmaps object.\n\n        \"\"\"\n        arr_0to1_resized = ia.imresize_single_image(self.arr_0to1, sizes, interpolation=interpolation)\n\n        # cubic interpolation can lead to values outside of [0.0, 1.0],\n        # see https://github.com/opencv/opencv/issues/7195\n        # TODO area interpolation too?\n        arr_0to1_resized = np.clip(arr_0to1_resized, 0.0, 1.0)\n\n        return HeatmapsOnImage.from_0to1(arr_0to1_resized, shape=self.shape, min_value=self.min_value,\n                                         max_value=self.max_value)",
    "doc": "Resize the heatmap(s) array to the provided size given the provided interpolation.\n\n        Parameters\n        ----------\n        sizes : float or iterable of int or iterable of float\n            New size of the array in ``(height, width)``.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        interpolation : None or str or int, optional\n            The interpolation to use during resize.\n            See :func:`imgaug.imgaug.imresize_single_image` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Resized heatmaps object."
  },
  {
    "code": "def to_uint8(self):\n        \"\"\"\n        Convert this heatmaps object to a 0-to-255 array.\n\n        Returns\n        -------\n        arr_uint8 : (H,W,C) ndarray\n            Heatmap as a 0-to-255 array (dtype is uint8).\n\n        \"\"\"\n        # TODO this always returns (H,W,C), even if input ndarray was originall (H,W)\n        # does it make sense here to also return (H,W) if self.arr_was_2d?\n        arr_0to255 = np.clip(np.round(self.arr_0to1 * 255), 0, 255)\n        arr_uint8 = arr_0to255.astype(np.uint8)\n        return arr_uint8",
    "doc": "Convert this heatmaps object to a 0-to-255 array.\n\n        Returns\n        -------\n        arr_uint8 : (H,W,C) ndarray\n            Heatmap as a 0-to-255 array (dtype is uint8)."
  },
  {
    "code": "def from_uint8(arr_uint8, shape, min_value=0.0, max_value=1.0):\n        \"\"\"\n        Create a heatmaps object from an heatmap array containing values ranging from 0 to 255.\n\n        Parameters\n        ----------\n        arr_uint8 : (H,W) ndarray or (H,W,C) ndarray\n            Heatmap(s) array, where ``H`` is height, ``W`` is width and ``C`` is the number of heatmap channels.\n            Expected dtype is uint8.\n\n        shape : tuple of int\n            Shape of the image on which the heatmap(s) is/are placed. NOT the shape of the\n            heatmap(s) array, unless it is identical to the image shape (note the likely\n            difference between the arrays in the number of channels).\n            If there is not a corresponding image, use the shape of the heatmaps array.\n\n        min_value : float, optional\n            Minimum value for the heatmaps that the 0-to-255 array represents. This will usually\n            be 0.0. It is used when calling :func:`imgaug.HeatmapsOnImage.get_arr`, which converts the\n            underlying ``(0, 255)`` array to value range ``(min_value, max_value)``.\n\n        max_value : float, optional\n            Maximum value for the heatmaps that 0-to-255 array represents.\n            See parameter `min_value` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps object.\n\n        \"\"\"\n        arr_0to1 = arr_uint8.astype(np.float32) / 255.0\n        return HeatmapsOnImage.from_0to1(arr_0to1, shape, min_value=min_value, max_value=max_value)",
    "doc": "Create a heatmaps object from an heatmap array containing values ranging from 0 to 255.\n\n        Parameters\n        ----------\n        arr_uint8 : (H,W) ndarray or (H,W,C) ndarray\n            Heatmap(s) array, where ``H`` is height, ``W`` is width and ``C`` is the number of heatmap channels.\n            Expected dtype is uint8.\n\n        shape : tuple of int\n            Shape of the image on which the heatmap(s) is/are placed. NOT the shape of the\n            heatmap(s) array, unless it is identical to the image shape (note the likely\n            difference between the arrays in the number of channels).\n            If there is not a corresponding image, use the shape of the heatmaps array.\n\n        min_value : float, optional\n            Minimum value for the heatmaps that the 0-to-255 array represents. This will usually\n            be 0.0. It is used when calling :func:`imgaug.HeatmapsOnImage.get_arr`, which converts the\n            underlying ``(0, 255)`` array to value range ``(min_value, max_value)``.\n\n        max_value : float, optional\n            Maximum value for the heatmaps that 0-to-255 array represents.\n            See parameter `min_value` for details.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Heatmaps object."
  },
  {
    "code": "def from_0to1(arr_0to1, shape, min_value=0.0, max_value=1.0):\n        \"\"\"\n        Create a heatmaps object from an heatmap array containing values ranging from 0.0 to 1.0.\n\n        Parameters\n        ----------\n        arr_0to1 : (H,W) or (H,W,C) ndarray\n            Heatmap(s) array, where ``H`` is height, ``W`` is width and ``C`` is the number of heatmap channels.\n            Expected dtype is float32.\n\n        shape : tuple of ints\n            Shape of the image on which the heatmap(s) is/are placed. NOT the shape of the\n            heatmap(s) array, unless it is identical to the image shape (note the likely\n            difference between the arrays in the number of channels).\n            If there is not a corresponding image, use the shape of the heatmaps array.\n\n        min_value : float, optional\n            Minimum value for the heatmaps that the 0-to-1 array represents. This will usually\n            be 0.0. It is used when calling :func:`imgaug.HeatmapsOnImage.get_arr`, which converts the\n            underlying ``(0.0, 1.0)`` array to value range ``(min_value, max_value)``.\n            E.g. if you started with heatmaps in the range ``(-1.0, 1.0)`` and projected these\n            to (0.0, 1.0), you should call this function with ``min_value=-1.0``, ``max_value=1.0``\n            so that :func:`imgaug.HeatmapsOnImage.get_arr` returns heatmap arrays having value\n            range (-1.0, 1.0).\n\n        max_value : float, optional\n            Maximum value for the heatmaps that to 0-to-255 array represents.\n            See parameter min_value for details.\n\n        Returns\n        -------\n        heatmaps : imgaug.HeatmapsOnImage\n            Heatmaps object.\n\n        \"\"\"\n        heatmaps = HeatmapsOnImage(arr_0to1, shape, min_value=0.0, max_value=1.0)\n        heatmaps.min_value = min_value\n        heatmaps.max_value = max_value\n        return heatmaps",
    "doc": "Create a heatmaps object from an heatmap array containing values ranging from 0.0 to 1.0.\n\n        Parameters\n        ----------\n        arr_0to1 : (H,W) or (H,W,C) ndarray\n            Heatmap(s) array, where ``H`` is height, ``W`` is width and ``C`` is the number of heatmap channels.\n            Expected dtype is float32.\n\n        shape : tuple of ints\n            Shape of the image on which the heatmap(s) is/are placed. NOT the shape of the\n            heatmap(s) array, unless it is identical to the image shape (note the likely\n            difference between the arrays in the number of channels).\n            If there is not a corresponding image, use the shape of the heatmaps array.\n\n        min_value : float, optional\n            Minimum value for the heatmaps that the 0-to-1 array represents. This will usually\n            be 0.0. It is used when calling :func:`imgaug.HeatmapsOnImage.get_arr`, which converts the\n            underlying ``(0.0, 1.0)`` array to value range ``(min_value, max_value)``.\n            E.g. if you started with heatmaps in the range ``(-1.0, 1.0)`` and projected these\n            to (0.0, 1.0), you should call this function with ``min_value=-1.0``, ``max_value=1.0``\n            so that :func:`imgaug.HeatmapsOnImage.get_arr` returns heatmap arrays having value\n            range (-1.0, 1.0).\n\n        max_value : float, optional\n            Maximum value for the heatmaps that to 0-to-255 array represents.\n            See parameter min_value for details.\n\n        Returns\n        -------\n        heatmaps : imgaug.HeatmapsOnImage\n            Heatmaps object."
  },
  {
    "code": "def change_normalization(cls, arr, source, target):\n        \"\"\"\n        Change the value range of a heatmap from one min-max to another min-max.\n\n        E.g. the value range may be changed from min=0.0, max=1.0 to min=-1.0, max=1.0.\n\n        Parameters\n        ----------\n        arr : ndarray\n            Heatmap array to modify.\n\n        source : tuple of float\n            Current value range of the input array, given as (min, max), where both are float values.\n\n        target : tuple of float\n            Desired output value range of the array, given as (min, max), where both are float values.\n\n        Returns\n        -------\n        arr_target : ndarray\n            Input array, with value range projected to the desired target value range.\n\n        \"\"\"\n        ia.do_assert(ia.is_np_array(arr))\n\n        if isinstance(source, HeatmapsOnImage):\n            source = (source.min_value, source.max_value)\n        else:\n            ia.do_assert(isinstance(source, tuple))\n            ia.do_assert(len(source) == 2)\n            ia.do_assert(source[0] < source[1])\n\n        if isinstance(target, HeatmapsOnImage):\n            target = (target.min_value, target.max_value)\n        else:\n            ia.do_assert(isinstance(target, tuple))\n            ia.do_assert(len(target) == 2)\n            ia.do_assert(target[0] < target[1])\n\n        # Check if source and target are the same (with a tiny bit of tolerance)\n        # if so, evade compuation and just copy the array instead.\n        # This is reasonable, as source and target will often both be (0.0, 1.0).\n        eps = np.finfo(arr.dtype).eps\n        mins_same = source[0] - 10*eps < target[0] < source[0] + 10*eps\n        maxs_same = source[1] - 10*eps < target[1] < source[1] + 10*eps\n        if mins_same and maxs_same:\n            return np.copy(arr)\n\n        min_source, max_source = source\n        min_target, max_target = target\n\n        diff_source = max_source - min_source\n        diff_target = max_target - min_target\n\n        arr_0to1 = (arr - min_source) / diff_source\n        arr_target = min_target + arr_0to1 * diff_target\n\n        return arr_target",
    "doc": "Change the value range of a heatmap from one min-max to another min-max.\n\n        E.g. the value range may be changed from min=0.0, max=1.0 to min=-1.0, max=1.0.\n\n        Parameters\n        ----------\n        arr : ndarray\n            Heatmap array to modify.\n\n        source : tuple of float\n            Current value range of the input array, given as (min, max), where both are float values.\n\n        target : tuple of float\n            Desired output value range of the array, given as (min, max), where both are float values.\n\n        Returns\n        -------\n        arr_target : ndarray\n            Input array, with value range projected to the desired target value range."
  },
  {
    "code": "def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the Heatmaps object.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Deep copy.\n\n        \"\"\"\n        return HeatmapsOnImage(self.get_arr(), shape=self.shape, min_value=self.min_value, max_value=self.max_value)",
    "doc": "Create a deep copy of the Heatmaps object.\n\n        Returns\n        -------\n        imgaug.HeatmapsOnImage\n            Deep copy."
  },
  {
    "code": "def setdefault(self, key: str, value: str) -> str:\n        \"\"\"\n        If the header `key` does not exist, then set it to `value`.\n        Returns the header value.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                return item_value.decode(\"latin-1\")\n        self._list.append((set_key, set_value))\n        return value",
    "doc": "If the header `key` does not exist, then set it to `value`.\n        Returns the header value."
  },
  {
    "code": "def append(self, key: str, value: str) -> None:\n        \"\"\"\n        Append a header, preserving any duplicate entries.\n        \"\"\"\n        append_key = key.lower().encode(\"latin-1\")\n        append_value = value.encode(\"latin-1\")\n        self._list.append((append_key, append_value))",
    "doc": "Append a header, preserving any duplicate entries."
  },
  {
    "code": "def request_response(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    is_coroutine = asyncio.iscoroutinefunction(func)\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive=receive)\n        if is_coroutine:\n            response = await func(request)\n        else:\n            response = await run_in_threadpool(func, request)\n        await response(scope, receive, send)\n\n    return app",
    "doc": "Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application."
  },
  {
    "code": "def websocket_session(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\"\n    # assert asyncio.iscoroutinefunction(func), \"WebSocket endpoints must be async\"\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n        await func(session)\n\n    return app",
    "doc": "Takes a coroutine `func(session)`, and returns an ASGI application."
  },
  {
    "code": "def compile_path(\n    path: str\n) -> typing.Tuple[typing.Pattern, str, typing.Dict[str, Convertor]]:\n    \"\"\"\n    Given a path string, like: \"/{username:str}\", return a three-tuple\n    of (regex, format, {param_name:convertor}).\n\n    regex:      \"/(?P<username>[^/]+)\"\n    format:     \"/{username}\"\n    convertors: {\"username\": StringConvertor()}\n    \"\"\"\n    path_regex = \"^\"\n    path_format = \"\"\n\n    idx = 0\n    param_convertors = {}\n    for match in PARAM_REGEX.finditer(path):\n        param_name, convertor_type = match.groups(\"str\")\n        convertor_type = convertor_type.lstrip(\":\")\n        assert (\n            convertor_type in CONVERTOR_TYPES\n        ), f\"Unknown path convertor '{convertor_type}'\"\n        convertor = CONVERTOR_TYPES[convertor_type]\n\n        path_regex += path[idx : match.start()]\n        path_regex += f\"(?P<{param_name}>{convertor.regex})\"\n\n        path_format += path[idx : match.start()]\n        path_format += \"{%s}\" % param_name\n\n        param_convertors[param_name] = convertor\n\n        idx = match.end()\n\n    path_regex += path[idx:] + \"$\"\n    path_format += path[idx:]\n\n    return re.compile(path_regex), path_format, param_convertors",
    "doc": "Given a path string, like: \"/{username:str}\", return a three-tuple\n    of (regex, format, {param_name:convertor}).\n\n    regex:      \"/(?P<username>[^/]+)\"\n    format:     \"/{username}\"\n    convertors: {\"username\": StringConvertor()}"
  },
  {
    "code": "def get_endpoints(\n        self, routes: typing.List[BaseRoute]\n    ) -> typing.List[EndpointInfo]:\n        \"\"\"\n        Given the routes, yields the following information:\n\n        - path\n            eg: /users/\n        - http_method\n            one of 'get', 'post', 'put', 'patch', 'delete', 'options'\n        - func\n            method ready to extract the docstring\n        \"\"\"\n        endpoints_info: list = []\n\n        for route in routes:\n            if isinstance(route, Mount):\n                routes = route.routes or []\n                sub_endpoints = [\n                    EndpointInfo(\n                        path=\"\".join((route.path, sub_endpoint.path)),\n                        http_method=sub_endpoint.http_method,\n                        func=sub_endpoint.func,\n                    )\n                    for sub_endpoint in self.get_endpoints(routes)\n                ]\n                endpoints_info.extend(sub_endpoints)\n\n            elif not isinstance(route, Route) or not route.include_in_schema:\n                continue\n\n            elif inspect.isfunction(route.endpoint) or inspect.ismethod(route.endpoint):\n                for method in route.methods or [\"GET\"]:\n                    if method == \"HEAD\":\n                        continue\n                    endpoints_info.append(\n                        EndpointInfo(route.path, method.lower(), route.endpoint)\n                    )\n            else:\n                for method in [\"get\", \"post\", \"put\", \"patch\", \"delete\", \"options\"]:\n                    if not hasattr(route.endpoint, method):\n                        continue\n                    func = getattr(route.endpoint, method)\n                    endpoints_info.append(\n                        EndpointInfo(route.path, method.lower(), func)\n                    )\n\n        return endpoints_info",
    "doc": "Given the routes, yields the following information:\n\n        - path\n            eg: /users/\n        - http_method\n            one of 'get', 'post', 'put', 'patch', 'delete', 'options'\n        - func\n            method ready to extract the docstring"
  },
  {
    "code": "def parse_docstring(self, func_or_method: typing.Callable) -> dict:\n        \"\"\"\n        Given a function, parse the docstring as YAML and return a dictionary of info.\n        \"\"\"\n        docstring = func_or_method.__doc__\n        if not docstring:\n            return {}\n\n        # We support having regular docstrings before the schema\n        # definition. Here we return just the schema part from\n        # the docstring.\n        docstring = docstring.split(\"---\")[-1]\n\n        parsed = yaml.safe_load(docstring)\n\n        if not isinstance(parsed, dict):\n            # A regular docstring (not yaml formatted) can return\n            # a simple string here, which wouldn't follow the schema.\n            return {}\n\n        return parsed",
    "doc": "Given a function, parse the docstring as YAML and return a dictionary of info."
  },
  {
    "code": "def get_directories(\n        self, directory: str = None, packages: typing.List[str] = None\n    ) -> typing.List[str]:\n        \"\"\"\n        Given `directory` and `packages` arugments, return a list of all the\n        directories that should be used for serving static files from.\n        \"\"\"\n        directories = []\n        if directory is not None:\n            directories.append(directory)\n\n        for package in packages or []:\n            spec = importlib.util.find_spec(package)\n            assert spec is not None, f\"Package {package!r} could not be found.\"\n            assert (\n                spec.origin is not None\n            ), \"Directory 'statics' in package {package!r} could not be found.\"\n            directory = os.path.normpath(os.path.join(spec.origin, \"..\", \"statics\"))\n            assert os.path.isdir(\n                directory\n            ), \"Directory 'statics' in package {package!r} could not be found.\"\n            directories.append(directory)\n\n        return directories",
    "doc": "Given `directory` and `packages` arugments, return a list of all the\n        directories that should be used for serving static files from."
  },
  {
    "code": "def get_path(self, scope: Scope) -> str:\n        \"\"\"\n        Given the ASGI scope, return the `path` string to serve up,\n        with OS specific path seperators, and any '..', '.' components removed.\n        \"\"\"\n        return os.path.normpath(os.path.join(*scope[\"path\"].split(\"/\")))",
    "doc": "Given the ASGI scope, return the `path` string to serve up,\n        with OS specific path seperators, and any '..', '.' components removed."
  },
  {
    "code": "async def get_response(self, path: str, scope: Scope) -> Response:\n        \"\"\"\n        Returns an HTTP response, given the incoming path, method and request headers.\n        \"\"\"\n        if scope[\"method\"] not in (\"GET\", \"HEAD\"):\n            return PlainTextResponse(\"Method Not Allowed\", status_code=405)\n\n        if path.startswith(\"..\"):\n            # Most clients will normalize the path, so we shouldn't normally\n            # get this, but don't allow misbehaving clients to break out of\n            # the static files directory.\n            return PlainTextResponse(\"Not Found\", status_code=404)\n\n        full_path, stat_result = await self.lookup_path(path)\n\n        if stat_result and stat.S_ISREG(stat_result.st_mode):\n            # We have a static file to serve.\n            return self.file_response(full_path, stat_result, scope)\n\n        elif stat_result and stat.S_ISDIR(stat_result.st_mode) and self.html:\n            # We're in HTML mode, and have got a directory URL.\n            # Check if we have 'index.html' file to serve.\n            index_path = os.path.join(path, \"index.html\")\n            full_path, stat_result = await self.lookup_path(index_path)\n            if stat_result is not None and stat.S_ISREG(stat_result.st_mode):\n                if not scope[\"path\"].endswith(\"/\"):\n                    # Directory URLs should redirect to always end in \"/\".\n                    url = URL(scope=scope)\n                    url = url.replace(path=url.path + \"/\")\n                    return RedirectResponse(url=url)\n                return self.file_response(full_path, stat_result, scope)\n\n        if self.html:\n            # Check for '404.html' if we're in HTML mode.\n            full_path, stat_result = await self.lookup_path(\"404.html\")\n            if stat_result is not None and stat.S_ISREG(stat_result.st_mode):\n                return self.file_response(\n                    full_path, stat_result, scope, status_code=404\n                )\n\n        return PlainTextResponse(\"Not Found\", status_code=404)",
    "doc": "Returns an HTTP response, given the incoming path, method and request headers."
  },
  {
    "code": "async def check_config(self) -> None:\n        \"\"\"\n        Perform a one-off configuration check that StaticFiles is actually\n        pointed at a directory, so that we can raise loud errors rather than\n        just returning 404 responses.\n        \"\"\"\n        if self.directory is None:\n            return\n\n        try:\n            stat_result = await aio_stat(self.directory)\n        except FileNotFoundError:\n            raise RuntimeError(\n                f\"StaticFiles directory '{self.directory}' does not exist.\"\n            )\n        if not (stat.S_ISDIR(stat_result.st_mode) or stat.S_ISLNK(stat_result.st_mode)):\n            raise RuntimeError(\n                f\"StaticFiles path '{self.directory}' is not a directory.\"\n            )",
    "doc": "Perform a one-off configuration check that StaticFiles is actually\n        pointed at a directory, so that we can raise loud errors rather than\n        just returning 404 responses."
  },
  {
    "code": "def is_not_modified(\n        self, response_headers: Headers, request_headers: Headers\n    ) -> bool:\n        \"\"\"\n        Given the request and response headers, return `True` if an HTTP\n        \"Not Modified\" response could be returned instead.\n        \"\"\"\n        try:\n            if_none_match = request_headers[\"if-none-match\"]\n            etag = response_headers[\"etag\"]\n            if if_none_match == etag:\n                return True\n        except KeyError:\n            pass\n\n        try:\n            if_modified_since = parsedate(request_headers[\"if-modified-since\"])\n            last_modified = parsedate(response_headers[\"last-modified\"])\n            if (\n                if_modified_since is not None\n                and last_modified is not None\n                and if_modified_since >= last_modified\n            ):\n                return True\n        except KeyError:\n            pass\n\n        return False",
    "doc": "Given the request and response headers, return `True` if an HTTP\n        \"Not Modified\" response could be returned instead."
  },
  {
    "code": "def build_environ(scope: Scope, body: bytes) -> dict:\n    \"\"\"\n    Builds a scope and request body into a WSGI environ object.\n    \"\"\"\n    environ = {\n        \"REQUEST_METHOD\": scope[\"method\"],\n        \"SCRIPT_NAME\": scope.get(\"root_path\", \"\"),\n        \"PATH_INFO\": scope[\"path\"],\n        \"QUERY_STRING\": scope[\"query_string\"].decode(\"ascii\"),\n        \"SERVER_PROTOCOL\": f\"HTTP/{scope['http_version']}\",\n        \"wsgi.version\": (1, 0),\n        \"wsgi.url_scheme\": scope.get(\"scheme\", \"http\"),\n        \"wsgi.input\": io.BytesIO(body),\n        \"wsgi.errors\": sys.stdout,\n        \"wsgi.multithread\": True,\n        \"wsgi.multiprocess\": True,\n        \"wsgi.run_once\": False,\n    }\n\n    # Get server name and port - required in WSGI, not in ASGI\n    server = scope.get(\"server\") or (\"localhost\", 80)\n    environ[\"SERVER_NAME\"] = server[0]\n    environ[\"SERVER_PORT\"] = server[1]\n\n    # Get client IP address\n    if scope.get(\"client\"):\n        environ[\"REMOTE_ADDR\"] = scope[\"client\"][0]\n\n    # Go through headers and make them into environ entries\n    for name, value in scope.get(\"headers\", []):\n        name = name.decode(\"latin1\")\n        if name == \"content-length\":\n            corrected_name = \"CONTENT_LENGTH\"\n        elif name == \"content-type\":\n            corrected_name = \"CONTENT_TYPE\"\n        else:\n            corrected_name = f\"HTTP_{name}\".upper().replace(\"-\", \"_\")\n        # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case\n        value = value.decode(\"latin1\")\n        if corrected_name in environ:\n            value = environ[corrected_name] + \",\" + value\n        environ[corrected_name] = value\n    return environ",
    "doc": "Builds a scope and request body into a WSGI environ object."
  },
  {
    "code": "async def receive(self) -> Message:\n        \"\"\"\n        Receive ASGI websocket messages, ensuring valid state transitions.\n        \"\"\"\n        if self.client_state == WebSocketState.CONNECTING:\n            message = await self._receive()\n            message_type = message[\"type\"]\n            assert message_type == \"websocket.connect\"\n            self.client_state = WebSocketState.CONNECTED\n            return message\n        elif self.client_state == WebSocketState.CONNECTED:\n            message = await self._receive()\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.receive\", \"websocket.disconnect\"}\n            if message_type == \"websocket.disconnect\":\n                self.client_state = WebSocketState.DISCONNECTED\n            return message\n        else:\n            raise RuntimeError(\n                'Cannot call \"receive\" once a disconnect message has been received.'\n            )",
    "doc": "Receive ASGI websocket messages, ensuring valid state transitions."
  },
  {
    "code": "async def send(self, message: Message) -> None:\n        \"\"\"\n        Send ASGI websocket messages, ensuring valid state transitions.\n        \"\"\"\n        if self.application_state == WebSocketState.CONNECTING:\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.accept\", \"websocket.close\"}\n            if message_type == \"websocket.close\":\n                self.application_state = WebSocketState.DISCONNECTED\n            else:\n                self.application_state = WebSocketState.CONNECTED\n            await self._send(message)\n        elif self.application_state == WebSocketState.CONNECTED:\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.send\", \"websocket.close\"}\n            if message_type == \"websocket.close\":\n                self.application_state = WebSocketState.DISCONNECTED\n            await self._send(message)\n        else:\n            raise RuntimeError('Cannot call \"send\" once a close message has been sent.')",
    "doc": "Send ASGI websocket messages, ensuring valid state transitions."
  },
  {
    "code": "def get_top_long_short_abs(positions, top=10):\n    \"\"\"\n    Finds the top long, short, and absolute positions.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    top : int, optional\n        How many of each to find (default 10).\n\n    Returns\n    -------\n    df_top_long : pd.DataFrame\n        Top long positions.\n    df_top_short : pd.DataFrame\n        Top short positions.\n    df_top_abs : pd.DataFrame\n        Top absolute positions.\n    \"\"\"\n\n    positions = positions.drop('cash', axis='columns')\n    df_max = positions.max()\n    df_min = positions.min()\n    df_abs_max = positions.abs().max()\n    df_top_long = df_max[df_max > 0].nlargest(top)\n    df_top_short = df_min[df_min < 0].nsmallest(top)\n    df_top_abs = df_abs_max.nlargest(top)\n    return df_top_long, df_top_short, df_top_abs",
    "doc": "Finds the top long, short, and absolute positions.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    top : int, optional\n        How many of each to find (default 10).\n\n    Returns\n    -------\n    df_top_long : pd.DataFrame\n        Top long positions.\n    df_top_short : pd.DataFrame\n        Top short positions.\n    df_top_abs : pd.DataFrame\n        Top absolute positions."
  },
  {
    "code": "def get_max_median_position_concentration(positions):\n    \"\"\"\n    Finds the max and median long and short position concentrations\n    in each time period specified by the index of positions.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n\n    Returns\n    -------\n    pd.DataFrame\n        Columns are max long, max short, median long, and median short\n        position concentrations. Rows are timeperiods.\n    \"\"\"\n\n    expos = get_percent_alloc(positions)\n    expos = expos.drop('cash', axis=1)\n\n    longs = expos.where(expos.applymap(lambda x: x > 0))\n    shorts = expos.where(expos.applymap(lambda x: x < 0))\n\n    alloc_summary = pd.DataFrame()\n    alloc_summary['max_long'] = longs.max(axis=1)\n    alloc_summary['median_long'] = longs.median(axis=1)\n    alloc_summary['median_short'] = shorts.median(axis=1)\n    alloc_summary['max_short'] = shorts.min(axis=1)\n\n    return alloc_summary",
    "doc": "Finds the max and median long and short position concentrations\n    in each time period specified by the index of positions.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n\n    Returns\n    -------\n    pd.DataFrame\n        Columns are max long, max short, median long, and median short\n        position concentrations. Rows are timeperiods."
  },
  {
    "code": "def extract_pos(positions, cash):\n    \"\"\"\n    Extract position values from backtest object as returned by\n    get_backtest() on the Quantopian research platform.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        timeseries containing one row per symbol (and potentially\n        duplicate datetime indices) and columns for amount and\n        last_sale_price.\n    cash : pd.Series\n        timeseries containing cash in the portfolio.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    \"\"\"\n\n    positions = positions.copy()\n    positions['values'] = positions.amount * positions.last_sale_price\n\n    cash.name = 'cash'\n\n    values = positions.reset_index().pivot_table(index='index',\n                                                 columns='sid',\n                                                 values='values')\n\n    if ZIPLINE:\n        for asset in values.columns:\n            if type(asset) in [Equity, Future]:\n                values[asset] = values[asset] * asset.price_multiplier\n\n    values = values.join(cash).fillna(0)\n\n    # NOTE: Set name of DataFrame.columns to sid, to match the behavior\n    # of DataFrame.join in earlier versions of pandas.\n    values.columns.name = 'sid'\n\n    return values",
    "doc": "Extract position values from backtest object as returned by\n    get_backtest() on the Quantopian research platform.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        timeseries containing one row per symbol (and potentially\n        duplicate datetime indices) and columns for amount and\n        last_sale_price.\n    cash : pd.Series\n        timeseries containing cash in the portfolio.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet."
  },
  {
    "code": "def get_sector_exposures(positions, symbol_sector_map):\n    \"\"\"\n    Sum position exposures by sector.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Contains position values or amounts.\n        - Example\n            index         'AAPL'         'MSFT'        'CHK'        cash\n            2004-01-09    13939.380     -15012.993    -403.870      1477.483\n            2004-01-12    14492.630     -18624.870    142.630       3989.610\n            2004-01-13    -13853.280    13653.640     -100.980      100.000\n    symbol_sector_map : dict or pd.Series\n        Security identifier to sector mapping.\n        Security ids as keys/index, sectors as values.\n        - Example:\n            {'AAPL' : 'Technology'\n             'MSFT' : 'Technology'\n             'CHK' : 'Natural Resources'}\n\n    Returns\n    -------\n    sector_exp : pd.DataFrame\n        Sectors and their allocations.\n        - Example:\n            index         'Technology'    'Natural Resources' cash\n            2004-01-09    -1073.613       -403.870            1477.4830\n            2004-01-12    -4132.240       142.630             3989.6100\n            2004-01-13    -199.640        -100.980            100.0000\n    \"\"\"\n\n    cash = positions['cash']\n    positions = positions.drop('cash', axis=1)\n\n    unmapped_pos = np.setdiff1d(positions.columns.values,\n                                list(symbol_sector_map.keys()))\n    if len(unmapped_pos) > 0:\n        warn_message = \"\"\"Warning: Symbols {} have no sector mapping.\n        They will not be included in sector allocations\"\"\".format(\n            \", \".join(map(str, unmapped_pos)))\n        warnings.warn(warn_message, UserWarning)\n\n    sector_exp = positions.groupby(\n        by=symbol_sector_map, axis=1).sum()\n\n    sector_exp['cash'] = cash\n\n    return sector_exp",
    "doc": "Sum position exposures by sector.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Contains position values or amounts.\n        - Example\n            index         'AAPL'         'MSFT'        'CHK'        cash\n            2004-01-09    13939.380     -15012.993    -403.870      1477.483\n            2004-01-12    14492.630     -18624.870    142.630       3989.610\n            2004-01-13    -13853.280    13653.640     -100.980      100.000\n    symbol_sector_map : dict or pd.Series\n        Security identifier to sector mapping.\n        Security ids as keys/index, sectors as values.\n        - Example:\n            {'AAPL' : 'Technology'\n             'MSFT' : 'Technology'\n             'CHK' : 'Natural Resources'}\n\n    Returns\n    -------\n    sector_exp : pd.DataFrame\n        Sectors and their allocations.\n        - Example:\n            index         'Technology'    'Natural Resources' cash\n            2004-01-09    -1073.613       -403.870            1477.4830\n            2004-01-12    -4132.240       142.630             3989.6100\n            2004-01-13    -199.640        -100.980            100.0000"
  },
  {
    "code": "def get_long_short_pos(positions):\n    \"\"\"\n    Determines the long and short allocations in a portfolio.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n\n    Returns\n    -------\n    df_long_short : pd.DataFrame\n        Long and short allocations as a decimal\n        percentage of the total net liquidation\n    \"\"\"\n\n    pos_wo_cash = positions.drop('cash', axis=1)\n    longs = pos_wo_cash[pos_wo_cash > 0].sum(axis=1).fillna(0)\n    shorts = pos_wo_cash[pos_wo_cash < 0].sum(axis=1).fillna(0)\n    cash = positions.cash\n    net_liquidation = longs + shorts + cash\n    df_pos = pd.DataFrame({'long': longs.divide(net_liquidation, axis='index'),\n                           'short': shorts.divide(net_liquidation,\n                                                  axis='index')})\n    df_pos['net exposure'] = df_pos['long'] + df_pos['short']\n    return df_pos",
    "doc": "Determines the long and short allocations in a portfolio.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n\n    Returns\n    -------\n    df_long_short : pd.DataFrame\n        Long and short allocations as a decimal\n        percentage of the total net liquidation"
  },
  {
    "code": "def compute_style_factor_exposures(positions, risk_factor):\n    \"\"\"\n    Returns style factor exposure of an algorithm's positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in create_risk_tear_sheet\n\n    risk_factor : pd.DataFrame\n        Daily risk factor per asset.\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                         Equity(24   Equity(62\n                           [AAPL])      [ABT])\n        2017-04-03\t  -0.51284     1.39173\n        2017-04-04\t  -0.73381     0.98149\n        2017-04-05\t  -0.90132     1.13981\n    \"\"\"\n\n    positions_wo_cash = positions.drop('cash', axis='columns')\n    gross_exposure = positions_wo_cash.abs().sum(axis='columns')\n\n    style_factor_exposure = positions_wo_cash.multiply(risk_factor) \\\n        .divide(gross_exposure, axis='index')\n    tot_style_factor_exposure = style_factor_exposure.sum(axis='columns',\n                                                          skipna=True)\n\n    return tot_style_factor_exposure",
    "doc": "Returns style factor exposure of an algorithm's positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in create_risk_tear_sheet\n\n    risk_factor : pd.DataFrame\n        Daily risk factor per asset.\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                         Equity(24   Equity(62\n                           [AAPL])      [ABT])\n        2017-04-03\t  -0.51284     1.39173\n        2017-04-04\t  -0.73381     0.98149\n        2017-04-05\t  -0.90132     1.13981"
  },
  {
    "code": "def plot_style_factor_exposures(tot_style_factor_exposure, factor_name=None,\n                                ax=None):\n    \"\"\"\n    Plots DataFrame output of compute_style_factor_exposures as a line graph\n\n    Parameters\n    ----------\n    tot_style_factor_exposure : pd.Series\n        Daily style factor exposures (output of compute_style_factor_exposures)\n        - Time series with decimal style factor exposures\n        - Example:\n            2017-04-24    0.037820\n            2017-04-25    0.016413\n            2017-04-26   -0.021472\n            2017-04-27   -0.024859\n\n    factor_name : string\n        Name of style factor, for use in graph title\n        - Defaults to tot_style_factor_exposure.name\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if factor_name is None:\n        factor_name = tot_style_factor_exposure.name\n\n    ax.plot(tot_style_factor_exposure.index, tot_style_factor_exposure,\n            label=factor_name)\n    avg = tot_style_factor_exposure.mean()\n    ax.axhline(avg, linestyle='-.', label='Mean = {:.3}'.format(avg))\n    ax.axhline(0, color='k', linestyle='-')\n    _, _, y1, y2 = plt.axis()\n    lim = max(abs(y1), abs(y2))\n    ax.set(title='Exposure to {}'.format(factor_name),\n           ylabel='{} \\n weighted exposure'.format(factor_name),\n           ylim=(-lim, lim))\n    ax.legend(frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots DataFrame output of compute_style_factor_exposures as a line graph\n\n    Parameters\n    ----------\n    tot_style_factor_exposure : pd.Series\n        Daily style factor exposures (output of compute_style_factor_exposures)\n        - Time series with decimal style factor exposures\n        - Example:\n            2017-04-24    0.037820\n            2017-04-25    0.016413\n            2017-04-26   -0.021472\n            2017-04-27   -0.024859\n\n    factor_name : string\n        Name of style factor, for use in graph title\n        - Defaults to tot_style_factor_exposure.name"
  },
  {
    "code": "def compute_sector_exposures(positions, sectors, sector_dict=SECTORS):\n    \"\"\"\n    Returns arrays of long, short and gross sector exposures of an algorithm's\n    positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in compute_style_factor_exposures.\n\n    sectors : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - See full explanation in create_risk_tear_sheet\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - Keys are sector codes (e.g. ints or strings) and values are sector\n          names (which must be strings)\n        - Defaults to Morningstar sectors\n    \"\"\"\n\n    sector_ids = sector_dict.keys()\n\n    long_exposures = []\n    short_exposures = []\n    gross_exposures = []\n    net_exposures = []\n\n    positions_wo_cash = positions.drop('cash', axis='columns')\n    long_exposure = positions_wo_cash[positions_wo_cash > 0] \\\n        .sum(axis='columns')\n    short_exposure = positions_wo_cash[positions_wo_cash < 0] \\\n        .abs().sum(axis='columns')\n    gross_exposure = positions_wo_cash.abs().sum(axis='columns')\n\n    for sector_id in sector_ids:\n        in_sector = positions_wo_cash[sectors == sector_id]\n\n        long_sector = in_sector[in_sector > 0] \\\n            .sum(axis='columns').divide(long_exposure)\n        short_sector = in_sector[in_sector < 0] \\\n            .sum(axis='columns').divide(short_exposure)\n        gross_sector = in_sector.abs().sum(axis='columns') \\\n            .divide(gross_exposure)\n        net_sector = long_sector.subtract(short_sector)\n\n        long_exposures.append(long_sector)\n        short_exposures.append(short_sector)\n        gross_exposures.append(gross_sector)\n        net_exposures.append(net_sector)\n\n    return long_exposures, short_exposures, gross_exposures, net_exposures",
    "doc": "Returns arrays of long, short and gross sector exposures of an algorithm's\n    positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in compute_style_factor_exposures.\n\n    sectors : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - See full explanation in create_risk_tear_sheet\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - Keys are sector codes (e.g. ints or strings) and values are sector\n          names (which must be strings)\n        - Defaults to Morningstar sectors"
  },
  {
    "code": "def plot_sector_exposures_longshort(long_exposures, short_exposures,\n                                    sector_dict=SECTORS, ax=None):\n    \"\"\"\n    Plots outputs of compute_sector_exposures as area charts\n\n    Parameters\n    ----------\n    long_exposures, short_exposures : arrays\n        Arrays of long and short sector exposures (output of\n        compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if sector_dict is None:\n        sector_names = SECTORS.values()\n    else:\n        sector_names = sector_dict.values()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))\n\n    ax.stackplot(long_exposures[0].index, long_exposures,\n                 labels=sector_names, colors=color_list, alpha=0.8,\n                 baseline='zero')\n    ax.stackplot(long_exposures[0].index, short_exposures,\n                 colors=color_list, alpha=0.8, baseline='zero')\n    ax.axhline(0, color='k', linestyle='-')\n    ax.set(title='Long and short exposures to sectors',\n           ylabel='Proportion of long/short exposure in sectors')\n    ax.legend(loc='upper left', frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots outputs of compute_sector_exposures as area charts\n\n    Parameters\n    ----------\n    long_exposures, short_exposures : arrays\n        Arrays of long and short sector exposures (output of\n        compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures"
  },
  {
    "code": "def plot_sector_exposures_gross(gross_exposures, sector_dict=None, ax=None):\n    \"\"\"\n    Plots output of compute_sector_exposures as area charts\n\n    Parameters\n    ----------\n    gross_exposures : arrays\n        Arrays of gross sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if sector_dict is None:\n        sector_names = SECTORS.values()\n    else:\n        sector_names = sector_dict.values()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))\n\n    ax.stackplot(gross_exposures[0].index, gross_exposures,\n                 labels=sector_names, colors=color_list, alpha=0.8,\n                 baseline='zero')\n    ax.axhline(0, color='k', linestyle='-')\n    ax.set(title='Gross exposure to sectors',\n           ylabel='Proportion of gross exposure \\n in sectors')\n\n    return ax",
    "doc": "Plots output of compute_sector_exposures as area charts\n\n    Parameters\n    ----------\n    gross_exposures : arrays\n        Arrays of gross sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures"
  },
  {
    "code": "def plot_sector_exposures_net(net_exposures, sector_dict=None, ax=None):\n    \"\"\"\n    Plots output of compute_sector_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : arrays\n        Arrays of net sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if sector_dict is None:\n        sector_names = SECTORS.values()\n    else:\n        sector_names = sector_dict.values()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))\n\n    for i in range(len(net_exposures)):\n        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,\n                label=sector_names[i])\n    ax.set(title='Net exposures to sectors',\n           ylabel='Proportion of net exposure \\n in sectors')\n\n    return ax",
    "doc": "Plots output of compute_sector_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : arrays\n        Arrays of net sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures"
  },
  {
    "code": "def compute_cap_exposures(positions, caps):\n    \"\"\"\n    Returns arrays of long, short and gross market cap exposures of an\n    algorithm's positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in compute_style_factor_exposures.\n\n    caps : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - See full explanation in create_risk_tear_sheet\n    \"\"\"\n\n    long_exposures = []\n    short_exposures = []\n    gross_exposures = []\n    net_exposures = []\n\n    positions_wo_cash = positions.drop('cash', axis='columns')\n    tot_gross_exposure = positions_wo_cash.abs().sum(axis='columns')\n    tot_long_exposure = positions_wo_cash[positions_wo_cash > 0] \\\n        .sum(axis='columns')\n    tot_short_exposure = positions_wo_cash[positions_wo_cash < 0] \\\n        .abs().sum(axis='columns')\n\n    for bucket_name, boundaries in CAP_BUCKETS.items():\n        in_bucket = positions_wo_cash[(caps >= boundaries[0]) &\n                                      (caps <= boundaries[1])]\n\n        gross_bucket = in_bucket.abs().sum(axis='columns') \\\n            .divide(tot_gross_exposure)\n        long_bucket = in_bucket[in_bucket > 0] \\\n            .sum(axis='columns').divide(tot_long_exposure)\n        short_bucket = in_bucket[in_bucket < 0] \\\n            .sum(axis='columns').divide(tot_short_exposure)\n        net_bucket = long_bucket.subtract(short_bucket)\n\n        gross_exposures.append(gross_bucket)\n        long_exposures.append(long_bucket)\n        short_exposures.append(short_bucket)\n        net_exposures.append(net_bucket)\n\n    return long_exposures, short_exposures, gross_exposures, net_exposures",
    "doc": "Returns arrays of long, short and gross market cap exposures of an\n    algorithm's positions\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - See full explanation in compute_style_factor_exposures.\n\n    caps : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - See full explanation in create_risk_tear_sheet"
  },
  {
    "code": "def plot_cap_exposures_longshort(long_exposures, short_exposures, ax=None):\n    \"\"\"\n    Plots outputs of compute_cap_exposures as area charts\n\n    Parameters\n    ----------\n    long_exposures, short_exposures : arrays\n        Arrays of long and short market cap exposures (output of\n        compute_cap_exposures).\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))\n\n    ax.stackplot(long_exposures[0].index, long_exposures,\n                 labels=CAP_BUCKETS.keys(), colors=color_list, alpha=0.8,\n                 baseline='zero')\n    ax.stackplot(long_exposures[0].index, short_exposures, colors=color_list,\n                 alpha=0.8, baseline='zero')\n    ax.axhline(0, color='k', linestyle='-')\n    ax.set(title='Long and short exposures to market caps',\n           ylabel='Proportion of long/short exposure in market cap buckets')\n    ax.legend(loc='upper left', frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots outputs of compute_cap_exposures as area charts\n\n    Parameters\n    ----------\n    long_exposures, short_exposures : arrays\n        Arrays of long and short market cap exposures (output of\n        compute_cap_exposures)."
  },
  {
    "code": "def plot_cap_exposures_gross(gross_exposures, ax=None):\n    \"\"\"\n    Plots outputs of compute_cap_exposures as area charts\n\n    Parameters\n    ----------\n    gross_exposures : array\n        Arrays of gross market cap exposures (output of compute_cap_exposures).\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))\n\n    ax.stackplot(gross_exposures[0].index, gross_exposures,\n                 labels=CAP_BUCKETS.keys(), colors=color_list, alpha=0.8,\n                 baseline='zero')\n    ax.axhline(0, color='k', linestyle='-')\n    ax.set(title='Gross exposure to market caps',\n           ylabel='Proportion of gross exposure \\n in market cap buckets')\n\n    return ax",
    "doc": "Plots outputs of compute_cap_exposures as area charts\n\n    Parameters\n    ----------\n    gross_exposures : array\n        Arrays of gross market cap exposures (output of compute_cap_exposures)."
  },
  {
    "code": "def plot_cap_exposures_net(net_exposures, ax=None):\n    \"\"\"\n    Plots outputs of compute_cap_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : array\n        Arrays of gross market cap exposures (output of compute_cap_exposures).\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))\n\n    cap_names = CAP_BUCKETS.keys()\n    for i in range(len(net_exposures)):\n        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,\n                label=cap_names[i])\n    ax.axhline(0, color='k', linestyle='-')\n    ax.set(title='Net exposure to market caps',\n           ylabel='Proportion of net exposure \\n in market cap buckets')\n\n    return ax",
    "doc": "Plots outputs of compute_cap_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : array\n        Arrays of gross market cap exposures (output of compute_cap_exposures)."
  },
  {
    "code": "def compute_volume_exposures(shares_held, volumes, percentile):\n    \"\"\"\n    Returns arrays of pth percentile of long, short and gross volume exposures\n    of an algorithm's held shares\n\n    Parameters\n    ----------\n    shares_held : pd.DataFrame\n        Daily number of shares held by an algorithm.\n        - See full explanation in create_risk_tear_sheet\n\n    volume : pd.DataFrame\n        Daily volume per asset\n        - See full explanation in create_risk_tear_sheet\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures\n        - See full explanation in create_risk_tear_sheet\n    \"\"\"\n\n    shares_held = shares_held.replace(0, np.nan)\n\n    shares_longed = shares_held[shares_held > 0]\n    shares_shorted = -1 * shares_held[shares_held < 0]\n    shares_grossed = shares_held.abs()\n\n    longed_frac = shares_longed.divide(volumes)\n    shorted_frac = shares_shorted.divide(volumes)\n    grossed_frac = shares_grossed.divide(volumes)\n\n    # NOTE: To work around a bug in `quantile` with nan-handling in\n    #       pandas 0.18, use np.nanpercentile by applying to each row of\n    #       the dataframe. This is fixed in pandas 0.19.\n    #\n    # longed_threshold = 100*longed_frac.quantile(percentile, axis='columns')\n    # shorted_threshold = 100*shorted_frac.quantile(percentile, axis='columns')\n    # grossed_threshold = 100*grossed_frac.quantile(percentile, axis='columns')\n\n    longed_threshold = 100 * longed_frac.apply(\n        partial(np.nanpercentile, q=100 * percentile),\n        axis='columns',\n    )\n    shorted_threshold = 100 * shorted_frac.apply(\n        partial(np.nanpercentile, q=100 * percentile),\n        axis='columns',\n    )\n    grossed_threshold = 100 * grossed_frac.apply(\n        partial(np.nanpercentile, q=100 * percentile),\n        axis='columns',\n    )\n\n    return longed_threshold, shorted_threshold, grossed_threshold",
    "doc": "Returns arrays of pth percentile of long, short and gross volume exposures\n    of an algorithm's held shares\n\n    Parameters\n    ----------\n    shares_held : pd.DataFrame\n        Daily number of shares held by an algorithm.\n        - See full explanation in create_risk_tear_sheet\n\n    volume : pd.DataFrame\n        Daily volume per asset\n        - See full explanation in create_risk_tear_sheet\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures\n        - See full explanation in create_risk_tear_sheet"
  },
  {
    "code": "def plot_volume_exposures_longshort(longed_threshold, shorted_threshold,\n                                    percentile, ax=None):\n    \"\"\"\n    Plots outputs of compute_volume_exposures as line graphs\n\n    Parameters\n    ----------\n    longed_threshold, shorted_threshold : pd.Series\n        Series of longed and shorted volume exposures (output of\n        compute_volume_exposures).\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures.\n        - See full explanation in create_risk_tear_sheet\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    ax.plot(longed_threshold.index, longed_threshold,\n            color='b', label='long')\n    ax.plot(shorted_threshold.index, shorted_threshold,\n            color='r', label='short')\n    ax.axhline(0, color='k')\n    ax.set(title='Long and short exposures to illiquidity',\n           ylabel='{}th percentile of proportion of volume (%)'\n           .format(100 * percentile))\n    ax.legend(frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots outputs of compute_volume_exposures as line graphs\n\n    Parameters\n    ----------\n    longed_threshold, shorted_threshold : pd.Series\n        Series of longed and shorted volume exposures (output of\n        compute_volume_exposures).\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures.\n        - See full explanation in create_risk_tear_sheet"
  },
  {
    "code": "def plot_volume_exposures_gross(grossed_threshold, percentile, ax=None):\n    \"\"\"\n    Plots outputs of compute_volume_exposures as line graphs\n\n    Parameters\n    ----------\n    grossed_threshold : pd.Series\n        Series of grossed volume exposures (output of\n        compute_volume_exposures).\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures\n        - See full explanation in create_risk_tear_sheet\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    ax.plot(grossed_threshold.index, grossed_threshold,\n            color='b', label='gross')\n    ax.axhline(0, color='k')\n    ax.set(title='Gross exposure to illiquidity',\n           ylabel='{}th percentile of \\n proportion of volume (%)'\n           .format(100 * percentile))\n    ax.legend(frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots outputs of compute_volume_exposures as line graphs\n\n    Parameters\n    ----------\n    grossed_threshold : pd.Series\n        Series of grossed volume exposures (output of\n        compute_volume_exposures).\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures\n        - See full explanation in create_risk_tear_sheet"
  },
  {
    "code": "def create_full_tear_sheet(returns,\n                           positions=None,\n                           transactions=None,\n                           market_data=None,\n                           benchmark_rets=None,\n                           slippage=None,\n                           live_start_date=None,\n                           sector_mappings=None,\n                           bayesian=False,\n                           round_trips=False,\n                           estimate_intraday='infer',\n                           hide_positions=False,\n                           cone_std=(1.0, 1.5, 2.0),\n                           bootstrap=False,\n                           unadjusted_returns=None,\n                           style_factor_panel=None,\n                           sectors=None,\n                           caps=None,\n                           shares_held=None,\n                           volumes=None,\n                           percentile=None,\n                           turnover_denom='AGB',\n                           set_context=True,\n                           factor_returns=None,\n                           factor_loadings=None,\n                           pos_in_dollars=True,\n                           header_rows=None,\n                           factor_partitions=FACTOR_PARTITIONS):\n    \"\"\"\n    Generate a number of tear sheets that are useful\n    for analyzing a strategy's performance.\n\n    - Fetches benchmarks if needed.\n    - Creates tear sheets for returns, and significant events.\n        If possible, also creates tear sheets for position analysis,\n        transaction analysis, and Bayesian analysis.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    market_data : pd.Panel, optional\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    bayesian: boolean, optional\n        If True, causes the generation of a Bayesian tear sheet.\n    round_trips: boolean, optional\n        If True, causes the generation of a round trip tear sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Instead of using the end-of-day positions, use the point in the day\n        where we have the most $ invested. This will adjust positions to\n        better approximate and represent how an intraday strategy behaves.\n        By default, this is 'infer', and an attempt will be made to detect\n        an intraday strategy. Specifying this value will prevent detection.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    bootstrap : boolean (optional)\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    factor_returns : pd.Dataframe, optional\n        Returns by factor, with date as index and factors as columns\n    factor_loadings : pd.Dataframe, optional\n        Factor loadings for all days in the date range, with date and\n        ticker as index, and factors as columns.\n    pos_in_dollars : boolean, optional\n        indicates whether positions is in dollars\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context.\n         - See plotting.context().\n    factor_partitions : dict, optional\n        dict specifying how factors should be separated in perf attrib\n        factor returns and risk exposures plots\n        - See create_perf_attrib_tear_sheet().\n    \"\"\"\n\n    if (unadjusted_returns is None) and (slippage is not None) and\\\n       (transactions is not None):\n        unadjusted_returns = returns.copy()\n        returns = txn.adjust_returns_for_slippage(returns, positions,\n                                                  transactions, slippage)\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    create_returns_tear_sheet(\n        returns,\n        positions=positions,\n        transactions=transactions,\n        live_start_date=live_start_date,\n        cone_std=cone_std,\n        benchmark_rets=benchmark_rets,\n        bootstrap=bootstrap,\n        turnover_denom=turnover_denom,\n        header_rows=header_rows,\n        set_context=set_context)\n\n    create_interesting_times_tear_sheet(returns,\n                                        benchmark_rets=benchmark_rets,\n                                        set_context=set_context)\n\n    if positions is not None:\n        create_position_tear_sheet(returns, positions,\n                                   hide_positions=hide_positions,\n                                   set_context=set_context,\n                                   sector_mappings=sector_mappings,\n                                   estimate_intraday=False)\n\n        if transactions is not None:\n            create_txn_tear_sheet(returns, positions, transactions,\n                                  unadjusted_returns=unadjusted_returns,\n                                  estimate_intraday=False,\n                                  set_context=set_context)\n            if round_trips:\n                create_round_trip_tear_sheet(\n                    returns=returns,\n                    positions=positions,\n                    transactions=transactions,\n                    sector_mappings=sector_mappings,\n                    estimate_intraday=False)\n\n            if market_data is not None:\n                create_capacity_tear_sheet(returns, positions, transactions,\n                                           market_data,\n                                           liquidation_daily_vol_limit=0.2,\n                                           last_n_days=125,\n                                           estimate_intraday=False)\n\n        if style_factor_panel is not None:\n            create_risk_tear_sheet(positions, style_factor_panel, sectors,\n                                   caps, shares_held, volumes, percentile)\n\n        if factor_returns is not None and factor_loadings is not None:\n            create_perf_attrib_tear_sheet(returns, positions, factor_returns,\n                                          factor_loadings, transactions,\n                                          pos_in_dollars=pos_in_dollars,\n                                          factor_partitions=factor_partitions)\n\n    if bayesian:\n        create_bayesian_tear_sheet(returns,\n                                   live_start_date=live_start_date,\n                                   benchmark_rets=benchmark_rets,\n                                   set_context=set_context)",
    "doc": "Generate a number of tear sheets that are useful\n    for analyzing a strategy's performance.\n\n    - Fetches benchmarks if needed.\n    - Creates tear sheets for returns, and significant events.\n        If possible, also creates tear sheets for position analysis,\n        transaction analysis, and Bayesian analysis.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    market_data : pd.Panel, optional\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    bayesian: boolean, optional\n        If True, causes the generation of a Bayesian tear sheet.\n    round_trips: boolean, optional\n        If True, causes the generation of a round trip tear sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Instead of using the end-of-day positions, use the point in the day\n        where we have the most $ invested. This will adjust positions to\n        better approximate and represent how an intraday strategy behaves.\n        By default, this is 'infer', and an attempt will be made to detect\n        an intraday strategy. Specifying this value will prevent detection.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    bootstrap : boolean (optional)\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    factor_returns : pd.Dataframe, optional\n        Returns by factor, with date as index and factors as columns\n    factor_loadings : pd.Dataframe, optional\n        Factor loadings for all days in the date range, with date and\n        ticker as index, and factors as columns.\n    pos_in_dollars : boolean, optional\n        indicates whether positions is in dollars\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context.\n         - See plotting.context().\n    factor_partitions : dict, optional\n        dict specifying how factors should be separated in perf attrib\n        factor returns and risk exposures plots\n        - See create_perf_attrib_tear_sheet()."
  },
  {
    "code": "def create_simple_tear_sheet(returns,\n                             positions=None,\n                             transactions=None,\n                             benchmark_rets=None,\n                             slippage=None,\n                             estimate_intraday='infer',\n                             live_start_date=None,\n                             turnover_denom='AGB',\n                             header_rows=None):\n    \"\"\"\n    Simpler version of create_full_tear_sheet; generates summary performance\n    statistics and important plots as a single image.\n\n    - Plots: cumulative returns, rolling beta, rolling Sharpe, underwater,\n        exposure, top 10 holdings, total holdings, long/short holdings,\n        daily turnover, transaction time distribution.\n    - Never accept market_data input (market_data = None)\n    - Never accept sector_mappings input (sector_mappings = None)\n    - Never perform bootstrap analysis (bootstrap = False)\n    - Never hide posistions on top 10 holdings plot (hide_positions = False)\n    - Always use default cone_std (cone_std = (1.0, 1.5, 2.0))\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    benchmark_rets : pd.Series, optional\n        Daily returns of the benchmark, noncumulative.\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context.\n    \"\"\"\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    if (slippage is not None) and (transactions is not None):\n        returns = txn.adjust_returns_for_slippage(returns, positions,\n                                                  transactions, slippage)\n\n    always_sections = 4\n    positions_sections = 4 if positions is not None else 0\n    transactions_sections = 2 if transactions is not None else 0\n    live_sections = 1 if live_start_date is not None else 0\n    benchmark_sections = 1 if benchmark_rets is not None else 0\n\n    vertical_sections = sum([\n        always_sections,\n        positions_sections,\n        transactions_sections,\n        live_sections,\n        benchmark_sections,\n    ])\n\n    if live_start_date is not None:\n        live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n\n    plotting.show_perf_stats(returns,\n                             benchmark_rets,\n                             positions=positions,\n                             transactions=transactions,\n                             turnover_denom=turnover_denom,\n                             live_start_date=live_start_date,\n                             header_rows=header_rows)\n\n    fig = plt.figure(figsize=(14, vertical_sections * 6))\n    gs = gridspec.GridSpec(vertical_sections, 3, wspace=0.5, hspace=0.5)\n\n    ax_rolling_returns = plt.subplot(gs[:2, :])\n    i = 2\n    if benchmark_rets is not None:\n        ax_rolling_beta = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n        i += 1\n    ax_rolling_sharpe = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n    ax_underwater = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n\n    plotting.plot_rolling_returns(returns,\n                                  factor_returns=benchmark_rets,\n                                  live_start_date=live_start_date,\n                                  cone_std=(1.0, 1.5, 2.0),\n                                  ax=ax_rolling_returns)\n    ax_rolling_returns.set_title('Cumulative returns')\n\n    if benchmark_rets is not None:\n        plotting.plot_rolling_beta(returns, benchmark_rets, ax=ax_rolling_beta)\n\n    plotting.plot_rolling_sharpe(returns, ax=ax_rolling_sharpe)\n\n    plotting.plot_drawdown_underwater(returns, ax=ax_underwater)\n\n    if positions is not None:\n        # Plot simple positions tear sheet\n        ax_exposures = plt.subplot(gs[i, :])\n        i += 1\n        ax_top_positions = plt.subplot(gs[i, :], sharex=ax_exposures)\n        i += 1\n        ax_holdings = plt.subplot(gs[i, :], sharex=ax_exposures)\n        i += 1\n        ax_long_short_holdings = plt.subplot(gs[i, :])\n        i += 1\n\n        positions_alloc = pos.get_percent_alloc(positions)\n\n        plotting.plot_exposures(returns, positions, ax=ax_exposures)\n\n        plotting.show_and_plot_top_positions(returns,\n                                             positions_alloc,\n                                             show_and_plot=0,\n                                             hide_positions=False,\n                                             ax=ax_top_positions)\n\n        plotting.plot_holdings(returns, positions_alloc, ax=ax_holdings)\n\n        plotting.plot_long_short_holdings(returns, positions_alloc,\n                                          ax=ax_long_short_holdings)\n\n        if transactions is not None:\n            # Plot simple transactions tear sheet\n            ax_turnover = plt.subplot(gs[i, :])\n            i += 1\n            ax_txn_timings = plt.subplot(gs[i, :])\n            i += 1\n\n            plotting.plot_turnover(returns,\n                                   transactions,\n                                   positions,\n                                   ax=ax_turnover)\n\n            plotting.plot_txn_time_hist(transactions, ax=ax_txn_timings)\n\n    for ax in fig.axes:\n        plt.setp(ax.get_xticklabels(), visible=True)",
    "doc": "Simpler version of create_full_tear_sheet; generates summary performance\n    statistics and important plots as a single image.\n\n    - Plots: cumulative returns, rolling beta, rolling Sharpe, underwater,\n        exposure, top 10 holdings, total holdings, long/short holdings,\n        daily turnover, transaction time distribution.\n    - Never accept market_data input (market_data = None)\n    - Never accept sector_mappings input (sector_mappings = None)\n    - Never perform bootstrap analysis (bootstrap = False)\n    - Never hide posistions on top 10 holdings plot (hide_positions = False)\n    - Always use default cone_std (cone_std = (1.0, 1.5, 2.0))\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    benchmark_rets : pd.Series, optional\n        Daily returns of the benchmark, noncumulative.\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context."
  },
  {
    "code": "def create_returns_tear_sheet(returns, positions=None,\n                              transactions=None,\n                              live_start_date=None,\n                              cone_std=(1.0, 1.5, 2.0),\n                              benchmark_rets=None,\n                              bootstrap=False,\n                              turnover_denom='AGB',\n                              header_rows=None,\n                              return_fig=False):\n    \"\"\"\n    Generate a number of plots for analyzing a strategy's returns.\n\n    - Fetches benchmarks, then creates the plots on a single figure.\n    - Plots: rolling returns (with cone), rolling beta, rolling sharpe,\n        rolling Fama-French risk factors, drawdowns, underwater plot, monthly\n        and annual return plots, daily similarity plots,\n        and return quantile box plot.\n    - Will also print the start and end dates of the strategy,\n        performance statistics, drawdown periods, and the return range.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - See full explanation in create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    benchmark_rets : pd.Series, optional\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    \"\"\"\n\n    if benchmark_rets is not None:\n        returns = utils.clip_returns_to_benchmark(returns, benchmark_rets)\n\n    plotting.show_perf_stats(returns, benchmark_rets,\n                             positions=positions,\n                             transactions=transactions,\n                             turnover_denom=turnover_denom,\n                             bootstrap=bootstrap,\n                             live_start_date=live_start_date,\n                             header_rows=header_rows)\n\n    plotting.show_worst_drawdown_periods(returns)\n\n    vertical_sections = 11\n\n    if live_start_date is not None:\n        vertical_sections += 1\n        live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n\n    if benchmark_rets is not None:\n        vertical_sections += 1\n\n    if bootstrap:\n        vertical_sections += 1\n\n    fig = plt.figure(figsize=(14, vertical_sections * 6))\n    gs = gridspec.GridSpec(vertical_sections, 3, wspace=0.5, hspace=0.5)\n    ax_rolling_returns = plt.subplot(gs[:2, :])\n\n    i = 2\n    ax_rolling_returns_vol_match = plt.subplot(gs[i, :],\n                                               sharex=ax_rolling_returns)\n    i += 1\n    ax_rolling_returns_log = plt.subplot(gs[i, :],\n                                         sharex=ax_rolling_returns)\n    i += 1\n    ax_returns = plt.subplot(gs[i, :],\n                             sharex=ax_rolling_returns)\n    i += 1\n    if benchmark_rets is not None:\n        ax_rolling_beta = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n        i += 1\n    ax_rolling_volatility = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n    ax_rolling_sharpe = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n    ax_drawdown = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n    ax_underwater = plt.subplot(gs[i, :], sharex=ax_rolling_returns)\n    i += 1\n    ax_monthly_heatmap = plt.subplot(gs[i, 0])\n    ax_annual_returns = plt.subplot(gs[i, 1])\n    ax_monthly_dist = plt.subplot(gs[i, 2])\n    i += 1\n    ax_return_quantiles = plt.subplot(gs[i, :])\n    i += 1\n\n    plotting.plot_rolling_returns(\n        returns,\n        factor_returns=benchmark_rets,\n        live_start_date=live_start_date,\n        cone_std=cone_std,\n        ax=ax_rolling_returns)\n    ax_rolling_returns.set_title(\n        'Cumulative returns')\n\n    plotting.plot_rolling_returns(\n        returns,\n        factor_returns=benchmark_rets,\n        live_start_date=live_start_date,\n        cone_std=None,\n        volatility_match=(benchmark_rets is not None),\n        legend_loc=None,\n        ax=ax_rolling_returns_vol_match)\n    ax_rolling_returns_vol_match.set_title(\n        'Cumulative returns volatility matched to benchmark')\n\n    plotting.plot_rolling_returns(\n        returns,\n        factor_returns=benchmark_rets,\n        logy=True,\n        live_start_date=live_start_date,\n        cone_std=cone_std,\n        ax=ax_rolling_returns_log)\n    ax_rolling_returns_log.set_title(\n        'Cumulative returns on logarithmic scale')\n\n    plotting.plot_returns(\n        returns,\n        live_start_date=live_start_date,\n        ax=ax_returns,\n    )\n    ax_returns.set_title(\n        'Returns')\n\n    if benchmark_rets is not None:\n        plotting.plot_rolling_beta(\n            returns, benchmark_rets, ax=ax_rolling_beta)\n\n    plotting.plot_rolling_volatility(\n        returns, factor_returns=benchmark_rets, ax=ax_rolling_volatility)\n\n    plotting.plot_rolling_sharpe(\n        returns, ax=ax_rolling_sharpe)\n\n    # Drawdowns\n    plotting.plot_drawdown_periods(\n        returns, top=5, ax=ax_drawdown)\n\n    plotting.plot_drawdown_underwater(\n        returns=returns, ax=ax_underwater)\n\n    plotting.plot_monthly_returns_heatmap(returns, ax=ax_monthly_heatmap)\n    plotting.plot_annual_returns(returns, ax=ax_annual_returns)\n    plotting.plot_monthly_returns_dist(returns, ax=ax_monthly_dist)\n\n    plotting.plot_return_quantiles(\n        returns,\n        live_start_date=live_start_date,\n        ax=ax_return_quantiles)\n\n    if bootstrap and (benchmark_rets is not None):\n        ax_bootstrap = plt.subplot(gs[i, :])\n        plotting.plot_perf_stats(returns, benchmark_rets,\n                                 ax=ax_bootstrap)\n    elif bootstrap:\n        raise ValueError('bootstrap requires passing of benchmark_rets.')\n\n    for ax in fig.axes:\n        plt.setp(ax.get_xticklabels(), visible=True)\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of plots for analyzing a strategy's returns.\n\n    - Fetches benchmarks, then creates the plots on a single figure.\n    - Plots: rolling returns (with cone), rolling beta, rolling sharpe,\n        rolling Fama-French risk factors, drawdowns, underwater plot, monthly\n        and annual return plots, daily similarity plots,\n        and return quantile box plot.\n    - Will also print the start and end dates of the strategy,\n        performance statistics, drawdown periods, and the return range.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - See full explanation in create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    benchmark_rets : pd.Series, optional\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on."
  },
  {
    "code": "def create_position_tear_sheet(returns, positions,\n                               show_and_plot_top_pos=2, hide_positions=False,\n                               return_fig=False, sector_mappings=None,\n                               transactions=None, estimate_intraday='infer'):\n    \"\"\"\n    Generate a number of plots for analyzing a\n    strategy's positions and holdings.\n\n    - Plots: gross leverage, exposures, top positions, and holdings.\n    - Will also print the top positions held.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    show_and_plot_top_pos : int, optional\n        By default, this is 2, and both prints and plots the\n        top 10 positions.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n        Overrides show_and_plot_top_pos to 0 to suppress text output.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    \"\"\"\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    if hide_positions:\n        show_and_plot_top_pos = 0\n    vertical_sections = 7 if sector_mappings is not None else 6\n\n    fig = plt.figure(figsize=(14, vertical_sections * 6))\n    gs = gridspec.GridSpec(vertical_sections, 3, wspace=0.5, hspace=0.5)\n    ax_exposures = plt.subplot(gs[0, :])\n    ax_top_positions = plt.subplot(gs[1, :], sharex=ax_exposures)\n    ax_max_median_pos = plt.subplot(gs[2, :], sharex=ax_exposures)\n    ax_holdings = plt.subplot(gs[3, :], sharex=ax_exposures)\n    ax_long_short_holdings = plt.subplot(gs[4, :])\n    ax_gross_leverage = plt.subplot(gs[5, :], sharex=ax_exposures)\n\n    positions_alloc = pos.get_percent_alloc(positions)\n\n    plotting.plot_exposures(returns, positions, ax=ax_exposures)\n\n    plotting.show_and_plot_top_positions(\n        returns,\n        positions_alloc,\n        show_and_plot=show_and_plot_top_pos,\n        hide_positions=hide_positions,\n        ax=ax_top_positions)\n\n    plotting.plot_max_median_position_concentration(positions,\n                                                    ax=ax_max_median_pos)\n\n    plotting.plot_holdings(returns, positions_alloc, ax=ax_holdings)\n\n    plotting.plot_long_short_holdings(returns, positions_alloc,\n                                      ax=ax_long_short_holdings)\n\n    plotting.plot_gross_leverage(returns, positions,\n                                 ax=ax_gross_leverage)\n\n    if sector_mappings is not None:\n        sector_exposures = pos.get_sector_exposures(positions,\n                                                    sector_mappings)\n        if len(sector_exposures.columns) > 1:\n            sector_alloc = pos.get_percent_alloc(sector_exposures)\n            sector_alloc = sector_alloc.drop('cash', axis='columns')\n            ax_sector_alloc = plt.subplot(gs[6, :], sharex=ax_exposures)\n            plotting.plot_sector_allocations(returns, sector_alloc,\n                                             ax=ax_sector_alloc)\n\n    for ax in fig.axes:\n        plt.setp(ax.get_xticklabels(), visible=True)\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of plots for analyzing a\n    strategy's positions and holdings.\n\n    - Plots: gross leverage, exposures, top positions, and holdings.\n    - Will also print the top positions held.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    show_and_plot_top_pos : int, optional\n        By default, this is 2, and both prints and plots the\n        top 10 positions.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n        Overrides show_and_plot_top_pos to 0 to suppress text output.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet."
  },
  {
    "code": "def create_txn_tear_sheet(returns, positions, transactions,\n                          unadjusted_returns=None, estimate_intraday='infer',\n                          return_fig=False):\n    \"\"\"\n    Generate a number of plots for analyzing a strategy's transactions.\n\n    Plots: turnover, daily volume, and a histogram of daily volume.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    unadjusted_returns : pd.Series, optional\n        Daily unadjusted returns of the strategy, noncumulative.\n        Will plot additional swippage sweep analysis.\n         - See pyfolio.plotting.plot_swippage_sleep and\n           pyfolio.plotting.plot_slippage_sensitivity\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    \"\"\"\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    vertical_sections = 6 if unadjusted_returns is not None else 4\n\n    fig = plt.figure(figsize=(14, vertical_sections * 6))\n    gs = gridspec.GridSpec(vertical_sections, 3, wspace=0.5, hspace=0.5)\n    ax_turnover = plt.subplot(gs[0, :])\n    ax_daily_volume = plt.subplot(gs[1, :], sharex=ax_turnover)\n    ax_turnover_hist = plt.subplot(gs[2, :])\n    ax_txn_timings = plt.subplot(gs[3, :])\n\n    plotting.plot_turnover(\n        returns,\n        transactions,\n        positions,\n        ax=ax_turnover)\n\n    plotting.plot_daily_volume(returns, transactions, ax=ax_daily_volume)\n\n    try:\n        plotting.plot_daily_turnover_hist(transactions, positions,\n                                          ax=ax_turnover_hist)\n    except ValueError:\n        warnings.warn('Unable to generate turnover plot.', UserWarning)\n\n    plotting.plot_txn_time_hist(transactions, ax=ax_txn_timings)\n\n    if unadjusted_returns is not None:\n        ax_slippage_sweep = plt.subplot(gs[4, :])\n        plotting.plot_slippage_sweep(unadjusted_returns,\n                                     positions,\n                                     transactions,\n                                     ax=ax_slippage_sweep\n                                     )\n        ax_slippage_sensitivity = plt.subplot(gs[5, :])\n        plotting.plot_slippage_sensitivity(unadjusted_returns,\n                                           positions,\n                                           transactions,\n                                           ax=ax_slippage_sensitivity\n                                           )\n    for ax in fig.axes:\n        plt.setp(ax.get_xticklabels(), visible=True)\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of plots for analyzing a strategy's transactions.\n\n    Plots: turnover, daily volume, and a histogram of daily volume.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    unadjusted_returns : pd.Series, optional\n        Daily unadjusted returns of the strategy, noncumulative.\n        Will plot additional swippage sweep analysis.\n         - See pyfolio.plotting.plot_swippage_sleep and\n           pyfolio.plotting.plot_slippage_sensitivity\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on."
  },
  {
    "code": "def create_round_trip_tear_sheet(returns, positions, transactions,\n                                 sector_mappings=None,\n                                 estimate_intraday='infer', return_fig=False):\n    \"\"\"\n    Generate a number of figures and plots describing the duration,\n    frequency, and profitability of trade \"round trips.\"\n    A round trip is started when a new long or short position is\n    opened and is only completed when the number of shares in that\n    position returns to or crosses zero.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    \"\"\"\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    transactions_closed = round_trips.add_closing_transactions(positions,\n                                                               transactions)\n    # extract_round_trips requires BoD portfolio_value\n    trades = round_trips.extract_round_trips(\n        transactions_closed,\n        portfolio_value=positions.sum(axis='columns') / (1 + returns)\n    )\n\n    if len(trades) < 5:\n        warnings.warn(\n            \"\"\"Fewer than 5 round-trip trades made.\n               Skipping round trip tearsheet.\"\"\", UserWarning)\n        return\n\n    round_trips.print_round_trip_stats(trades)\n\n    plotting.show_profit_attribution(trades)\n\n    if sector_mappings is not None:\n        sector_trades = round_trips.apply_sector_mappings_to_round_trips(\n            trades, sector_mappings)\n        plotting.show_profit_attribution(sector_trades)\n\n    fig = plt.figure(figsize=(14, 3 * 6))\n\n    gs = gridspec.GridSpec(3, 2, wspace=0.5, hspace=0.5)\n\n    ax_trade_lifetimes = plt.subplot(gs[0, :])\n    ax_prob_profit_trade = plt.subplot(gs[1, 0])\n    ax_holding_time = plt.subplot(gs[1, 1])\n    ax_pnl_per_round_trip_dollars = plt.subplot(gs[2, 0])\n    ax_pnl_per_round_trip_pct = plt.subplot(gs[2, 1])\n\n    plotting.plot_round_trip_lifetimes(trades, ax=ax_trade_lifetimes)\n\n    plotting.plot_prob_profit_trade(trades, ax=ax_prob_profit_trade)\n\n    trade_holding_times = [x.days for x in trades['duration']]\n    sns.distplot(trade_holding_times, kde=False, ax=ax_holding_time)\n    ax_holding_time.set(xlabel='Holding time in days')\n\n    sns.distplot(trades.pnl, kde=False, ax=ax_pnl_per_round_trip_dollars)\n    ax_pnl_per_round_trip_dollars.set(xlabel='PnL per round-trip trade in $')\n\n    sns.distplot(trades.returns.dropna() * 100, kde=False,\n                 ax=ax_pnl_per_round_trip_pct)\n    ax_pnl_per_round_trip_pct.set(\n        xlabel='Round-trip returns in %')\n\n    gs.tight_layout(fig)\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of figures and plots describing the duration,\n    frequency, and profitability of trade \"round trips.\"\n    A round trip is started when a new long or short position is\n    opened and is only completed when the number of shares in that\n    position returns to or crosses zero.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on."
  },
  {
    "code": "def create_interesting_times_tear_sheet(\n        returns, benchmark_rets=None, legend_loc='best', return_fig=False):\n    \"\"\"\n    Generate a number of returns plots around interesting points in time,\n    like the flash crash and 9/11.\n\n    Plots: returns around the dotcom bubble burst, Lehmann Brothers' failure,\n    9/11, US downgrade and EU debt crisis, Fukushima meltdown, US housing\n    bubble burst, EZB IR, Great Recession (August 2007, March and September\n    of 2008, Q1 & Q2 2009), flash crash, April and October 2014.\n\n    benchmark_rets must be passed, as it is meaningless to analyze performance\n    during interesting times without some benchmark to refer to.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    benchmark_rets : pd.Series\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    legend_loc : plt.legend_loc, optional\n         The legend's location.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    \"\"\"\n\n    rets_interesting = timeseries.extract_interesting_date_ranges(returns)\n\n    if not rets_interesting:\n        warnings.warn('Passed returns do not overlap with any'\n                      'interesting times.', UserWarning)\n        return\n\n    utils.print_table(pd.DataFrame(rets_interesting)\n                      .describe().transpose()\n                      .loc[:, ['mean', 'min', 'max']] * 100,\n                      name='Stress Events',\n                      float_format='{0:.2f}%'.format)\n\n    if benchmark_rets is not None:\n        returns = utils.clip_returns_to_benchmark(returns, benchmark_rets)\n\n        bmark_interesting = timeseries.extract_interesting_date_ranges(\n            benchmark_rets)\n\n    num_plots = len(rets_interesting)\n    # 2 plots, 1 row; 3 plots, 2 rows; 4 plots, 2 rows; etc.\n    num_rows = int((num_plots + 1) / 2.0)\n    fig = plt.figure(figsize=(14, num_rows * 6.0))\n    gs = gridspec.GridSpec(num_rows, 2, wspace=0.5, hspace=0.5)\n\n    for i, (name, rets_period) in enumerate(rets_interesting.items()):\n        # i=0 -> 0, i=1 -> 0, i=2 -> 1 ;; i=0 -> 0, i=1 -> 1, i=2 -> 0\n        ax = plt.subplot(gs[int(i / 2.0), i % 2])\n\n        ep.cum_returns(rets_period).plot(\n            ax=ax, color='forestgreen', label='algo', alpha=0.7, lw=2)\n\n        if benchmark_rets is not None:\n            ep.cum_returns(bmark_interesting[name]).plot(\n                ax=ax, color='gray', label='benchmark', alpha=0.6)\n            ax.legend(['Algo',\n                       'benchmark'],\n                      loc=legend_loc, frameon=True, framealpha=0.5)\n        else:\n            ax.legend(['Algo'],\n                      loc=legend_loc, frameon=True, framealpha=0.5)\n\n        ax.set_title(name)\n        ax.set_ylabel('Returns')\n        ax.set_xlabel('')\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of returns plots around interesting points in time,\n    like the flash crash and 9/11.\n\n    Plots: returns around the dotcom bubble burst, Lehmann Brothers' failure,\n    9/11, US downgrade and EU debt crisis, Fukushima meltdown, US housing\n    bubble burst, EZB IR, Great Recession (August 2007, March and September\n    of 2008, Q1 & Q2 2009), flash crash, April and October 2014.\n\n    benchmark_rets must be passed, as it is meaningless to analyze performance\n    during interesting times without some benchmark to refer to.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    benchmark_rets : pd.Series\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    legend_loc : plt.legend_loc, optional\n         The legend's location.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on."
  },
  {
    "code": "def create_capacity_tear_sheet(returns, positions, transactions,\n                               market_data,\n                               liquidation_daily_vol_limit=0.2,\n                               trade_daily_vol_limit=0.05,\n                               last_n_days=utils.APPROX_BDAYS_PER_MONTH * 6,\n                               days_to_liquidate_limit=1,\n                               estimate_intraday='infer'):\n    \"\"\"\n    Generates a report detailing portfolio size constraints set by\n    least liquid tickers. Plots a \"capacity sweep,\" a curve describing\n    projected sharpe ratio given the slippage penalties that are\n    applied at various capital bases.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    liquidation_daily_vol_limit : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position in the\n        \"days to liquidation\" analysis.\n    trade_daily_vol_limit : float\n        Flag daily transaction totals that exceed proportion of\n        daily bar.\n    last_n_days : integer\n        Compute max position allocation and dollar volume for only\n        the last N days of the backtest\n    days_to_liquidate_limit : integer\n        Display all tickers with greater max days to liquidation.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    \"\"\"\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    print(\"Max days to liquidation is computed for each traded name \"\n          \"assuming a 20% limit on daily bar consumption \\n\"\n          \"and trailing 5 day mean volume as the available bar volume.\\n\\n\"\n          \"Tickers with >1 day liquidation time at a\"\n          \" constant $1m capital base:\")\n\n    max_days_by_ticker = capacity.get_max_days_to_liquidate_by_ticker(\n        positions, market_data,\n        max_bar_consumption=liquidation_daily_vol_limit,\n        capital_base=1e6,\n        mean_volume_window=5)\n    max_days_by_ticker.index = (\n        max_days_by_ticker.index.map(utils.format_asset))\n\n    print(\"Whole backtest:\")\n    utils.print_table(\n        max_days_by_ticker[max_days_by_ticker.days_to_liquidate >\n                           days_to_liquidate_limit])\n\n    max_days_by_ticker_lnd = capacity.get_max_days_to_liquidate_by_ticker(\n        positions, market_data,\n        max_bar_consumption=liquidation_daily_vol_limit,\n        capital_base=1e6,\n        mean_volume_window=5,\n        last_n_days=last_n_days)\n    max_days_by_ticker_lnd.index = (\n        max_days_by_ticker_lnd.index.map(utils.format_asset))\n\n    print(\"Last {} trading days:\".format(last_n_days))\n    utils.print_table(\n        max_days_by_ticker_lnd[max_days_by_ticker_lnd.days_to_liquidate > 1])\n\n    llt = capacity.get_low_liquidity_transactions(transactions, market_data)\n    llt.index = llt.index.map(utils.format_asset)\n\n    print('Tickers with daily transactions consuming >{}% of daily bar \\n'\n          'all backtest:'.format(trade_daily_vol_limit * 100))\n    utils.print_table(\n        llt[llt['max_pct_bar_consumed'] > trade_daily_vol_limit * 100])\n\n    llt = capacity.get_low_liquidity_transactions(\n        transactions, market_data, last_n_days=last_n_days)\n\n    print(\"Last {} trading days:\".format(last_n_days))\n    utils.print_table(\n        llt[llt['max_pct_bar_consumed'] > trade_daily_vol_limit * 100])\n\n    bt_starting_capital = positions.iloc[0].sum() / (1 + returns.iloc[0])\n    fig, ax_capacity_sweep = plt.subplots(figsize=(14, 6))\n    plotting.plot_capacity_sweep(returns, transactions, market_data,\n                                 bt_starting_capital,\n                                 min_pv=100000,\n                                 max_pv=300000000,\n                                 step_size=1000000,\n                                 ax=ax_capacity_sweep)",
    "doc": "Generates a report detailing portfolio size constraints set by\n    least liquid tickers. Plots a \"capacity sweep,\" a curve describing\n    projected sharpe ratio given the slippage penalties that are\n    applied at various capital bases.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    liquidation_daily_vol_limit : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position in the\n        \"days to liquidation\" analysis.\n    trade_daily_vol_limit : float\n        Flag daily transaction totals that exceed proportion of\n        daily bar.\n    last_n_days : integer\n        Compute max position allocation and dollar volume for only\n        the last N days of the backtest\n    days_to_liquidate_limit : integer\n        Display all tickers with greater max days to liquidation.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet."
  },
  {
    "code": "def create_bayesian_tear_sheet(returns, benchmark_rets=None,\n                               live_start_date=None, samples=2000,\n                               return_fig=False, stoch_vol=False,\n                               progressbar=True):\n    \"\"\"\n    Generate a number of Bayesian distributions and a Bayesian\n    cone plot of returns.\n\n    Plots: Sharpe distribution, annual volatility distribution,\n    annual alpha distribution, beta distribution, predicted 1 and 5\n    day returns distributions, and a cumulative returns cone plot.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    benchmark_rets : pd.Series, optional\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live\n        trading, after its backtest period.\n    samples : int, optional\n        Number of posterior samples to draw.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    stoch_vol : boolean, optional\n        If True, run and plot the stochastic volatility model\n    progressbar : boolean, optional\n        If True, show a progress bar\n    \"\"\"\n\n    if not have_bayesian:\n        raise NotImplementedError(\n            \"Bayesian tear sheet requirements not found.\\n\"\n            \"Run 'pip install pyfolio[bayesian]' to install \"\n            \"bayesian requirements.\"\n        )\n\n    if live_start_date is None:\n        raise NotImplementedError(\n            'Bayesian tear sheet requires setting of live_start_date'\n        )\n\n    live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n    df_train = returns.loc[returns.index < live_start_date]\n    df_test = returns.loc[returns.index >= live_start_date]\n\n    # Run T model with missing data\n    print(\"Running T model\")\n    previous_time = time()\n    # track the total run time of the Bayesian tear sheet\n    start_time = previous_time\n\n    trace_t, ppc_t = bayesian.run_model('t', df_train,\n                                        returns_test=df_test,\n                                        samples=samples, ppc=True,\n                                        progressbar=progressbar)\n    previous_time = timer(\"T model\", previous_time)\n\n    # Compute BEST model\n    print(\"\\nRunning BEST model\")\n    trace_best = bayesian.run_model('best', df_train,\n                                    returns_test=df_test,\n                                    samples=samples,\n                                    progressbar=progressbar)\n    previous_time = timer(\"BEST model\", previous_time)\n\n    # Plot results\n\n    fig = plt.figure(figsize=(14, 10 * 2))\n    gs = gridspec.GridSpec(9, 2, wspace=0.3, hspace=0.3)\n\n    axs = []\n    row = 0\n\n    # Plot Bayesian cone\n    ax_cone = plt.subplot(gs[row, :])\n    bayesian.plot_bayes_cone(df_train, df_test, ppc_t, ax=ax_cone)\n    previous_time = timer(\"plotting Bayesian cone\", previous_time)\n\n    # Plot BEST results\n    row += 1\n    axs.append(plt.subplot(gs[row, 0]))\n    axs.append(plt.subplot(gs[row, 1]))\n    row += 1\n    axs.append(plt.subplot(gs[row, 0]))\n    axs.append(plt.subplot(gs[row, 1]))\n    row += 1\n    axs.append(plt.subplot(gs[row, 0]))\n    axs.append(plt.subplot(gs[row, 1]))\n    row += 1\n    # Effect size across two\n    axs.append(plt.subplot(gs[row, :]))\n\n    bayesian.plot_best(trace=trace_best, axs=axs)\n    previous_time = timer(\"plotting BEST results\", previous_time)\n\n    # Compute Bayesian predictions\n    row += 1\n    ax_ret_pred_day = plt.subplot(gs[row, 0])\n    ax_ret_pred_week = plt.subplot(gs[row, 1])\n    day_pred = ppc_t[:, 0]\n    p5 = scipy.stats.scoreatpercentile(day_pred, 5)\n    sns.distplot(day_pred,\n                 ax=ax_ret_pred_day\n                 )\n    ax_ret_pred_day.axvline(p5, linestyle='--', linewidth=3.)\n    ax_ret_pred_day.set_xlabel('Predicted returns 1 day')\n    ax_ret_pred_day.set_ylabel('Frequency')\n    ax_ret_pred_day.text(0.4, 0.9, 'Bayesian VaR = %.2f' % p5,\n                         verticalalignment='bottom',\n                         horizontalalignment='right',\n                         transform=ax_ret_pred_day.transAxes)\n    previous_time = timer(\"computing Bayesian predictions\", previous_time)\n\n    # Plot Bayesian VaRs\n    week_pred = (\n        np.cumprod(ppc_t[:, :5] + 1, 1) - 1)[:, -1]\n    p5 = scipy.stats.scoreatpercentile(week_pred, 5)\n    sns.distplot(week_pred,\n                 ax=ax_ret_pred_week\n                 )\n    ax_ret_pred_week.axvline(p5, linestyle='--', linewidth=3.)\n    ax_ret_pred_week.set_xlabel('Predicted cum returns 5 days')\n    ax_ret_pred_week.set_ylabel('Frequency')\n    ax_ret_pred_week.text(0.4, 0.9, 'Bayesian VaR = %.2f' % p5,\n                          verticalalignment='bottom',\n                          horizontalalignment='right',\n                          transform=ax_ret_pred_week.transAxes)\n    previous_time = timer(\"plotting Bayesian VaRs estimate\", previous_time)\n\n    # Run alpha beta model\n    if benchmark_rets is not None:\n        print(\"\\nRunning alpha beta model\")\n        benchmark_rets = benchmark_rets.loc[df_train.index]\n        trace_alpha_beta = bayesian.run_model('alpha_beta', df_train,\n                                              bmark=benchmark_rets,\n                                              samples=samples,\n                                              progressbar=progressbar)\n        previous_time = timer(\"running alpha beta model\", previous_time)\n\n        # Plot alpha and beta\n        row += 1\n        ax_alpha = plt.subplot(gs[row, 0])\n        ax_beta = plt.subplot(gs[row, 1])\n        sns.distplot((1 + trace_alpha_beta['alpha'][100:])**252 - 1,\n                     ax=ax_alpha)\n        sns.distplot(trace_alpha_beta['beta'][100:], ax=ax_beta)\n        ax_alpha.set_xlabel('Annual Alpha')\n        ax_alpha.set_ylabel('Belief')\n        ax_beta.set_xlabel('Beta')\n        ax_beta.set_ylabel('Belief')\n        previous_time = timer(\"plotting alpha beta model\", previous_time)\n\n    if stoch_vol:\n        # run stochastic volatility model\n        returns_cutoff = 400\n        print(\n            \"\\nRunning stochastic volatility model on \"\n            \"most recent {} days of returns.\".format(returns_cutoff)\n        )\n        if df_train.size > returns_cutoff:\n            df_train_truncated = df_train[-returns_cutoff:]\n        _, trace_stoch_vol = bayesian.model_stoch_vol(df_train_truncated)\n        previous_time = timer(\n            \"running stochastic volatility model\", previous_time)\n\n        # plot latent volatility\n        row += 1\n        ax_volatility = plt.subplot(gs[row, :])\n        bayesian.plot_stoch_vol(\n            df_train_truncated, trace=trace_stoch_vol, ax=ax_volatility)\n        previous_time = timer(\n            \"plotting stochastic volatility model\", previous_time)\n\n    total_time = time() - start_time\n    print(\"\\nTotal runtime was {:.2f} seconds.\".format(total_time))\n\n    gs.tight_layout(fig)\n\n    if return_fig:\n        return fig",
    "doc": "Generate a number of Bayesian distributions and a Bayesian\n    cone plot of returns.\n\n    Plots: Sharpe distribution, annual volatility distribution,\n    annual alpha distribution, beta distribution, predicted 1 and 5\n    day returns distributions, and a cumulative returns cone plot.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    benchmark_rets : pd.Series, optional\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live\n        trading, after its backtest period.\n    samples : int, optional\n        Number of posterior samples to draw.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    stoch_vol : boolean, optional\n        If True, run and plot the stochastic volatility model\n    progressbar : boolean, optional\n        If True, show a progress bar"
  },
  {
    "code": "def create_risk_tear_sheet(positions,\n                           style_factor_panel=None,\n                           sectors=None,\n                           caps=None,\n                           shares_held=None,\n                           volumes=None,\n                           percentile=None,\n                           returns=None,\n                           transactions=None,\n                           estimate_intraday='infer',\n                           return_fig=False):\n    '''\n    Creates risk tear sheet: computes and plots style factor exposures, sector\n    exposures, market cap exposures and volume exposures.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - DataFrame with dates as index, equities as columns\n        - Last column is cash held\n        - Example:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])             cash\n        2017-04-03\t-108062.40 \t  4401.540     2.247757e+07\n        2017-04-04\t-108852.00\t  4373.820     2.540999e+07\n        2017-04-05\t-119968.66\t  4336.200     2.839812e+07\n\n    style_factor_panel : pd.Panel\n        Panel where each item is a DataFrame that tabulates style factor per\n        equity per day.\n        - Each item has dates as index, equities as columns\n        - Example item:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])\n        2017-04-03\t  -0.51284     1.39173\n        2017-04-04\t  -0.73381     0.98149\n        2017-04-05\t  -0.90132\t   1.13981\n\n    sectors : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])\n        2017-04-03\t     311.0       206.0\n        2017-04-04\t     311.0       206.0\n        2017-04-05\t     311.0\t     206.0\n\n    caps : pd.DataFrame\n        Daily market cap per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03     1.327160e+10     6.402460e+10\n        2017-04-04\t   1.329620e+10     6.403694e+10\n        2017-04-05\t   1.297464e+10\t    6.397187e+10\n\n    shares_held : pd.DataFrame\n        Daily number of shares held by an algorithm.\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03             1915            -2595\n        2017-04-04\t           1968            -3272\n        2017-04-05\t           2104            -3917\n\n    volumes : pd.DataFrame\n        Daily volume per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03      34940859.00       4665573.80\n        2017-04-04\t    35603329.10       4818463.90\n        2017-04-05\t    41846731.75\t      4129153.10\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures.\n        - Defaults to 10th percentile\n    '''\n\n    positions = utils.check_intraday(estimate_intraday, returns,\n                                     positions, transactions)\n\n    idx = positions.index & style_factor_panel.iloc[0].index & sectors.index \\\n        & caps.index & shares_held.index & volumes.index\n    positions = positions.loc[idx]\n\n    vertical_sections = 0\n    if style_factor_panel is not None:\n        vertical_sections += len(style_factor_panel.items)\n        new_style_dict = {}\n        for item in style_factor_panel.items:\n            new_style_dict.update({item:\n                                   style_factor_panel.loc[item].loc[idx]})\n        style_factor_panel = pd.Panel()\n        style_factor_panel = style_factor_panel.from_dict(new_style_dict)\n    if sectors is not None:\n        vertical_sections += 4\n        sectors = sectors.loc[idx]\n    if caps is not None:\n        vertical_sections += 4\n        caps = caps.loc[idx]\n    if (shares_held is not None) & (volumes is not None) \\\n                                 & (percentile is not None):\n        vertical_sections += 3\n        shares_held = shares_held.loc[idx]\n        volumes = volumes.loc[idx]\n\n    if percentile is None:\n        percentile = 0.1\n\n    fig = plt.figure(figsize=[14, vertical_sections * 6])\n    gs = gridspec.GridSpec(vertical_sections, 3, wspace=0.5, hspace=0.5)\n\n    if style_factor_panel is not None:\n        style_axes = []\n        style_axes.append(plt.subplot(gs[0, :]))\n        for i in range(1, len(style_factor_panel.items)):\n            style_axes.append(plt.subplot(gs[i, :], sharex=style_axes[0]))\n\n        j = 0\n        for name, df in style_factor_panel.iteritems():\n            sfe = risk.compute_style_factor_exposures(positions, df)\n            risk.plot_style_factor_exposures(sfe, name, style_axes[j])\n            j += 1\n\n    if sectors is not None:\n        i += 1\n        ax_sector_longshort = plt.subplot(gs[i:i+2, :], sharex=style_axes[0])\n        i += 2\n        ax_sector_gross = plt.subplot(gs[i, :], sharex=style_axes[0])\n        i += 1\n        ax_sector_net = plt.subplot(gs[i, :], sharex=style_axes[0])\n        long_exposures, short_exposures, gross_exposures, net_exposures \\\n            = risk.compute_sector_exposures(positions, sectors)\n        risk.plot_sector_exposures_longshort(long_exposures, short_exposures,\n                                             ax=ax_sector_longshort)\n        risk.plot_sector_exposures_gross(gross_exposures, ax=ax_sector_gross)\n        risk.plot_sector_exposures_net(net_exposures, ax=ax_sector_net)\n\n    if caps is not None:\n        i += 1\n        ax_cap_longshort = plt.subplot(gs[i:i+2, :], sharex=style_axes[0])\n        i += 2\n        ax_cap_gross = plt.subplot(gs[i, :], sharex=style_axes[0])\n        i += 1\n        ax_cap_net = plt.subplot(gs[i, :], sharex=style_axes[0])\n        long_exposures, short_exposures, gross_exposures, net_exposures \\\n            = risk.compute_cap_exposures(positions, caps)\n        risk.plot_cap_exposures_longshort(long_exposures, short_exposures,\n                                          ax_cap_longshort)\n        risk.plot_cap_exposures_gross(gross_exposures, ax_cap_gross)\n        risk.plot_cap_exposures_net(net_exposures, ax_cap_net)\n\n    if volumes is not None:\n        i += 1\n        ax_vol_longshort = plt.subplot(gs[i:i+2, :], sharex=style_axes[0])\n        i += 2\n        ax_vol_gross = plt.subplot(gs[i, :], sharex=style_axes[0])\n        longed_threshold, shorted_threshold, grossed_threshold \\\n            = risk.compute_volume_exposures(positions, volumes, percentile)\n        risk.plot_volume_exposures_longshort(longed_threshold,\n                                             shorted_threshold, percentile,\n                                             ax_vol_longshort)\n        risk.plot_volume_exposures_gross(grossed_threshold, percentile,\n                                         ax_vol_gross)\n\n    for ax in fig.axes:\n        plt.setp(ax.get_xticklabels(), visible=True)\n\n    if return_fig:\n        return fig",
    "doc": "Creates risk tear sheet: computes and plots style factor exposures, sector\n    exposures, market cap exposures and volume exposures.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily equity positions of algorithm, in dollars.\n        - DataFrame with dates as index, equities as columns\n        - Last column is cash held\n        - Example:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])             cash\n        2017-04-03\t-108062.40 \t  4401.540     2.247757e+07\n        2017-04-04\t-108852.00\t  4373.820     2.540999e+07\n        2017-04-05\t-119968.66\t  4336.200     2.839812e+07\n\n    style_factor_panel : pd.Panel\n        Panel where each item is a DataFrame that tabulates style factor per\n        equity per day.\n        - Each item has dates as index, equities as columns\n        - Example item:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])\n        2017-04-03\t  -0.51284     1.39173\n        2017-04-04\t  -0.73381     0.98149\n        2017-04-05\t  -0.90132\t   1.13981\n\n    sectors : pd.DataFrame\n        Daily Morningstar sector code per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                     Equity(24   Equity(62\n                       [AAPL])      [ABT])\n        2017-04-03\t     311.0       206.0\n        2017-04-04\t     311.0       206.0\n        2017-04-05\t     311.0\t     206.0\n\n    caps : pd.DataFrame\n        Daily market cap per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03     1.327160e+10     6.402460e+10\n        2017-04-04\t   1.329620e+10     6.403694e+10\n        2017-04-05\t   1.297464e+10\t    6.397187e+10\n\n    shares_held : pd.DataFrame\n        Daily number of shares held by an algorithm.\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03             1915            -2595\n        2017-04-04\t           1968            -3272\n        2017-04-05\t           2104            -3917\n\n    volumes : pd.DataFrame\n        Daily volume per asset\n        - DataFrame with dates as index and equities as columns\n        - Example:\n                          Equity(24        Equity(62\n                            [AAPL])           [ABT])\n        2017-04-03      34940859.00       4665573.80\n        2017-04-04\t    35603329.10       4818463.90\n        2017-04-05\t    41846731.75\t      4129153.10\n\n    percentile : float\n        Percentile to use when computing and plotting volume exposures.\n        - Defaults to 10th percentile"
  },
  {
    "code": "def create_perf_attrib_tear_sheet(returns,\n                                  positions,\n                                  factor_returns,\n                                  factor_loadings,\n                                  transactions=None,\n                                  pos_in_dollars=True,\n                                  return_fig=False,\n                                  factor_partitions=FACTOR_PARTITIONS):\n    \"\"\"\n    Generate plots and tables for analyzing a strategy's performance.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date\n        and ticker as index, and factors as columns.\n\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n         - Default is None.\n\n    pos_in_dollars : boolean, optional\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n\n    factor_partitions : dict\n        dict specifying how factors should be separated in factor returns\n        and risk exposures plots\n        - Example:\n          {'style': ['momentum', 'size', 'value', ...],\n           'sector': ['technology', 'materials', ... ]}\n    \"\"\"\n    portfolio_exposures, perf_attrib_data = perf_attrib.perf_attrib(\n        returns, positions, factor_returns, factor_loadings, transactions,\n        pos_in_dollars=pos_in_dollars\n    )\n\n    display(Markdown(\"## Performance Relative to Common Risk Factors\"))\n\n    # aggregate perf attrib stats and show summary table\n    perf_attrib.show_perf_attrib_stats(returns, positions, factor_returns,\n                                       factor_loadings, transactions,\n                                       pos_in_dollars)\n\n    # one section for the returns plot, and for each factor grouping\n    # one section for factor returns, and one for risk exposures\n    vertical_sections = 1 + 2 * max(len(factor_partitions), 1)\n    current_section = 0\n\n    fig = plt.figure(figsize=[14, vertical_sections * 6])\n\n    gs = gridspec.GridSpec(vertical_sections, 1,\n                           wspace=0.5, hspace=0.5)\n\n    perf_attrib.plot_returns(perf_attrib_data,\n                             ax=plt.subplot(gs[current_section]))\n    current_section += 1\n\n    if factor_partitions is not None:\n\n        for factor_type, partitions in factor_partitions.iteritems():\n\n            columns_to_select = perf_attrib_data.columns.intersection(\n                partitions\n            )\n\n            perf_attrib.plot_factor_contribution_to_perf(\n                perf_attrib_data[columns_to_select],\n                ax=plt.subplot(gs[current_section]),\n                title=(\n                    'Cumulative common {} returns attribution'\n                ).format(factor_type)\n            )\n            current_section += 1\n\n        for factor_type, partitions in factor_partitions.iteritems():\n\n            perf_attrib.plot_risk_exposures(\n                portfolio_exposures[portfolio_exposures.columns\n                                    .intersection(partitions)],\n                ax=plt.subplot(gs[current_section]),\n                title='Daily {} factor exposures'.format(factor_type)\n            )\n            current_section += 1\n\n    else:\n\n        perf_attrib.plot_factor_contribution_to_perf(\n            perf_attrib_data,\n            ax=plt.subplot(gs[current_section])\n        )\n        current_section += 1\n\n        perf_attrib.plot_risk_exposures(\n            portfolio_exposures,\n            ax=plt.subplot(gs[current_section])\n        )\n\n    gs.tight_layout(fig)\n\n    if return_fig:\n        return fig",
    "doc": "Generate plots and tables for analyzing a strategy's performance.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date\n        and ticker as index, and factors as columns.\n\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n         - Default is None.\n\n    pos_in_dollars : boolean, optional\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n\n    factor_partitions : dict\n        dict specifying how factors should be separated in factor returns\n        and risk exposures plots\n        - Example:\n          {'style': ['momentum', 'size', 'value', ...],\n           'sector': ['technology', 'materials', ... ]}"
  },
  {
    "code": "def daily_txns_with_bar_data(transactions, market_data):\n    \"\"\"\n    Sums the absolute value of shares traded in each name on each day.\n    Adds columns containing the closing price and total daily volume for\n    each day-ticker combination.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Contains \"volume\" and \"price\" DataFrames for the tickers\n        in the passed positions DataFrames\n\n    Returns\n    -------\n    txn_daily : pd.DataFrame\n        Daily totals for transacted shares in each traded name.\n        price and volume columns for close price and daily volume for\n        the corresponding ticker, respectively.\n    \"\"\"\n\n    transactions.index.name = 'date'\n    txn_daily = pd.DataFrame(transactions.assign(\n        amount=abs(transactions.amount)).groupby(\n        ['symbol', pd.TimeGrouper('D')]).sum()['amount'])\n    txn_daily['price'] = market_data['price'].unstack()\n    txn_daily['volume'] = market_data['volume'].unstack()\n\n    txn_daily = txn_daily.reset_index().set_index('date')\n\n    return txn_daily",
    "doc": "Sums the absolute value of shares traded in each name on each day.\n    Adds columns containing the closing price and total daily volume for\n    each day-ticker combination.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Contains \"volume\" and \"price\" DataFrames for the tickers\n        in the passed positions DataFrames\n\n    Returns\n    -------\n    txn_daily : pd.DataFrame\n        Daily totals for transacted shares in each traded name.\n        price and volume columns for close price and daily volume for\n        the corresponding ticker, respectively."
  },
  {
    "code": "def days_to_liquidate_positions(positions, market_data,\n                                max_bar_consumption=0.2,\n                                capital_base=1e6,\n                                mean_volume_window=5):\n    \"\"\"\n    Compute the number of days that would have been required\n    to fully liquidate each position on each day based on the\n    trailing n day mean daily bar volume and a limit on the proportion\n    of a daily bar that we are allowed to consume.\n\n    This analysis uses portfolio allocations and a provided capital base\n    rather than the dollar values in the positions DataFrame to remove the\n    effect of compounding on days to liquidate. In other words, this function\n    assumes that the net liquidation portfolio value will always remain\n    constant at capital_base.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Contains daily position values including cash\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    max_bar_consumption : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position.\n    capital_base : integer\n        Capital base multiplied by portfolio allocation to compute\n        position value that needs liquidating.\n    mean_volume_window : float\n        Trailing window to use in mean volume calculation.\n\n    Returns\n    -------\n    days_to_liquidate : pd.DataFrame\n        Number of days required to fully liquidate daily positions.\n        Datetime index, symbols as columns.\n    \"\"\"\n\n    DV = market_data['volume'] * market_data['price']\n    roll_mean_dv = DV.rolling(window=mean_volume_window,\n                              center=False).mean().shift()\n    roll_mean_dv = roll_mean_dv.replace(0, np.nan)\n\n    positions_alloc = pos.get_percent_alloc(positions)\n    positions_alloc = positions_alloc.drop('cash', axis=1)\n\n    days_to_liquidate = (positions_alloc * capital_base) / \\\n        (max_bar_consumption * roll_mean_dv)\n\n    return days_to_liquidate.iloc[mean_volume_window:]",
    "doc": "Compute the number of days that would have been required\n    to fully liquidate each position on each day based on the\n    trailing n day mean daily bar volume and a limit on the proportion\n    of a daily bar that we are allowed to consume.\n\n    This analysis uses portfolio allocations and a provided capital base\n    rather than the dollar values in the positions DataFrame to remove the\n    effect of compounding on days to liquidate. In other words, this function\n    assumes that the net liquidation portfolio value will always remain\n    constant at capital_base.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Contains daily position values including cash\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    max_bar_consumption : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position.\n    capital_base : integer\n        Capital base multiplied by portfolio allocation to compute\n        position value that needs liquidating.\n    mean_volume_window : float\n        Trailing window to use in mean volume calculation.\n\n    Returns\n    -------\n    days_to_liquidate : pd.DataFrame\n        Number of days required to fully liquidate daily positions.\n        Datetime index, symbols as columns."
  },
  {
    "code": "def get_max_days_to_liquidate_by_ticker(positions, market_data,\n                                        max_bar_consumption=0.2,\n                                        capital_base=1e6,\n                                        mean_volume_window=5,\n                                        last_n_days=None):\n    \"\"\"\n    Finds the longest estimated liquidation time for each traded\n    name over the course of backtest (or last n days of the backtest).\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Contains daily position values including cash\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    max_bar_consumption : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position.\n    capital_base : integer\n        Capital base multiplied by portfolio allocation to compute\n        position value that needs liquidating.\n    mean_volume_window : float\n        Trailing window to use in mean volume calculation.\n    last_n_days : integer\n        Compute for only the last n days of the passed backtest data.\n\n    Returns\n    -------\n    days_to_liquidate : pd.DataFrame\n        Max Number of days required to fully liquidate each traded name.\n        Index of symbols. Columns for days_to_liquidate and the corresponding\n        date and position_alloc on that day.\n    \"\"\"\n\n    dtlp = days_to_liquidate_positions(positions, market_data,\n                                       max_bar_consumption=max_bar_consumption,\n                                       capital_base=capital_base,\n                                       mean_volume_window=mean_volume_window)\n\n    if last_n_days is not None:\n        dtlp = dtlp.loc[dtlp.index.max() - pd.Timedelta(days=last_n_days):]\n\n    pos_alloc = pos.get_percent_alloc(positions)\n    pos_alloc = pos_alloc.drop('cash', axis=1)\n\n    liq_desc = pd.DataFrame()\n    liq_desc['days_to_liquidate'] = dtlp.unstack()\n    liq_desc['pos_alloc_pct'] = pos_alloc.unstack() * 100\n    liq_desc.index.levels[0].name = 'symbol'\n    liq_desc.index.levels[1].name = 'date'\n\n    worst_liq = liq_desc.reset_index().sort_values(\n        'days_to_liquidate', ascending=False).groupby('symbol').first()\n\n    return worst_liq",
    "doc": "Finds the longest estimated liquidation time for each traded\n    name over the course of backtest (or last n days of the backtest).\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Contains daily position values including cash\n        - See full explanation in tears.create_full_tear_sheet\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    max_bar_consumption : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position.\n    capital_base : integer\n        Capital base multiplied by portfolio allocation to compute\n        position value that needs liquidating.\n    mean_volume_window : float\n        Trailing window to use in mean volume calculation.\n    last_n_days : integer\n        Compute for only the last n days of the passed backtest data.\n\n    Returns\n    -------\n    days_to_liquidate : pd.DataFrame\n        Max Number of days required to fully liquidate each traded name.\n        Index of symbols. Columns for days_to_liquidate and the corresponding\n        date and position_alloc on that day."
  },
  {
    "code": "def get_low_liquidity_transactions(transactions, market_data,\n                                   last_n_days=None):\n    \"\"\"\n    For each traded name, find the daily transaction total that consumed\n    the greatest proportion of available daily bar volume.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    last_n_days : integer\n        Compute for only the last n days of the passed backtest data.\n    \"\"\"\n\n    txn_daily_w_bar = daily_txns_with_bar_data(transactions, market_data)\n    txn_daily_w_bar.index.name = 'date'\n    txn_daily_w_bar = txn_daily_w_bar.reset_index()\n\n    if last_n_days is not None:\n        md = txn_daily_w_bar.date.max() - pd.Timedelta(days=last_n_days)\n        txn_daily_w_bar = txn_daily_w_bar[txn_daily_w_bar.date > md]\n\n    bar_consumption = txn_daily_w_bar.assign(\n        max_pct_bar_consumed=(\n            txn_daily_w_bar.amount/txn_daily_w_bar.volume)*100\n    ).sort_values('max_pct_bar_consumed', ascending=False)\n    max_bar_consumption = bar_consumption.groupby('symbol').first()\n\n    return max_bar_consumption[['date', 'max_pct_bar_consumed']]",
    "doc": "For each traded name, find the daily transaction total that consumed\n    the greatest proportion of available daily bar volume.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    market_data : pd.Panel\n        Panel with items axis of 'price' and 'volume' DataFrames.\n        The major and minor axes should match those of the\n        the passed positions DataFrame (same dates and symbols).\n    last_n_days : integer\n        Compute for only the last n days of the passed backtest data."
  },
  {
    "code": "def apply_slippage_penalty(returns, txn_daily, simulate_starting_capital,\n                           backtest_starting_capital, impact=0.1):\n    \"\"\"\n    Applies quadratic volumeshare slippage model to daily returns based\n    on the proportion of the observed historical daily bar dollar volume\n    consumed by the strategy's trades. Scales the size of trades based\n    on the ratio of the starting capital we wish to test to the starting\n    capital of the passed backtest data.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Time series of daily returns.\n    txn_daily : pd.Series\n        Daily transaciton totals, closing price, and daily volume for\n        each traded name. See price_volume_daily_txns for more details.\n    simulate_starting_capital : integer\n        capital at which we want to test\n    backtest_starting_capital: capital base at which backtest was\n        origionally run. impact: See Zipline volumeshare slippage model\n    impact : float\n        Scales the size of the slippage penalty.\n\n    Returns\n    -------\n    adj_returns : pd.Series\n        Slippage penalty adjusted daily returns.\n    \"\"\"\n\n    mult = simulate_starting_capital / backtest_starting_capital\n    simulate_traded_shares = abs(mult * txn_daily.amount)\n    simulate_traded_dollars = txn_daily.price * simulate_traded_shares\n    simulate_pct_volume_used = simulate_traded_shares / txn_daily.volume\n\n    penalties = simulate_pct_volume_used**2 \\\n        * impact * simulate_traded_dollars\n\n    daily_penalty = penalties.resample('D').sum()\n    daily_penalty = daily_penalty.reindex(returns.index).fillna(0)\n\n    # Since we are scaling the numerator of the penalties linearly\n    # by capital base, it makes the most sense to scale the denominator\n    # similarly. In other words, since we aren't applying compounding to\n    # simulate_traded_shares, we shouldn't apply compounding to pv.\n    portfolio_value = ep.cum_returns(\n        returns, starting_value=backtest_starting_capital) * mult\n\n    adj_returns = returns - (daily_penalty / portfolio_value)\n\n    return adj_returns",
    "doc": "Applies quadratic volumeshare slippage model to daily returns based\n    on the proportion of the observed historical daily bar dollar volume\n    consumed by the strategy's trades. Scales the size of trades based\n    on the ratio of the starting capital we wish to test to the starting\n    capital of the passed backtest data.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Time series of daily returns.\n    txn_daily : pd.Series\n        Daily transaciton totals, closing price, and daily volume for\n        each traded name. See price_volume_daily_txns for more details.\n    simulate_starting_capital : integer\n        capital at which we want to test\n    backtest_starting_capital: capital base at which backtest was\n        origionally run. impact: See Zipline volumeshare slippage model\n    impact : float\n        Scales the size of the slippage penalty.\n\n    Returns\n    -------\n    adj_returns : pd.Series\n        Slippage penalty adjusted daily returns."
  },
  {
    "code": "def map_transaction(txn):\n    \"\"\"\n    Maps a single transaction row to a dictionary.\n\n    Parameters\n    ----------\n    txn : pd.DataFrame\n        A single transaction object to convert to a dictionary.\n\n    Returns\n    -------\n    dict\n        Mapped transaction.\n    \"\"\"\n\n    if isinstance(txn['sid'], dict):\n        sid = txn['sid']['sid']\n        symbol = txn['sid']['symbol']\n    else:\n        sid = txn['sid']\n        symbol = txn['sid']\n\n    return {'sid': sid,\n            'symbol': symbol,\n            'price': txn['price'],\n            'order_id': txn['order_id'],\n            'amount': txn['amount'],\n            'commission': txn['commission'],\n            'dt': txn['dt']}",
    "doc": "Maps a single transaction row to a dictionary.\n\n    Parameters\n    ----------\n    txn : pd.DataFrame\n        A single transaction object to convert to a dictionary.\n\n    Returns\n    -------\n    dict\n        Mapped transaction."
  },
  {
    "code": "def make_transaction_frame(transactions):\n    \"\"\"\n    Formats a transaction DataFrame.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Contains improperly formatted transactional data.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        Daily transaction volume and dollar ammount.\n         - See full explanation in tears.create_full_tear_sheet.\n    \"\"\"\n\n    transaction_list = []\n    for dt in transactions.index:\n        txns = transactions.loc[dt]\n        if len(txns) == 0:\n            continue\n\n        for txn in txns:\n            txn = map_transaction(txn)\n            transaction_list.append(txn)\n    df = pd.DataFrame(sorted(transaction_list, key=lambda x: x['dt']))\n    df['txn_dollars'] = -df['amount'] * df['price']\n\n    df.index = list(map(pd.Timestamp, df.dt.values))\n    return df",
    "doc": "Formats a transaction DataFrame.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Contains improperly formatted transactional data.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        Daily transaction volume and dollar ammount.\n         - See full explanation in tears.create_full_tear_sheet."
  },
  {
    "code": "def get_txn_vol(transactions):\n    \"\"\"\n    Extract daily transaction data from set of transaction objects.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Time series containing one row per symbol (and potentially\n        duplicate datetime indices) and columns for amount and\n        price.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily transaction volume and number of shares.\n         - See full explanation in tears.create_full_tear_sheet.\n    \"\"\"\n\n    txn_norm = transactions.copy()\n    txn_norm.index = txn_norm.index.normalize()\n    amounts = txn_norm.amount.abs()\n    prices = txn_norm.price\n    values = amounts * prices\n    daily_amounts = amounts.groupby(amounts.index).sum()\n    daily_values = values.groupby(values.index).sum()\n    daily_amounts.name = \"txn_shares\"\n    daily_values.name = \"txn_volume\"\n    return pd.concat([daily_values, daily_amounts], axis=1)",
    "doc": "Extract daily transaction data from set of transaction objects.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Time series containing one row per symbol (and potentially\n        duplicate datetime indices) and columns for amount and\n        price.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily transaction volume and number of shares.\n         - See full explanation in tears.create_full_tear_sheet."
  },
  {
    "code": "def adjust_returns_for_slippage(returns, positions, transactions,\n                                slippage_bps):\n    \"\"\"\n    Apply a slippage penalty for every dollar traded.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    slippage_bps: int/float\n        Basis points of slippage to apply.\n\n    Returns\n    -------\n    pd.Series\n        Time series of daily returns, adjusted for slippage.\n    \"\"\"\n\n    slippage = 0.0001 * slippage_bps\n    portfolio_value = positions.sum(axis=1)\n    pnl = portfolio_value * returns\n    traded_value = get_txn_vol(transactions).txn_volume\n    slippage_dollars = traded_value * slippage\n    adjusted_pnl = pnl.add(-slippage_dollars, fill_value=0)\n    adjusted_returns = returns * adjusted_pnl / pnl\n\n    return adjusted_returns",
    "doc": "Apply a slippage penalty for every dollar traded.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    slippage_bps: int/float\n        Basis points of slippage to apply.\n\n    Returns\n    -------\n    pd.Series\n        Time series of daily returns, adjusted for slippage."
  },
  {
    "code": "def get_turnover(positions, transactions, denominator='AGB'):\n    \"\"\"\n     - Value of purchases and sales divided\n    by either the actual gross book or the portfolio value\n    for the time step.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Contains daily position values including cash.\n        - See full explanation in tears.create_full_tear_sheet\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    denominator : str, optional\n        Either 'AGB' or 'portfolio_value', default AGB.\n        - AGB (Actual gross book) is the gross market\n        value (GMV) of the specific algo being analyzed.\n        Swapping out an entire portfolio of stocks for\n        another will yield 200% turnover, not 100%, since\n        transactions are being made for both sides.\n        - We use average of the previous and the current end-of-period\n        AGB to avoid singularities when trading only into or\n        out of an entire book in one trading period.\n        - portfolio_value is the total value of the algo's\n        positions end-of-period, including cash.\n\n    Returns\n    -------\n    turnover_rate : pd.Series\n        timeseries of portfolio turnover rates.\n    \"\"\"\n\n    txn_vol = get_txn_vol(transactions)\n    traded_value = txn_vol.txn_volume\n\n    if denominator == 'AGB':\n        # Actual gross book is the same thing as the algo's GMV\n        # We want our denom to be avg(AGB previous, AGB current)\n        AGB = positions.drop('cash', axis=1).abs().sum(axis=1)\n        denom = AGB.rolling(2).mean()\n\n        # Since the first value of pd.rolling returns NaN, we\n        # set our \"day 0\" AGB to 0.\n        denom.iloc[0] = AGB.iloc[0] / 2\n    elif denominator == 'portfolio_value':\n        denom = positions.sum(axis=1)\n    else:\n        raise ValueError(\n            \"Unexpected value for denominator '{}'. The \"\n            \"denominator parameter must be either 'AGB'\"\n            \" or 'portfolio_value'.\".format(denominator)\n        )\n\n    denom.index = denom.index.normalize()\n    turnover = traded_value.div(denom, axis='index')\n    turnover = turnover.fillna(0)\n    return turnover",
    "doc": "- Value of purchases and sales divided\n    by either the actual gross book or the portfolio value\n    for the time step.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Contains daily position values including cash.\n        - See full explanation in tears.create_full_tear_sheet\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    denominator : str, optional\n        Either 'AGB' or 'portfolio_value', default AGB.\n        - AGB (Actual gross book) is the gross market\n        value (GMV) of the specific algo being analyzed.\n        Swapping out an entire portfolio of stocks for\n        another will yield 200% turnover, not 100%, since\n        transactions are being made for both sides.\n        - We use average of the previous and the current end-of-period\n        AGB to avoid singularities when trading only into or\n        out of an entire book in one trading period.\n        - portfolio_value is the total value of the algo's\n        positions end-of-period, including cash.\n\n    Returns\n    -------\n    turnover_rate : pd.Series\n        timeseries of portfolio turnover rates."
  },
  {
    "code": "def _groupby_consecutive(txn, max_delta=pd.Timedelta('8h')):\n    \"\"\"Merge transactions of the same direction separated by less than\n    max_delta time duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    max_delta : pandas.Timedelta (optional)\n        Merge transactions in the same direction separated by less\n        than max_delta time duration.\n\n\n    Returns\n    -------\n    transactions : pd.DataFrame\n\n    \"\"\"\n    def vwap(transaction):\n        if transaction.amount.sum() == 0:\n            warnings.warn('Zero transacted shares, setting vwap to nan.')\n            return np.nan\n        return (transaction.amount * transaction.price).sum() / \\\n            transaction.amount.sum()\n\n    out = []\n    for sym, t in txn.groupby('symbol'):\n        t = t.sort_index()\n        t.index.name = 'dt'\n        t = t.reset_index()\n\n        t['order_sign'] = t.amount > 0\n        t['block_dir'] = (t.order_sign.shift(\n            1) != t.order_sign).astype(int).cumsum()\n        t['block_time'] = ((t.dt.sub(t.dt.shift(1))) >\n                           max_delta).astype(int).cumsum()\n        grouped_price = (t.groupby(('block_dir',\n                                   'block_time'))\n                          .apply(vwap))\n        grouped_price.name = 'price'\n        grouped_rest = t.groupby(('block_dir', 'block_time')).agg({\n            'amount': 'sum',\n            'symbol': 'first',\n            'dt': 'first'})\n\n        grouped = grouped_rest.join(grouped_price)\n\n        out.append(grouped)\n\n    out = pd.concat(out)\n    out = out.set_index('dt')\n    return out",
    "doc": "Merge transactions of the same direction separated by less than\n    max_delta time duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    max_delta : pandas.Timedelta (optional)\n        Merge transactions in the same direction separated by less\n        than max_delta time duration.\n\n\n    Returns\n    -------\n    transactions : pd.DataFrame"
  },
  {
    "code": "def extract_round_trips(transactions,\n                        portfolio_value=None):\n    \"\"\"Group transactions into \"round trips\". First, transactions are\n    grouped by day and directionality. Then, long and short\n    transactions are matched to create round-trip round_trips for which\n    PnL, duration and returns are computed. Crossings where a position\n    changes from long to short and vice-versa are handled correctly.\n\n    Under the hood, we reconstruct the individual shares in a\n    portfolio over time and match round_trips in a FIFO-order.\n\n    For example, the following transactions would constitute one round trip:\n    index                  amount   price    symbol\n    2004-01-09 12:18:01    10       50      'AAPL'\n    2004-01-09 15:12:53    10       100      'AAPL'\n    2004-01-13 14:41:23    -10      100      'AAPL'\n    2004-01-13 15:23:34    -10      200       'AAPL'\n\n    First, the first two and last two round_trips will be merged into a two\n    single transactions (computing the price via vwap). Then, during\n    the portfolio reconstruction, the two resulting transactions will\n    be merged and result in 1 round-trip trade with a PnL of\n    (150 * 20) - (75 * 20) = 1500.\n\n    Note, that round trips do not have to close out positions\n    completely. For example, we could have removed the last\n    transaction in the example above and still generated a round-trip\n    over 10 shares with 10 shares left in the portfolio to be matched\n    with a later transaction.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    portfolio_value : pd.Series (optional)\n        Portfolio value (all net assets including cash) over time.\n        Note that portfolio_value needs to beginning of day, so either\n        use .shift() or positions.sum(axis='columns') / (1+returns).\n\n    Returns\n    -------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip.  The returns column\n        contains returns in respect to the portfolio value while\n        rt_returns are the returns in regards to the invested capital\n        into that partiulcar round-trip.\n    \"\"\"\n\n    transactions = _groupby_consecutive(transactions)\n    roundtrips = []\n\n    for sym, trans_sym in transactions.groupby('symbol'):\n        trans_sym = trans_sym.sort_index()\n        price_stack = deque()\n        dt_stack = deque()\n        trans_sym['signed_price'] = trans_sym.price * \\\n            np.sign(trans_sym.amount)\n        trans_sym['abs_amount'] = trans_sym.amount.abs().astype(int)\n        for dt, t in trans_sym.iterrows():\n            if t.price < 0:\n                warnings.warn('Negative price detected, ignoring for'\n                              'round-trip.')\n                continue\n\n            indiv_prices = [t.signed_price] * t.abs_amount\n            if (len(price_stack) == 0) or \\\n               (copysign(1, price_stack[-1]) == copysign(1, t.amount)):\n                price_stack.extend(indiv_prices)\n                dt_stack.extend([dt] * len(indiv_prices))\n            else:\n                # Close round-trip\n                pnl = 0\n                invested = 0\n                cur_open_dts = []\n\n                for price in indiv_prices:\n                    if len(price_stack) != 0 and \\\n                       (copysign(1, price_stack[-1]) != copysign(1, price)):\n                        # Retrieve first dt, stock-price pair from\n                        # stack\n                        prev_price = price_stack.popleft()\n                        prev_dt = dt_stack.popleft()\n\n                        pnl += -(price + prev_price)\n                        cur_open_dts.append(prev_dt)\n                        invested += abs(prev_price)\n\n                    else:\n                        # Push additional stock-prices onto stack\n                        price_stack.append(price)\n                        dt_stack.append(dt)\n\n                roundtrips.append({'pnl': pnl,\n                                   'open_dt': cur_open_dts[0],\n                                   'close_dt': dt,\n                                   'long': price < 0,\n                                   'rt_returns': pnl / invested,\n                                   'symbol': sym,\n                                   })\n\n    roundtrips = pd.DataFrame(roundtrips)\n\n    roundtrips['duration'] = roundtrips['close_dt'].sub(roundtrips['open_dt'])\n\n    if portfolio_value is not None:\n        # Need to normalize so that we can join\n        pv = pd.DataFrame(portfolio_value,\n                          columns=['portfolio_value'])\\\n            .assign(date=portfolio_value.index)\n\n        roundtrips['date'] = roundtrips.close_dt.apply(lambda x:\n                                                       x.replace(hour=0,\n                                                                 minute=0,\n                                                                 second=0))\n\n        tmp = roundtrips.join(pv, on='date', lsuffix='_')\n\n        roundtrips['returns'] = tmp.pnl / tmp.portfolio_value\n        roundtrips = roundtrips.drop('date', axis='columns')\n\n    return roundtrips",
    "doc": "Group transactions into \"round trips\". First, transactions are\n    grouped by day and directionality. Then, long and short\n    transactions are matched to create round-trip round_trips for which\n    PnL, duration and returns are computed. Crossings where a position\n    changes from long to short and vice-versa are handled correctly.\n\n    Under the hood, we reconstruct the individual shares in a\n    portfolio over time and match round_trips in a FIFO-order.\n\n    For example, the following transactions would constitute one round trip:\n    index                  amount   price    symbol\n    2004-01-09 12:18:01    10       50      'AAPL'\n    2004-01-09 15:12:53    10       100      'AAPL'\n    2004-01-13 14:41:23    -10      100      'AAPL'\n    2004-01-13 15:23:34    -10      200       'AAPL'\n\n    First, the first two and last two round_trips will be merged into a two\n    single transactions (computing the price via vwap). Then, during\n    the portfolio reconstruction, the two resulting transactions will\n    be merged and result in 1 round-trip trade with a PnL of\n    (150 * 20) - (75 * 20) = 1500.\n\n    Note, that round trips do not have to close out positions\n    completely. For example, we could have removed the last\n    transaction in the example above and still generated a round-trip\n    over 10 shares with 10 shares left in the portfolio to be matched\n    with a later transaction.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    portfolio_value : pd.Series (optional)\n        Portfolio value (all net assets including cash) over time.\n        Note that portfolio_value needs to beginning of day, so either\n        use .shift() or positions.sum(axis='columns') / (1+returns).\n\n    Returns\n    -------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip.  The returns column\n        contains returns in respect to the portfolio value while\n        rt_returns are the returns in regards to the invested capital\n        into that partiulcar round-trip."
  },
  {
    "code": "def add_closing_transactions(positions, transactions):\n    \"\"\"\n    Appends transactions that close out all positions at the end of\n    the timespan covered by positions data. Utilizes pricing information\n    in the positions DataFrame to determine closing price.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    Returns\n    -------\n    closed_txns : pd.DataFrame\n        Transactions with closing transactions appended.\n    \"\"\"\n\n    closed_txns = transactions[['symbol', 'amount', 'price']]\n\n    pos_at_end = positions.drop('cash', axis=1).iloc[-1]\n    open_pos = pos_at_end.replace(0, np.nan).dropna()\n    # Add closing round_trips one second after the close to be sure\n    # they don't conflict with other round_trips executed at that time.\n    end_dt = open_pos.name + pd.Timedelta(seconds=1)\n\n    for sym, ending_val in open_pos.iteritems():\n        txn_sym = transactions[transactions.symbol == sym]\n\n        ending_amount = txn_sym.amount.sum()\n\n        ending_price = ending_val / ending_amount\n        closing_txn = {'symbol': sym,\n                       'amount': -ending_amount,\n                       'price': ending_price}\n\n        closing_txn = pd.DataFrame(closing_txn, index=[end_dt])\n        closed_txns = closed_txns.append(closing_txn)\n\n    closed_txns = closed_txns[closed_txns.amount != 0]\n\n    return closed_txns",
    "doc": "Appends transactions that close out all positions at the end of\n    the timespan covered by positions data. Utilizes pricing information\n    in the positions DataFrame to determine closing price.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    Returns\n    -------\n    closed_txns : pd.DataFrame\n        Transactions with closing transactions appended."
  },
  {
    "code": "def apply_sector_mappings_to_round_trips(round_trips, sector_mappings):\n    \"\"\"\n    Translates round trip symbols to sectors.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n\n    Returns\n    -------\n    sector_round_trips : pd.DataFrame\n        Round trips with symbol names replaced by sector names.\n    \"\"\"\n\n    sector_round_trips = round_trips.copy()\n    sector_round_trips.symbol = sector_round_trips.symbol.apply(\n        lambda x: sector_mappings.get(x, 'No Sector Mapping'))\n    sector_round_trips = sector_round_trips.dropna(axis=0)\n\n    return sector_round_trips",
    "doc": "Translates round trip symbols to sectors.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n\n    Returns\n    -------\n    sector_round_trips : pd.DataFrame\n        Round trips with symbol names replaced by sector names."
  },
  {
    "code": "def gen_round_trip_stats(round_trips):\n    \"\"\"Generate various round-trip statistics.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n\n    Returns\n    -------\n    stats : dict\n       A dictionary where each value is a pandas DataFrame containing\n       various round-trip statistics.\n\n    See also\n    --------\n    round_trips.print_round_trip_stats\n    \"\"\"\n\n    stats = {}\n    stats['pnl'] = agg_all_long_short(round_trips, 'pnl', PNL_STATS)\n    stats['summary'] = agg_all_long_short(round_trips, 'pnl',\n                                          SUMMARY_STATS)\n    stats['duration'] = agg_all_long_short(round_trips, 'duration',\n                                           DURATION_STATS)\n    stats['returns'] = agg_all_long_short(round_trips, 'returns',\n                                          RETURN_STATS)\n\n    stats['symbols'] = \\\n        round_trips.groupby('symbol')['returns'].agg(RETURN_STATS).T\n\n    return stats",
    "doc": "Generate various round-trip statistics.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n\n    Returns\n    -------\n    stats : dict\n       A dictionary where each value is a pandas DataFrame containing\n       various round-trip statistics.\n\n    See also\n    --------\n    round_trips.print_round_trip_stats"
  },
  {
    "code": "def print_round_trip_stats(round_trips, hide_pos=False):\n    \"\"\"Print various round-trip statistics. Tries to pretty-print tables\n    with HTML output if run inside IPython NB.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n\n    See also\n    --------\n    round_trips.gen_round_trip_stats\n    \"\"\"\n\n    stats = gen_round_trip_stats(round_trips)\n\n    print_table(stats['summary'], float_format='{:.2f}'.format,\n                name='Summary stats')\n    print_table(stats['pnl'], float_format='${:.2f}'.format, name='PnL stats')\n    print_table(stats['duration'], float_format='{:.2f}'.format,\n                name='Duration stats')\n    print_table(stats['returns'] * 100, float_format='{:.2f}%'.format,\n                name='Return stats')\n\n    if not hide_pos:\n        stats['symbols'].columns = stats['symbols'].columns.map(format_asset)\n        print_table(stats['symbols'] * 100,\n                    float_format='{:.2f}%'.format, name='Symbol stats')",
    "doc": "Print various round-trip statistics. Tries to pretty-print tables\n    with HTML output if run inside IPython NB.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n\n    See also\n    --------\n    round_trips.gen_round_trip_stats"
  },
  {
    "code": "def perf_attrib(returns,\n                positions,\n                factor_returns,\n                factor_loadings,\n                transactions=None,\n                pos_in_dollars=True):\n    \"\"\"\n    Attributes the performance of a returns stream to a set of risk factors.\n\n    Preprocesses inputs, and then calls empyrical.perf_attrib. See\n    empyrical.perf_attrib for more info.\n\n    Performance attribution determines how much each risk factor, e.g.,\n    momentum, the technology sector, etc., contributed to total returns, as\n    well as the daily exposure to each of the risk factors. The returns that\n    can be attributed to one of the given risk factors are the\n    `common_returns`, and the returns that _cannot_ be attributed to a risk\n    factor are the `specific_returns`, or the alpha. The common_returns and\n    specific_returns summed together will always equal the total returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n        - Example:\n            2017-01-01   -0.017098\n            2017-01-02    0.002683\n            2017-01-03   -0.008669\n\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n        - Examples:\n                        AAPL  TLT  XOM  cash\n            2017-01-01    34   58   10     0\n            2017-01-02    22   77   18     0\n            2017-01-03   -15   27   30    15\n\n                            AAPL       TLT       XOM  cash\n            2017-01-01  0.333333  0.568627  0.098039   0.0\n            2017-01-02  0.188034  0.658120  0.153846   0.0\n            2017-01-03  0.208333  0.375000  0.416667   0.0\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n        - Example:\n                        momentum  reversal\n            2017-01-01  0.002779 -0.005453\n            2017-01-02  0.001096  0.010290\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices. Used to check the turnover of\n        the algorithm. Default is None, in which case the turnover check is\n        skipped.\n\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    Returns\n    -------\n    tuple of (risk_exposures_portfolio, perf_attribution)\n\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n\n    perf_attribution : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n    \"\"\"\n    (returns,\n     positions,\n     factor_returns,\n     factor_loadings) = _align_and_warn(returns,\n                                        positions,\n                                        factor_returns,\n                                        factor_loadings,\n                                        transactions=transactions,\n                                        pos_in_dollars=pos_in_dollars)\n\n    # Note that we convert positions to percentages *after* the checks\n    # above, since get_turnover() expects positions in dollars.\n    positions = _stack_positions(positions, pos_in_dollars=pos_in_dollars)\n\n    return ep.perf_attrib(returns, positions, factor_returns, factor_loadings)",
    "doc": "Attributes the performance of a returns stream to a set of risk factors.\n\n    Preprocesses inputs, and then calls empyrical.perf_attrib. See\n    empyrical.perf_attrib for more info.\n\n    Performance attribution determines how much each risk factor, e.g.,\n    momentum, the technology sector, etc., contributed to total returns, as\n    well as the daily exposure to each of the risk factors. The returns that\n    can be attributed to one of the given risk factors are the\n    `common_returns`, and the returns that _cannot_ be attributed to a risk\n    factor are the `specific_returns`, or the alpha. The common_returns and\n    specific_returns summed together will always equal the total returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n        - Example:\n            2017-01-01   -0.017098\n            2017-01-02    0.002683\n            2017-01-03   -0.008669\n\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n        - Examples:\n                        AAPL  TLT  XOM  cash\n            2017-01-01    34   58   10     0\n            2017-01-02    22   77   18     0\n            2017-01-03   -15   27   30    15\n\n                            AAPL       TLT       XOM  cash\n            2017-01-01  0.333333  0.568627  0.098039   0.0\n            2017-01-02  0.188034  0.658120  0.153846   0.0\n            2017-01-03  0.208333  0.375000  0.416667   0.0\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n        - Example:\n                        momentum  reversal\n            2017-01-01  0.002779 -0.005453\n            2017-01-02  0.001096  0.010290\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices. Used to check the turnover of\n        the algorithm. Default is None, in which case the turnover check is\n        skipped.\n\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    Returns\n    -------\n    tuple of (risk_exposures_portfolio, perf_attribution)\n\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n\n    perf_attribution : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980"
  },
  {
    "code": "def compute_exposures(positions, factor_loadings, stack_positions=True,\n                      pos_in_dollars=True):\n    \"\"\"\n    Compute daily risk factor exposures.\n\n    Normalizes positions (if necessary) and calls ep.compute_exposures.\n    See empyrical.compute_exposures for more info.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame or pd.Series\n        Daily holdings (in dollars or percentages), indexed by date, OR\n        a series of holdings indexed by date and ticker.\n        - Examples:\n                        AAPL  TLT  XOM  cash\n            2017-01-01    34   58   10     0\n            2017-01-02    22   77   18     0\n            2017-01-03   -15   27   30    15\n\n                            AAPL       TLT       XOM  cash\n            2017-01-01  0.333333  0.568627  0.098039   0.0\n            2017-01-02  0.188034  0.658120  0.153846   0.0\n            2017-01-03  0.208333  0.375000  0.416667   0.0\n\n            dt          ticker\n            2017-01-01  AAPL      0.417582\n                        TLT       0.010989\n                        XOM       0.571429\n            2017-01-02  AAPL      0.202381\n                        TLT       0.535714\n                        XOM       0.261905\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n    stack_positions : bool\n        Flag indicating whether `positions` should be converted to long format.\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    Returns\n    -------\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns.\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n    \"\"\"\n    if stack_positions:\n        positions = _stack_positions(positions, pos_in_dollars=pos_in_dollars)\n\n    return ep.compute_exposures(positions, factor_loadings)",
    "doc": "Compute daily risk factor exposures.\n\n    Normalizes positions (if necessary) and calls ep.compute_exposures.\n    See empyrical.compute_exposures for more info.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame or pd.Series\n        Daily holdings (in dollars or percentages), indexed by date, OR\n        a series of holdings indexed by date and ticker.\n        - Examples:\n                        AAPL  TLT  XOM  cash\n            2017-01-01    34   58   10     0\n            2017-01-02    22   77   18     0\n            2017-01-03   -15   27   30    15\n\n                            AAPL       TLT       XOM  cash\n            2017-01-01  0.333333  0.568627  0.098039   0.0\n            2017-01-02  0.188034  0.658120  0.153846   0.0\n            2017-01-03  0.208333  0.375000  0.416667   0.0\n\n            dt          ticker\n            2017-01-01  AAPL      0.417582\n                        TLT       0.010989\n                        XOM       0.571429\n            2017-01-02  AAPL      0.202381\n                        TLT       0.535714\n                        XOM       0.261905\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n    stack_positions : bool\n        Flag indicating whether `positions` should be converted to long format.\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    Returns\n    -------\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns.\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515"
  },
  {
    "code": "def create_perf_attrib_stats(perf_attrib, risk_exposures):\n    \"\"\"\n    Takes perf attribution data over a period of time and computes annualized\n    multifactor alpha, multifactor sharpe, risk exposures.\n    \"\"\"\n    summary = OrderedDict()\n    total_returns = perf_attrib['total_returns']\n    specific_returns = perf_attrib['specific_returns']\n    common_returns = perf_attrib['common_returns']\n\n    summary['Annualized Specific Return'] =\\\n        ep.annual_return(specific_returns)\n    summary['Annualized Common Return'] =\\\n        ep.annual_return(common_returns)\n    summary['Annualized Total Return'] =\\\n        ep.annual_return(total_returns)\n\n    summary['Specific Sharpe Ratio'] =\\\n        ep.sharpe_ratio(specific_returns)\n\n    summary['Cumulative Specific Return'] =\\\n        ep.cum_returns_final(specific_returns)\n    summary['Cumulative Common Return'] =\\\n        ep.cum_returns_final(common_returns)\n    summary['Total Returns'] =\\\n        ep.cum_returns_final(total_returns)\n\n    summary = pd.Series(summary, name='')\n\n    annualized_returns_by_factor = [ep.annual_return(perf_attrib[c])\n                                    for c in risk_exposures.columns]\n    cumulative_returns_by_factor = [ep.cum_returns_final(perf_attrib[c])\n                                    for c in risk_exposures.columns]\n\n    risk_exposure_summary = pd.DataFrame(\n        data=OrderedDict([\n            (\n                'Average Risk Factor Exposure',\n                risk_exposures.mean(axis='rows')\n            ),\n            ('Annualized Return', annualized_returns_by_factor),\n            ('Cumulative Return', cumulative_returns_by_factor),\n        ]),\n        index=risk_exposures.columns,\n    )\n\n    return summary, risk_exposure_summary",
    "doc": "Takes perf attribution data over a period of time and computes annualized\n    multifactor alpha, multifactor sharpe, risk exposures."
  },
  {
    "code": "def show_perf_attrib_stats(returns,\n                           positions,\n                           factor_returns,\n                           factor_loadings,\n                           transactions=None,\n                           pos_in_dollars=True):\n    \"\"\"\n    Calls `perf_attrib` using inputs, and displays outputs using\n    `utils.print_table`.\n    \"\"\"\n    risk_exposures, perf_attrib_data = perf_attrib(\n        returns,\n        positions,\n        factor_returns,\n        factor_loadings,\n        transactions,\n        pos_in_dollars=pos_in_dollars,\n    )\n\n    perf_attrib_stats, risk_exposure_stats =\\\n        create_perf_attrib_stats(perf_attrib_data, risk_exposures)\n\n    percentage_formatter = '{:.2%}'.format\n    float_formatter = '{:.2f}'.format\n\n    summary_stats = perf_attrib_stats.loc[['Annualized Specific Return',\n                                           'Annualized Common Return',\n                                           'Annualized Total Return',\n                                           'Specific Sharpe Ratio']]\n\n    # Format return rows in summary stats table as percentages.\n    for col_name in (\n        'Annualized Specific Return',\n        'Annualized Common Return',\n        'Annualized Total Return',\n    ):\n        summary_stats[col_name] = percentage_formatter(summary_stats[col_name])\n\n    # Display sharpe to two decimal places.\n    summary_stats['Specific Sharpe Ratio'] = float_formatter(\n        summary_stats['Specific Sharpe Ratio']\n    )\n\n    print_table(summary_stats, name='Summary Statistics')\n\n    print_table(\n        risk_exposure_stats,\n        name='Exposures Summary',\n        # In exposures table, format exposure column to 2 decimal places, and\n        # return columns  as percentages.\n        formatters={\n            'Average Risk Factor Exposure': float_formatter,\n            'Annualized Return': percentage_formatter,\n            'Cumulative Return': percentage_formatter,\n        },\n    )",
    "doc": "Calls `perf_attrib` using inputs, and displays outputs using\n    `utils.print_table`."
  },
  {
    "code": "def plot_returns(perf_attrib_data, cost=None, ax=None):\n    \"\"\"\n    Plot total, specific, and common returns.\n\n    Parameters\n    ----------\n    perf_attrib_data : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index. Assumes the `total_returns` column is NOT\n        cost adjusted.\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n\n    cost : pd.Series, optional\n        if present, gets subtracted from `perf_attrib_data['total_returns']`,\n        and gets plotted separately\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    returns = perf_attrib_data['total_returns']\n    total_returns_label = 'Total returns'\n\n    cumulative_returns_less_costs = _cumulative_returns_less_costs(\n        returns,\n        cost\n    )\n    if cost is not None:\n        total_returns_label += ' (adjusted)'\n\n    specific_returns = perf_attrib_data['specific_returns']\n    common_returns = perf_attrib_data['common_returns']\n\n    ax.plot(cumulative_returns_less_costs, color='b',\n            label=total_returns_label)\n    ax.plot(ep.cum_returns(specific_returns), color='g',\n            label='Cumulative specific returns')\n    ax.plot(ep.cum_returns(common_returns), color='r',\n            label='Cumulative common returns')\n\n    if cost is not None:\n        ax.plot(-ep.cum_returns(cost), color='k',\n                label='Cumulative cost spent')\n\n    ax.set_title('Time series of cumulative returns')\n    ax.set_ylabel('Returns')\n\n    configure_legend(ax)\n\n    return ax",
    "doc": "Plot total, specific, and common returns.\n\n    Parameters\n    ----------\n    perf_attrib_data : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index. Assumes the `total_returns` column is NOT\n        cost adjusted.\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n\n    cost : pd.Series, optional\n        if present, gets subtracted from `perf_attrib_data['total_returns']`,\n        and gets plotted separately\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes"
  },
  {
    "code": "def plot_alpha_returns(alpha_returns, ax=None):\n    \"\"\"\n    Plot histogram of daily multi-factor alpha returns (specific returns).\n\n    Parameters\n    ----------\n    alpha_returns : pd.Series\n        series of daily alpha returns indexed by datetime\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    ax.hist(alpha_returns, color='g', label='Multi-factor alpha')\n    ax.set_title('Histogram of alphas')\n    ax.axvline(0, color='k', linestyle='--', label='Zero')\n\n    avg = alpha_returns.mean()\n    ax.axvline(avg, color='b', label='Mean = {: 0.5f}'.format(avg))\n    configure_legend(ax)\n\n    return ax",
    "doc": "Plot histogram of daily multi-factor alpha returns (specific returns).\n\n    Parameters\n    ----------\n    alpha_returns : pd.Series\n        series of daily alpha returns indexed by datetime\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes"
  },
  {
    "code": "def plot_factor_contribution_to_perf(\n        perf_attrib_data,\n        ax=None,\n        title='Cumulative common returns attribution',\n):\n    \"\"\"\n    Plot each factor's contribution to performance.\n\n    Parameters\n    ----------\n    perf_attrib_data : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    title : str, optional\n        title of plot\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    factors_to_plot = perf_attrib_data.drop(\n        ['total_returns', 'common_returns'], axis='columns', errors='ignore'\n    )\n\n    factors_cumulative = pd.DataFrame()\n    for factor in factors_to_plot:\n        factors_cumulative[factor] = ep.cum_returns(factors_to_plot[factor])\n\n    for col in factors_cumulative:\n        ax.plot(factors_cumulative[col])\n\n    ax.axhline(0, color='k')\n    configure_legend(ax, change_colors=True)\n\n    ax.set_ylabel('Cumulative returns by factor')\n    ax.set_title(title)\n\n    return ax",
    "doc": "Plot each factor's contribution to performance.\n\n    Parameters\n    ----------\n    perf_attrib_data : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    title : str, optional\n        title of plot\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes"
  },
  {
    "code": "def plot_risk_exposures(exposures, ax=None,\n                        title='Daily risk factor exposures'):\n    \"\"\"\n    Parameters\n    ----------\n    exposures : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    for col in exposures:\n        ax.plot(exposures[col])\n\n    configure_legend(ax, change_colors=True)\n    ax.set_ylabel('Factor exposures')\n    ax.set_title(title)\n\n    return ax",
    "doc": "Parameters\n    ----------\n    exposures : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n\n    ax :  matplotlib.axes.Axes\n        axes on which plots are made. if None, current axes will be used\n\n    Returns\n    -------\n    ax :  matplotlib.axes.Axes"
  },
  {
    "code": "def _align_and_warn(returns,\n                    positions,\n                    factor_returns,\n                    factor_loadings,\n                    transactions=None,\n                    pos_in_dollars=True):\n    \"\"\"\n    Make sure that all inputs have matching dates and tickers,\n    and raise warnings if necessary.\n    \"\"\"\n    missing_stocks = positions.columns.difference(\n        factor_loadings.index.get_level_values(1).unique()\n    )\n\n    # cash will not be in factor_loadings\n    num_stocks = len(positions.columns) - 1\n    missing_stocks = missing_stocks.drop('cash')\n    num_stocks_covered = num_stocks - len(missing_stocks)\n    missing_ratio = round(len(missing_stocks) / num_stocks, ndigits=3)\n\n    if num_stocks_covered == 0:\n        raise ValueError(\"Could not perform performance attribution. \"\n                         \"No factor loadings were available for this \"\n                         \"algorithm's positions.\")\n\n    if len(missing_stocks) > 0:\n\n        if len(missing_stocks) > 5:\n\n            missing_stocks_displayed = (\n                \" {} assets were missing factor loadings, including: {}..{}\"\n            ).format(len(missing_stocks),\n                     ', '.join(missing_stocks[:5].map(str)),\n                     missing_stocks[-1])\n            avg_allocation_msg = \"selected missing assets\"\n\n        else:\n            missing_stocks_displayed = (\n                \"The following assets were missing factor loadings: {}.\"\n            ).format(list(missing_stocks))\n            avg_allocation_msg = \"missing assets\"\n\n        missing_stocks_warning_msg = (\n            \"Could not determine risk exposures for some of this algorithm's \"\n            \"positions. Returns from the missing assets will not be properly \"\n            \"accounted for in performance attribution.\\n\"\n            \"\\n\"\n            \"{}. \"\n            \"Ignoring for exposure calculation and performance attribution. \"\n            \"Ratio of assets missing: {}. Average allocation of {}:\\n\"\n            \"\\n\"\n            \"{}.\\n\"\n        ).format(\n            missing_stocks_displayed,\n            missing_ratio,\n            avg_allocation_msg,\n            positions[missing_stocks[:5].union(missing_stocks[[-1]])].mean(),\n        )\n\n        warnings.warn(missing_stocks_warning_msg)\n\n        positions = positions.drop(missing_stocks, axis='columns',\n                                   errors='ignore')\n\n    missing_factor_loadings_index = positions.index.difference(\n        factor_loadings.index.get_level_values(0).unique()\n    )\n\n    missing_factor_loadings_index = positions.index.difference(\n        factor_loadings.index.get_level_values(0).unique()\n    )\n\n    if len(missing_factor_loadings_index) > 0:\n\n        if len(missing_factor_loadings_index) > 5:\n            missing_dates_displayed = (\n                \"(first missing is {}, last missing is {})\"\n            ).format(\n                missing_factor_loadings_index[0],\n                missing_factor_loadings_index[-1]\n            )\n        else:\n            missing_dates_displayed = list(missing_factor_loadings_index)\n\n        warning_msg = (\n            \"Could not find factor loadings for {} dates: {}. \"\n            \"Truncating date range for performance attribution. \"\n        ).format(len(missing_factor_loadings_index), missing_dates_displayed)\n\n        warnings.warn(warning_msg)\n\n        positions = positions.drop(missing_factor_loadings_index,\n                                   errors='ignore')\n        returns = returns.drop(missing_factor_loadings_index, errors='ignore')\n        factor_returns = factor_returns.drop(missing_factor_loadings_index,\n                                             errors='ignore')\n\n    if transactions is not None and pos_in_dollars:\n        turnover = get_turnover(positions, transactions).mean()\n        if turnover > PERF_ATTRIB_TURNOVER_THRESHOLD:\n            warning_msg = (\n                \"This algorithm has relatively high turnover of its \"\n                \"positions. As a result, performance attribution might not be \"\n                \"fully accurate.\\n\"\n                \"\\n\"\n                \"Performance attribution is calculated based \"\n                \"on end-of-day holdings and does not account for intraday \"\n                \"activity. Algorithms that derive a high percentage of \"\n                \"returns from buying and selling within the same day may \"\n                \"receive inaccurate performance attribution.\\n\"\n            )\n            warnings.warn(warning_msg)\n\n    return (returns, positions, factor_returns, factor_loadings)",
    "doc": "Make sure that all inputs have matching dates and tickers,\n    and raise warnings if necessary."
  },
  {
    "code": "def _stack_positions(positions, pos_in_dollars=True):\n    \"\"\"\n    Convert positions to percentages if necessary, and change them\n    to long format.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n    \"\"\"\n    if pos_in_dollars:\n        # convert holdings to percentages\n        positions = get_percent_alloc(positions)\n\n    # remove cash after normalizing positions\n    positions = positions.drop('cash', axis='columns')\n\n    # convert positions to long format\n    positions = positions.stack()\n    positions.index = positions.index.set_names(['dt', 'ticker'])\n\n    return positions",
    "doc": "Convert positions to percentages if necessary, and change them\n    to long format.\n\n    Parameters\n    ----------\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n\n    pos_in_dollars : bool\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars."
  },
  {
    "code": "def _cumulative_returns_less_costs(returns, costs):\n    \"\"\"\n    Compute cumulative returns, less costs.\n    \"\"\"\n    if costs is None:\n        return ep.cum_returns(returns)\n    return ep.cum_returns(returns - costs)",
    "doc": "Compute cumulative returns, less costs."
  },
  {
    "code": "def format_asset(asset):\n    \"\"\"\n    If zipline asset objects are used, we want to print them out prettily\n    within the tear sheet. This function should only be applied directly\n    before displaying.\n    \"\"\"\n\n    try:\n        import zipline.assets\n    except ImportError:\n        return asset\n\n    if isinstance(asset, zipline.assets.Asset):\n        return asset.symbol\n    else:\n        return asset",
    "doc": "If zipline asset objects are used, we want to print them out prettily\n    within the tear sheet. This function should only be applied directly\n    before displaying."
  },
  {
    "code": "def vectorize(func):\n    \"\"\"\n    Decorator so that functions can be written to work on Series but\n    may still be called with DataFrames.\n    \"\"\"\n\n    def wrapper(df, *args, **kwargs):\n        if df.ndim == 1:\n            return func(df, *args, **kwargs)\n        elif df.ndim == 2:\n            return df.apply(func, *args, **kwargs)\n\n    return wrapper",
    "doc": "Decorator so that functions can be written to work on Series but\n    may still be called with DataFrames."
  },
  {
    "code": "def extract_rets_pos_txn_from_zipline(backtest):\n    \"\"\"\n    Extract returns, positions, transactions and leverage from the\n    backtest data structure returned by zipline.TradingAlgorithm.run().\n\n    The returned data structures are in a format compatible with the\n    rest of pyfolio and can be directly passed to\n    e.g. tears.create_full_tear_sheet().\n\n    Parameters\n    ----------\n    backtest : pd.DataFrame\n        DataFrame returned by zipline.TradingAlgorithm.run()\n\n    Returns\n    -------\n    returns : pd.Series\n        Daily returns of strategy.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n\n\n    Example (on the Quantopian research platform)\n    ---------------------------------------------\n    >>> backtest = my_algo.run()\n    >>> returns, positions, transactions =\n    >>>     pyfolio.utils.extract_rets_pos_txn_from_zipline(backtest)\n    >>> pyfolio.tears.create_full_tear_sheet(returns,\n    >>>     positions, transactions)\n    \"\"\"\n\n    backtest.index = backtest.index.normalize()\n    if backtest.index.tzinfo is None:\n        backtest.index = backtest.index.tz_localize('UTC')\n    returns = backtest.returns\n    raw_positions = []\n    for dt, pos_row in backtest.positions.iteritems():\n        df = pd.DataFrame(pos_row)\n        df.index = [dt] * len(df)\n        raw_positions.append(df)\n    if not raw_positions:\n        raise ValueError(\"The backtest does not have any positions.\")\n    positions = pd.concat(raw_positions)\n    positions = pos.extract_pos(positions, backtest.ending_cash)\n    transactions = txn.make_transaction_frame(backtest.transactions)\n    if transactions.index.tzinfo is None:\n        transactions.index = transactions.index.tz_localize('utc')\n\n    return returns, positions, transactions",
    "doc": "Extract returns, positions, transactions and leverage from the\n    backtest data structure returned by zipline.TradingAlgorithm.run().\n\n    The returned data structures are in a format compatible with the\n    rest of pyfolio and can be directly passed to\n    e.g. tears.create_full_tear_sheet().\n\n    Parameters\n    ----------\n    backtest : pd.DataFrame\n        DataFrame returned by zipline.TradingAlgorithm.run()\n\n    Returns\n    -------\n    returns : pd.Series\n        Daily returns of strategy.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n\n\n    Example (on the Quantopian research platform)\n    ---------------------------------------------\n    >>> backtest = my_algo.run()\n    >>> returns, positions, transactions =\n    >>>     pyfolio.utils.extract_rets_pos_txn_from_zipline(backtest)\n    >>> pyfolio.tears.create_full_tear_sheet(returns,\n    >>>     positions, transactions)"
  },
  {
    "code": "def print_table(table,\n                name=None,\n                float_format=None,\n                formatters=None,\n                header_rows=None):\n    \"\"\"\n    Pretty print a pandas DataFrame.\n\n    Uses HTML output if running inside Jupyter Notebook, otherwise\n    formatted text output.\n\n    Parameters\n    ----------\n    table : pandas.Series or pandas.DataFrame\n        Table to pretty-print.\n    name : str, optional\n        Table name to display in upper left corner.\n    float_format : function, optional\n        Formatter to use for displaying table elements, passed as the\n        `float_format` arg to pd.Dataframe.to_html.\n        E.g. `'{0:.2%}'.format` for displaying 100 as '100.00%'.\n    formatters : list or dict, optional\n        Formatters to use by column, passed as the `formatters` arg to\n        pd.Dataframe.to_html.\n    header_rows : dict, optional\n        Extra rows to display at the top of the table.\n    \"\"\"\n\n    if isinstance(table, pd.Series):\n        table = pd.DataFrame(table)\n\n    if name is not None:\n        table.columns.name = name\n\n    html = table.to_html(float_format=float_format, formatters=formatters)\n\n    if header_rows is not None:\n        # Count the number of columns for the text to span\n        n_cols = html.split('<thead>')[1].split('</thead>')[0].count('<th>')\n\n        # Generate the HTML for the extra rows\n        rows = ''\n        for name, value in header_rows.items():\n            rows += ('\\n    <tr style=\"text-align: right;\"><th>%s</th>' +\n                     '<td colspan=%d>%s</td></tr>') % (name, n_cols, value)\n\n        # Inject the new HTML\n        html = html.replace('<thead>', '<thead>' + rows)\n\n    display(HTML(html))",
    "doc": "Pretty print a pandas DataFrame.\n\n    Uses HTML output if running inside Jupyter Notebook, otherwise\n    formatted text output.\n\n    Parameters\n    ----------\n    table : pandas.Series or pandas.DataFrame\n        Table to pretty-print.\n    name : str, optional\n        Table name to display in upper left corner.\n    float_format : function, optional\n        Formatter to use for displaying table elements, passed as the\n        `float_format` arg to pd.Dataframe.to_html.\n        E.g. `'{0:.2%}'.format` for displaying 100 as '100.00%'.\n    formatters : list or dict, optional\n        Formatters to use by column, passed as the `formatters` arg to\n        pd.Dataframe.to_html.\n    header_rows : dict, optional\n        Extra rows to display at the top of the table."
  },
  {
    "code": "def detect_intraday(positions, transactions, threshold=0.25):\n    \"\"\"\n    Attempt to detect an intraday strategy. Get the number of\n    positions held at the end of the day, and divide that by the\n    number of unique stocks transacted every day. If the average quotient\n    is below a threshold, then an intraday strategy is detected.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    boolean\n        True if an intraday strategy is detected.\n    \"\"\"\n\n    daily_txn = transactions.copy()\n    daily_txn.index = daily_txn.index.date\n    txn_count = daily_txn.groupby(level=0).symbol.nunique().sum()\n    daily_pos = positions.drop('cash', axis=1).replace(0, np.nan)\n    return daily_pos.count(axis=1).sum() / txn_count < threshold",
    "doc": "Attempt to detect an intraday strategy. Get the number of\n    positions held at the end of the day, and divide that by the\n    number of unique stocks transacted every day. If the average quotient\n    is below a threshold, then an intraday strategy is detected.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    boolean\n        True if an intraday strategy is detected."
  },
  {
    "code": "def check_intraday(estimate, returns, positions, transactions):\n    \"\"\"\n    Logic for checking if a strategy is intraday and processing it.\n\n    Parameters\n    ----------\n    estimate: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in tears.create_full_tear_sheet.\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values, adjusted for intraday movement.\n    \"\"\"\n\n    if estimate == 'infer':\n        if positions is not None and transactions is not None:\n            if detect_intraday(positions, transactions):\n                warnings.warn('Detected intraday strategy; inferring positi' +\n                              'ons from transactions. Set estimate_intraday' +\n                              '=False to disable.')\n                return estimate_intraday(returns, positions, transactions)\n            else:\n                return positions\n        else:\n            return positions\n\n    elif estimate:\n        if positions is not None and transactions is not None:\n            return estimate_intraday(returns, positions, transactions)\n        else:\n            raise ValueError('Positions and txns needed to estimate intraday')\n    else:\n        return positions",
    "doc": "Logic for checking if a strategy is intraday and processing it.\n\n    Parameters\n    ----------\n    estimate: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in tears.create_full_tear_sheet.\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values, adjusted for intraday movement."
  },
  {
    "code": "def estimate_intraday(returns, positions, transactions, EOD_hour=23):\n    \"\"\"\n    Intraday strategies will often not hold positions at the day end.\n    This attempts to find the point in the day that best represents\n    the activity of the strategy on that day, and effectively resamples\n    the end-of-day positions with the positions at this point of day.\n    The point of day is found by detecting when our exposure in the\n    market is at its maximum point. Note that this is an estimate.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values, resampled for intraday behavior.\n    \"\"\"\n\n    # Construct DataFrame of transaction amounts\n    txn_val = transactions.copy()\n    txn_val.index.names = ['date']\n    txn_val['value'] = txn_val.amount * txn_val.price\n    txn_val = txn_val.reset_index().pivot_table(\n        index='date', values='value',\n        columns='symbol').replace(np.nan, 0)\n\n    # Cumulate transaction amounts each day\n    txn_val['date'] = txn_val.index.date\n    txn_val = txn_val.groupby('date').cumsum()\n\n    # Calculate exposure, then take peak of exposure every day\n    txn_val['exposure'] = txn_val.abs().sum(axis=1)\n    condition = (txn_val['exposure'] == txn_val.groupby(\n        pd.TimeGrouper('24H'))['exposure'].transform(max))\n    txn_val = txn_val[condition].drop('exposure', axis=1)\n\n    # Compute cash delta\n    txn_val['cash'] = -txn_val.sum(axis=1)\n\n    # Shift EOD positions to positions at start of next trading day\n    positions_shifted = positions.copy().shift(1).fillna(0)\n    starting_capital = positions.iloc[0].sum() / (1 + returns[0])\n    positions_shifted.cash[0] = starting_capital\n\n    # Format and add start positions to intraday position changes\n    txn_val.index = txn_val.index.normalize()\n    corrected_positions = positions_shifted.add(txn_val, fill_value=0)\n    corrected_positions.index.name = 'period_close'\n    corrected_positions.columns.name = 'sid'\n\n    return corrected_positions",
    "doc": "Intraday strategies will often not hold positions at the day end.\n    This attempts to find the point in the day that best represents\n    the activity of the strategy on that day, and effectively resamples\n    the end-of-day positions with the positions at this point of day.\n    The point of day is found by detecting when our exposure in the\n    market is at its maximum point. Note that this is an estimate.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.DataFrame\n        Daily net position values, resampled for intraday behavior."
  },
  {
    "code": "def clip_returns_to_benchmark(rets, benchmark_rets):\n    \"\"\"\n    Drop entries from rets so that the start and end dates of rets match those\n    of benchmark_rets.\n\n    Parameters\n    ----------\n    rets : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See pf.tears.create_full_tear_sheet for more details\n\n    benchmark_rets : pd.Series\n        Daily returns of the benchmark, noncumulative.\n\n    Returns\n    -------\n    clipped_rets : pd.Series\n        Daily noncumulative returns with index clipped to match that of\n        benchmark returns.\n    \"\"\"\n\n    if (rets.index[0] < benchmark_rets.index[0]) \\\n            or (rets.index[-1] > benchmark_rets.index[-1]):\n        clipped_rets = rets[benchmark_rets.index]\n    else:\n        clipped_rets = rets\n\n    return clipped_rets",
    "doc": "Drop entries from rets so that the start and end dates of rets match those\n    of benchmark_rets.\n\n    Parameters\n    ----------\n    rets : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See pf.tears.create_full_tear_sheet for more details\n\n    benchmark_rets : pd.Series\n        Daily returns of the benchmark, noncumulative.\n\n    Returns\n    -------\n    clipped_rets : pd.Series\n        Daily noncumulative returns with index clipped to match that of\n        benchmark returns."
  },
  {
    "code": "def to_utc(df):\n    \"\"\"\n    For use in tests; applied UTC timestamp to DataFrame.\n    \"\"\"\n\n    try:\n        df.index = df.index.tz_localize('UTC')\n    except TypeError:\n        df.index = df.index.tz_convert('UTC')\n\n    return df",
    "doc": "For use in tests; applied UTC timestamp to DataFrame."
  },
  {
    "code": "def get_symbol_rets(symbol, start=None, end=None):\n    \"\"\"\n    Calls the currently registered 'returns_func'\n\n    Parameters\n    ----------\n    symbol : object\n        An identifier for the asset whose return\n        series is desired.\n        e.g. ticker symbol or database ID\n    start : date, optional\n        Earliest date to fetch data for.\n        Defaults to earliest date available.\n    end : date, optional\n        Latest date to fetch data for.\n        Defaults to latest date available.\n\n    Returns\n    -------\n    pandas.Series\n        Returned by the current 'returns_func'\n    \"\"\"\n\n    return SETTINGS['returns_func'](symbol,\n                                    start=start,\n                                    end=end)",
    "doc": "Calls the currently registered 'returns_func'\n\n    Parameters\n    ----------\n    symbol : object\n        An identifier for the asset whose return\n        series is desired.\n        e.g. ticker symbol or database ID\n    start : date, optional\n        Earliest date to fetch data for.\n        Defaults to earliest date available.\n    end : date, optional\n        Latest date to fetch data for.\n        Defaults to latest date available.\n\n    Returns\n    -------\n    pandas.Series\n        Returned by the current 'returns_func'"
  },
  {
    "code": "def configure_legend(ax, autofmt_xdate=True, change_colors=False,\n                     rotation=30, ha='right'):\n    \"\"\"\n    Format legend for perf attribution plots:\n    - put legend to the right of plot instead of overlapping with it\n    - make legend order match up with graph lines\n    - set colors according to colormap\n    \"\"\"\n    chartBox = ax.get_position()\n    ax.set_position([chartBox.x0, chartBox.y0,\n                     chartBox.width * 0.75, chartBox.height])\n\n    # make legend order match graph lines\n    handles, labels = ax.get_legend_handles_labels()\n    handles_and_labels_sorted = sorted(zip(handles, labels),\n                                       key=lambda x: x[0].get_ydata()[-1],\n                                       reverse=True)\n\n    handles_sorted = [h[0] for h in handles_and_labels_sorted]\n    labels_sorted = [h[1] for h in handles_and_labels_sorted]\n\n    if change_colors:\n        for handle, color in zip(handles_sorted,\n                                 cycle(COLORS)):\n\n            handle.set_color(color)\n\n    ax.legend(handles=handles_sorted,\n              labels=labels_sorted,\n              frameon=True,\n              framealpha=0.5,\n              loc='upper left',\n              bbox_to_anchor=(1.05, 1),\n              fontsize='large')\n\n    # manually rotate xticklabels instead of using matplotlib's autofmt_xdate\n    # because it disables xticklabels for all but the last plot\n    if autofmt_xdate:\n        for label in ax.get_xticklabels():\n            label.set_ha(ha)\n            label.set_rotation(rotation)",
    "doc": "Format legend for perf attribution plots:\n    - put legend to the right of plot instead of overlapping with it\n    - make legend order match up with graph lines\n    - set colors according to colormap"
  },
  {
    "code": "def sample_colormap(cmap_name, n_samples):\n    \"\"\"\n    Sample a colormap from matplotlib\n    \"\"\"\n    colors = []\n    colormap = cm.cmap_d[cmap_name]\n    for i in np.linspace(0, 1, n_samples):\n        colors.append(colormap(i))\n\n    return colors",
    "doc": "Sample a colormap from matplotlib"
  },
  {
    "code": "def customize(func):\n    \"\"\"\n    Decorator to set plotting context and axes style during function call.\n    \"\"\"\n    @wraps(func)\n    def call_w_context(*args, **kwargs):\n        set_context = kwargs.pop('set_context', True)\n        if set_context:\n            with plotting_context(), axes_style():\n                return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n    return call_w_context",
    "doc": "Decorator to set plotting context and axes style during function call."
  },
  {
    "code": "def plotting_context(context='notebook', font_scale=1.5, rc=None):\n    \"\"\"\n    Create pyfolio default plotting style context.\n\n    Under the hood, calls and returns seaborn.plotting_context() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    context : str, optional\n        Name of seaborn context.\n    font_scale : float, optional\n        Scale font by factor font_scale.\n    rc : dict, optional\n        Config flags.\n        By default, {'lines.linewidth': 1.5}\n        is being used and will be added to any\n        rc passed in, unless explicitly overriden.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.plotting_context(font_scale=2):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context().\n\n    \"\"\"\n    if rc is None:\n        rc = {}\n\n    rc_default = {'lines.linewidth': 1.5}\n\n    # Add defaults if they do not exist\n    for name, val in rc_default.items():\n        rc.setdefault(name, val)\n\n    return sns.plotting_context(context=context, font_scale=font_scale, rc=rc)",
    "doc": "Create pyfolio default plotting style context.\n\n    Under the hood, calls and returns seaborn.plotting_context() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    context : str, optional\n        Name of seaborn context.\n    font_scale : float, optional\n        Scale font by factor font_scale.\n    rc : dict, optional\n        Config flags.\n        By default, {'lines.linewidth': 1.5}\n        is being used and will be added to any\n        rc passed in, unless explicitly overriden.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.plotting_context(font_scale=2):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context()."
  },
  {
    "code": "def axes_style(style='darkgrid', rc=None):\n    \"\"\"\n    Create pyfolio default axes style context.\n\n    Under the hood, calls and returns seaborn.axes_style() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    style : str, optional\n        Name of seaborn style.\n    rc : dict, optional\n        Config flags.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.axes_style(style='whitegrid'):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context().\n\n    \"\"\"\n    if rc is None:\n        rc = {}\n\n    rc_default = {}\n\n    # Add defaults if they do not exist\n    for name, val in rc_default.items():\n        rc.setdefault(name, val)\n\n    return sns.axes_style(style=style, rc=rc)",
    "doc": "Create pyfolio default axes style context.\n\n    Under the hood, calls and returns seaborn.axes_style() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    style : str, optional\n        Name of seaborn style.\n    rc : dict, optional\n        Config flags.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.axes_style(style='whitegrid'):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context()."
  },
  {
    "code": "def plot_monthly_returns_heatmap(returns, ax=None, **kwargs):\n    \"\"\"\n    Plots a heatmap of returns by month.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    monthly_ret_table = ep.aggregate_returns(returns, 'monthly')\n    monthly_ret_table = monthly_ret_table.unstack().round(3)\n\n    sns.heatmap(\n        monthly_ret_table.fillna(0) *\n        100.0,\n        annot=True,\n        annot_kws={\"size\": 9},\n        alpha=1.0,\n        center=0.0,\n        cbar=False,\n        cmap=matplotlib.cm.RdYlGn,\n        ax=ax, **kwargs)\n    ax.set_ylabel('Year')\n    ax.set_xlabel('Month')\n    ax.set_title(\"Monthly returns (%)\")\n    return ax",
    "doc": "Plots a heatmap of returns by month.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_annual_returns(returns, ax=None, **kwargs):\n    \"\"\"\n    Plots a bar graph of returns by year.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_axis_formatter = FuncFormatter(utils.percentage)\n    ax.xaxis.set_major_formatter(FuncFormatter(x_axis_formatter))\n    ax.tick_params(axis='x', which='major')\n\n    ann_ret_df = pd.DataFrame(\n        ep.aggregate_returns(\n            returns,\n            'yearly'))\n\n    ax.axvline(\n        100 *\n        ann_ret_df.values.mean(),\n        color='steelblue',\n        linestyle='--',\n        lw=4,\n        alpha=0.7)\n    (100 * ann_ret_df.sort_index(ascending=False)\n     ).plot(ax=ax, kind='barh', alpha=0.70, **kwargs)\n    ax.axvline(0.0, color='black', linestyle='-', lw=3)\n\n    ax.set_ylabel('Year')\n    ax.set_xlabel('Returns')\n    ax.set_title(\"Annual returns\")\n    ax.legend(['Mean'], frameon=True, framealpha=0.5)\n    return ax",
    "doc": "Plots a bar graph of returns by year.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_monthly_returns_dist(returns, ax=None, **kwargs):\n    \"\"\"\n    Plots a distribution of monthly returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    x_axis_formatter = FuncFormatter(utils.percentage)\n    ax.xaxis.set_major_formatter(FuncFormatter(x_axis_formatter))\n    ax.tick_params(axis='x', which='major')\n\n    monthly_ret_table = ep.aggregate_returns(returns, 'monthly')\n\n    ax.hist(\n        100 * monthly_ret_table,\n        color='orangered',\n        alpha=0.80,\n        bins=20,\n        **kwargs)\n\n    ax.axvline(\n        100 * monthly_ret_table.mean(),\n        color='gold',\n        linestyle='--',\n        lw=4,\n        alpha=1.0)\n\n    ax.axvline(0.0, color='black', linestyle='-', lw=3, alpha=0.75)\n    ax.legend(['Mean'], frameon=True, framealpha=0.5)\n    ax.set_ylabel('Number of months')\n    ax.set_xlabel('Returns')\n    ax.set_title(\"Distribution of monthly returns\")\n    return ax",
    "doc": "Plots a distribution of monthly returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_holdings(returns, positions, legend_loc='best', ax=None, **kwargs):\n    \"\"\"\n    Plots total amount of stocks with an active position, either short\n    or long. Displays daily total, daily average per month, and\n    all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    positions = positions.copy().drop('cash', axis='columns')\n    df_holdings = positions.replace(0, np.nan).count(axis=1)\n    df_holdings_by_month = df_holdings.resample('1M').mean()\n    df_holdings.plot(color='steelblue', alpha=0.6, lw=0.5, ax=ax, **kwargs)\n    df_holdings_by_month.plot(\n        color='orangered',\n        lw=2,\n        ax=ax,\n        **kwargs)\n    ax.axhline(\n        df_holdings.values.mean(),\n        color='steelblue',\n        ls='--',\n        lw=3)\n\n    ax.set_xlim((returns.index[0], returns.index[-1]))\n\n    leg = ax.legend(['Daily holdings',\n                     'Average daily holdings, by month',\n                     'Average daily holdings, overall'],\n                    loc=legend_loc, frameon=True,\n                    framealpha=0.5)\n    leg.get_frame().set_edgecolor('black')\n\n    ax.set_title('Total holdings')\n    ax.set_ylabel('Holdings')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots total amount of stocks with an active position, either short\n    or long. Displays daily total, daily average per month, and\n    all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_long_short_holdings(returns, positions,\n                             legend_loc='upper left', ax=None, **kwargs):\n    \"\"\"\n    Plots total amount of stocks with an active position, breaking out\n    short and long into transparent filled regions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    positions = positions.drop('cash', axis='columns')\n    positions = positions.replace(0, np.nan)\n    df_longs = positions[positions > 0].count(axis=1)\n    df_shorts = positions[positions < 0].count(axis=1)\n    lf = ax.fill_between(df_longs.index, 0, df_longs.values,\n                         color='g', alpha=0.5, lw=2.0)\n    sf = ax.fill_between(df_shorts.index, 0, df_shorts.values,\n                         color='r', alpha=0.5, lw=2.0)\n\n    bf = patches.Rectangle([0, 0], 1, 1, color='darkgoldenrod')\n    leg = ax.legend([lf, sf, bf],\n                    ['Long (max: %s, min: %s)' % (df_longs.max(),\n                                                  df_longs.min()),\n                     'Short (max: %s, min: %s)' % (df_shorts.max(),\n                                                   df_shorts.min()),\n                     'Overlap'], loc=legend_loc, frameon=True,\n                    framealpha=0.5)\n    leg.get_frame().set_edgecolor('black')\n\n    ax.set_xlim((returns.index[0], returns.index[-1]))\n    ax.set_title('Long and short holdings')\n    ax.set_ylabel('Holdings')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots total amount of stocks with an active position, breaking out\n    short and long into transparent filled regions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_drawdown_periods(returns, top=10, ax=None, **kwargs):\n    \"\"\"\n    Plots cumulative returns highlighting top drawdown periods.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 10).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    df_cum_rets = ep.cum_returns(returns, starting_value=1.0)\n    df_drawdowns = timeseries.gen_drawdown_table(returns, top=top)\n\n    df_cum_rets.plot(ax=ax, **kwargs)\n\n    lim = ax.get_ylim()\n    colors = sns.cubehelix_palette(len(df_drawdowns))[::-1]\n    for i, (peak, recovery) in df_drawdowns[\n            ['Peak date', 'Recovery date']].iterrows():\n        if pd.isnull(recovery):\n            recovery = returns.index[-1]\n        ax.fill_between((peak, recovery),\n                        lim[0],\n                        lim[1],\n                        alpha=.4,\n                        color=colors[i])\n    ax.set_ylim(lim)\n    ax.set_title('Top %i drawdown periods' % top)\n    ax.set_ylabel('Cumulative returns')\n    ax.legend(['Portfolio'], loc='upper left',\n              frameon=True, framealpha=0.5)\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots cumulative returns highlighting top drawdown periods.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 10).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_drawdown_underwater(returns, ax=None, **kwargs):\n    \"\"\"\n    Plots how far underwaterr returns are over time, or plots current\n    drawdown vs. date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.percentage)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    df_cum_rets = ep.cum_returns(returns, starting_value=1.0)\n    running_max = np.maximum.accumulate(df_cum_rets)\n    underwater = -100 * ((running_max - df_cum_rets) / running_max)\n    (underwater).plot(ax=ax, kind='area', color='coral', alpha=0.7, **kwargs)\n    ax.set_ylabel('Drawdown')\n    ax.set_title('Underwater plot')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots how far underwaterr returns are over time, or plots current\n    drawdown vs. date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_perf_stats(returns, factor_returns, ax=None):\n    \"\"\"\n    Create box plot of some performance metrics of the strategy.\n    The width of the box whiskers is determined by a bootstrap.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    bootstrap_values = timeseries.perf_stats_bootstrap(returns,\n                                                       factor_returns,\n                                                       return_stats=False)\n    bootstrap_values = bootstrap_values.drop('Kurtosis', axis='columns')\n\n    sns.boxplot(data=bootstrap_values, orient='h', ax=ax)\n\n    return ax",
    "doc": "Create box plot of some performance metrics of the strategy.\n    The width of the box whiskers is determined by a bootstrap.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def show_perf_stats(returns, factor_returns=None, positions=None,\n                    transactions=None, turnover_denom='AGB',\n                    live_start_date=None, bootstrap=False,\n                    header_rows=None):\n    \"\"\"\n    Prints some performance metrics of the strategy.\n\n    - Shows amount of time the strategy has been run in backtest and\n      out-of-sample (in live trading).\n\n    - Shows Omega ratio, max drawdown, Calmar ratio, annual return,\n      stability, Sharpe ratio, annual volatility, alpha, and beta.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics.\n         - For more information, see timeseries.perf_stats_bootstrap\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the displayed table.\n    \"\"\"\n\n    if bootstrap:\n        perf_func = timeseries.perf_stats_bootstrap\n    else:\n        perf_func = timeseries.perf_stats\n\n    perf_stats_all = perf_func(\n        returns,\n        factor_returns=factor_returns,\n        positions=positions,\n        transactions=transactions,\n        turnover_denom=turnover_denom)\n\n    date_rows = OrderedDict()\n    if len(returns.index) > 0:\n        date_rows['Start date'] = returns.index[0].strftime('%Y-%m-%d')\n        date_rows['End date'] = returns.index[-1].strftime('%Y-%m-%d')\n\n    if live_start_date is not None:\n        live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n        returns_is = returns[returns.index < live_start_date]\n        returns_oos = returns[returns.index >= live_start_date]\n\n        positions_is = None\n        positions_oos = None\n        transactions_is = None\n        transactions_oos = None\n\n        if positions is not None:\n            positions_is = positions[positions.index < live_start_date]\n            positions_oos = positions[positions.index >= live_start_date]\n            if transactions is not None:\n                transactions_is = transactions[(transactions.index <\n                                                live_start_date)]\n                transactions_oos = transactions[(transactions.index >\n                                                 live_start_date)]\n\n        perf_stats_is = perf_func(\n            returns_is,\n            factor_returns=factor_returns,\n            positions=positions_is,\n            transactions=transactions_is,\n            turnover_denom=turnover_denom)\n\n        perf_stats_oos = perf_func(\n            returns_oos,\n            factor_returns=factor_returns,\n            positions=positions_oos,\n            transactions=transactions_oos,\n            turnover_denom=turnover_denom)\n        if len(returns.index) > 0:\n            date_rows['In-sample months'] = int(len(returns_is) /\n                                                APPROX_BDAYS_PER_MONTH)\n            date_rows['Out-of-sample months'] = int(len(returns_oos) /\n                                                    APPROX_BDAYS_PER_MONTH)\n\n        perf_stats = pd.concat(OrderedDict([\n            ('In-sample', perf_stats_is),\n            ('Out-of-sample', perf_stats_oos),\n            ('All', perf_stats_all),\n        ]), axis=1)\n    else:\n        if len(returns.index) > 0:\n            date_rows['Total months'] = int(len(returns) /\n                                            APPROX_BDAYS_PER_MONTH)\n        perf_stats = pd.DataFrame(perf_stats_all, columns=['Backtest'])\n\n    for column in perf_stats.columns:\n        for stat, value in perf_stats[column].iteritems():\n            if stat in STAT_FUNCS_PCT:\n                perf_stats.loc[stat, column] = str(np.round(value * 100,\n                                                            1)) + '%'\n    if header_rows is None:\n        header_rows = date_rows\n    else:\n        header_rows = OrderedDict(header_rows)\n        header_rows.update(date_rows)\n\n    utils.print_table(\n        perf_stats,\n        float_format='{0:.2f}'.format,\n        header_rows=header_rows,\n    )",
    "doc": "Prints some performance metrics of the strategy.\n\n    - Shows amount of time the strategy has been run in backtest and\n      out-of-sample (in live trading).\n\n    - Shows Omega ratio, max drawdown, Calmar ratio, annual return,\n      stability, Sharpe ratio, annual volatility, alpha, and beta.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics.\n         - For more information, see timeseries.perf_stats_bootstrap\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the displayed table."
  },
  {
    "code": "def plot_returns(returns,\n                 live_start_date=None,\n                 ax=None):\n    \"\"\"\n    Plots raw returns over time.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    ax.set_label('')\n    ax.set_ylabel('Returns')\n\n    if live_start_date is not None:\n        live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n        is_returns = returns.loc[returns.index < live_start_date]\n        oos_returns = returns.loc[returns.index >= live_start_date]\n        is_returns.plot(ax=ax, color='g')\n        oos_returns.plot(ax=ax, color='r')\n\n    else:\n        returns.plot(ax=ax, color='g')\n\n    return ax",
    "doc": "Plots raw returns over time.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_rolling_returns(returns,\n                         factor_returns=None,\n                         live_start_date=None,\n                         logy=False,\n                         cone_std=None,\n                         legend_loc='best',\n                         volatility_match=False,\n                         cone_function=timeseries.forecast_cone_bootstrap,\n                         ax=None, **kwargs):\n    \"\"\"\n    Plots cumulative rolling returns versus some benchmarks'.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Additionally, a non-parametric cone plot may be added to the\n    out-of-sample returns region.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    logy : bool, optional\n        Whether to log-scale the y-axis.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - See timeseries.forecast_cone_bounds for more details.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    volatility_match : bool, optional\n        Whether to normalize the volatility of the returns to those of the\n        benchmark returns. This helps compare strategies with different\n        volatilities. Requires passing of benchmark_rets.\n    cone_function : function, optional\n        Function to use when generating forecast probability cone.\n        The function signiture must follow the form:\n        def cone(in_sample_returns (pd.Series),\n                 days_to_project_forward (int),\n                 cone_std= (float, or tuple),\n                 starting_value= (int, or float))\n        See timeseries.forecast_cone_bootstrap for an example.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    ax.set_xlabel('')\n    ax.set_ylabel('Cumulative returns')\n    ax.set_yscale('log' if logy else 'linear')\n\n    if volatility_match and factor_returns is None:\n        raise ValueError('volatility_match requires passing of '\n                         'factor_returns.')\n    elif volatility_match and factor_returns is not None:\n        bmark_vol = factor_returns.loc[returns.index].std()\n        returns = (returns / returns.std()) * bmark_vol\n\n    cum_rets = ep.cum_returns(returns, 1.0)\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    if factor_returns is not None:\n        cum_factor_returns = ep.cum_returns(\n            factor_returns[cum_rets.index], 1.0)\n        cum_factor_returns.plot(lw=2, color='gray',\n                                label=factor_returns.name, alpha=0.60,\n                                ax=ax, **kwargs)\n\n    if live_start_date is not None:\n        live_start_date = ep.utils.get_utc_timestamp(live_start_date)\n        is_cum_returns = cum_rets.loc[cum_rets.index < live_start_date]\n        oos_cum_returns = cum_rets.loc[cum_rets.index >= live_start_date]\n    else:\n        is_cum_returns = cum_rets\n        oos_cum_returns = pd.Series([])\n\n    is_cum_returns.plot(lw=3, color='forestgreen', alpha=0.6,\n                        label='Backtest', ax=ax, **kwargs)\n\n    if len(oos_cum_returns) > 0:\n        oos_cum_returns.plot(lw=4, color='red', alpha=0.6,\n                             label='Live', ax=ax, **kwargs)\n\n        if cone_std is not None:\n            if isinstance(cone_std, (float, int)):\n                cone_std = [cone_std]\n\n            is_returns = returns.loc[returns.index < live_start_date]\n            cone_bounds = cone_function(\n                is_returns,\n                len(oos_cum_returns),\n                cone_std=cone_std,\n                starting_value=is_cum_returns[-1])\n\n            cone_bounds = cone_bounds.set_index(oos_cum_returns.index)\n            for std in cone_std:\n                ax.fill_between(cone_bounds.index,\n                                cone_bounds[float(std)],\n                                cone_bounds[float(-std)],\n                                color='steelblue', alpha=0.5)\n\n    if legend_loc is not None:\n        ax.legend(loc=legend_loc, frameon=True, framealpha=0.5)\n    ax.axhline(1.0, linestyle='--', color='black', lw=2)\n\n    return ax",
    "doc": "Plots cumulative rolling returns versus some benchmarks'.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Additionally, a non-parametric cone plot may be added to the\n    out-of-sample returns region.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    logy : bool, optional\n        Whether to log-scale the y-axis.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - See timeseries.forecast_cone_bounds for more details.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    volatility_match : bool, optional\n        Whether to normalize the volatility of the returns to those of the\n        benchmark returns. This helps compare strategies with different\n        volatilities. Requires passing of benchmark_rets.\n    cone_function : function, optional\n        Function to use when generating forecast probability cone.\n        The function signiture must follow the form:\n        def cone(in_sample_returns (pd.Series),\n                 days_to_project_forward (int),\n                 cone_std= (float, or tuple),\n                 starting_value= (int, or float))\n        See timeseries.forecast_cone_bootstrap for an example.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_rolling_beta(returns, factor_returns, legend_loc='best',\n                      ax=None, **kwargs):\n    \"\"\"\n    Plots the rolling 6-month and 12-month beta versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    ax.set_title(\"Rolling portfolio beta to \" + str(factor_returns.name))\n    ax.set_ylabel('Beta')\n    rb_1 = timeseries.rolling_beta(\n        returns, factor_returns, rolling_window=APPROX_BDAYS_PER_MONTH * 6)\n    rb_1.plot(color='steelblue', lw=3, alpha=0.6, ax=ax, **kwargs)\n    rb_2 = timeseries.rolling_beta(\n        returns, factor_returns, rolling_window=APPROX_BDAYS_PER_MONTH * 12)\n    rb_2.plot(color='grey', lw=3, alpha=0.4, ax=ax, **kwargs)\n    ax.axhline(rb_1.mean(), color='steelblue', linestyle='--', lw=3)\n    ax.axhline(0.0, color='black', linestyle='-', lw=2)\n\n    ax.set_xlabel('')\n    ax.legend(['6-mo',\n               '12-mo'],\n              loc=legend_loc, frameon=True, framealpha=0.5)\n    ax.set_ylim((-1.0, 1.0))\n    return ax",
    "doc": "Plots the rolling 6-month and 12-month beta versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_rolling_volatility(returns, factor_returns=None,\n                            rolling_window=APPROX_BDAYS_PER_MONTH * 6,\n                            legend_loc='best', ax=None, **kwargs):\n    \"\"\"\n    Plots the rolling volatility versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the volatility.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    rolling_vol_ts = timeseries.rolling_volatility(\n        returns, rolling_window)\n    rolling_vol_ts.plot(alpha=.7, lw=3, color='orangered', ax=ax,\n                        **kwargs)\n    if factor_returns is not None:\n        rolling_vol_ts_factor = timeseries.rolling_volatility(\n            factor_returns, rolling_window)\n        rolling_vol_ts_factor.plot(alpha=.7, lw=3, color='grey', ax=ax,\n                                   **kwargs)\n\n    ax.set_title('Rolling volatility (6-month)')\n    ax.axhline(\n        rolling_vol_ts.mean(),\n        color='steelblue',\n        linestyle='--',\n        lw=3)\n\n    ax.axhline(0.0, color='black', linestyle='-', lw=2)\n\n    ax.set_ylabel('Volatility')\n    ax.set_xlabel('')\n    if factor_returns is None:\n        ax.legend(['Volatility', 'Average volatility'],\n                  loc=legend_loc, frameon=True, framealpha=0.5)\n    else:\n        ax.legend(['Volatility', 'Benchmark volatility', 'Average volatility'],\n                  loc=legend_loc, frameon=True, framealpha=0.5)\n    return ax",
    "doc": "Plots the rolling volatility versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the volatility.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_rolling_sharpe(returns, factor_returns=None,\n                        rolling_window=APPROX_BDAYS_PER_MONTH * 6,\n                        legend_loc='best', ax=None, **kwargs):\n    \"\"\"\n    Plots the rolling Sharpe ratio versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor for\n        which the benchmark rolling Sharpe is computed. Usually\n        a benchmark such as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the sharpe ratio.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    rolling_sharpe_ts = timeseries.rolling_sharpe(\n        returns, rolling_window)\n    rolling_sharpe_ts.plot(alpha=.7, lw=3, color='orangered', ax=ax,\n                           **kwargs)\n\n    if factor_returns is not None:\n        rolling_sharpe_ts_factor = timeseries.rolling_sharpe(\n            factor_returns, rolling_window)\n        rolling_sharpe_ts_factor.plot(alpha=.7, lw=3, color='grey', ax=ax,\n                                      **kwargs)\n\n    ax.set_title('Rolling Sharpe ratio (6-month)')\n    ax.axhline(\n        rolling_sharpe_ts.mean(),\n        color='steelblue',\n        linestyle='--',\n        lw=3)\n    ax.axhline(0.0, color='black', linestyle='-', lw=3)\n\n    ax.set_ylabel('Sharpe ratio')\n    ax.set_xlabel('')\n    if factor_returns is None:\n        ax.legend(['Sharpe', 'Average'],\n                  loc=legend_loc, frameon=True, framealpha=0.5)\n    else:\n        ax.legend(['Sharpe', 'Benchmark Sharpe', 'Average'],\n                  loc=legend_loc, frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots the rolling Sharpe ratio versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor for\n        which the benchmark rolling Sharpe is computed. Usually\n        a benchmark such as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the sharpe ratio.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_gross_leverage(returns, positions, ax=None, **kwargs):\n    \"\"\"\n    Plots gross leverage versus date.\n\n    Gross leverage is the sum of long and short exposure per share\n    divided by net asset value.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    gl = timeseries.gross_lev(positions)\n    gl.plot(lw=0.5, color='limegreen', legend=False, ax=ax, **kwargs)\n\n    ax.axhline(gl.mean(), color='g', linestyle='--', lw=3)\n\n    ax.set_title('Gross leverage')\n    ax.set_ylabel('Gross leverage')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots gross leverage versus date.\n\n    Gross leverage is the sum of long and short exposure per share\n    divided by net asset value.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_exposures(returns, positions, ax=None, **kwargs):\n    \"\"\"\n    Plots a cake chart of the long and short exposure.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See\n        pos.get_percent_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    pos_no_cash = positions.drop('cash', axis=1)\n    l_exp = pos_no_cash[pos_no_cash > 0].sum(axis=1) / positions.sum(axis=1)\n    s_exp = pos_no_cash[pos_no_cash < 0].sum(axis=1) / positions.sum(axis=1)\n    net_exp = pos_no_cash.sum(axis=1) / positions.sum(axis=1)\n\n    ax.fill_between(l_exp.index,\n                    0,\n                    l_exp.values,\n                    label='Long', color='green', alpha=0.5)\n    ax.fill_between(s_exp.index,\n                    0,\n                    s_exp.values,\n                    label='Short', color='red', alpha=0.5)\n    ax.plot(net_exp.index, net_exp.values,\n            label='Net', color='black', linestyle='dotted')\n\n    ax.set_xlim((returns.index[0], returns.index[-1]))\n    ax.set_title(\"Exposure\")\n    ax.set_ylabel('Exposure')\n    ax.legend(loc='lower left', frameon=True, framealpha=0.5)\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots a cake chart of the long and short exposure.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See\n        pos.get_percent_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def show_and_plot_top_positions(returns, positions_alloc,\n                                show_and_plot=2, hide_positions=False,\n                                legend_loc='real_best', ax=None,\n                                **kwargs):\n    \"\"\"\n    Prints and/or plots the exposures of the top 10 held positions of\n    all time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_percent_alloc.\n    show_and_plot : int, optional\n        By default, this is 2, and both prints and plots.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n        By default, the legend will display below the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes, conditional\n        The axes that were plotted on.\n\n    \"\"\"\n    positions_alloc = positions_alloc.copy()\n    positions_alloc.columns = positions_alloc.columns.map(utils.format_asset)\n\n    df_top_long, df_top_short, df_top_abs = pos.get_top_long_short_abs(\n        positions_alloc)\n\n    if show_and_plot == 1 or show_and_plot == 2:\n        utils.print_table(pd.DataFrame(df_top_long * 100, columns=['max']),\n                          float_format='{0:.2f}%'.format,\n                          name='Top 10 long positions of all time')\n\n        utils.print_table(pd.DataFrame(df_top_short * 100, columns=['max']),\n                          float_format='{0:.2f}%'.format,\n                          name='Top 10 short positions of all time')\n\n        utils.print_table(pd.DataFrame(df_top_abs * 100, columns=['max']),\n                          float_format='{0:.2f}%'.format,\n                          name='Top 10 positions of all time')\n\n    if show_and_plot == 0 or show_and_plot == 2:\n\n        if ax is None:\n            ax = plt.gca()\n\n        positions_alloc[df_top_abs.index].plot(\n            title='Portfolio allocation over time, only top 10 holdings',\n            alpha=0.5, ax=ax, **kwargs)\n\n        # Place legend below plot, shrink plot by 20%\n        if legend_loc == 'real_best':\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0 + box.height * 0.1,\n                             box.width, box.height * 0.9])\n\n            # Put a legend below current axis\n            ax.legend(loc='upper center', frameon=True, framealpha=0.5,\n                      bbox_to_anchor=(0.5, -0.14), ncol=5)\n        else:\n            ax.legend(loc=legend_loc)\n\n        ax.set_xlim((returns.index[0], returns.index[-1]))\n        ax.set_ylabel('Exposure by holding')\n\n        if hide_positions:\n            ax.legend_.remove()\n\n        return ax",
    "doc": "Prints and/or plots the exposures of the top 10 held positions of\n    all time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_percent_alloc.\n    show_and_plot : int, optional\n        By default, this is 2, and both prints and plots.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n        By default, the legend will display below the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes, conditional\n        The axes that were plotted on."
  },
  {
    "code": "def plot_max_median_position_concentration(positions, ax=None, **kwargs):\n    \"\"\"\n    Plots the max and median of long and short position concentrations\n    over the time.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    alloc_summary = pos.get_max_median_position_concentration(positions)\n    colors = ['mediumblue', 'steelblue', 'tomato', 'firebrick']\n    alloc_summary.plot(linewidth=1, color=colors, alpha=0.6, ax=ax)\n\n    ax.legend(loc='center left', frameon=True, framealpha=0.5)\n    ax.set_ylabel('Exposure')\n    ax.set_title('Long/short max and median position concentration')\n\n    return ax",
    "doc": "Plots the max and median of long and short position concentrations\n    over the time.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_sector_allocations(returns, sector_alloc, ax=None, **kwargs):\n    \"\"\"\n    Plots the sector exposures of the portfolio over time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    sector_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_sector_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    sector_alloc.plot(title='Sector allocation over time',\n                      alpha=0.5, ax=ax, **kwargs)\n\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0 + box.height * 0.1,\n                     box.width, box.height * 0.9])\n\n    # Put a legend below current axis\n    ax.legend(loc='upper center', frameon=True, framealpha=0.5,\n              bbox_to_anchor=(0.5, -0.14), ncol=5)\n\n    ax.set_xlim((sector_alloc.index[0], sector_alloc.index[-1]))\n    ax.set_ylabel('Exposure by sector')\n    ax.set_xlabel('')\n\n    return ax",
    "doc": "Plots the sector exposures of the portfolio over time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    sector_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_sector_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_return_quantiles(returns, live_start_date=None, ax=None, **kwargs):\n    \"\"\"\n    Creates a box plot of daily, weekly, and monthly return\n    distributions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    is_returns = returns if live_start_date is None \\\n        else returns.loc[returns.index < live_start_date]\n    is_weekly = ep.aggregate_returns(is_returns, 'weekly')\n    is_monthly = ep.aggregate_returns(is_returns, 'monthly')\n    sns.boxplot(data=[is_returns, is_weekly, is_monthly],\n                palette=[\"#4c72B0\", \"#55A868\", \"#CCB974\"],\n                ax=ax, **kwargs)\n\n    if live_start_date is not None:\n        oos_returns = returns.loc[returns.index >= live_start_date]\n        oos_weekly = ep.aggregate_returns(oos_returns, 'weekly')\n        oos_monthly = ep.aggregate_returns(oos_returns, 'monthly')\n\n        sns.swarmplot(data=[oos_returns, oos_weekly, oos_monthly], ax=ax,\n                      color=\"red\",\n                      marker=\"d\", **kwargs)\n        red_dots = matplotlib.lines.Line2D([], [], color=\"red\", marker=\"d\",\n                                           label=\"Out-of-sample data\",\n                                           linestyle='')\n        ax.legend(handles=[red_dots], frameon=True, framealpha=0.5)\n    ax.set_xticklabels(['Daily', 'Weekly', 'Monthly'])\n    ax.set_title('Return quantiles')\n\n    return ax",
    "doc": "Creates a box plot of daily, weekly, and monthly return\n    distributions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_turnover(returns, transactions, positions,\n                  legend_loc='best', ax=None, **kwargs):\n    \"\"\"\n    Plots turnover vs. date.\n\n    Turnover is the number of shares traded for a period as a fraction\n    of total shares.\n\n    Displays daily total, daily average per month, and all-time daily\n    average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    y_axis_formatter = FuncFormatter(utils.two_dec_places)\n    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))\n\n    df_turnover = txn.get_turnover(positions, transactions)\n    df_turnover_by_month = df_turnover.resample(\"M\").mean()\n    df_turnover.plot(color='steelblue', alpha=1.0, lw=0.5, ax=ax, **kwargs)\n    df_turnover_by_month.plot(\n        color='orangered',\n        alpha=0.5,\n        lw=2,\n        ax=ax,\n        **kwargs)\n    ax.axhline(\n        df_turnover.mean(), color='steelblue', linestyle='--', lw=3, alpha=1.0)\n    ax.legend(['Daily turnover',\n               'Average daily turnover, by month',\n               'Average daily turnover, net'],\n              loc=legend_loc, frameon=True, framealpha=0.5)\n    ax.set_title('Daily turnover')\n    ax.set_xlim((returns.index[0], returns.index[-1]))\n    ax.set_ylim((0, 2))\n    ax.set_ylabel('Turnover')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots turnover vs. date.\n\n    Turnover is the number of shares traded for a period as a fraction\n    of total shares.\n\n    Displays daily total, daily average per month, and all-time daily\n    average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_slippage_sweep(returns, positions, transactions,\n                        slippage_params=(3, 8, 10, 12, 15, 20, 50),\n                        ax=None, **kwargs):\n    \"\"\"\n    Plots equity curves at different per-dollar slippage assumptions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    slippage_params: tuple\n        Slippage pameters to apply to the return time series (in\n        basis points).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    slippage_sweep = pd.DataFrame()\n    for bps in slippage_params:\n        adj_returns = txn.adjust_returns_for_slippage(returns, positions,\n                                                      transactions, bps)\n        label = str(bps) + \" bps\"\n        slippage_sweep[label] = ep.cum_returns(adj_returns, 1)\n\n    slippage_sweep.plot(alpha=1.0, lw=0.5, ax=ax)\n\n    ax.set_title('Cumulative returns given additional per-dollar slippage')\n    ax.set_ylabel('')\n\n    ax.legend(loc='center left', frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Plots equity curves at different per-dollar slippage assumptions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    slippage_params: tuple\n        Slippage pameters to apply to the return time series (in\n        basis points).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_slippage_sensitivity(returns, positions, transactions,\n                              ax=None, **kwargs):\n    \"\"\"\n    Plots curve relating per-dollar slippage to average annual returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    avg_returns_given_slippage = pd.Series()\n    for bps in range(1, 100):\n        adj_returns = txn.adjust_returns_for_slippage(returns, positions,\n                                                      transactions, bps)\n        avg_returns = ep.annual_return(adj_returns)\n        avg_returns_given_slippage.loc[bps] = avg_returns\n\n    avg_returns_given_slippage.plot(alpha=1.0, lw=2, ax=ax)\n\n    ax.set_title('Average annual returns given additional per-dollar slippage')\n    ax.set_xticks(np.arange(0, 100, 10))\n    ax.set_ylabel('Average annual return')\n    ax.set_xlabel('Per-dollar slippage (bps)')\n\n    return ax",
    "doc": "Plots curve relating per-dollar slippage to average annual returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_daily_turnover_hist(transactions, positions,\n                             ax=None, **kwargs):\n    \"\"\"\n    Plots a histogram of daily turnover rates.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    turnover = txn.get_turnover(positions, transactions)\n    sns.distplot(turnover, ax=ax, **kwargs)\n    ax.set_title('Distribution of daily turnover rates')\n    ax.set_xlabel('Turnover rate')\n    return ax",
    "doc": "Plots a histogram of daily turnover rates.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_daily_volume(returns, transactions, ax=None, **kwargs):\n    \"\"\"\n    Plots trading volume per day vs. date.\n\n    Also displays all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    daily_txn = txn.get_txn_vol(transactions)\n    daily_txn.txn_shares.plot(alpha=1.0, lw=0.5, ax=ax, **kwargs)\n    ax.axhline(daily_txn.txn_shares.mean(), color='steelblue',\n               linestyle='--', lw=3, alpha=1.0)\n    ax.set_title('Daily trading volume')\n    ax.set_xlim((returns.index[0], returns.index[-1]))\n    ax.set_ylabel('Amount of shares traded')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots trading volume per day vs. date.\n\n    Also displays all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_txn_time_hist(transactions, bin_minutes=5, tz='America/New_York',\n                       ax=None, **kwargs):\n    \"\"\"\n    Plots a histogram of transaction times, binning the times into\n    buckets of a given duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    bin_minutes : float, optional\n        Sizes of the bins in minutes, defaults to 5 minutes.\n    tz : str, optional\n        Time zone to plot against. Note that if the specified\n        zone does not apply daylight savings, the distribution\n        may be partially offset.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    txn_time = transactions.copy()\n\n    txn_time.index = txn_time.index.tz_convert(pytz.timezone(tz))\n    txn_time.index = txn_time.index.map(lambda x: x.hour * 60 + x.minute)\n    txn_time['trade_value'] = (txn_time.amount * txn_time.price).abs()\n    txn_time = txn_time.groupby(level=0).sum().reindex(index=range(570, 961))\n    txn_time.index = (txn_time.index / bin_minutes).astype(int) * bin_minutes\n    txn_time = txn_time.groupby(level=0).sum()\n\n    txn_time['time_str'] = txn_time.index.map(lambda x:\n                                              str(datetime.time(int(x / 60),\n                                                                x % 60))[:-3])\n\n    trade_value_sum = txn_time.trade_value.sum()\n    txn_time.trade_value = txn_time.trade_value.fillna(0) / trade_value_sum\n\n    ax.bar(txn_time.index, txn_time.trade_value, width=bin_minutes, **kwargs)\n\n    ax.set_xlim(570, 960)\n    ax.set_xticks(txn_time.index[::int(30 / bin_minutes)])\n    ax.set_xticklabels(txn_time.time_str[::int(30 / bin_minutes)])\n    ax.set_title('Transaction time distribution')\n    ax.set_ylabel('Proportion')\n    ax.set_xlabel('')\n    return ax",
    "doc": "Plots a histogram of transaction times, binning the times into\n    buckets of a given duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    bin_minutes : float, optional\n        Sizes of the bins in minutes, defaults to 5 minutes.\n    tz : str, optional\n        Time zone to plot against. Note that if the specified\n        zone does not apply daylight savings, the distribution\n        may be partially offset.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def show_worst_drawdown_periods(returns, top=5):\n    \"\"\"\n    Prints information about the worst drawdown periods.\n\n    Prints peak dates, valley dates, recovery dates, and net\n    drawdowns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 5).\n    \"\"\"\n\n    drawdown_df = timeseries.gen_drawdown_table(returns, top=top)\n    utils.print_table(\n        drawdown_df.sort_values('Net drawdown in %', ascending=False),\n        name='Worst drawdown periods',\n        float_format='{0:.2f}'.format,\n    )",
    "doc": "Prints information about the worst drawdown periods.\n\n    Prints peak dates, valley dates, recovery dates, and net\n    drawdowns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 5)."
  },
  {
    "code": "def plot_monthly_returns_timeseries(returns, ax=None, **kwargs):\n    \"\"\"\n    Plots monthly returns as a timeseries.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    def cumulate_returns(x):\n        return ep.cum_returns(x)[-1]\n\n    if ax is None:\n        ax = plt.gca()\n\n    monthly_rets = returns.resample('M').apply(lambda x: cumulate_returns(x))\n    monthly_rets = monthly_rets.to_period()\n\n    sns.barplot(x=monthly_rets.index,\n                y=monthly_rets.values,\n                color='steelblue')\n\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=90)\n\n    # only show x-labels on year boundary\n    xticks_coord = []\n    xticks_label = []\n    count = 0\n    for i in monthly_rets.index:\n        if i.month == 1:\n            xticks_label.append(i)\n            xticks_coord.append(count)\n            # plot yearly boundary line\n            ax.axvline(count, color='gray', ls='--', alpha=0.3)\n\n        count += 1\n\n    ax.axhline(0.0, color='darkgray', ls='-')\n    ax.set_xticks(xticks_coord)\n    ax.set_xticklabels(xticks_label)\n\n    return ax",
    "doc": "Plots monthly returns as a timeseries.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_round_trip_lifetimes(round_trips, disp_amount=16, lsize=18, ax=None):\n    \"\"\"\n    Plots timespans and directions of a sample of round trip trades.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.subplot()\n\n    symbols_sample = round_trips.symbol.unique()\n    np.random.seed(1)\n    sample = np.random.choice(round_trips.symbol.unique(), replace=False,\n                              size=min(disp_amount, len(symbols_sample)))\n    sample_round_trips = round_trips[round_trips.symbol.isin(sample)]\n\n    symbol_idx = pd.Series(np.arange(len(sample)), index=sample)\n\n    for symbol, sym_round_trips in sample_round_trips.groupby('symbol'):\n        for _, row in sym_round_trips.iterrows():\n            c = 'b' if row.long else 'r'\n            y_ix = symbol_idx[symbol] + 0.05\n            ax.plot([row['open_dt'], row['close_dt']],\n                    [y_ix, y_ix], color=c,\n                    linewidth=lsize, solid_capstyle='butt')\n\n    ax.set_yticks(range(disp_amount))\n    ax.set_yticklabels([utils.format_asset(s) for s in sample])\n\n    ax.set_ylim((-0.5, min(len(sample), disp_amount) - 0.5))\n    blue = patches.Rectangle([0, 0], 1, 1, color='b', label='Long')\n    red = patches.Rectangle([0, 0], 1, 1, color='r', label='Short')\n    leg = ax.legend(handles=[blue, red], loc='lower left',\n                    frameon=True, framealpha=0.5)\n    leg.get_frame().set_edgecolor('black')\n    ax.grid(False)\n\n    return ax",
    "doc": "Plots timespans and directions of a sample of round trip trades.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def show_profit_attribution(round_trips):\n    \"\"\"\n    Prints the share of total PnL contributed by each\n    traded name.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    total_pnl = round_trips['pnl'].sum()\n    pnl_attribution = round_trips.groupby('symbol')['pnl'].sum() / total_pnl\n    pnl_attribution.name = ''\n\n    pnl_attribution.index = pnl_attribution.index.map(utils.format_asset)\n    utils.print_table(\n        pnl_attribution.sort_values(\n            inplace=False,\n            ascending=False,\n        ),\n        name='Profitability (PnL / PnL total) per name',\n        float_format='{:.2%}'.format,\n    )",
    "doc": "Prints the share of total PnL contributed by each\n    traded name.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_prob_profit_trade(round_trips, ax=None):\n    \"\"\"\n    Plots a probability distribution for the event of making\n    a profitable trade.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    x = np.linspace(0, 1., 500)\n\n    round_trips['profitable'] = round_trips.pnl > 0\n\n    dist = sp.stats.beta(round_trips.profitable.sum(),\n                         (~round_trips.profitable).sum())\n    y = dist.pdf(x)\n    lower_perc = dist.ppf(.025)\n    upper_perc = dist.ppf(.975)\n\n    lower_plot = dist.ppf(.001)\n    upper_plot = dist.ppf(.999)\n\n    if ax is None:\n        ax = plt.subplot()\n\n    ax.plot(x, y)\n    ax.axvline(lower_perc, color='0.5')\n    ax.axvline(upper_perc, color='0.5')\n\n    ax.set_xlabel('Probability of making a profitable decision')\n    ax.set_ylabel('Belief')\n    ax.set_xlim(lower_plot, upper_plot)\n    ax.set_ylim((0, y.max() + 1.))\n\n    return ax",
    "doc": "Plots a probability distribution for the event of making\n    a profitable trade.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on."
  },
  {
    "code": "def plot_cones(name, bounds, oos_returns, num_samples=1000, ax=None,\n               cone_std=(1., 1.5, 2.), random_seed=None, num_strikes=3):\n    \"\"\"\n    Plots the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns. Redraws a new cone when\n    cumulative returns fall outside of last cone drawn.\n\n    Parameters\n    ----------\n    name : str\n        Account name to be used as figure title.\n    bounds : pandas.core.frame.DataFrame\n        Contains upper and lower cone boundaries. Column names are\n        strings corresponding to the number of standard devations\n        above (positive) or below (negative) the projected mean\n        cumulative returns.\n    oos_returns : pandas.core.frame.DataFrame\n        Non-cumulative out-of-sample returns.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    cone_std : list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n    num_strikes : int\n        Upper limit for number of cones drawn. Can be anything from 0 to 3.\n\n    Returns\n    -------\n    Returns are either an ax or fig option, but not both. If a\n    matplotlib.Axes instance is passed in as ax, then it will be modified\n    and returned. This allows for users to plot interactively in jupyter\n    notebook. When no ax object is passed in, a matplotlib.figure instance\n    is generated and returned. This figure can then be used to save\n    the plot as an image without viewing it.\n\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    fig : matplotlib.figure\n        The figure instance which contains all the plot elements.\n    \"\"\"\n\n    if ax is None:\n        fig = figure.Figure(figsize=(10, 8))\n        FigureCanvasAgg(fig)\n        axes = fig.add_subplot(111)\n    else:\n        axes = ax\n\n    returns = ep.cum_returns(oos_returns, starting_value=1.)\n    bounds_tmp = bounds.copy()\n    returns_tmp = returns.copy()\n    cone_start = returns.index[0]\n    colors = [\"green\", \"orange\", \"orangered\", \"darkred\"]\n\n    for c in range(num_strikes + 1):\n        if c > 0:\n            tmp = returns.loc[cone_start:]\n            bounds_tmp = bounds_tmp.iloc[0:len(tmp)]\n            bounds_tmp = bounds_tmp.set_index(tmp.index)\n            crossing = (tmp < bounds_tmp[float(-2.)].iloc[:len(tmp)])\n            if crossing.sum() <= 0:\n                break\n            cone_start = crossing.loc[crossing].index[0]\n            returns_tmp = returns.loc[cone_start:]\n            bounds_tmp = (bounds - (1 - returns.loc[cone_start]))\n        for std in cone_std:\n            x = returns_tmp.index\n            y1 = bounds_tmp[float(std)].iloc[:len(returns_tmp)]\n            y2 = bounds_tmp[float(-std)].iloc[:len(returns_tmp)]\n            axes.fill_between(x, y1, y2, color=colors[c], alpha=0.5)\n\n    # Plot returns line graph\n    label = 'Cumulative returns = {:.2f}%'.format((returns.iloc[-1] - 1) * 100)\n    axes.plot(returns.index, returns.values, color='black', lw=3.,\n              label=label)\n\n    if name is not None:\n        axes.set_title(name)\n    axes.axhline(1, color='black', alpha=0.2)\n    axes.legend(frameon=True, framealpha=0.5)\n\n    if ax is None:\n        return fig\n    else:\n        return axes",
    "doc": "Plots the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns. Redraws a new cone when\n    cumulative returns fall outside of last cone drawn.\n\n    Parameters\n    ----------\n    name : str\n        Account name to be used as figure title.\n    bounds : pandas.core.frame.DataFrame\n        Contains upper and lower cone boundaries. Column names are\n        strings corresponding to the number of standard devations\n        above (positive) or below (negative) the projected mean\n        cumulative returns.\n    oos_returns : pandas.core.frame.DataFrame\n        Non-cumulative out-of-sample returns.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    cone_std : list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n    num_strikes : int\n        Upper limit for number of cones drawn. Can be anything from 0 to 3.\n\n    Returns\n    -------\n    Returns are either an ax or fig option, but not both. If a\n    matplotlib.Axes instance is passed in as ax, then it will be modified\n    and returned. This allows for users to plot interactively in jupyter\n    notebook. When no ax object is passed in, a matplotlib.figure instance\n    is generated and returned. This figure can then be used to save\n    the plot as an image without viewing it.\n\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    fig : matplotlib.figure\n        The figure instance which contains all the plot elements."
  },
  {
    "code": "def var_cov_var_normal(P, c, mu=0, sigma=1):\n    \"\"\"\n    Variance-covariance calculation of daily Value-at-Risk in a\n    portfolio.\n\n    Parameters\n    ----------\n    P : float\n        Portfolio value.\n    c : float\n        Confidence level.\n    mu : float, optional\n        Mean.\n\n    Returns\n    -------\n    float\n        Variance-covariance.\n    \"\"\"\n\n    alpha = sp.stats.norm.ppf(1 - c, mu, sigma)\n    return P - P * (alpha + 1)",
    "doc": "Variance-covariance calculation of daily Value-at-Risk in a\n    portfolio.\n\n    Parameters\n    ----------\n    P : float\n        Portfolio value.\n    c : float\n        Confidence level.\n    mu : float, optional\n        Mean.\n\n    Returns\n    -------\n    float\n        Variance-covariance."
  },
  {
    "code": "def sortino_ratio(returns, required_return=0, period=DAILY):\n    \"\"\"\n    Determines the Sortino ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    depends on input type\n    series ==> float\n    DataFrame ==> np.array\n\n        Annualized Sortino ratio.\n    \"\"\"\n\n    return ep.sortino_ratio(returns, required_return=required_return)",
    "doc": "Determines the Sortino ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    depends on input type\n    series ==> float\n    DataFrame ==> np.array\n\n        Annualized Sortino ratio."
  },
  {
    "code": "def downside_risk(returns, required_return=0, period=DAILY):\n    \"\"\"\n    Determines the downside deviation below a threshold\n\n    Parameters\n    ----------\n    returns : pd.Series or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    depends on input type\n    series ==> float\n    DataFrame ==> np.array\n\n        Annualized downside deviation\n    \"\"\"\n\n    return ep.downside_risk(returns,\n                            required_return=required_return,\n                            period=period)",
    "doc": "Determines the downside deviation below a threshold\n\n    Parameters\n    ----------\n    returns : pd.Series or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    depends on input type\n    series ==> float\n    DataFrame ==> np.array\n\n        Annualized downside deviation"
  },
  {
    "code": "def sharpe_ratio(returns, risk_free=0, period=DAILY):\n    \"\"\"\n    Determines the Sharpe ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    risk_free : int, float\n        Constant risk-free return throughout the period.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    float\n        Sharpe ratio.\n    np.nan\n        If insufficient length of returns or if if adjusted returns are 0.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Sharpe_ratio for more details.\n    \"\"\"\n\n    return ep.sharpe_ratio(returns, risk_free=risk_free, period=period)",
    "doc": "Determines the Sharpe ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n    risk_free : int, float\n        Constant risk-free return throughout the period.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Can be 'monthly', 'weekly', or 'daily'.\n        - Defaults to 'daily'.\n\n    Returns\n    -------\n    float\n        Sharpe ratio.\n    np.nan\n        If insufficient length of returns or if if adjusted returns are 0.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Sharpe_ratio for more details."
  },
  {
    "code": "def rolling_beta(returns, factor_returns,\n                 rolling_window=APPROX_BDAYS_PER_MONTH * 6):\n    \"\"\"\n    Determines the rolling beta of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series or pd.DataFrame\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - If DataFrame is passed, computes rolling beta for each column.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The size of the rolling window, in days, over which to compute\n        beta (default 6 months).\n\n    Returns\n    -------\n    pd.Series\n        Rolling beta.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Beta_(finance) for more details.\n    \"\"\"\n\n    if factor_returns.ndim > 1:\n        # Apply column-wise\n        return factor_returns.apply(partial(rolling_beta, returns),\n                                    rolling_window=rolling_window)\n    else:\n        out = pd.Series(index=returns.index)\n        for beg, end in zip(returns.index[0:-rolling_window],\n                            returns.index[rolling_window:]):\n            out.loc[end] = ep.beta(\n                returns.loc[beg:end],\n                factor_returns.loc[beg:end])\n\n        return out",
    "doc": "Determines the rolling beta of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series or pd.DataFrame\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - If DataFrame is passed, computes rolling beta for each column.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The size of the rolling window, in days, over which to compute\n        beta (default 6 months).\n\n    Returns\n    -------\n    pd.Series\n        Rolling beta.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Beta_(finance) for more details."
  },
  {
    "code": "def rolling_regression(returns, factor_returns,\n                       rolling_window=APPROX_BDAYS_PER_MONTH * 6,\n                       nan_threshold=0.1):\n    \"\"\"\n    Computes rolling factor betas using a multivariate linear regression\n    (separate linear regressions is problematic because the factors may be\n    confounded).\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.DataFrame\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - Computes rolling beta for each column.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the beta. Defaults to 6 months.\n    nan_threshold : float, optional\n        If there are more than this fraction of NaNs, the rolling regression\n        for the given date will be skipped.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing rolling beta coefficients to SMB, HML and UMD\n    \"\"\"\n\n    # We need to drop NaNs to regress\n    ret_no_na = returns.dropna()\n\n    columns = ['alpha'] + factor_returns.columns.tolist()\n    rolling_risk = pd.DataFrame(columns=columns,\n                                index=ret_no_na.index)\n\n    rolling_risk.index.name = 'dt'\n\n    for beg, end in zip(ret_no_na.index[:-rolling_window],\n                        ret_no_na.index[rolling_window:]):\n        returns_period = ret_no_na[beg:end]\n        factor_returns_period = factor_returns.loc[returns_period.index]\n\n        if np.all(factor_returns_period.isnull().mean()) < nan_threshold:\n            factor_returns_period_dnan = factor_returns_period.dropna()\n            reg = linear_model.LinearRegression(fit_intercept=True).fit(\n                factor_returns_period_dnan,\n                returns_period.loc[factor_returns_period_dnan.index])\n            rolling_risk.loc[end, factor_returns.columns] = reg.coef_\n            rolling_risk.loc[end, 'alpha'] = reg.intercept_\n\n    return rolling_risk",
    "doc": "Computes rolling factor betas using a multivariate linear regression\n    (separate linear regressions is problematic because the factors may be\n    confounded).\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.DataFrame\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - Computes rolling beta for each column.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the beta. Defaults to 6 months.\n    nan_threshold : float, optional\n        If there are more than this fraction of NaNs, the rolling regression\n        for the given date will be skipped.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing rolling beta coefficients to SMB, HML and UMD"
  },
  {
    "code": "def gross_lev(positions):\n    \"\"\"\n    Calculates the gross leverage of a strategy.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.Series\n        Gross leverage.\n    \"\"\"\n\n    exposure = positions.drop('cash', axis=1).abs().sum(axis=1)\n    return exposure / positions.sum(axis=1)",
    "doc": "Calculates the gross leverage of a strategy.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n\n    Returns\n    -------\n    pd.Series\n        Gross leverage."
  },
  {
    "code": "def value_at_risk(returns, period=None, sigma=2.0):\n    \"\"\"\n    Get value at risk (VaR).\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    period : str, optional\n        Period over which to calculate VaR. Set to 'weekly',\n        'monthly', or 'yearly', otherwise defaults to period of\n        returns (typically daily).\n    sigma : float, optional\n        Standard deviations of VaR, default 2.\n    \"\"\"\n    if period is not None:\n        returns_agg = ep.aggregate_returns(returns, period)\n    else:\n        returns_agg = returns.copy()\n\n    value_at_risk = returns_agg.mean() - sigma * returns_agg.std()\n    return value_at_risk",
    "doc": "Get value at risk (VaR).\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    period : str, optional\n        Period over which to calculate VaR. Set to 'weekly',\n        'monthly', or 'yearly', otherwise defaults to period of\n        returns (typically daily).\n    sigma : float, optional\n        Standard deviations of VaR, default 2."
  },
  {
    "code": "def perf_stats(returns, factor_returns=None, positions=None,\n               transactions=None, turnover_denom='AGB'):\n    \"\"\"\n    Calculates various performance metrics of a strategy, for use in\n    plotting.show_perf_stats.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n         - If None, do not compute alpha, beta, and information ratio.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet.\n    turnover_denom : str\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n\n    Returns\n    -------\n    pd.Series\n        Performance metrics.\n    \"\"\"\n\n    stats = pd.Series()\n    for stat_func in SIMPLE_STAT_FUNCS:\n        stats[STAT_FUNC_NAMES[stat_func.__name__]] = stat_func(returns)\n\n    if positions is not None:\n        stats['Gross leverage'] = gross_lev(positions).mean()\n        if transactions is not None:\n            stats['Daily turnover'] = get_turnover(positions,\n                                                   transactions,\n                                                   turnover_denom).mean()\n    if factor_returns is not None:\n        for stat_func in FACTOR_STAT_FUNCS:\n            res = stat_func(returns, factor_returns)\n            stats[STAT_FUNC_NAMES[stat_func.__name__]] = res\n\n    return stats",
    "doc": "Calculates various performance metrics of a strategy, for use in\n    plotting.show_perf_stats.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n         - If None, do not compute alpha, beta, and information ratio.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet.\n    turnover_denom : str\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n\n    Returns\n    -------\n    pd.Series\n        Performance metrics."
  },
  {
    "code": "def perf_stats_bootstrap(returns, factor_returns=None, return_stats=True,\n                         **kwargs):\n    \"\"\"Calculates various bootstrapped performance metrics of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n         - If None, do not compute alpha, beta, and information ratio.\n    return_stats : boolean (optional)\n        If True, returns a DataFrame of mean, median, 5 and 95 percentiles\n        for each perf metric.\n        If False, returns a DataFrame with the bootstrap samples for\n        each perf metric.\n\n    Returns\n    -------\n    pd.DataFrame\n        if return_stats is True:\n        - Distributional statistics of bootstrapped sampling\n        distribution of performance metrics.\n        if return_stats is False:\n        - Bootstrap samples for each performance metric.\n    \"\"\"\n\n    bootstrap_values = OrderedDict()\n\n    for stat_func in SIMPLE_STAT_FUNCS:\n        stat_name = STAT_FUNC_NAMES[stat_func.__name__]\n        bootstrap_values[stat_name] = calc_bootstrap(stat_func,\n                                                     returns)\n\n    if factor_returns is not None:\n        for stat_func in FACTOR_STAT_FUNCS:\n            stat_name = STAT_FUNC_NAMES[stat_func.__name__]\n            bootstrap_values[stat_name] = calc_bootstrap(\n                stat_func,\n                returns,\n                factor_returns=factor_returns)\n\n    bootstrap_values = pd.DataFrame(bootstrap_values)\n\n    if return_stats:\n        stats = bootstrap_values.apply(calc_distribution_stats)\n        return stats.T[['mean', 'median', '5%', '95%']]\n    else:\n        return bootstrap_values",
    "doc": "Calculates various bootstrapped performance metrics of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n         - If None, do not compute alpha, beta, and information ratio.\n    return_stats : boolean (optional)\n        If True, returns a DataFrame of mean, median, 5 and 95 percentiles\n        for each perf metric.\n        If False, returns a DataFrame with the bootstrap samples for\n        each perf metric.\n\n    Returns\n    -------\n    pd.DataFrame\n        if return_stats is True:\n        - Distributional statistics of bootstrapped sampling\n        distribution of performance metrics.\n        if return_stats is False:\n        - Bootstrap samples for each performance metric."
  },
  {
    "code": "def calc_bootstrap(func, returns, *args, **kwargs):\n    \"\"\"Performs a bootstrap analysis on a user-defined function returning\n    a summary statistic.\n\n    Parameters\n    ----------\n    func : function\n        Function that either takes a single array (commonly returns)\n        or two arrays (commonly returns and factor returns) and\n        returns a single value (commonly a summary\n        statistic). Additional args and kwargs are passed as well.\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    n_samples : int, optional\n        Number of bootstrap samples to draw. Default is 1000.\n        Increasing this will lead to more stable / accurate estimates.\n\n    Returns\n    -------\n    numpy.ndarray\n        Bootstrapped sampling distribution of passed in func.\n    \"\"\"\n\n    n_samples = kwargs.pop('n_samples', 1000)\n    out = np.empty(n_samples)\n\n    factor_returns = kwargs.pop('factor_returns', None)\n\n    for i in range(n_samples):\n        idx = np.random.randint(len(returns), size=len(returns))\n        returns_i = returns.iloc[idx].reset_index(drop=True)\n        if factor_returns is not None:\n            factor_returns_i = factor_returns.iloc[idx].reset_index(drop=True)\n            out[i] = func(returns_i, factor_returns_i,\n                          *args, **kwargs)\n        else:\n            out[i] = func(returns_i,\n                          *args, **kwargs)\n\n    return out",
    "doc": "Performs a bootstrap analysis on a user-defined function returning\n    a summary statistic.\n\n    Parameters\n    ----------\n    func : function\n        Function that either takes a single array (commonly returns)\n        or two arrays (commonly returns and factor returns) and\n        returns a single value (commonly a summary\n        statistic). Additional args and kwargs are passed as well.\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    n_samples : int, optional\n        Number of bootstrap samples to draw. Default is 1000.\n        Increasing this will lead to more stable / accurate estimates.\n\n    Returns\n    -------\n    numpy.ndarray\n        Bootstrapped sampling distribution of passed in func."
  },
  {
    "code": "def calc_distribution_stats(x):\n    \"\"\"Calculate various summary statistics of data.\n\n    Parameters\n    ----------\n    x : numpy.ndarray or pandas.Series\n        Array to compute summary statistics for.\n\n    Returns\n    -------\n    pandas.Series\n        Series containing mean, median, std, as well as 5, 25, 75 and\n        95 percentiles of passed in values.\n    \"\"\"\n\n    return pd.Series({'mean': np.mean(x),\n                      'median': np.median(x),\n                      'std': np.std(x),\n                      '5%': np.percentile(x, 5),\n                      '25%': np.percentile(x, 25),\n                      '75%': np.percentile(x, 75),\n                      '95%': np.percentile(x, 95),\n                      'IQR': np.subtract.reduce(\n                          np.percentile(x, [75, 25])),\n                      })",
    "doc": "Calculate various summary statistics of data.\n\n    Parameters\n    ----------\n    x : numpy.ndarray or pandas.Series\n        Array to compute summary statistics for.\n\n    Returns\n    -------\n    pandas.Series\n        Series containing mean, median, std, as well as 5, 25, 75 and\n        95 percentiles of passed in values."
  },
  {
    "code": "def get_max_drawdown_underwater(underwater):\n    \"\"\"\n    Determines peak, valley, and recovery dates given an 'underwater'\n    DataFrame.\n\n    An underwater DataFrame is a DataFrame that has precomputed\n    rolling drawdown.\n\n    Parameters\n    ----------\n    underwater : pd.Series\n       Underwater returns (rolling drawdown) of a strategy.\n\n    Returns\n    -------\n    peak : datetime\n        The maximum drawdown's peak.\n    valley : datetime\n        The maximum drawdown's valley.\n    recovery : datetime\n        The maximum drawdown's recovery.\n    \"\"\"\n\n    valley = np.argmin(underwater)  # end of the period\n    # Find first 0\n    peak = underwater[:valley][underwater[:valley] == 0].index[-1]\n    # Find last 0\n    try:\n        recovery = underwater[valley:][underwater[valley:] == 0].index[0]\n    except IndexError:\n        recovery = np.nan  # drawdown not recovered\n    return peak, valley, recovery",
    "doc": "Determines peak, valley, and recovery dates given an 'underwater'\n    DataFrame.\n\n    An underwater DataFrame is a DataFrame that has precomputed\n    rolling drawdown.\n\n    Parameters\n    ----------\n    underwater : pd.Series\n       Underwater returns (rolling drawdown) of a strategy.\n\n    Returns\n    -------\n    peak : datetime\n        The maximum drawdown's peak.\n    valley : datetime\n        The maximum drawdown's valley.\n    recovery : datetime\n        The maximum drawdown's recovery."
  },
  {
    "code": "def get_max_drawdown(returns):\n    \"\"\"\n    Determines the maximum drawdown of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n\n    Returns\n    -------\n    float\n        Maximum drawdown.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Drawdown_(economics) for more details.\n    \"\"\"\n\n    returns = returns.copy()\n    df_cum = cum_returns(returns, 1.0)\n    running_max = np.maximum.accumulate(df_cum)\n    underwater = df_cum / running_max - 1\n    return get_max_drawdown_underwater(underwater)",
    "doc": "Determines the maximum drawdown of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~pyfolio.timeseries.cum_returns`.\n\n    Returns\n    -------\n    float\n        Maximum drawdown.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Drawdown_(economics) for more details."
  },
  {
    "code": "def get_top_drawdowns(returns, top=10):\n    \"\"\"\n    Finds top drawdowns, sorted by drawdown amount.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        The amount of top drawdowns to find (default 10).\n\n    Returns\n    -------\n    drawdowns : list\n        List of drawdown peaks, valleys, and recoveries. See get_max_drawdown.\n    \"\"\"\n\n    returns = returns.copy()\n    df_cum = ep.cum_returns(returns, 1.0)\n    running_max = np.maximum.accumulate(df_cum)\n    underwater = df_cum / running_max - 1\n\n    drawdowns = []\n    for t in range(top):\n        peak, valley, recovery = get_max_drawdown_underwater(underwater)\n        # Slice out draw-down period\n        if not pd.isnull(recovery):\n            underwater.drop(underwater[peak: recovery].index[1:-1],\n                            inplace=True)\n        else:\n            # drawdown has not ended yet\n            underwater = underwater.loc[:peak]\n\n        drawdowns.append((peak, valley, recovery))\n        if (len(returns) == 0) or (len(underwater) == 0):\n            break\n\n    return drawdowns",
    "doc": "Finds top drawdowns, sorted by drawdown amount.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        The amount of top drawdowns to find (default 10).\n\n    Returns\n    -------\n    drawdowns : list\n        List of drawdown peaks, valleys, and recoveries. See get_max_drawdown."
  },
  {
    "code": "def gen_drawdown_table(returns, top=10):\n    \"\"\"\n    Places top drawdowns in a table.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        The amount of top drawdowns to find (default 10).\n\n    Returns\n    -------\n    df_drawdowns : pd.DataFrame\n        Information about top drawdowns.\n    \"\"\"\n\n    df_cum = ep.cum_returns(returns, 1.0)\n    drawdown_periods = get_top_drawdowns(returns, top=top)\n    df_drawdowns = pd.DataFrame(index=list(range(top)),\n                                columns=['Net drawdown in %',\n                                         'Peak date',\n                                         'Valley date',\n                                         'Recovery date',\n                                         'Duration'])\n\n    for i, (peak, valley, recovery) in enumerate(drawdown_periods):\n        if pd.isnull(recovery):\n            df_drawdowns.loc[i, 'Duration'] = np.nan\n        else:\n            df_drawdowns.loc[i, 'Duration'] = len(pd.date_range(peak,\n                                                                recovery,\n                                                                freq='B'))\n        df_drawdowns.loc[i, 'Peak date'] = (peak.to_pydatetime()\n                                            .strftime('%Y-%m-%d'))\n        df_drawdowns.loc[i, 'Valley date'] = (valley.to_pydatetime()\n                                              .strftime('%Y-%m-%d'))\n        if isinstance(recovery, float):\n            df_drawdowns.loc[i, 'Recovery date'] = recovery\n        else:\n            df_drawdowns.loc[i, 'Recovery date'] = (recovery.to_pydatetime()\n                                                    .strftime('%Y-%m-%d'))\n        df_drawdowns.loc[i, 'Net drawdown in %'] = (\n            (df_cum.loc[peak] - df_cum.loc[valley]) / df_cum.loc[peak]) * 100\n\n    df_drawdowns['Peak date'] = pd.to_datetime(df_drawdowns['Peak date'])\n    df_drawdowns['Valley date'] = pd.to_datetime(df_drawdowns['Valley date'])\n    df_drawdowns['Recovery date'] = pd.to_datetime(\n        df_drawdowns['Recovery date'])\n\n    return df_drawdowns",
    "doc": "Places top drawdowns in a table.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        The amount of top drawdowns to find (default 10).\n\n    Returns\n    -------\n    df_drawdowns : pd.DataFrame\n        Information about top drawdowns."
  },
  {
    "code": "def rolling_volatility(returns, rolling_vol_window):\n    \"\"\"\n    Determines the rolling volatility of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    rolling_vol_window : int\n        Length of rolling window, in days, over which to compute.\n\n    Returns\n    -------\n    pd.Series\n        Rolling volatility.\n    \"\"\"\n\n    return returns.rolling(rolling_vol_window).std() \\\n        * np.sqrt(APPROX_BDAYS_PER_YEAR)",
    "doc": "Determines the rolling volatility of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    rolling_vol_window : int\n        Length of rolling window, in days, over which to compute.\n\n    Returns\n    -------\n    pd.Series\n        Rolling volatility."
  },
  {
    "code": "def rolling_sharpe(returns, rolling_sharpe_window):\n    \"\"\"\n    Determines the rolling Sharpe ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    rolling_sharpe_window : int\n        Length of rolling window, in days, over which to compute.\n\n    Returns\n    -------\n    pd.Series\n        Rolling Sharpe ratio.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Sharpe_ratio for more details.\n    \"\"\"\n\n    return returns.rolling(rolling_sharpe_window).mean() \\\n        / returns.rolling(rolling_sharpe_window).std() \\\n        * np.sqrt(APPROX_BDAYS_PER_YEAR)",
    "doc": "Determines the rolling Sharpe ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    rolling_sharpe_window : int\n        Length of rolling window, in days, over which to compute.\n\n    Returns\n    -------\n    pd.Series\n        Rolling Sharpe ratio.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Sharpe_ratio for more details."
  },
  {
    "code": "def simulate_paths(is_returns, num_days,\n                   starting_value=1, num_samples=1000, random_seed=None):\n    \"\"\"\n    Gnerate alternate paths using available values from in-sample returns.\n\n    Parameters\n    ----------\n    is_returns : pandas.core.frame.DataFrame\n        Non-cumulative in-sample returns.\n    num_days : int\n        Number of days to project the probability cone forward.\n    starting_value : int or float\n        Starting value of the out of sample period.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n\n    Returns\n    -------\n    samples : numpy.ndarray\n    \"\"\"\n\n    samples = np.empty((num_samples, num_days))\n    seed = np.random.RandomState(seed=random_seed)\n    for i in range(num_samples):\n        samples[i, :] = is_returns.sample(num_days, replace=True,\n                                          random_state=seed)\n\n    return samples",
    "doc": "Gnerate alternate paths using available values from in-sample returns.\n\n    Parameters\n    ----------\n    is_returns : pandas.core.frame.DataFrame\n        Non-cumulative in-sample returns.\n    num_days : int\n        Number of days to project the probability cone forward.\n    starting_value : int or float\n        Starting value of the out of sample period.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n\n    Returns\n    -------\n    samples : numpy.ndarray"
  },
  {
    "code": "def summarize_paths(samples, cone_std=(1., 1.5, 2.), starting_value=1.):\n    \"\"\"\n    Gnerate the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns.\n\n    Parameters\n    ----------\n    samples : numpy.ndarray\n        Alternative paths, or series of possible outcomes.\n    cone_std : list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n\n    Returns\n    -------\n    samples : pandas.core.frame.DataFrame\n    \"\"\"\n\n    cum_samples = ep.cum_returns(samples.T,\n                                 starting_value=starting_value).T\n\n    cum_mean = cum_samples.mean(axis=0)\n    cum_std = cum_samples.std(axis=0)\n\n    if isinstance(cone_std, (float, int)):\n        cone_std = [cone_std]\n\n    cone_bounds = pd.DataFrame(columns=pd.Float64Index([]))\n    for num_std in cone_std:\n        cone_bounds.loc[:, float(num_std)] = cum_mean + cum_std * num_std\n        cone_bounds.loc[:, float(-num_std)] = cum_mean - cum_std * num_std\n\n    return cone_bounds",
    "doc": "Gnerate the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns.\n\n    Parameters\n    ----------\n    samples : numpy.ndarray\n        Alternative paths, or series of possible outcomes.\n    cone_std : list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n\n    Returns\n    -------\n    samples : pandas.core.frame.DataFrame"
  },
  {
    "code": "def forecast_cone_bootstrap(is_returns, num_days, cone_std=(1., 1.5, 2.),\n                            starting_value=1, num_samples=1000,\n                            random_seed=None):\n    \"\"\"\n    Determines the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns. Future cumulative mean and\n    standard devation are computed by repeatedly sampling from the\n    in-sample daily returns (i.e. bootstrap). This cone is non-parametric,\n    meaning it does not assume that returns are normally distributed.\n\n    Parameters\n    ----------\n    is_returns : pd.Series\n        In-sample daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    num_days : int\n        Number of days to project the probability cone forward.\n    cone_std : int, float, or list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n    starting_value : int or float\n        Starting value of the out of sample period.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n\n    Returns\n    -------\n    pd.DataFrame\n        Contains upper and lower cone boundaries. Column names are\n        strings corresponding to the number of standard devations\n        above (positive) or below (negative) the projected mean\n        cumulative returns.\n    \"\"\"\n\n    samples = simulate_paths(\n        is_returns=is_returns,\n        num_days=num_days,\n        starting_value=starting_value,\n        num_samples=num_samples,\n        random_seed=random_seed\n    )\n\n    cone_bounds = summarize_paths(\n        samples=samples,\n        cone_std=cone_std,\n        starting_value=starting_value\n    )\n\n    return cone_bounds",
    "doc": "Determines the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns. Future cumulative mean and\n    standard devation are computed by repeatedly sampling from the\n    in-sample daily returns (i.e. bootstrap). This cone is non-parametric,\n    meaning it does not assume that returns are normally distributed.\n\n    Parameters\n    ----------\n    is_returns : pd.Series\n        In-sample daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    num_days : int\n        Number of days to project the probability cone forward.\n    cone_std : int, float, or list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n    starting_value : int or float\n        Starting value of the out of sample period.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n\n    Returns\n    -------\n    pd.DataFrame\n        Contains upper and lower cone boundaries. Column names are\n        strings corresponding to the number of standard devations\n        above (positive) or below (negative) the projected mean\n        cumulative returns."
  },
  {
    "code": "def extract_interesting_date_ranges(returns):\n    \"\"\"\n    Extracts returns based on interesting events. See\n    gen_date_range_interesting.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n\n    Returns\n    -------\n    ranges : OrderedDict\n        Date ranges, with returns, of all valid events.\n    \"\"\"\n\n    returns_dupe = returns.copy()\n    returns_dupe.index = returns_dupe.index.map(pd.Timestamp)\n    ranges = OrderedDict()\n    for name, (start, end) in PERIODS.items():\n        try:\n            period = returns_dupe.loc[start:end]\n            if len(period) == 0:\n                continue\n            ranges[name] = period\n        except BaseException:\n            continue\n\n    return ranges",
    "doc": "Extracts returns based on interesting events. See\n    gen_date_range_interesting.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n\n    Returns\n    -------\n    ranges : OrderedDict\n        Date ranges, with returns, of all valid events."
  },
  {
    "code": "def model_returns_t_alpha_beta(data, bmark, samples=2000, progressbar=True):\n    \"\"\"\n    Run Bayesian alpha-beta-model with T distributed returns.\n\n    This model estimates intercept (alpha) and slope (beta) of two\n    return sets. Usually, these will be algorithm returns and\n    benchmark returns (e.g. S&P500). The data is assumed to be T\n    distributed and thus is robust to outliers and takes tail events\n    into account.  If a pandas.DataFrame is passed as a benchmark, then\n    multiple linear regression is used to estimate alpha and beta.\n\n    Parameters\n    ----------\n    returns : pandas.Series\n        Series of simple returns of an algorithm or stock.\n    bmark : pandas.DataFrame\n        DataFrame of benchmark returns (e.g., S&P500) or risk factors (e.g.,\n        Fama-French SMB, HML, and UMD).\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n    samples : int (optional)\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n    \"\"\"\n\n    data_bmark = pd.concat([data, bmark], axis=1).dropna()\n\n    with pm.Model() as model:\n        sigma = pm.HalfCauchy(\n            'sigma',\n            beta=1)\n        nu = pm.Exponential('nu_minus_two', 1. / 10.)\n\n        # alpha and beta\n        X = data_bmark.iloc[:, 1]\n        y = data_bmark.iloc[:, 0]\n\n        alpha_reg = pm.Normal('alpha', mu=0, sd=.1)\n        beta_reg = pm.Normal('beta', mu=0, sd=1)\n\n        mu_reg = alpha_reg + beta_reg * X\n        pm.StudentT('returns',\n                    nu=nu + 2,\n                    mu=mu_reg,\n                    sd=sigma,\n                    observed=y)\n        trace = pm.sample(samples, progressbar=progressbar)\n\n    return model, trace",
    "doc": "Run Bayesian alpha-beta-model with T distributed returns.\n\n    This model estimates intercept (alpha) and slope (beta) of two\n    return sets. Usually, these will be algorithm returns and\n    benchmark returns (e.g. S&P500). The data is assumed to be T\n    distributed and thus is robust to outliers and takes tail events\n    into account.  If a pandas.DataFrame is passed as a benchmark, then\n    multiple linear regression is used to estimate alpha and beta.\n\n    Parameters\n    ----------\n    returns : pandas.Series\n        Series of simple returns of an algorithm or stock.\n    bmark : pandas.DataFrame\n        DataFrame of benchmark returns (e.g., S&P500) or risk factors (e.g.,\n        Fama-French SMB, HML, and UMD).\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n    samples : int (optional)\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior."
  },
  {
    "code": "def model_returns_normal(data, samples=500, progressbar=True):\n    \"\"\"\n    Run Bayesian model assuming returns are normally distributed.\n\n    Parameters\n    ----------\n    returns : pandas.Series\n        Series of simple returns of an algorithm or stock.\n    samples : int (optional)\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n    \"\"\"\n\n    with pm.Model() as model:\n        mu = pm.Normal('mean returns', mu=0, sd=.01, testval=data.mean())\n        sigma = pm.HalfCauchy('volatility', beta=1, testval=data.std())\n        returns = pm.Normal('returns', mu=mu, sd=sigma, observed=data)\n        pm.Deterministic(\n            'annual volatility',\n            returns.distribution.variance**.5 *\n            np.sqrt(252))\n        pm.Deterministic(\n            'sharpe',\n            returns.distribution.mean /\n            returns.distribution.variance**.5 *\n            np.sqrt(252))\n\n        trace = pm.sample(samples, progressbar=progressbar)\n    return model, trace",
    "doc": "Run Bayesian model assuming returns are normally distributed.\n\n    Parameters\n    ----------\n    returns : pandas.Series\n        Series of simple returns of an algorithm or stock.\n    samples : int (optional)\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior."
  },
  {
    "code": "def model_best(y1, y2, samples=1000, progressbar=True):\n    \"\"\"\n    Bayesian Estimation Supersedes the T-Test\n\n    This model runs a Bayesian hypothesis comparing if y1 and y2 come\n    from the same distribution. Returns are assumed to be T-distributed.\n\n    In addition, computes annual volatility and Sharpe of in and\n    out-of-sample periods.\n\n    This model replicates the example used in:\n    Kruschke, John. (2012) Bayesian estimation supersedes the t\n    test. Journal of Experimental Psychology: General.\n\n    Parameters\n    ----------\n    y1 : array-like\n        Array of returns (e.g. in-sample)\n    y2 : array-like\n        Array of returns (e.g. out-of-sample)\n    samples : int, optional\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    See Also\n    --------\n    plot_stoch_vol : plotting of tochastic volatility model\n    \"\"\"\n\n    y = np.concatenate((y1, y2))\n\n    mu_m = np.mean(y)\n    mu_p = 0.000001 * 1 / np.std(y)**2\n\n    sigma_low = np.std(y) / 1000\n    sigma_high = np.std(y) * 1000\n    with pm.Model() as model:\n        group1_mean = pm.Normal('group1_mean', mu=mu_m, tau=mu_p,\n                                testval=y1.mean())\n        group2_mean = pm.Normal('group2_mean', mu=mu_m, tau=mu_p,\n                                testval=y2.mean())\n        group1_std = pm.Uniform('group1_std', lower=sigma_low,\n                                upper=sigma_high, testval=y1.std())\n        group2_std = pm.Uniform('group2_std', lower=sigma_low,\n                                upper=sigma_high, testval=y2.std())\n        nu = pm.Exponential('nu_minus_two', 1 / 29., testval=4.) + 2.\n\n        returns_group1 = pm.StudentT('group1', nu=nu, mu=group1_mean,\n                                     lam=group1_std**-2, observed=y1)\n        returns_group2 = pm.StudentT('group2', nu=nu, mu=group2_mean,\n                                     lam=group2_std**-2, observed=y2)\n\n        diff_of_means = pm.Deterministic('difference of means',\n                                         group2_mean - group1_mean)\n        pm.Deterministic('difference of stds',\n                         group2_std - group1_std)\n        pm.Deterministic('effect size', diff_of_means /\n                         pm.math.sqrt((group1_std**2 +\n                                       group2_std**2) / 2))\n\n        pm.Deterministic('group1_annual_volatility',\n                         returns_group1.distribution.variance**.5 *\n                         np.sqrt(252))\n        pm.Deterministic('group2_annual_volatility',\n                         returns_group2.distribution.variance**.5 *\n                         np.sqrt(252))\n\n        pm.Deterministic('group1_sharpe', returns_group1.distribution.mean /\n                         returns_group1.distribution.variance**.5 *\n                         np.sqrt(252))\n        pm.Deterministic('group2_sharpe', returns_group2.distribution.mean /\n                         returns_group2.distribution.variance**.5 *\n                         np.sqrt(252))\n\n        trace = pm.sample(samples, progressbar=progressbar)\n    return model, trace",
    "doc": "Bayesian Estimation Supersedes the T-Test\n\n    This model runs a Bayesian hypothesis comparing if y1 and y2 come\n    from the same distribution. Returns are assumed to be T-distributed.\n\n    In addition, computes annual volatility and Sharpe of in and\n    out-of-sample periods.\n\n    This model replicates the example used in:\n    Kruschke, John. (2012) Bayesian estimation supersedes the t\n    test. Journal of Experimental Psychology: General.\n\n    Parameters\n    ----------\n    y1 : array-like\n        Array of returns (e.g. in-sample)\n    y2 : array-like\n        Array of returns (e.g. out-of-sample)\n    samples : int, optional\n        Number of posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    See Also\n    --------\n    plot_stoch_vol : plotting of tochastic volatility model"
  },
  {
    "code": "def plot_best(trace=None, data_train=None, data_test=None,\n              samples=1000, burn=200, axs=None):\n    \"\"\"\n    Plot BEST significance analysis.\n\n    Parameters\n    ----------\n    trace : pymc3.sampling.BaseTrace, optional\n        trace object as returned by model_best()\n        If not passed, will run model_best(), for which\n        data_train and data_test are required.\n    data_train : pandas.Series, optional\n        Returns of in-sample period.\n        Required if trace=None.\n    data_test : pandas.Series, optional\n        Returns of out-of-sample period.\n        Required if trace=None.\n    samples : int, optional\n        Posterior samples to draw.\n    burn : int\n        Posterior sampels to discard as burn-in.\n    axs : array of matplotlib.axes objects, optional\n        Plot into passed axes objects. Needs 6 axes.\n\n    Returns\n    -------\n    None\n\n    See Also\n    --------\n    model_best : Estimation of BEST model.\n    \"\"\"\n\n    if trace is None:\n        if (data_train is not None) or (data_test is not None):\n            raise ValueError('Either pass trace or data_train and data_test')\n        trace = model_best(data_train, data_test, samples=samples)\n\n    trace = trace[burn:]\n    if axs is None:\n        fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(16, 4))\n\n    def distplot_w_perc(trace, ax):\n        sns.distplot(trace, ax=ax)\n        ax.axvline(\n            stats.scoreatpercentile(trace, 2.5),\n            color='0.5', label='2.5 and 97.5 percentiles')\n        ax.axvline(\n            stats.scoreatpercentile(trace, 97.5),\n            color='0.5')\n\n    sns.distplot(trace['group1_mean'], ax=axs[0], label='Backtest')\n    sns.distplot(trace['group2_mean'], ax=axs[0], label='Forward')\n    axs[0].legend(loc=0, frameon=True, framealpha=0.5)\n    axs[1].legend(loc=0, frameon=True, framealpha=0.5)\n\n    distplot_w_perc(trace['difference of means'], axs[1])\n\n    axs[0].set(xlabel='Mean', ylabel='Belief', yticklabels=[])\n    axs[1].set(xlabel='Difference of means', yticklabels=[])\n\n    sns.distplot(trace['group1_annual_volatility'], ax=axs[2],\n                 label='Backtest')\n    sns.distplot(trace['group2_annual_volatility'], ax=axs[2],\n                 label='Forward')\n    distplot_w_perc(trace['group2_annual_volatility'] -\n                    trace['group1_annual_volatility'], axs[3])\n    axs[2].set(xlabel='Annual volatility', ylabel='Belief',\n               yticklabels=[])\n    axs[2].legend(loc=0, frameon=True, framealpha=0.5)\n    axs[3].set(xlabel='Difference of volatility', yticklabels=[])\n\n    sns.distplot(trace['group1_sharpe'], ax=axs[4], label='Backtest')\n    sns.distplot(trace['group2_sharpe'], ax=axs[4], label='Forward')\n    distplot_w_perc(trace['group2_sharpe'] - trace['group1_sharpe'],\n                    axs[5])\n    axs[4].set(xlabel='Sharpe', ylabel='Belief', yticklabels=[])\n    axs[4].legend(loc=0, frameon=True, framealpha=0.5)\n    axs[5].set(xlabel='Difference of Sharpes', yticklabels=[])\n\n    sns.distplot(trace['effect size'], ax=axs[6])\n    axs[6].axvline(\n        stats.scoreatpercentile(trace['effect size'], 2.5),\n        color='0.5')\n    axs[6].axvline(\n        stats.scoreatpercentile(trace['effect size'], 97.5),\n        color='0.5')\n    axs[6].set(xlabel='Difference of means normalized by volatility',\n               ylabel='Belief', yticklabels=[])",
    "doc": "Plot BEST significance analysis.\n\n    Parameters\n    ----------\n    trace : pymc3.sampling.BaseTrace, optional\n        trace object as returned by model_best()\n        If not passed, will run model_best(), for which\n        data_train and data_test are required.\n    data_train : pandas.Series, optional\n        Returns of in-sample period.\n        Required if trace=None.\n    data_test : pandas.Series, optional\n        Returns of out-of-sample period.\n        Required if trace=None.\n    samples : int, optional\n        Posterior samples to draw.\n    burn : int\n        Posterior sampels to discard as burn-in.\n    axs : array of matplotlib.axes objects, optional\n        Plot into passed axes objects. Needs 6 axes.\n\n    Returns\n    -------\n    None\n\n    See Also\n    --------\n    model_best : Estimation of BEST model."
  },
  {
    "code": "def model_stoch_vol(data, samples=2000, progressbar=True):\n    \"\"\"\n    Run stochastic volatility model.\n\n    This model estimates the volatility of a returns series over time.\n    Returns are assumed to be T-distributed. lambda (width of\n    T-distributed) is assumed to follow a random-walk.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Return series to model.\n    samples : int, optional\n        Posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    See Also\n    --------\n    plot_stoch_vol : plotting of tochastic volatility model\n    \"\"\"\n\n    from pymc3.distributions.timeseries import GaussianRandomWalk\n\n    with pm.Model() as model:\n        nu = pm.Exponential('nu', 1. / 10, testval=5.)\n        sigma = pm.Exponential('sigma', 1. / .02, testval=.1)\n        s = GaussianRandomWalk('s', sigma**-2, shape=len(data))\n        volatility_process = pm.Deterministic('volatility_process',\n                                              pm.math.exp(-2 * s))\n        pm.StudentT('r', nu, lam=volatility_process, observed=data)\n\n        trace = pm.sample(samples, progressbar=progressbar)\n\n    return model, trace",
    "doc": "Run stochastic volatility model.\n\n    This model estimates the volatility of a returns series over time.\n    Returns are assumed to be T-distributed. lambda (width of\n    T-distributed) is assumed to follow a random-walk.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Return series to model.\n    samples : int, optional\n        Posterior samples to draw.\n\n    Returns\n    -------\n    model : pymc.Model object\n        PyMC3 model containing all random variables.\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    See Also\n    --------\n    plot_stoch_vol : plotting of tochastic volatility model"
  },
  {
    "code": "def plot_stoch_vol(data, trace=None, ax=None):\n    \"\"\"\n    Generate plot for stochastic volatility model.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Returns to model.\n    trace : pymc3.sampling.BaseTrace object, optional\n        trace as returned by model_stoch_vol\n        If not passed, sample from model.\n    ax : matplotlib.axes object, optional\n        Plot into axes object\n\n    Returns\n    -------\n    ax object\n\n    See Also\n    --------\n    model_stoch_vol : run stochastic volatility model\n    \"\"\"\n\n    if trace is None:\n        trace = model_stoch_vol(data)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(15, 8))\n\n    data.abs().plot(ax=ax)\n    ax.plot(data.index, np.exp(trace['s', ::30].T), 'r', alpha=.03)\n    ax.set(title='Stochastic volatility', xlabel='Time', ylabel='Volatility')\n    ax.legend(['Abs returns', 'Stochastic volatility process'],\n              frameon=True, framealpha=0.5)\n\n    return ax",
    "doc": "Generate plot for stochastic volatility model.\n\n    Parameters\n    ----------\n    data : pandas.Series\n        Returns to model.\n    trace : pymc3.sampling.BaseTrace object, optional\n        trace as returned by model_stoch_vol\n        If not passed, sample from model.\n    ax : matplotlib.axes object, optional\n        Plot into axes object\n\n    Returns\n    -------\n    ax object\n\n    See Also\n    --------\n    model_stoch_vol : run stochastic volatility model"
  },
  {
    "code": "def compute_bayes_cone(preds, starting_value=1.):\n    \"\"\"\n    Compute 5, 25, 75 and 95 percentiles of cumulative returns, used\n    for the Bayesian cone.\n\n    Parameters\n    ----------\n    preds : numpy.array\n        Multiple (simulated) cumulative returns.\n    starting_value : int (optional)\n        Have cumulative returns start around this value.\n        Default = 1.\n\n    Returns\n    -------\n    dict of percentiles over time\n        Dictionary mapping percentiles (5, 25, 75, 95) to a\n        timeseries.\n    \"\"\"\n\n    def scoreatpercentile(cum_preds, p):\n        return [stats.scoreatpercentile(\n            c, p) for c in cum_preds.T]\n\n    cum_preds = np.cumprod(preds + 1, 1) * starting_value\n    perc = {p: scoreatpercentile(cum_preds, p) for p in (5, 25, 75, 95)}\n\n    return perc",
    "doc": "Compute 5, 25, 75 and 95 percentiles of cumulative returns, used\n    for the Bayesian cone.\n\n    Parameters\n    ----------\n    preds : numpy.array\n        Multiple (simulated) cumulative returns.\n    starting_value : int (optional)\n        Have cumulative returns start around this value.\n        Default = 1.\n\n    Returns\n    -------\n    dict of percentiles over time\n        Dictionary mapping percentiles (5, 25, 75, 95) to a\n        timeseries."
  },
  {
    "code": "def compute_consistency_score(returns_test, preds):\n    \"\"\"\n    Compute Bayesian consistency score.\n\n    Parameters\n    ----------\n    returns_test : pd.Series\n        Observed cumulative returns.\n    preds : numpy.array\n        Multiple (simulated) cumulative returns.\n\n    Returns\n    -------\n    Consistency score\n        Score from 100 (returns_test perfectly on the median line of the\n        Bayesian cone spanned by preds) to 0 (returns_test completely\n        outside of Bayesian cone.)\n    \"\"\"\n\n    returns_test_cum = cum_returns(returns_test, starting_value=1.)\n    cum_preds = np.cumprod(preds + 1, 1)\n\n    q = [sp.stats.percentileofscore(cum_preds[:, i],\n                                    returns_test_cum.iloc[i],\n                                    kind='weak')\n         for i in range(len(returns_test_cum))]\n    # normalize to be from 100 (perfect median line) to 0 (completely outside\n    # of cone)\n    return 100 - np.abs(50 - np.mean(q)) / .5",
    "doc": "Compute Bayesian consistency score.\n\n    Parameters\n    ----------\n    returns_test : pd.Series\n        Observed cumulative returns.\n    preds : numpy.array\n        Multiple (simulated) cumulative returns.\n\n    Returns\n    -------\n    Consistency score\n        Score from 100 (returns_test perfectly on the median line of the\n        Bayesian cone spanned by preds) to 0 (returns_test completely\n        outside of Bayesian cone.)"
  },
  {
    "code": "def run_model(model, returns_train, returns_test=None,\n              bmark=None, samples=500, ppc=False, progressbar=True):\n    \"\"\"\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series or pd.DataFrame (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n    samples : int (optional)\n        Number of posterior samples to draw.\n    ppc : boolean (optional)\n        Whether to run a posterior predictive check. Will generate\n        samples of length returns_test.  Returns a second argument\n        that contains the PPC of shape samples x len(returns_test).\n\n    Returns\n    -------\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    ppc : numpy.array (if ppc==True)\n       PPC of shape samples x len(returns_test).\n    \"\"\"\n\n    if model == 'alpha_beta':\n        model, trace = model_returns_t_alpha_beta(returns_train,\n                                                  bmark, samples,\n                                                  progressbar=progressbar)\n    elif model == 't':\n        model, trace = model_returns_t(returns_train, samples,\n                                       progressbar=progressbar)\n    elif model == 'normal':\n        model, trace = model_returns_normal(returns_train, samples,\n                                            progressbar=progressbar)\n    elif model == 'best':\n        model, trace = model_best(returns_train, returns_test,\n                                  samples=samples,\n                                  progressbar=progressbar)\n    else:\n        raise NotImplementedError(\n            'Model {} not found.'\n            'Use alpha_beta, t, normal, or best.'.format(model))\n\n    if ppc:\n        ppc_samples = pm.sample_ppc(trace, samples=samples,\n                                    model=model, size=len(returns_test),\n                                    progressbar=progressbar)\n        return trace, ppc_samples['returns']\n\n    return trace",
    "doc": "Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series or pd.DataFrame (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n    samples : int (optional)\n        Number of posterior samples to draw.\n    ppc : boolean (optional)\n        Whether to run a posterior predictive check. Will generate\n        samples of length returns_test.  Returns a second argument\n        that contains the PPC of shape samples x len(returns_test).\n\n    Returns\n    -------\n    trace : pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n    ppc : numpy.array (if ppc==True)\n       PPC of shape samples x len(returns_test)."
  },
  {
    "code": "def plot_bayes_cone(returns_train, returns_test, ppc,\n                    plot_train_len=50, ax=None):\n    \"\"\"\n    Generate cumulative returns plot with Bayesian cone.\n\n    Parameters\n    ----------\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    ppc : np.array\n        Posterior predictive samples of shape samples x\n        len(returns_test).\n    plot_train_len : int (optional)\n        How many data points to plot of returns_train. Useful to zoom in on\n        the prediction if there is a long backtest period.\n    ax : matplotlib.Axis (optional)\n        Axes upon which to plot.\n\n    Returns\n    -------\n    score : float\n        Consistency score (see compute_consistency_score)\n    trace : pymc3.sampling.BaseTrace\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n    \"\"\"\n\n    score = compute_consistency_score(returns_test,\n                                      ppc)\n\n    ax = _plot_bayes_cone(\n        returns_train,\n        returns_test,\n        ppc,\n        plot_train_len=plot_train_len,\n        ax=ax)\n    ax.text(\n        0.40,\n        0.90,\n        'Consistency score: %.1f' %\n        score,\n        verticalalignment='bottom',\n        horizontalalignment='right',\n        transform=ax.transAxes,\n    )\n\n    ax.set_ylabel('Cumulative returns')\n    return score",
    "doc": "Generate cumulative returns plot with Bayesian cone.\n\n    Parameters\n    ----------\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    ppc : np.array\n        Posterior predictive samples of shape samples x\n        len(returns_test).\n    plot_train_len : int (optional)\n        How many data points to plot of returns_train. Useful to zoom in on\n        the prediction if there is a long backtest period.\n    ax : matplotlib.Axis (optional)\n        Axes upon which to plot.\n\n    Returns\n    -------\n    score : float\n        Consistency score (see compute_consistency_score)\n    trace : pymc3.sampling.BaseTrace\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior."
  },
  {
    "code": "def load_voc_dataset(path='data', dataset='2012', contain_classes_in_person=False):\n    \"\"\"Pascal VOC 2007/2012 Dataset.\n\n    It has 20 objects:\n    aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor\n    and additional 3 classes : head, hand, foot for person.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/VOC``.\n    dataset : str\n        The VOC dataset version, `2012`, `2007`, `2007test` or `2012test`. We usually train model on `2007+2012` and test it on `2007test`.\n    contain_classes_in_person : boolean\n        Whether include head, hand and foot annotation, default is False.\n\n    Returns\n    ---------\n    imgs_file_list : list of str\n        Full paths of all images.\n    imgs_semseg_file_list : list of str\n        Full paths of all maps for semantic segmentation. Note that not all images have this map!\n    imgs_insseg_file_list : list of str\n        Full paths of all maps for instance segmentation. Note that not all images have this map!\n    imgs_ann_file_list : list of str\n        Full paths of all annotations for bounding box and object class, all images have this annotations.\n    classes : list of str\n        Classes in order.\n    classes_in_person : list of str\n        Classes in person.\n    classes_dict : dictionary\n        Class label to integer.\n    n_objs_list : list of int\n        Number of objects in all images in ``imgs_file_list`` in order.\n    objs_info_list : list of str\n        Darknet format for the annotation of all images in ``imgs_file_list`` in order. ``[class_id x_centre y_centre width height]`` in ratio format.\n    objs_info_dicts : dictionary\n        The annotation of all images in ``imgs_file_list``, ``{imgs_file_list : dictionary for annotation}``,\n        format from `TensorFlow/Models/object-detection <https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py>`__.\n\n    Examples\n    ----------\n    >>> imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list,\n    >>>     classes, classes_in_person, classes_dict,\n    >>>     n_objs_list, objs_info_list, objs_info_dicts = tl.files.load_voc_dataset(dataset=\"2012\", contain_classes_in_person=False)\n    >>> idx = 26\n    >>> print(classes)\n    ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n    >>> print(classes_dict)\n    {'sheep': 16, 'horse': 12, 'bicycle': 1, 'bottle': 4, 'cow': 9, 'sofa': 17, 'car': 6, 'dog': 11, 'cat': 7, 'person': 14, 'train': 18, 'diningtable': 10, 'aeroplane': 0, 'bus': 5, 'pottedplant': 15, 'tvmonitor': 19, 'chair': 8, 'bird': 2, 'boat': 3, 'motorbike': 13}\n    >>> print(imgs_file_list[idx])\n    data/VOC/VOC2012/JPEGImages/2007_000423.jpg\n    >>> print(n_objs_list[idx])\n    2\n    >>> print(imgs_ann_file_list[idx])\n    data/VOC/VOC2012/Annotations/2007_000423.xml\n    >>> print(objs_info_list[idx])\n    14 0.173 0.461333333333 0.142 0.496\n    14 0.828 0.542666666667 0.188 0.594666666667\n    >>> ann = tl.prepro.parse_darknet_ann_str_to_list(objs_info_list[idx])\n    >>> print(ann)\n    [[14, 0.173, 0.461333333333, 0.142, 0.496], [14, 0.828, 0.542666666667, 0.188, 0.594666666667]]\n    >>> c, b = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    >>> print(c, b)\n    [14, 14] [[0.173, 0.461333333333, 0.142, 0.496], [0.828, 0.542666666667, 0.188, 0.594666666667]]\n\n    References\n    -------------\n    - `Pascal VOC2012 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__.\n    - `Pascal VOC2007 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__.\n\n    \"\"\"\n    path = os.path.join(path, 'VOC')\n\n    def _recursive_parse_xml_to_dict(xml):\n        \"\"\"Recursively parses XML contents to python dict.\n\n        We assume that `object` tags are the only ones that can appear\n        multiple times at the same level of a tree.\n\n        Args:\n            xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n        Returns:\n            Python dictionary holding XML contents.\n\n        \"\"\"\n        if xml is not None:\n            return {xml.tag: xml.text}\n        result = {}\n        for child in xml:\n            child_result = _recursive_parse_xml_to_dict(child)\n            if child.tag != 'object':\n                result[child.tag] = child_result[child.tag]\n            else:\n                if child.tag not in result:\n                    result[child.tag] = []\n                result[child.tag].append(child_result[child.tag])\n        return {xml.tag: result}\n\n    import xml.etree.ElementTree as ET\n\n    if dataset == \"2012\":\n        url = \"http://pjreddie.com/media/files/\"\n        tar_filename = \"VOCtrainval_11-May-2012.tar\"\n        extracted_filename = \"VOC2012\"  #\"VOCdevkit/VOC2012\"\n        logging.info(\"    [============= VOC 2012 =============]\")\n    elif dataset == \"2012test\":\n        extracted_filename = \"VOC2012test\"  #\"VOCdevkit/VOC2012\"\n        logging.info(\"    [============= VOC 2012 Test Set =============]\")\n        logging.info(\n            \"    \\nAuthor: 2012test only have person annotation, so 2007test is highly recommended for testing !\\n\"\n        )\n        import time\n        time.sleep(3)\n        if os.path.isdir(os.path.join(path, extracted_filename)) is False:\n            logging.info(\"For VOC 2012 Test data - online registration required\")\n            logging.info(\n                \" Please download VOC2012test.tar from:  \\n register: http://host.robots.ox.ac.uk:8080 \\n voc2012 : http://host.robots.ox.ac.uk:8080/eval/challenges/voc2012/ \\ndownload: http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar\"\n            )\n            logging.info(\" unzip VOC2012test.tar,rename the folder to VOC2012test and put it into %s\" % path)\n            exit()\n        # # http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar\n        # url = \"http://host.robots.ox.ac.uk:8080/eval/downloads/\"\n        # tar_filename = \"VOC2012test.tar\"\n    elif dataset == \"2007\":\n        url = \"http://pjreddie.com/media/files/\"\n        tar_filename = \"VOCtrainval_06-Nov-2007.tar\"\n        extracted_filename = \"VOC2007\"\n        logging.info(\"    [============= VOC 2007 =============]\")\n    elif dataset == \"2007test\":\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html#testdata\n        # http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n        url = \"http://pjreddie.com/media/files/\"\n        tar_filename = \"VOCtest_06-Nov-2007.tar\"\n        extracted_filename = \"VOC2007test\"\n        logging.info(\"    [============= VOC 2007 Test Set =============]\")\n    else:\n        raise Exception(\"Please set the dataset aug to 2012, 2012test or 2007.\")\n\n    # download dataset\n    if dataset != \"2012test\":\n        from sys import platform as _platform\n        if folder_exists(os.path.join(path, extracted_filename)) is False:\n            logging.info(\"[VOC] {} is nonexistent in {}\".format(extracted_filename, path))\n            maybe_download_and_extract(tar_filename, path, url, extract=True)\n            del_file(os.path.join(path, tar_filename))\n            if dataset == \"2012\":\n                if _platform == \"win32\":\n                    os.system(\"move {}\\VOCdevkit\\VOC2012 {}\\VOC2012\".format(path, path))\n                else:\n                    os.system(\"mv {}/VOCdevkit/VOC2012 {}/VOC2012\".format(path, path))\n            elif dataset == \"2007\":\n                if _platform == \"win32\":\n                    os.system(\"move {}\\VOCdevkit\\VOC2007 {}\\VOC2007\".format(path, path))\n                else:\n                    os.system(\"mv {}/VOCdevkit/VOC2007 {}/VOC2007\".format(path, path))\n            elif dataset == \"2007test\":\n                if _platform == \"win32\":\n                    os.system(\"move {}\\VOCdevkit\\VOC2007 {}\\VOC2007test\".format(path, path))\n                else:\n                    os.system(\"mv {}/VOCdevkit/VOC2007 {}/VOC2007test\".format(path, path))\n            del_folder(os.path.join(path, 'VOCdevkit'))\n    # object classes(labels)  NOTE: YOU CAN CUSTOMIZE THIS LIST\n    classes = [\n        \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\",\n        \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n    ]\n    if contain_classes_in_person:\n        classes_in_person = [\"head\", \"hand\", \"foot\"]\n    else:\n        classes_in_person = []\n\n    classes += classes_in_person  # use extra 3 classes for person\n\n    classes_dict = utils.list_string_to_dict(classes)\n    logging.info(\"[VOC] object classes {}\".format(classes_dict))\n\n    # 1. image path list\n    # folder_imgs = path+\"/\"+extracted_filename+\"/JPEGImages/\"\n    folder_imgs = os.path.join(path, extracted_filename, \"JPEGImages\")\n    imgs_file_list = load_file_list(path=folder_imgs, regx='\\\\.jpg', printable=False)\n    logging.info(\"[VOC] {} images found\".format(len(imgs_file_list)))\n\n    imgs_file_list.sort(\n        key=lambda s: int(s.replace('.', ' ').replace('_', '').split(' ')[-2])\n    )  # 2007_000027.jpg --> 2007000027\n\n    imgs_file_list = [os.path.join(folder_imgs, s) for s in imgs_file_list]\n    # logging.info('IM',imgs_file_list[0::3333], imgs_file_list[-1])\n    if dataset != \"2012test\":\n        ##======== 2. semantic segmentation maps path list\n        # folder_semseg = path+\"/\"+extracted_filename+\"/SegmentationClass/\"\n        folder_semseg = os.path.join(path, extracted_filename, \"SegmentationClass\")\n        imgs_semseg_file_list = load_file_list(path=folder_semseg, regx='\\\\.png', printable=False)\n        logging.info(\"[VOC] {} maps for semantic segmentation found\".format(len(imgs_semseg_file_list)))\n        imgs_semseg_file_list.sort(\n            key=lambda s: int(s.replace('.', ' ').replace('_', '').split(' ')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_semseg_file_list = [os.path.join(folder_semseg, s) for s in imgs_semseg_file_list]\n        # logging.info('Semantic Seg IM',imgs_semseg_file_list[0::333], imgs_semseg_file_list[-1])\n        ##======== 3. instance segmentation maps path list\n        # folder_insseg = path+\"/\"+extracted_filename+\"/SegmentationObject/\"\n        folder_insseg = os.path.join(path, extracted_filename, \"SegmentationObject\")\n        imgs_insseg_file_list = load_file_list(path=folder_insseg, regx='\\\\.png', printable=False)\n        logging.info(\"[VOC] {} maps for instance segmentation found\".format(len(imgs_semseg_file_list)))\n        imgs_insseg_file_list.sort(\n            key=lambda s: int(s.replace('.', ' ').replace('_', '').split(' ')[-2])\n        )  # 2007_000032.png --> 2007000032\n        imgs_insseg_file_list = [os.path.join(folder_insseg, s) for s in imgs_insseg_file_list]\n        # logging.info('Instance Seg IM',imgs_insseg_file_list[0::333], imgs_insseg_file_list[-1])\n    else:\n        imgs_semseg_file_list = []\n        imgs_insseg_file_list = []\n    # 4. annotations for bounding box and object class\n    # folder_ann = path+\"/\"+extracted_filename+\"/Annotations/\"\n    folder_ann = os.path.join(path, extracted_filename, \"Annotations\")\n    imgs_ann_file_list = load_file_list(path=folder_ann, regx='\\\\.xml', printable=False)\n    logging.info(\n        \"[VOC] {} XML annotation files for bounding box and object class found\".format(len(imgs_ann_file_list))\n    )\n    imgs_ann_file_list.sort(\n        key=lambda s: int(s.replace('.', ' ').replace('_', '').split(' ')[-2])\n    )  # 2007_000027.xml --> 2007000027\n    imgs_ann_file_list = [os.path.join(folder_ann, s) for s in imgs_ann_file_list]\n    # logging.info('ANN',imgs_ann_file_list[0::3333], imgs_ann_file_list[-1])\n\n    if dataset == \"2012test\":  # remove unused images in JPEG folder\n        imgs_file_list_new = []\n        for ann in imgs_ann_file_list:\n            ann = os.path.split(ann)[-1].split('.')[0]\n            for im in imgs_file_list:\n                if ann in im:\n                    imgs_file_list_new.append(im)\n                    break\n        imgs_file_list = imgs_file_list_new\n        logging.info(\"[VOC] keep %d images\" % len(imgs_file_list_new))\n\n    # parse XML annotations\n    def convert(size, box):\n        dw = 1. / size[0]\n        dh = 1. / size[1]\n        x = (box[0] + box[1]) / 2.0\n        y = (box[2] + box[3]) / 2.0\n        w = box[1] - box[0]\n        h = box[3] - box[2]\n        x = x * dw\n        w = w * dw\n        y = y * dh\n        h = h * dh\n        return x, y, w, h\n\n    def convert_annotation(file_name):\n        \"\"\"Given VOC2012 XML Annotations, returns number of objects and info.\"\"\"\n        in_file = open(file_name)\n        out_file = \"\"\n        tree = ET.parse(in_file)\n        root = tree.getroot()\n        size = root.find('size')\n        w = int(size.find('width').text)\n        h = int(size.find('height').text)\n        n_objs = 0\n\n        for obj in root.iter('object'):\n            if dataset != \"2012test\":\n                difficult = obj.find('difficult').text\n                cls = obj.find('name').text\n                if cls not in classes or int(difficult) == 1:\n                    continue\n            else:\n                cls = obj.find('name').text\n                if cls not in classes:\n                    continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find('bndbox')\n            b = (\n                float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),\n                float(xmlbox.find('ymax').text)\n            )\n            bb = convert((w, h), b)\n\n            out_file += str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n'\n            n_objs += 1\n            if cls in \"person\":\n                for part in obj.iter('part'):\n                    cls = part.find('name').text\n                    if cls not in classes_in_person:\n                        continue\n                    cls_id = classes.index(cls)\n                    xmlbox = part.find('bndbox')\n                    b = (\n                        float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),\n                        float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)\n                    )\n                    bb = convert((w, h), b)\n                    # out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n                    out_file += str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n'\n                    n_objs += 1\n        in_file.close()\n        return n_objs, out_file\n\n    logging.info(\"[VOC] Parsing xml annotations files\")\n    n_objs_list = []\n    objs_info_list = []  # Darknet Format list of string\n    objs_info_dicts = {}\n    for idx, ann_file in enumerate(imgs_ann_file_list):\n        n_objs, objs_info = convert_annotation(ann_file)\n        n_objs_list.append(n_objs)\n        objs_info_list.append(objs_info)\n        with tf.gfile.GFile(ann_file, 'r') as fid:\n            xml_str = fid.read()\n        xml = etree.fromstring(xml_str)\n        data = _recursive_parse_xml_to_dict(xml)['annotation']\n        objs_info_dicts.update({imgs_file_list[idx]: data})\n\n    return imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list, classes, classes_in_person, classes_dict, n_objs_list, objs_info_list, objs_info_dicts",
    "doc": "Pascal VOC 2007/2012 Dataset.\n\n    It has 20 objects:\n    aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor\n    and additional 3 classes : head, hand, foot for person.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/VOC``.\n    dataset : str\n        The VOC dataset version, `2012`, `2007`, `2007test` or `2012test`. We usually train model on `2007+2012` and test it on `2007test`.\n    contain_classes_in_person : boolean\n        Whether include head, hand and foot annotation, default is False.\n\n    Returns\n    ---------\n    imgs_file_list : list of str\n        Full paths of all images.\n    imgs_semseg_file_list : list of str\n        Full paths of all maps for semantic segmentation. Note that not all images have this map!\n    imgs_insseg_file_list : list of str\n        Full paths of all maps for instance segmentation. Note that not all images have this map!\n    imgs_ann_file_list : list of str\n        Full paths of all annotations for bounding box and object class, all images have this annotations.\n    classes : list of str\n        Classes in order.\n    classes_in_person : list of str\n        Classes in person.\n    classes_dict : dictionary\n        Class label to integer.\n    n_objs_list : list of int\n        Number of objects in all images in ``imgs_file_list`` in order.\n    objs_info_list : list of str\n        Darknet format for the annotation of all images in ``imgs_file_list`` in order. ``[class_id x_centre y_centre width height]`` in ratio format.\n    objs_info_dicts : dictionary\n        The annotation of all images in ``imgs_file_list``, ``{imgs_file_list : dictionary for annotation}``,\n        format from `TensorFlow/Models/object-detection <https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py>`__.\n\n    Examples\n    ----------\n    >>> imgs_file_list, imgs_semseg_file_list, imgs_insseg_file_list, imgs_ann_file_list,\n    >>>     classes, classes_in_person, classes_dict,\n    >>>     n_objs_list, objs_info_list, objs_info_dicts = tl.files.load_voc_dataset(dataset=\"2012\", contain_classes_in_person=False)\n    >>> idx = 26\n    >>> print(classes)\n    ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n    >>> print(classes_dict)\n    {'sheep': 16, 'horse': 12, 'bicycle': 1, 'bottle': 4, 'cow': 9, 'sofa': 17, 'car': 6, 'dog': 11, 'cat': 7, 'person': 14, 'train': 18, 'diningtable': 10, 'aeroplane': 0, 'bus': 5, 'pottedplant': 15, 'tvmonitor': 19, 'chair': 8, 'bird': 2, 'boat': 3, 'motorbike': 13}\n    >>> print(imgs_file_list[idx])\n    data/VOC/VOC2012/JPEGImages/2007_000423.jpg\n    >>> print(n_objs_list[idx])\n    2\n    >>> print(imgs_ann_file_list[idx])\n    data/VOC/VOC2012/Annotations/2007_000423.xml\n    >>> print(objs_info_list[idx])\n    14 0.173 0.461333333333 0.142 0.496\n    14 0.828 0.542666666667 0.188 0.594666666667\n    >>> ann = tl.prepro.parse_darknet_ann_str_to_list(objs_info_list[idx])\n    >>> print(ann)\n    [[14, 0.173, 0.461333333333, 0.142, 0.496], [14, 0.828, 0.542666666667, 0.188, 0.594666666667]]\n    >>> c, b = tl.prepro.parse_darknet_ann_list_to_cls_box(ann)\n    >>> print(c, b)\n    [14, 14] [[0.173, 0.461333333333, 0.142, 0.496], [0.828, 0.542666666667, 0.188, 0.594666666667]]\n\n    References\n    -------------\n    - `Pascal VOC2012 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__.\n    - `Pascal VOC2007 Website <https://pjreddie.com/projects/pascal-voc-dataset-mirror/>`__."
  },
  {
    "code": "def main(_):\n    \"\"\"\n    The core of the model consists of an LSTM cell that processes one word at\n    a time and computes probabilities of the possible continuations of the\n    sentence. The memory state of the network is initialized with a vector\n    of zeros and gets updated after reading each word. Also, for computational\n    reasons, we will process data in mini-batches of size batch_size.\n\n    \"\"\"\n    if FLAGS.model == \"small\":\n        init_scale = 0.1\n        learning_rate = 1.0\n        max_grad_norm = 5\n        num_steps = 20\n        hidden_size = 200\n        max_epoch = 4\n        max_max_epoch = 13\n        keep_prob = 1.0\n        lr_decay = 0.5\n        batch_size = 20\n        vocab_size = 10000\n    elif FLAGS.model == \"medium\":\n        init_scale = 0.05\n        learning_rate = 1.0\n        max_grad_norm = 5\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 650\n        max_epoch = 6\n        max_max_epoch = 39\n        keep_prob = 0.5\n        lr_decay = 0.8\n        batch_size = 20\n        vocab_size = 10000\n    elif FLAGS.model == \"large\":\n        init_scale = 0.04\n        learning_rate = 1.0\n        max_grad_norm = 10\n        # num_layers = 2\n        num_steps = 35\n        hidden_size = 1500\n        max_epoch = 14\n        max_max_epoch = 55\n        keep_prob = 0.35\n        lr_decay = 1 / 1.15\n        batch_size = 20\n        vocab_size = 10000\n    else:\n        raise ValueError(\"Invalid model: %s\", FLAGS.model)\n\n    # Load PTB dataset\n    train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n    # train_data = train_data[0:int(100000/5)]    # for fast testing\n    print('len(train_data) {}'.format(len(train_data)))  # 929589 a list of int\n    print('len(valid_data) {}'.format(len(valid_data)))  # 73760  a list of int\n    print('len(test_data)  {}'.format(len(test_data)))  # 82430  a list of int\n    print('vocab_size      {}'.format(vocab_size))  # 10000\n\n    sess = tf.InteractiveSession()\n\n    # One int represents one word, the meaning of batch_size here is not the\n    # same with MNIST example, it is the number of concurrent processes for\n    # computational reasons.\n\n    # Training and Validing\n    input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n    # Testing (Evaluation)\n    input_data_test = tf.placeholder(tf.int32, [1, 1])\n    targets_test = tf.placeholder(tf.int32, [1, 1])\n\n    def inference(x, is_training, num_steps, reuse=None):\n        \"\"\"If reuse is True, the inferences use the existing parameters,\n        then different inferences share the same parameters.\n\n        Note :\n        - For DynamicRNNLayer, you can set dropout and the number of RNN layer internally.\n        \"\"\"\n        print(\"\\nnum_steps : %d, is_training : %s, reuse : %s\" % (num_steps, is_training, reuse))\n        init = tf.random_uniform_initializer(-init_scale, init_scale)\n        with tf.variable_scope(\"model\", reuse=reuse):\n            net = tl.layers.EmbeddingInputlayer(x, vocab_size, hidden_size, init, name='embedding')\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop1')\n            net = tl.layers.RNNLayer(\n                net,\n                cell_fn=tf.contrib.rnn.BasicLSTMCell,  # tf.nn.rnn_cell.BasicLSTMCell,\n                cell_init_args={'forget_bias': 0.0},  # 'state_is_tuple': True},\n                n_hidden=hidden_size,\n                initializer=init,\n                n_steps=num_steps,\n                return_last=False,\n                name='basic_lstm_layer1'\n            )\n            lstm1 = net\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop2')\n            net = tl.layers.RNNLayer(\n                net,\n                cell_fn=tf.contrib.rnn.BasicLSTMCell,  # tf.nn.rnn_cell.BasicLSTMCell,\n                cell_init_args={'forget_bias': 0.0},  # 'state_is_tuple': True},\n                n_hidden=hidden_size,\n                initializer=init,\n                n_steps=num_steps,\n                return_last=False,\n                return_seq_2d=True,\n                name='basic_lstm_layer2'\n            )\n            lstm2 = net\n            # Alternatively, if return_seq_2d=False, in the above RNN layer,\n            # you can reshape the outputs as follow:\n            # net = tl.layers.ReshapeLayer(net,\n            #       shape=[-1, int(net.outputs._shape[-1])], name='reshape')\n            net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop3')\n            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')\n        return net, lstm1, lstm2\n\n    # Inference for Training\n    net, lstm1, lstm2 = inference(input_data, is_training=True, num_steps=num_steps, reuse=None)\n    # Inference for Validating\n    net_val, lstm1_val, lstm2_val = inference(input_data, is_training=False, num_steps=num_steps, reuse=True)\n    # Inference for Testing (Evaluation)\n    net_test, lstm1_test, lstm2_test = inference(input_data_test, is_training=False, num_steps=1, reuse=True)\n\n    # sess.run(tf.global_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    def loss_fn(outputs, targets):  # , batch_size, num_steps):\n        # See tl.cost.cross_entropy_seq()\n        # Returns the cost function of Cross-entropy of two sequences, implement\n        # softmax internally.\n        # outputs : 2D tensor [batch_size*num_steps, n_units of output layer]\n        # targets : 2D tensor [batch_size, num_steps], need to be reshaped.\n        # batch_size : RNN batch_size, number of concurrent processes.\n        # n_examples = batch_size * num_steps\n        # so\n        # cost is the averaged cost of each mini-batch (concurrent process).\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]\n        )\n        # [tf.ones([batch_size * num_steps])])\n        cost = tf.reduce_sum(loss) / batch_size\n        return cost\n\n    # Cost for Training\n    cost = loss_fn(net.outputs, targets)  # , batch_size, num_steps)\n    # Cost for Validating\n    cost_val = loss_fn(net_val.outputs, targets)  # , batch_size, num_steps)\n    # Cost for Testing (Evaluation)\n    cost_test = loss_fn(net_test.outputs, targets_test)  # , 1, 1)\n\n    # Truncated Backpropagation for training\n    with tf.variable_scope('learning_rate'):\n        lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(lr)\n    train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    sess.run(tf.global_variables_initializer())\n\n    net.print_params()\n    net.print_layers()\n    tl.layers.print_all_variables()\n\n    print(\"\\nStart learning a language model by using PTB dataset\")\n    for i in range(max_max_epoch):\n        # decreases the initial learning rate after several\n        # epoachs (defined by ``max_epoch``), by multipling a ``lr_decay``.\n        new_lr_decay = lr_decay**max(i - max_epoch, 0.0)\n        sess.run(tf.assign(lr, learning_rate * new_lr_decay))\n\n        # Training\n        print(\"Epoch: %d/%d Learning rate: %.3f\" % (i + 1, max_max_epoch, sess.run(lr)))\n        epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)\n        state2 = tl.layers.initialize_rnn_state(lstm2.initial_state)\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data, batch_size, num_steps)):\n            feed_dict = {\n                input_data: x,\n                targets: y,\n                lstm1.initial_state: state1,\n                lstm2.initial_state: state2,\n            }\n            # For training, enable dropout\n            feed_dict.update(net.all_drop)\n            _cost, state1, state2, _ = sess.run(\n                [cost, lstm1.final_state, lstm2.final_state, train_op], feed_dict=feed_dict\n            )\n            costs += _cost\n            iters += num_steps\n\n            if step % (epoch_size // 10) == 10:\n                print(\n                    \"%.3f perplexity: %.3f speed: %.0f wps\" %\n                    (step * 1.0 / epoch_size, np.exp(costs / iters), iters * batch_size / (time.time() - start_time))\n                )\n        train_perplexity = np.exp(costs / iters)\n        print(\"Epoch: %d/%d Train Perplexity: %.3f\" % (i + 1, max_max_epoch, train_perplexity))\n\n        # Validing\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        state1 = tl.layers.initialize_rnn_state(lstm1_val.initial_state)\n        state2 = tl.layers.initialize_rnn_state(lstm2_val.initial_state)\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(valid_data, batch_size, num_steps)):\n            feed_dict = {\n                input_data: x,\n                targets: y,\n                lstm1_val.initial_state: state1,\n                lstm2_val.initial_state: state2,\n            }\n            _cost, state1, state2, _ = sess.run(\n                [cost_val, lstm1_val.final_state, lstm2_val.final_state,\n                 tf.no_op()], feed_dict=feed_dict\n            )\n            costs += _cost\n            iters += num_steps\n        valid_perplexity = np.exp(costs / iters)\n        print(\"Epoch: %d/%d Valid Perplexity: %.3f\" % (i + 1, max_max_epoch, valid_perplexity))\n\n    print(\"Evaluation\")\n    # Testing\n    # go through the test set step by step, it will take a while.\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n    # reset all states at the begining\n    state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)\n    state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)\n    for step, (x, y) in enumerate(tl.iterate.ptb_iterator(test_data, batch_size=1, num_steps=1)):\n        feed_dict = {\n            input_data_test: x,\n            targets_test: y,\n            lstm1_test.initial_state: state1,\n            lstm2_test.initial_state: state2,\n        }\n        _cost, state1, state2 = sess.run(\n            [cost_test, lstm1_test.final_state, lstm2_test.final_state], feed_dict=feed_dict\n        )\n        costs += _cost\n        iters += 1\n    test_perplexity = np.exp(costs / iters)\n    print(\"Test Perplexity: %.3f took %.2fs\" % (test_perplexity, time.time() - start_time))\n\n    print(\n        \"More example: Text generation using Trump's speech data: https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_generate_text.py -- def main_lstm_generate_text():\"\n    )",
    "doc": "The core of the model consists of an LSTM cell that processes one word at\n    a time and computes probabilities of the possible continuations of the\n    sentence. The memory state of the network is initialized with a vector\n    of zeros and gets updated after reading each word. Also, for computational\n    reasons, we will process data in mini-batches of size batch_size."
  },
  {
    "code": "def private_method(func):\n    \"\"\"Decorator for making an instance method private.\"\"\"\n\n    def func_wrapper(*args, **kwargs):\n        \"\"\"Decorator wrapper function.\"\"\"\n        outer_frame = inspect.stack()[1][0]\n        if 'self' not in outer_frame.f_locals or outer_frame.f_locals['self'] is not args[0]:\n            raise RuntimeError('%s.%s is a private method' % (args[0].__class__.__name__, func.__name__))\n\n        return func(*args, **kwargs)\n\n    return func_wrapper",
    "doc": "Decorator for making an instance method private."
  },
  {
    "code": "def protected_method(func):\n    \"\"\"Decorator for making an instance method private.\"\"\"\n\n    def func_wrapper(*args, **kwargs):\n        \"\"\"Decorator wrapper function.\"\"\"\n        outer_frame = inspect.stack()[1][0]\n\n        caller = inspect.getmro(outer_frame.f_locals['self'].__class__)[:-1]\n        target = inspect.getmro(args[0].__class__)[:-1]\n\n        share_subsclass = False\n\n        for cls_ in target:\n            if issubclass(caller[0], cls_) or caller[0] is cls_:\n                share_subsclass = True\n                break\n\n        if ('self' not in outer_frame.f_locals or\n                outer_frame.f_locals['self'] is not args[0]) and (not share_subsclass):\n            raise RuntimeError('%s.%s is a protected method' % (args[0].__class__.__name__, func.__name__))\n\n        return func(*args, **kwargs)\n\n    return func_wrapper",
    "doc": "Decorator for making an instance method private."
  },
  {
    "code": "def atrous_conv1d(\n        prev_layer,\n        n_filter=32,\n        filter_size=2,\n        stride=1,\n        dilation=1,\n        act=None,\n        padding='SAME',\n        data_format='NWC',\n        W_init=tf.truncated_normal_initializer(stddev=0.02),\n        b_init=tf.constant_initializer(value=0.0),\n        W_init_args=None,\n        b_init_args=None,\n        name='atrous_1d',\n):\n    \"\"\"Simplified version of :class:`AtrousConv1dLayer`.\n\n    Parameters\n    ----------\n    prev_layer : :class:`Layer`\n        Previous layer.\n    n_filter : int\n        The number of filters.\n    filter_size : int\n        The filter size.\n    stride : tuple of int\n        The strides: (height, width).\n    dilation : int\n        The filter dilation size.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: \"SAME\" or \"VALID\".\n    data_format : str\n        Default is 'NWC' as it is a 1D CNN.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weight matrix initializer.\n    b_init_args : dictionary\n        The arguments for the bias vector initializer.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    :class:`Layer`\n        A :class:`AtrousConv1dLayer` object\n\n    \"\"\"\n    return Conv1dLayer(\n        prev_layer=prev_layer,\n        act=act,\n        shape=(filter_size, int(prev_layer.outputs.get_shape()[-1]), n_filter),\n        stride=stride,\n        padding=padding,\n        dilation_rate=dilation,\n        data_format=data_format,\n        W_init=W_init,\n        b_init=b_init,\n        W_init_args=W_init_args,\n        b_init_args=b_init_args,\n        name=name,\n    )",
    "doc": "Simplified version of :class:`AtrousConv1dLayer`.\n\n    Parameters\n    ----------\n    prev_layer : :class:`Layer`\n        Previous layer.\n    n_filter : int\n        The number of filters.\n    filter_size : int\n        The filter size.\n    stride : tuple of int\n        The strides: (height, width).\n    dilation : int\n        The filter dilation size.\n    act : activation function\n        The activation function of this layer.\n    padding : str\n        The padding algorithm type: \"SAME\" or \"VALID\".\n    data_format : str\n        Default is 'NWC' as it is a 1D CNN.\n    W_init : initializer\n        The initializer for the weight matrix.\n    b_init : initializer or None\n        The initializer for the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weight matrix initializer.\n    b_init_args : dictionary\n        The arguments for the bias vector initializer.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    :class:`Layer`\n        A :class:`AtrousConv1dLayer` object"
  },
  {
    "code": "def _GetNextLogCountPerToken(token):\n    \"\"\"Wrapper for _log_counter_per_token.\n\n    Args:\n    token: The token for which to look up the count.\n\n    Returns:\n    The number of times this function has been called with\n    *token* as an argument (starting at 0)\n    \"\"\"\n    global _log_counter_per_token  # pylint: disable=global-variable-not-assigned\n    _log_counter_per_token[token] = 1 + _log_counter_per_token.get(token, -1)\n    return _log_counter_per_token[token]",
    "doc": "Wrapper for _log_counter_per_token.\n\n    Args:\n    token: The token for which to look up the count.\n\n    Returns:\n    The number of times this function has been called with\n    *token* as an argument (starting at 0)"
  },
  {
    "code": "def log_every_n(level, msg, n, *args):\n    \"\"\"Log 'msg % args' at level 'level' once per 'n' times.\n\n    Logs the 1st call, (N+1)st call, (2N+1)st call,  etc.\n    Not threadsafe.\n\n    Args:\n    level: The level at which to log.\n    msg: The message to be logged.\n    n: The number of times this should be called before it is logged.\n    *args: The args to be substituted into the msg.\n    \"\"\"\n    count = _GetNextLogCountPerToken(_GetFileAndLine())\n    log_if(level, msg, not (count % n), *args)",
    "doc": "Log 'msg % args' at level 'level' once per 'n' times.\n\n    Logs the 1st call, (N+1)st call, (2N+1)st call,  etc.\n    Not threadsafe.\n\n    Args:\n    level: The level at which to log.\n    msg: The message to be logged.\n    n: The number of times this should be called before it is logged.\n    *args: The args to be substituted into the msg."
  },
  {
    "code": "def log_if(level, msg, condition, *args):\n    \"\"\"Log 'msg % args' at level 'level' only if condition is fulfilled.\"\"\"\n    if condition:\n        vlog(level, msg, *args)",
    "doc": "Log 'msg % args' at level 'level' only if condition is fulfilled."
  },
  {
    "code": "def _GetFileAndLine():\n    \"\"\"Returns (filename, linenumber) for the stack frame.\"\"\"\n    # Use sys._getframe().  This avoids creating a traceback object.\n    # pylint: disable=protected-access\n    f = _sys._getframe()\n    # pylint: enable=protected-access\n    our_file = f.f_code.co_filename\n    f = f.f_back\n    while f:\n        code = f.f_code\n        if code.co_filename != our_file:\n            return (code.co_filename, f.f_lineno)\n        f = f.f_back\n    return ('<unknown>', 0)",
    "doc": "Returns (filename, linenumber) for the stack frame."
  },
  {
    "code": "def google2_log_prefix(level, timestamp=None, file_and_line=None):\n    \"\"\"Assemble a logline prefix using the google2 format.\"\"\"\n    # pylint: disable=global-variable-not-assigned\n    global _level_names\n    # pylint: enable=global-variable-not-assigned\n\n    # Record current time\n    now = timestamp or _time.time()\n    now_tuple = _time.localtime(now)\n    now_microsecond = int(1e6 * (now % 1.0))\n\n    (filename, line) = file_and_line or _GetFileAndLine()\n    basename = _os.path.basename(filename)\n\n    # Severity string\n    severity = 'I'\n    if level in _level_names:\n        severity = _level_names[level][0]\n\n    s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % (\n        severity,\n        now_tuple[1],  # month\n        now_tuple[2],  # day\n        now_tuple[3],  # hour\n        now_tuple[4],  # min\n        now_tuple[5],  # sec\n        now_microsecond,\n        _get_thread_id(),\n        basename,\n        line\n    )\n\n    return s",
    "doc": "Assemble a logline prefix using the google2 format."
  },
  {
    "code": "def load_mpii_pose_dataset(path='data', is_16_pos_only=False):\n    \"\"\"Load MPII Human Pose Dataset.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to.\n    is_16_pos_only : boolean\n        If True, only return the peoples contain 16 pose keypoints. (Usually be used for single person pose estimation)\n\n    Returns\n    ----------\n    img_train_list : list of str\n        The image directories of training data.\n    ann_train_list : list of dict\n        The annotations of training data.\n    img_test_list : list of str\n        The image directories of testing data.\n    ann_test_list : list of dict\n        The annotations of testing data.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], 'image.png')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Human Pose Dataset. CVPR 14 <http://human-pose.mpi-inf.mpg.de>`__\n    - `MPII Human Pose Models. CVPR 16 <http://pose.mpi-inf.mpg.de>`__\n    - `MPII Human Shape, Poselet Conditioned Pictorial Structures and etc <http://pose.mpi-inf.mpg.de/#related>`__\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__\n    \"\"\"\n    path = os.path.join(path, 'mpii_human_pose')\n    logging.info(\"Load or Download MPII Human Pose > {}\".format(path))\n\n    # annotation\n    url = \"http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/\"\n    tar_filename = \"mpii_human_pose_v1_u12_2.zip\"\n    extracted_filename = \"mpii_human_pose_v1_u12_2\"\n    if folder_exists(os.path.join(path, extracted_filename)) is False:\n        logging.info(\"[MPII] (annotation) {} is nonexistent in {}\".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # images\n    url = \"http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/\"\n    tar_filename = \"mpii_human_pose_v1.tar.gz\"\n    extracted_filename2 = \"images\"\n    if folder_exists(os.path.join(path, extracted_filename2)) is False:\n        logging.info(\"[MPII] (images) {} is nonexistent in {}\".format(extracted_filename, path))\n        maybe_download_and_extract(tar_filename, path, url, extract=True)\n        del_file(os.path.join(path, tar_filename))\n\n    # parse annotation, format see http://human-pose.mpi-inf.mpg.de/#download\n    import scipy.io as sio\n    logging.info(\"reading annotations from mat file ...\")\n    # mat = sio.loadmat(os.path.join(path, extracted_filename, \"mpii_human_pose_v1_u12_1.mat\"))\n\n    # def fix_wrong_joints(joint):    # https://github.com/mitmul/deeppose/blob/master/datasets/mpii_dataset.py\n    #     if '12' in joint and '13' in joint and '2' in joint and '3' in joint:\n    #         if ((joint['12'][0] < joint['13'][0]) and\n    #                 (joint['3'][0] < joint['2'][0])):\n    #             joint['2'], joint['3'] = joint['3'], joint['2']\n    #         if ((joint['12'][0] > joint['13'][0]) and\n    #                 (joint['3'][0] > joint['2'][0])):\n    #             joint['2'], joint['3'] = joint['3'], joint['2']\n    #     return joint\n\n    ann_train_list = []\n    ann_test_list = []\n    img_train_list = []\n    img_test_list = []\n\n    def save_joints():\n        # joint_data_fn = os.path.join(path, 'data.json')\n        # fp = open(joint_data_fn, 'w')\n        mat = sio.loadmat(os.path.join(path, extracted_filename, \"mpii_human_pose_v1_u12_1.mat\"))\n\n        for _, (anno, train_flag) in enumerate(  # all images\n                zip(mat['RELEASE']['annolist'][0, 0][0], mat['RELEASE']['img_train'][0, 0][0])):\n\n            img_fn = anno['image']['name'][0, 0][0]\n            train_flag = int(train_flag)\n\n            # print(i, img_fn, train_flag) # DEBUG print all images\n\n            if train_flag:\n                img_train_list.append(img_fn)\n                ann_train_list.append([])\n            else:\n                img_test_list.append(img_fn)\n                ann_test_list.append([])\n\n            head_rect = []\n            if 'x1' in str(anno['annorect'].dtype):\n                head_rect = zip(\n                    [x1[0, 0] for x1 in anno['annorect']['x1'][0]], [y1[0, 0] for y1 in anno['annorect']['y1'][0]],\n                    [x2[0, 0] for x2 in anno['annorect']['x2'][0]], [y2[0, 0] for y2 in anno['annorect']['y2'][0]]\n                )\n            else:\n                head_rect = []  # TODO\n\n            if 'annopoints' in str(anno['annorect'].dtype):\n                annopoints = anno['annorect']['annopoints'][0]\n                head_x1s = anno['annorect']['x1'][0]\n                head_y1s = anno['annorect']['y1'][0]\n                head_x2s = anno['annorect']['x2'][0]\n                head_y2s = anno['annorect']['y2'][0]\n\n                for annopoint, head_x1, head_y1, head_x2, head_y2 in zip(annopoints, head_x1s, head_y1s, head_x2s,\n                                                                         head_y2s):\n                    # if annopoint != []:\n                    # if len(annopoint) != 0:\n                    if annopoint.size:\n                        head_rect = [\n                            float(head_x1[0, 0]),\n                            float(head_y1[0, 0]),\n                            float(head_x2[0, 0]),\n                            float(head_y2[0, 0])\n                        ]\n\n                        # joint coordinates\n                        annopoint = annopoint['point'][0, 0]\n                        j_id = [str(j_i[0, 0]) for j_i in annopoint['id'][0]]\n                        x = [x[0, 0] for x in annopoint['x'][0]]\n                        y = [y[0, 0] for y in annopoint['y'][0]]\n                        joint_pos = {}\n                        for _j_id, (_x, _y) in zip(j_id, zip(x, y)):\n                            joint_pos[int(_j_id)] = [float(_x), float(_y)]\n                        # joint_pos = fix_wrong_joints(joint_pos)\n\n                        # visibility list\n                        if 'is_visible' in str(annopoint.dtype):\n                            vis = [v[0] if v.size > 0 else [0] for v in annopoint['is_visible'][0]]\n                            vis = dict([(k, int(v[0])) if len(v) > 0 else v for k, v in zip(j_id, vis)])\n                        else:\n                            vis = None\n\n                        # if len(joint_pos) == 16:\n                        if ((is_16_pos_only ==True) and (len(joint_pos) == 16)) or (is_16_pos_only == False):\n                            # only use image with 16 key points / or use all\n                            data = {\n                                'filename': img_fn,\n                                'train': train_flag,\n                                'head_rect': head_rect,\n                                'is_visible': vis,\n                                'joint_pos': joint_pos\n                            }\n                            # print(json.dumps(data), file=fp)  # py3\n                            if train_flag:\n                                ann_train_list[-1].append(data)\n                            else:\n                                ann_test_list[-1].append(data)\n\n    # def write_line(datum, fp):\n    #     joints = sorted([[int(k), v] for k, v in datum['joint_pos'].items()])\n    #     joints = np.array([j for i, j in joints]).flatten()\n    #\n    #     out = [datum['filename']]\n    #     out.extend(joints)\n    #     out = [str(o) for o in out]\n    #     out = ','.join(out)\n    #\n    #     print(out, file=fp)\n\n    # def split_train_test():\n    #     # fp_test = open('data/mpii/test_joints.csv', 'w')\n    #     fp_test = open(os.path.join(path, 'test_joints.csv'), 'w')\n    #     # fp_train = open('data/mpii/train_joints.csv', 'w')\n    #     fp_train = open(os.path.join(path, 'train_joints.csv'), 'w')\n    #     # all_data = open('data/mpii/data.json').readlines()\n    #     all_data = open(os.path.join(path, 'data.json')).readlines()\n    #     N = len(all_data)\n    #     N_test = int(N * 0.1)\n    #     N_train = N - N_test\n    #\n    #     print('N:{}'.format(N))\n    #     print('N_train:{}'.format(N_train))\n    #     print('N_test:{}'.format(N_test))\n    #\n    #     np.random.seed(1701)\n    #     perm = np.random.permutation(N)\n    #     test_indices = perm[:N_test]\n    #     train_indices = perm[N_test:]\n    #\n    #     print('train_indices:{}'.format(len(train_indices)))\n    #     print('test_indices:{}'.format(len(test_indices)))\n    #\n    #     for i in train_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_train)\n    #\n    #     for i in test_indices:\n    #         datum = json.loads(all_data[i].strip())\n    #         write_line(datum, fp_test)\n\n    save_joints()\n    # split_train_test()  #\n\n    ## read images dir\n    logging.info(\"reading images list ...\")\n    img_dir = os.path.join(path, extracted_filename2)\n    _img_list = load_file_list(path=os.path.join(path, extracted_filename2), regx='\\\\.jpg', printable=False)\n    # ann_list = json.load(open(os.path.join(path, 'data.json')))\n    for i, im in enumerate(img_train_list):\n        if im not in _img_list:\n            print('missing training image {} in {} (remove from img(ann)_train_list)'.format(im, img_dir))\n            # img_train_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n    for i, im in enumerate(img_test_list):\n        if im not in _img_list:\n            print('missing testing image {} in {} (remove from img(ann)_test_list)'.format(im, img_dir))\n            # img_test_list.remove(im)\n            del img_train_list[i]\n            del ann_train_list[i]\n\n    ## check annotation and images\n    n_train_images = len(img_train_list)\n    n_test_images = len(img_test_list)\n    n_images = n_train_images + n_test_images\n    logging.info(\"n_images: {} n_train_images: {} n_test_images: {}\".format(n_images, n_train_images, n_test_images))\n    n_train_ann = len(ann_train_list)\n    n_test_ann = len(ann_test_list)\n    n_ann = n_train_ann + n_test_ann\n    logging.info(\"n_ann: {} n_train_ann: {} n_test_ann: {}\".format(n_ann, n_train_ann, n_test_ann))\n    n_train_people = len(sum(ann_train_list, []))\n    n_test_people = len(sum(ann_test_list, []))\n    n_people = n_train_people + n_test_people\n    logging.info(\"n_people: {} n_train_people: {} n_test_people: {}\".format(n_people, n_train_people, n_test_people))\n    # add path to all image file name\n    for i, value in enumerate(img_train_list):\n        img_train_list[i] = os.path.join(img_dir, value)\n    for i, value in enumerate(img_test_list):\n        img_test_list[i] = os.path.join(img_dir, value)\n    return img_train_list, ann_train_list, img_test_list, ann_test_list",
    "doc": "Load MPII Human Pose Dataset.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to.\n    is_16_pos_only : boolean\n        If True, only return the peoples contain 16 pose keypoints. (Usually be used for single person pose estimation)\n\n    Returns\n    ----------\n    img_train_list : list of str\n        The image directories of training data.\n    ann_train_list : list of dict\n        The annotations of training data.\n    img_test_list : list of str\n        The image directories of testing data.\n    ann_test_list : list of dict\n        The annotations of testing data.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], 'image.png')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Human Pose Dataset. CVPR 14 <http://human-pose.mpi-inf.mpg.de>`__\n    - `MPII Human Pose Models. CVPR 16 <http://pose.mpi-inf.mpg.de>`__\n    - `MPII Human Shape, Poselet Conditioned Pictorial Structures and etc <http://pose.mpi-inf.mpg.de/#related>`__\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__"
  },
  {
    "code": "def transformer(U, theta, out_size, name='SpatialTransformer2dAffine'):\n    \"\"\"Spatial Transformer Layer for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__\n    , see :class:`SpatialTransformer2dAffineLayer` class.\n\n    Parameters\n    ----------\n    U : list of float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the localisation network should be [num_batch, 6], value range should be [0, 1] (via tanh).\n    out_size: tuple of int\n        The size of the output of the network (height, width)\n    name: str\n        Optional function name\n\n    Returns\n    -------\n    Tensor\n        The transformed tensor.\n\n    References\n    ----------\n    - `Spatial Transformer Networks <https://arxiv.org/abs/1506.02025>`__\n    - `TensorFlow/Models <https://github.com/tensorflow/models/tree/master/transformer>`__\n\n    Notes\n    -----\n    To initialize the network to the identity transform init.\n\n    >>> import tensorflow as tf\n    >>> # ``theta`` to\n    >>> identity = np.array([[1., 0., 0.], [0., 1., 0.]])\n    >>> identity = identity.flatten()\n    >>> theta = tf.Variable(initial_value=identity)\n\n    \"\"\"\n\n    def _repeat(x, n_repeats):\n        with tf.variable_scope('_repeat'):\n            rep = tf.transpose(tf.expand_dims(tf.ones(shape=tf.stack([\n                n_repeats,\n            ])), 1), [1, 0])\n            rep = tf.cast(rep, 'int32')\n            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n            return tf.reshape(x, [-1])\n\n    def _interpolate(im, x, y, out_size):\n        with tf.variable_scope('_interpolate'):\n            # constants\n            num_batch = tf.shape(im)[0]\n            height = tf.shape(im)[1]\n            width = tf.shape(im)[2]\n            channels = tf.shape(im)[3]\n\n            x = tf.cast(x, 'float32')\n            y = tf.cast(y, 'float32')\n            height_f = tf.cast(height, 'float32')\n            width_f = tf.cast(width, 'float32')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            zero = tf.zeros([], dtype='int32')\n            max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')\n            max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')\n\n            # scale indices from [-1, 1] to [0, width/height]\n            x = (x + 1.0) * (width_f) / 2.0\n            y = (y + 1.0) * (height_f) / 2.0\n\n            # do sampling\n            x0 = tf.cast(tf.floor(x), 'int32')\n            x1 = x0 + 1\n            y0 = tf.cast(tf.floor(y), 'int32')\n            y1 = y0 + 1\n\n            x0 = tf.clip_by_value(x0, zero, max_x)\n            x1 = tf.clip_by_value(x1, zero, max_x)\n            y0 = tf.clip_by_value(y0, zero, max_y)\n            y1 = tf.clip_by_value(y1, zero, max_y)\n            dim2 = width\n            dim1 = width * height\n            base = _repeat(tf.range(num_batch) * dim1, out_height * out_width)\n            base_y0 = base + y0 * dim2\n            base_y1 = base + y1 * dim2\n            idx_a = base_y0 + x0\n            idx_b = base_y1 + x0\n            idx_c = base_y0 + x1\n            idx_d = base_y1 + x1\n\n            # use indices to lookup pixels in the flat image and restore\n            # channels dim\n            im_flat = tf.reshape(im, tf.stack([-1, channels]))\n            im_flat = tf.cast(im_flat, 'float32')\n            Ia = tf.gather(im_flat, idx_a)\n            Ib = tf.gather(im_flat, idx_b)\n            Ic = tf.gather(im_flat, idx_c)\n            Id = tf.gather(im_flat, idx_d)\n\n            # and finally calculate interpolated values\n            x0_f = tf.cast(x0, 'float32')\n            x1_f = tf.cast(x1, 'float32')\n            y0_f = tf.cast(y0, 'float32')\n            y1_f = tf.cast(y1, 'float32')\n            wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)\n            wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)\n            wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)\n            wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)\n            output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n            return output\n\n    def _meshgrid(height, width):\n        with tf.variable_scope('_meshgrid'):\n            # This should be equivalent to:\n            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n            #                         np.linspace(-1, 1, height))\n            #  ones = np.ones(np.prod(x_t.shape))\n            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n            x_t = tf.matmul(\n                tf.ones(shape=tf.stack([height, 1])),\n                tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0])\n            )\n            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1), tf.ones(shape=tf.stack([1, width])))\n\n            x_t_flat = tf.reshape(x_t, (1, -1))\n            y_t_flat = tf.reshape(y_t, (1, -1))\n\n            ones = tf.ones_like(x_t_flat)\n            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])\n            return grid\n\n    def _transform(theta, input_dim, out_size):\n        with tf.variable_scope('_transform'):\n            num_batch = tf.shape(input_dim)[0]\n            num_channels = tf.shape(input_dim)[3]\n            theta = tf.reshape(theta, (-1, 2, 3))\n            theta = tf.cast(theta, 'float32')\n\n            # grid of (x_t, y_t, 1), eq (1) in ref [1]\n            out_height = out_size[0]\n            out_width = out_size[1]\n            grid = _meshgrid(out_height, out_width)\n            grid = tf.expand_dims(grid, 0)\n            grid = tf.reshape(grid, [-1])\n            grid = tf.tile(grid, tf.stack([num_batch]))\n            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))\n\n            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n            T_g = tf.matmul(theta, grid)\n            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n            x_s_flat = tf.reshape(x_s, [-1])\n            y_s_flat = tf.reshape(y_s, [-1])\n\n            input_transformed = _interpolate(input_dim, x_s_flat, y_s_flat, out_size)\n\n            output = tf.reshape(input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))\n            return output\n\n    with tf.variable_scope(name):\n        output = _transform(theta, U, out_size)\n        return output",
    "doc": "Spatial Transformer Layer for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__\n    , see :class:`SpatialTransformer2dAffineLayer` class.\n\n    Parameters\n    ----------\n    U : list of float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the localisation network should be [num_batch, 6], value range should be [0, 1] (via tanh).\n    out_size: tuple of int\n        The size of the output of the network (height, width)\n    name: str\n        Optional function name\n\n    Returns\n    -------\n    Tensor\n        The transformed tensor.\n\n    References\n    ----------\n    - `Spatial Transformer Networks <https://arxiv.org/abs/1506.02025>`__\n    - `TensorFlow/Models <https://github.com/tensorflow/models/tree/master/transformer>`__\n\n    Notes\n    -----\n    To initialize the network to the identity transform init.\n\n    >>> import tensorflow as tf\n    >>> # ``theta`` to\n    >>> identity = np.array([[1., 0., 0.], [0., 1., 0.]])\n    >>> identity = identity.flatten()\n    >>> theta = tf.Variable(initial_value=identity)"
  },
  {
    "code": "def batch_transformer(U, thetas, out_size, name='BatchSpatialTransformer2dAffine'):\n    \"\"\"Batch Spatial Transformer function for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__.\n\n    Parameters\n    ----------\n    U : list of float\n        tensor of inputs [batch, height, width, num_channels]\n    thetas : list of float\n        a set of transformations for each input [batch, num_transforms, 6]\n    out_size : list of int\n        the size of the output [out_height, out_width]\n    name : str\n        optional function name\n\n    Returns\n    ------\n    float\n        Tensor of size [batch * num_transforms, out_height, out_width, num_channels]\n\n    \"\"\"\n    with tf.variable_scope(name):\n        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])\n        indices = [[i] * num_transforms for i in xrange(num_batch)]\n        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))\n        return transformer(input_repeated, thetas, out_size)",
    "doc": "Batch Spatial Transformer function for `2D Affine Transformation <https://en.wikipedia.org/wiki/Affine_transformation>`__.\n\n    Parameters\n    ----------\n    U : list of float\n        tensor of inputs [batch, height, width, num_channels]\n    thetas : list of float\n        a set of transformations for each input [batch, num_transforms, 6]\n    out_size : list of int\n        the size of the output [out_height, out_width]\n    name : str\n        optional function name\n\n    Returns\n    ------\n    float\n        Tensor of size [batch * num_transforms, out_height, out_width, num_channels]"
  },
  {
    "code": "def create_task_spec_def():\n    \"\"\"Returns the a :class:`TaskSpecDef` based on the environment variables for distributed training.\n\n    References\n    ----------\n    - `ML-engine trainer considerations <https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config>`__\n    - `TensorPort Distributed Computing <https://www.tensorport.com/documentation/code-details/>`__\n\n    \"\"\"\n    if 'TF_CONFIG' in os.environ:\n        # TF_CONFIG is used in ML-engine\n        env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n        task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n        cluster_data = env.get('cluster', None) or {'ps': None, 'worker': None, 'master': None}\n        return TaskSpecDef(\n            task_type=task_data['type'], index=task_data['index'],\n            trial=task_data['trial'] if 'trial' in task_data else None, ps_hosts=cluster_data['ps'],\n            worker_hosts=cluster_data['worker'], master=cluster_data['master'] if 'master' in cluster_data else None\n        )\n    elif 'JOB_NAME' in os.environ:\n        # JOB_NAME, TASK_INDEX, PS_HOSTS, WORKER_HOSTS and MASTER_HOST are used in TensorPort\n        return TaskSpecDef(\n            task_type=os.environ['JOB_NAME'], index=os.environ['TASK_INDEX'], ps_hosts=os.environ.get('PS_HOSTS', None),\n            worker_hosts=os.environ.get('WORKER_HOSTS', None), master=os.environ.get('MASTER_HOST', None)\n        )\n    else:\n        raise Exception('You need to setup TF_CONFIG or JOB_NAME to define the task.')",
    "doc": "Returns the a :class:`TaskSpecDef` based on the environment variables for distributed training.\n\n    References\n    ----------\n    - `ML-engine trainer considerations <https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config>`__\n    - `TensorPort Distributed Computing <https://www.tensorport.com/documentation/code-details/>`__"
  },
  {
    "code": "def create_distributed_session(\n        task_spec=None, checkpoint_dir=None, scaffold=None, hooks=None, chief_only_hooks=None, save_checkpoint_secs=600,\n        save_summaries_steps=object(), save_summaries_secs=object(), config=None, stop_grace_period_secs=120,\n        log_step_count_steps=100\n):\n    \"\"\"Creates a distributed session.\n\n    It calls `MonitoredTrainingSession` to create a :class:`MonitoredSession` for distributed training.\n\n    Parameters\n    ----------\n    task_spec : :class:`TaskSpecDef`.\n        The task spec definition from create_task_spec_def()\n    checkpoint_dir : str.\n        Optional path to a directory where to restore variables.\n    scaffold : ``Scaffold``\n        A `Scaffold` used for gathering or building supportive ops.\n        If not specified, a default one is created. It's used to finalize the graph.\n    hooks : list of ``SessionRunHook`` objects.\n        Optional\n    chief_only_hooks : list of ``SessionRunHook`` objects.\n        Activate these hooks if `is_chief==True`, ignore otherwise.\n    save_checkpoint_secs : int\n        The frequency, in seconds, that a checkpoint is saved\n        using a default checkpoint saver. If `save_checkpoint_secs` is set to\n        `None`, then the default checkpoint saver isn't used.\n    save_summaries_steps : int\n        The frequency, in number of global steps, that the\n        summaries are written to disk using a default summary saver. If both\n        `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n        the default summary saver isn't used. Default 100.\n    save_summaries_secs : int\n        The frequency, in secs, that the summaries are written\n        to disk using a default summary saver.  If both `save_summaries_steps` and\n        `save_summaries_secs` are set to `None`, then the default summary saver\n        isn't used. Default not enabled.\n    config : ``tf.ConfigProto``\n        an instance of `tf.ConfigProto` proto used to configure the session.\n        It's the `config` argument of constructor of `tf.Session`.\n    stop_grace_period_secs : int\n        Number of seconds given to threads to stop after\n        `close()` has been called.\n    log_step_count_steps : int\n        The frequency, in number of global steps, that the\n        global step/sec is logged.\n\n    Examples\n    --------\n    A simple example for distributed training where all the workers use the same dataset:\n\n    >>> task_spec = TaskSpec()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      tensors = create_graph()\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir='/tmp/ckpt') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    An example where the dataset is shared among the workers\n    (see https://www.tensorflow.org/programmers_guide/datasets):\n\n    >>> task_spec = TaskSpec()\n    >>> # dataset is a :class:`tf.data.Dataset` with the raw data\n    >>> dataset = create_dataset()\n    >>> if task_spec is not None:\n    >>>     dataset = dataset.shard(task_spec.num_workers, task_spec.shard_index)\n    >>> # shuffle or apply a map function to the new sharded dataset, for example:\n    >>> dataset = dataset.shuffle(buffer_size=10000)\n    >>> dataset = dataset.batch(batch_size)\n    >>> dataset = dataset.repeat(num_epochs)\n    >>> # create the iterator for the dataset and the input tensor\n    >>> iterator = dataset.make_one_shot_iterator()\n    >>> next_element = iterator.get_next()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      # next_element is the input for the graph\n    >>>      tensors = create_graph(next_element)\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir='/tmp/ckpt') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    References\n    ----------\n    - `MonitoredTrainingSession <https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession>`__\n\n    \"\"\"\n    target = task_spec.target() if task_spec is not None else None\n    is_chief = task_spec.is_master() if task_spec is not None else True\n    return tf.train.MonitoredTrainingSession(\n        master=target, is_chief=is_chief, checkpoint_dir=checkpoint_dir, scaffold=scaffold,\n        save_checkpoint_secs=save_checkpoint_secs, save_summaries_steps=save_summaries_steps,\n        save_summaries_secs=save_summaries_secs, log_step_count_steps=log_step_count_steps,\n        stop_grace_period_secs=stop_grace_period_secs, config=config, hooks=hooks, chief_only_hooks=chief_only_hooks\n    )",
    "doc": "Creates a distributed session.\n\n    It calls `MonitoredTrainingSession` to create a :class:`MonitoredSession` for distributed training.\n\n    Parameters\n    ----------\n    task_spec : :class:`TaskSpecDef`.\n        The task spec definition from create_task_spec_def()\n    checkpoint_dir : str.\n        Optional path to a directory where to restore variables.\n    scaffold : ``Scaffold``\n        A `Scaffold` used for gathering or building supportive ops.\n        If not specified, a default one is created. It's used to finalize the graph.\n    hooks : list of ``SessionRunHook`` objects.\n        Optional\n    chief_only_hooks : list of ``SessionRunHook`` objects.\n        Activate these hooks if `is_chief==True`, ignore otherwise.\n    save_checkpoint_secs : int\n        The frequency, in seconds, that a checkpoint is saved\n        using a default checkpoint saver. If `save_checkpoint_secs` is set to\n        `None`, then the default checkpoint saver isn't used.\n    save_summaries_steps : int\n        The frequency, in number of global steps, that the\n        summaries are written to disk using a default summary saver. If both\n        `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n        the default summary saver isn't used. Default 100.\n    save_summaries_secs : int\n        The frequency, in secs, that the summaries are written\n        to disk using a default summary saver.  If both `save_summaries_steps` and\n        `save_summaries_secs` are set to `None`, then the default summary saver\n        isn't used. Default not enabled.\n    config : ``tf.ConfigProto``\n        an instance of `tf.ConfigProto` proto used to configure the session.\n        It's the `config` argument of constructor of `tf.Session`.\n    stop_grace_period_secs : int\n        Number of seconds given to threads to stop after\n        `close()` has been called.\n    log_step_count_steps : int\n        The frequency, in number of global steps, that the\n        global step/sec is logged.\n\n    Examples\n    --------\n    A simple example for distributed training where all the workers use the same dataset:\n\n    >>> task_spec = TaskSpec()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      tensors = create_graph()\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir='/tmp/ckpt') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    An example where the dataset is shared among the workers\n    (see https://www.tensorflow.org/programmers_guide/datasets):\n\n    >>> task_spec = TaskSpec()\n    >>> # dataset is a :class:`tf.data.Dataset` with the raw data\n    >>> dataset = create_dataset()\n    >>> if task_spec is not None:\n    >>>     dataset = dataset.shard(task_spec.num_workers, task_spec.shard_index)\n    >>> # shuffle or apply a map function to the new sharded dataset, for example:\n    >>> dataset = dataset.shuffle(buffer_size=10000)\n    >>> dataset = dataset.batch(batch_size)\n    >>> dataset = dataset.repeat(num_epochs)\n    >>> # create the iterator for the dataset and the input tensor\n    >>> iterator = dataset.make_one_shot_iterator()\n    >>> next_element = iterator.get_next()\n    >>> with tf.device(task_spec.device_fn()):\n    >>>      # next_element is the input for the graph\n    >>>      tensors = create_graph(next_element)\n    >>> with tl.DistributedSession(task_spec=task_spec,\n    ...                            checkpoint_dir='/tmp/ckpt') as session:\n    >>>      while not session.should_stop():\n    >>>           session.run(tensors)\n\n    References\n    ----------\n    - `MonitoredTrainingSession <https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession>`__"
  },
  {
    "code": "def validation_metrics(self):\n        \"\"\"A helper function to compute validation related metrics\"\"\"\n\n        if (self._validation_iterator is None) or (self._validation_metrics is None):\n            raise AttributeError('Validation is not setup.')\n\n        n = 0.0\n        metric_sums = [0.0] * len(self._validation_metrics)\n        self._sess.run(self._validation_iterator.initializer)\n        while True:\n            try:\n                metrics = self._sess.run(self._validation_metrics)\n                for i, m in enumerate(metrics):\n                    metric_sums[i] += m\n                n += 1.0\n            except tf.errors.OutOfRangeError:\n                break\n        for i, m in enumerate(metric_sums):\n            metric_sums[i] = metric_sums[i] / n\n        return zip(self._validation_metrics, metric_sums)",
    "doc": "A helper function to compute validation related metrics"
  },
  {
    "code": "def train_and_validate_to_end(self, validate_step_size=50):\n        \"\"\"A helper function that shows how to train and validate a model at the same time.\n\n        Parameters\n        ----------\n        validate_step_size : int\n            Validate the training network every N steps.\n\n        \"\"\"\n        while not self._sess.should_stop():\n            self.train_on_batch()  # Run a training step synchronously.\n            if self.global_step % validate_step_size == 0:\n                # logging.info(\"Average loss for validation dataset: %s\" % self.get_validation_metrics())\n                log_str = 'step: %d, ' % self.global_step\n                for n, m in self.validation_metrics:\n                    log_str += '%s: %f, ' % (n.name, m)\n                logging.info(log_str)",
    "doc": "A helper function that shows how to train and validate a model at the same time.\n\n        Parameters\n        ----------\n        validate_step_size : int\n            Validate the training network every N steps."
  },
  {
    "code": "def _load_mnist_dataset(shape, path, name='mnist', url='http://yann.lecun.com/exdb/mnist/'):\n    \"\"\"A generic function to load mnist-like dataset.\n\n    Parameters:\n    ----------\n    shape : tuple\n        The shape of digit images.\n    path : str\n        The path that the data is downloaded to.\n    name : str\n        The dataset name you want to use(the default is 'mnist').\n    url : str\n        The url of dataset(the default is 'http://yann.lecun.com/exdb/mnist/').\n    \"\"\"\n    path = os.path.join(path, name)\n\n    # Define functions for loading mnist-like data's images and labels.\n    # For convenience, they also download the requested files if needed.\n    def load_mnist_images(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n\n        logging.info(filepath)\n        # Read the inputs in Yann LeCun's binary format.\n        with gzip.open(filepath, 'rb') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(shape)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return data / np.float32(256)\n\n    def load_mnist_labels(path, filename):\n        filepath = maybe_download_and_extract(filename, path, url)\n        # Read the labels in Yann LeCun's binary format.\n        with gzip.open(filepath, 'rb') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that's exactly what we want.\n        return data\n\n    # Download and read the training and test set images and labels.\n    logging.info(\"Load or Download {0} > {1}\".format(name.upper(), path))\n    X_train = load_mnist_images(path, 'train-images-idx3-ubyte.gz')\n    y_train = load_mnist_labels(path, 'train-labels-idx1-ubyte.gz')\n    X_test = load_mnist_images(path, 't10k-images-idx3-ubyte.gz')\n    y_test = load_mnist_labels(path, 't10k-labels-idx1-ubyte.gz')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn't matter how we do this as long as we can read them again.)\n    X_train = np.asarray(X_train, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    X_val = np.asarray(X_val, dtype=np.float32)\n    y_val = np.asarray(y_val, dtype=np.int32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n    return X_train, y_train, X_val, y_val, X_test, y_test",
    "doc": "A generic function to load mnist-like dataset.\n\n    Parameters:\n    ----------\n    shape : tuple\n        The shape of digit images.\n    path : str\n        The path that the data is downloaded to.\n    name : str\n        The dataset name you want to use(the default is 'mnist').\n    url : str\n        The url of dataset(the default is 'http://yann.lecun.com/exdb/mnist/')."
  },
  {
    "code": "def load_cifar10_dataset(shape=(-1, 32, 32, 3), path='data', plotable=False):\n    \"\"\"Load CIFAR-10 dataset.\n\n    It consists of 60000 32x32 colour images in 10 classes, with\n    6000 images per class. There are 50000 training images and 10000 test images.\n\n    The dataset is divided into five training batches and one test batch, each with\n    10000 images. The test batch contains exactly 1000 randomly-selected images from\n    each class. The training batches contain the remaining images in random order,\n    but some training batches may contain more images from one class than another.\n    Between them, the training batches contain exactly 5000 images from each class.\n\n    Parameters\n    ----------\n    shape : tupe\n        The shape of digit images e.g. (-1, 3, 32, 32) and (-1, 32, 32, 3).\n    path : str\n        The path that the data is downloaded to, defaults is ``data/cifar10/``.\n    plotable : boolean\n        Whether to plot some image examples, False as default.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n\n    References\n    ----------\n    - `CIFAR website <https://www.cs.toronto.edu/~kriz/cifar.html>`__\n    - `Data download link <https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>`__\n    - `<https://teratail.com/questions/28932>`__\n\n    \"\"\"\n    path = os.path.join(path, 'cifar10')\n    logging.info(\"Load or Download cifar10 > {}\".format(path))\n\n    # Helper function to unpickle the data\n    def unpickle(file):\n        fp = open(file, 'rb')\n        if sys.version_info.major == 2:\n            data = pickle.load(fp)\n        elif sys.version_info.major == 3:\n            data = pickle.load(fp, encoding='latin-1')\n        fp.close()\n        return data\n\n    filename = 'cifar-10-python.tar.gz'\n    url = 'https://www.cs.toronto.edu/~kriz/'\n    # Download and uncompress file\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    # Unpickle file and fill in data\n    X_train = None\n    y_train = []\n    for i in range(1, 6):\n        data_dic = unpickle(os.path.join(path, 'cifar-10-batches-py/', \"data_batch_{}\".format(i)))\n        if i == 1:\n            X_train = data_dic['data']\n        else:\n            X_train = np.vstack((X_train, data_dic['data']))\n        y_train += data_dic['labels']\n\n    test_data_dic = unpickle(os.path.join(path, 'cifar-10-batches-py/', \"test_batch\"))\n    X_test = test_data_dic['data']\n    y_test = np.array(test_data_dic['labels'])\n\n    if shape == (-1, 3, 32, 32):\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n    elif shape == (-1, 32, 32, 3):\n        X_test = X_test.reshape(shape, order='F')\n        X_train = X_train.reshape(shape, order='F')\n        X_test = np.transpose(X_test, (0, 2, 1, 3))\n        X_train = np.transpose(X_train, (0, 2, 1, 3))\n    else:\n        X_test = X_test.reshape(shape)\n        X_train = X_train.reshape(shape)\n\n    y_train = np.array(y_train)\n\n    if plotable:\n        logging.info('\\nCIFAR-10')\n        fig = plt.figure(1)\n\n        logging.info('Shape of a training image: X_train[0] %s' % X_train[0].shape)\n\n        plt.ion()  # interactive mode\n        count = 1\n        for _ in range(10):  # each row\n            for _ in range(10):  # each column\n                _ = fig.add_subplot(10, 10, count)\n                if shape == (-1, 3, 32, 32):\n                    # plt.imshow(X_train[count-1], interpolation='nearest')\n                    plt.imshow(np.transpose(X_train[count - 1], (1, 2, 0)), interpolation='nearest')\n                    # plt.imshow(np.transpose(X_train[count-1], (2, 1, 0)), interpolation='nearest')\n                elif shape == (-1, 32, 32, 3):\n                    plt.imshow(X_train[count - 1], interpolation='nearest')\n                    # plt.imshow(np.transpose(X_train[count-1], (1, 0, 2)), interpolation='nearest')\n                else:\n                    raise Exception(\"Do not support the given 'shape' to plot the image examples\")\n                plt.gca().xaxis.set_major_locator(plt.NullLocator())  # \u4e0d\u663e\u793a\u523b\u5ea6(tick)\n                plt.gca().yaxis.set_major_locator(plt.NullLocator())\n                count = count + 1\n        plt.draw()  # interactive mode\n        plt.pause(3)  # interactive mode\n\n        logging.info(\"X_train: %s\" % X_train.shape)\n        logging.info(\"y_train: %s\" % y_train.shape)\n        logging.info(\"X_test:  %s\" % X_test.shape)\n        logging.info(\"y_test:  %s\" % y_test.shape)\n\n    X_train = np.asarray(X_train, dtype=np.float32)\n    X_test = np.asarray(X_test, dtype=np.float32)\n    y_train = np.asarray(y_train, dtype=np.int32)\n    y_test = np.asarray(y_test, dtype=np.int32)\n\n    return X_train, y_train, X_test, y_test",
    "doc": "Load CIFAR-10 dataset.\n\n    It consists of 60000 32x32 colour images in 10 classes, with\n    6000 images per class. There are 50000 training images and 10000 test images.\n\n    The dataset is divided into five training batches and one test batch, each with\n    10000 images. The test batch contains exactly 1000 randomly-selected images from\n    each class. The training batches contain the remaining images in random order,\n    but some training batches may contain more images from one class than another.\n    Between them, the training batches contain exactly 5000 images from each class.\n\n    Parameters\n    ----------\n    shape : tupe\n        The shape of digit images e.g. (-1, 3, 32, 32) and (-1, 32, 32, 3).\n    path : str\n        The path that the data is downloaded to, defaults is ``data/cifar10/``.\n    plotable : boolean\n        Whether to plot some image examples, False as default.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n\n    References\n    ----------\n    - `CIFAR website <https://www.cs.toronto.edu/~kriz/cifar.html>`__\n    - `Data download link <https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>`__\n    - `<https://teratail.com/questions/28932>`__"
  },
  {
    "code": "def load_cropped_svhn(path='data', include_extra=True):\n    \"\"\"Load Cropped SVHN.\n\n    The Cropped Street View House Numbers (SVHN) Dataset contains 32x32x3 RGB images.\n    Digit '1' has label 1, '9' has label 9 and '0' has label 0 (the original dataset uses 10 to represent '0'), see `ufldl website <http://ufldl.stanford.edu/housenumbers/>`__.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to.\n    include_extra : boolean\n        If True (default), add extra images to the training set.\n\n    Returns\n    -------\n    X_train, y_train, X_test, y_test: tuple\n        Return splitted training/test set respectively.\n\n    Examples\n    ---------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cropped_svhn(include_extra=False)\n    >>> tl.vis.save_images(X_train[0:100], [10, 10], 'svhn.png')\n\n    \"\"\"\n    start_time = time.time()\n\n    path = os.path.join(path, 'cropped_svhn')\n    logging.info(\"Load or Download Cropped SVHN > {} | include extra images: {}\".format(path, include_extra))\n    url = \"http://ufldl.stanford.edu/housenumbers/\"\n\n    np_file = os.path.join(path, \"train_32x32.npz\")\n    if file_exists(np_file) is False:\n        filename = \"train_32x32.mat\"\n        filepath = maybe_download_and_extract(filename, path, url)\n        mat = sio.loadmat(filepath)\n        X_train = mat['X'] / 255.0  # to [0, 1]\n        X_train = np.transpose(X_train, (3, 0, 1, 2))\n        y_train = np.squeeze(mat['y'], axis=1)\n        y_train[y_train == 10] = 0  # replace 10 to 0\n        np.savez(np_file, X=X_train, y=y_train)\n        del_file(filepath)\n    else:\n        v = np.load(np_file)\n        X_train = v['X']\n        y_train = v['y']\n    logging.info(\"  n_train: {}\".format(len(y_train)))\n\n    np_file = os.path.join(path, \"test_32x32.npz\")\n    if file_exists(np_file) is False:\n        filename = \"test_32x32.mat\"\n        filepath = maybe_download_and_extract(filename, path, url)\n        mat = sio.loadmat(filepath)\n        X_test = mat['X'] / 255.0\n        X_test = np.transpose(X_test, (3, 0, 1, 2))\n        y_test = np.squeeze(mat['y'], axis=1)\n        y_test[y_test == 10] = 0\n        np.savez(np_file, X=X_test, y=y_test)\n        del_file(filepath)\n    else:\n        v = np.load(np_file)\n        X_test = v['X']\n        y_test = v['y']\n    logging.info(\"  n_test: {}\".format(len(y_test)))\n\n    if include_extra:\n        logging.info(\"  getting extra 531131 images, please wait ...\")\n        np_file = os.path.join(path, \"extra_32x32.npz\")\n        if file_exists(np_file) is False:\n            logging.info(\"  the first time to load extra images will take long time to convert the file format ...\")\n            filename = \"extra_32x32.mat\"\n            filepath = maybe_download_and_extract(filename, path, url)\n            mat = sio.loadmat(filepath)\n            X_extra = mat['X'] / 255.0\n            X_extra = np.transpose(X_extra, (3, 0, 1, 2))\n            y_extra = np.squeeze(mat['y'], axis=1)\n            y_extra[y_extra == 10] = 0\n            np.savez(np_file, X=X_extra, y=y_extra)\n            del_file(filepath)\n        else:\n            v = np.load(np_file)\n            X_extra = v['X']\n            y_extra = v['y']\n        # print(X_train.shape, X_extra.shape)\n        logging.info(\"  adding n_extra {} to n_train {}\".format(len(y_extra), len(y_train)))\n        t = time.time()\n        X_train = np.concatenate((X_train, X_extra), 0)\n        y_train = np.concatenate((y_train, y_extra), 0)\n        # X_train = np.append(X_train, X_extra, axis=0)\n        # y_train = np.append(y_train, y_extra, axis=0)\n        logging.info(\"  added n_extra {} to n_train {} took {}s\".format(len(y_extra), len(y_train), time.time() - t))\n    else:\n        logging.info(\"  no extra images are included\")\n    logging.info(\"  image size: %s n_train: %d n_test: %d\" % (str(X_train.shape[1:4]), len(y_train), len(y_test)))\n    logging.info(\"  took: {}s\".format(int(time.time() - start_time)))\n    return X_train, y_train, X_test, y_test",
    "doc": "Load Cropped SVHN.\n\n    The Cropped Street View House Numbers (SVHN) Dataset contains 32x32x3 RGB images.\n    Digit '1' has label 1, '9' has label 9 and '0' has label 0 (the original dataset uses 10 to represent '0'), see `ufldl website <http://ufldl.stanford.edu/housenumbers/>`__.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to.\n    include_extra : boolean\n        If True (default), add extra images to the training set.\n\n    Returns\n    -------\n    X_train, y_train, X_test, y_test: tuple\n        Return splitted training/test set respectively.\n\n    Examples\n    ---------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_cropped_svhn(include_extra=False)\n    >>> tl.vis.save_images(X_train[0:100], [10, 10], 'svhn.png')"
  },
  {
    "code": "def load_ptb_dataset(path='data'):\n    \"\"\"Load Penn TreeBank (PTB) dataset.\n\n    It is used in many LANGUAGE MODELING papers,\n    including \"Empirical Evaluation and Combination of Advanced Language\n    Modeling Techniques\", \"Recurrent Neural Network Regularization\".\n    It consists of 929k training words, 73k validation words, and 82k test\n    words. It has 10k words in its vocabulary.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/ptb/``.\n\n    Returns\n    --------\n    train_data, valid_data, test_data : list of int\n        The training, validating and testing data in integer format.\n    vocab_size : int\n        The vocabulary size.\n\n    Examples\n    --------\n    >>> train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n\n    References\n    ---------------\n    - ``tensorflow.models.rnn.ptb import reader``\n    - `Manual download <http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz>`__\n\n    Notes\n    ------\n    - If you want to get the raw data, see the source code.\n\n    \"\"\"\n    path = os.path.join(path, 'ptb')\n    logging.info(\"Load or Download Penn TreeBank (PTB) dataset > {}\".format(path))\n\n    # Maybe dowload and uncompress tar, or load exsisting files\n    filename = 'simple-examples.tgz'\n    url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/'\n    maybe_download_and_extract(filename, path, url, extract=True)\n\n    data_path = os.path.join(path, 'simple-examples', 'data')\n    train_path = os.path.join(data_path, \"ptb.train.txt\")\n    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n    test_path = os.path.join(data_path, \"ptb.test.txt\")\n\n    word_to_id = nlp.build_vocab(nlp.read_words(train_path))\n\n    train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)\n    valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)\n    test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)\n    vocab_size = len(word_to_id)\n\n    # logging.info(nlp.read_words(train_path)) # ... 'according', 'to', 'mr.', '<unk>', '<eos>']\n    # logging.info(train_data)                 # ...  214,         5,    23,    1,       2]\n    # logging.info(word_to_id)                 # ... 'beyond': 1295, 'anti-nuclear': 9599, 'trouble': 1520, '<eos>': 2 ... }\n    # logging.info(vocabulary)                 # 10000\n    # exit()\n    return train_data, valid_data, test_data, vocab_size",
    "doc": "Load Penn TreeBank (PTB) dataset.\n\n    It is used in many LANGUAGE MODELING papers,\n    including \"Empirical Evaluation and Combination of Advanced Language\n    Modeling Techniques\", \"Recurrent Neural Network Regularization\".\n    It consists of 929k training words, 73k validation words, and 82k test\n    words. It has 10k words in its vocabulary.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/ptb/``.\n\n    Returns\n    --------\n    train_data, valid_data, test_data : list of int\n        The training, validating and testing data in integer format.\n    vocab_size : int\n        The vocabulary size.\n\n    Examples\n    --------\n    >>> train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n\n    References\n    ---------------\n    - ``tensorflow.models.rnn.ptb import reader``\n    - `Manual download <http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz>`__\n\n    Notes\n    ------\n    - If you want to get the raw data, see the source code."
  },
  {
    "code": "def load_matt_mahoney_text8_dataset(path='data'):\n    \"\"\"Load Matt Mahoney's dataset.\n\n    Download a text file from Matt Mahoney's website\n    if not present, and make sure it's the right size.\n    Extract the first file enclosed in a zip file as a list of words.\n    This dataset can be used for Word Embedding.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/mm_test8/``.\n\n    Returns\n    --------\n    list of str\n        The raw text data e.g. [.... 'their', 'families', 'who', 'were', 'expelled', 'from', 'jerusalem', ...]\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> print('Data size', len(words))\n\n    \"\"\"\n    path = os.path.join(path, 'mm_test8')\n    logging.info(\"Load or Download matt_mahoney_text8 Dataset> {}\".format(path))\n\n    filename = 'text8.zip'\n    url = 'http://mattmahoney.net/dc/'\n    maybe_download_and_extract(filename, path, url, expected_bytes=31344016)\n\n    with zipfile.ZipFile(os.path.join(path, filename)) as f:\n        word_list = f.read(f.namelist()[0]).split()\n        for idx, _ in enumerate(word_list):\n            word_list[idx] = word_list[idx].decode()\n    return word_list",
    "doc": "Load Matt Mahoney's dataset.\n\n    Download a text file from Matt Mahoney's website\n    if not present, and make sure it's the right size.\n    Extract the first file enclosed in a zip file as a list of words.\n    This dataset can be used for Word Embedding.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/mm_test8/``.\n\n    Returns\n    --------\n    list of str\n        The raw text data e.g. [.... 'their', 'families', 'who', 'were', 'expelled', 'from', 'jerusalem', ...]\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> print('Data size', len(words))"
  },
  {
    "code": "def load_imdb_dataset(\n        path='data', nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2,\n        index_from=3\n):\n    \"\"\"Load IMDB dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\n    nb_words : int\n        Number of words to get.\n    skip_top : int\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n    maxlen : int\n        Maximum sequence length. Any longer sequence will be truncated.\n    seed : int\n        Seed for reproducible data shuffling.\n    start_char : int\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n    oov_char : int\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n    index_from : int\n        Index actual words with this index and higher.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\n    ...                                 nb_words=20000, test_split=0.2)\n    >>> print('X_train.shape', X_train.shape)\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\n    >>> print('y_train.shape', y_train.shape)\n    (20000,)  [1 0 0 ..., 1 0 1]\n\n    References\n    -----------\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__\n\n    \"\"\"\n    path = os.path.join(path, 'imdb')\n\n    filename = \"imdb.pkl\"\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    maybe_download_and_extract(filename, path, url)\n\n    if filename.endswith(\".gz\"):\n        f = gzip.open(os.path.join(path, filename), 'rb')\n    else:\n        f = open(os.path.join(path, filename), 'rb')\n\n    X, labels = cPickle.load(f)\n    f.close()\n\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for x, y in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n    if not X:\n        raise Exception(\n            'After filtering for sequences shorter than maxlen=' + str(maxlen) + ', no sequence was kept. '\n            'Increase maxlen.'\n        )\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if (w >= nb_words or w < skip_top):\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n\n    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n\n    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n\n    return X_train, y_train, X_test, y_test",
    "doc": "Load IMDB dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/imdb/``.\n    nb_words : int\n        Number of words to get.\n    skip_top : int\n        Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n    maxlen : int\n        Maximum sequence length. Any longer sequence will be truncated.\n    seed : int\n        Seed for reproducible data shuffling.\n    start_char : int\n        The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n    oov_char : int\n        Words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n    index_from : int\n        Index actual words with this index and higher.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(\n    ...                                 nb_words=20000, test_split=0.2)\n    >>> print('X_train.shape', X_train.shape)\n    (20000,)  [[1, 62, 74, ... 1033, 507, 27],[1, 60, 33, ... 13, 1053, 7]..]\n    >>> print('y_train.shape', y_train.shape)\n    (20000,)  [1 0 0 ..., 1 0 1]\n\n    References\n    -----------\n    - `Modified from keras. <https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py>`__"
  },
  {
    "code": "def load_nietzsche_dataset(path='data'):\n    \"\"\"Load Nietzsche dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/nietzsche/``.\n\n    Returns\n    --------\n    str\n        The content.\n\n    Examples\n    --------\n    >>> see tutorial_generate_text.py\n    >>> words = tl.files.load_nietzsche_dataset()\n    >>> words = basic_clean_str(words)\n    >>> words = words.split()\n\n    \"\"\"\n    logging.info(\"Load or Download nietzsche dataset > {}\".format(path))\n    path = os.path.join(path, 'nietzsche')\n\n    filename = \"nietzsche.txt\"\n    url = 'https://s3.amazonaws.com/text-datasets/'\n    filepath = maybe_download_and_extract(filename, path, url)\n\n    with open(filepath, \"r\") as f:\n        words = f.read()\n        return words",
    "doc": "Load Nietzsche dataset.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/nietzsche/``.\n\n    Returns\n    --------\n    str\n        The content.\n\n    Examples\n    --------\n    >>> see tutorial_generate_text.py\n    >>> words = tl.files.load_nietzsche_dataset()\n    >>> words = basic_clean_str(words)\n    >>> words = words.split()"
  },
  {
    "code": "def load_wmt_en_fr_dataset(path='data'):\n    \"\"\"Load WMT'15 English-to-French translation dataset.\n\n    It will download the data from the WMT'15 Website (10^9-French-English corpus), and the 2013 news test from the same site as development set.\n    Returns the directories of training data and test data.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/wmt_en_fr/``.\n\n    References\n    ----------\n    - Code modified from /tensorflow/models/rnn/translation/data_utils.py\n\n    Notes\n    -----\n    Usually, it will take a long time to download this dataset.\n\n    \"\"\"\n    path = os.path.join(path, 'wmt_en_fr')\n    # URLs for WMT data.\n    _WMT_ENFR_TRAIN_URL = \"http://www.statmt.org/wmt10/\"\n    _WMT_ENFR_DEV_URL = \"http://www.statmt.org/wmt15/\"\n\n    def gunzip_file(gz_path, new_path):\n        \"\"\"Unzips from gz_path into new_path.\"\"\"\n        logging.info(\"Unpacking %s to %s\" % (gz_path, new_path))\n        with gzip.open(gz_path, \"rb\") as gz_file:\n            with open(new_path, \"wb\") as new_file:\n                for line in gz_file:\n                    new_file.write(line)\n\n    def get_wmt_enfr_train_set(path):\n        \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n        filename = \"training-giga-fren.tar\"\n        maybe_download_and_extract(filename, path, _WMT_ENFR_TRAIN_URL, extract=True)\n        train_path = os.path.join(path, \"giga-fren.release2.fixed\")\n        gunzip_file(train_path + \".fr.gz\", train_path + \".fr\")\n        gunzip_file(train_path + \".en.gz\", train_path + \".en\")\n        return train_path\n\n    def get_wmt_enfr_dev_set(path):\n        \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n        filename = \"dev-v2.tgz\"\n        dev_file = maybe_download_and_extract(filename, path, _WMT_ENFR_DEV_URL, extract=False)\n        dev_name = \"newstest2013\"\n        dev_path = os.path.join(path, \"newstest2013\")\n        if not (gfile.Exists(dev_path + \".fr\") and gfile.Exists(dev_path + \".en\")):\n            logging.info(\"Extracting tgz file %s\" % dev_file)\n            with tarfile.open(dev_file, \"r:gz\") as dev_tar:\n                fr_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".fr\")\n                en_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".en\")\n                fr_dev_file.name = dev_name + \".fr\"  # Extract without \"dev/\" prefix.\n                en_dev_file.name = dev_name + \".en\"\n                dev_tar.extract(fr_dev_file, path)\n                dev_tar.extract(en_dev_file, path)\n        return dev_path\n\n    logging.info(\"Load or Download WMT English-to-French translation > {}\".format(path))\n\n    train_path = get_wmt_enfr_train_set(path)\n    dev_path = get_wmt_enfr_dev_set(path)\n\n    return train_path, dev_path",
    "doc": "Load WMT'15 English-to-French translation dataset.\n\n    It will download the data from the WMT'15 Website (10^9-French-English corpus), and the 2013 news test from the same site as development set.\n    Returns the directories of training data and test data.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/wmt_en_fr/``.\n\n    References\n    ----------\n    - Code modified from /tensorflow/models/rnn/translation/data_utils.py\n\n    Notes\n    -----\n    Usually, it will take a long time to download this dataset."
  },
  {
    "code": "def load_flickr25k_dataset(tag='sky', path=\"data\", n_threads=50, printable=False):\n    \"\"\"Load Flickr25K dataset.\n\n    Returns a list of images by a given tag from Flick25k dataset,\n    it will download Flickr25k from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like 'dog', 'red', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    -----------\n    Get images with tag of sky\n\n    >>> images = tl.files.load_flickr25k_dataset(tag='sky')\n\n    Get all images\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=None, n_threads=100, printable=True)\n\n    \"\"\"\n    path = os.path.join(path, 'flickr25k')\n\n    filename = 'mirflickr25k.zip'\n    url = 'http://press.liacs.nl/mirflickr/mirflickr25k/'\n\n    # download dataset\n    if folder_exists(os.path.join(path, \"mirflickr\")) is False:\n        logging.info(\"[*] Flickr25k is nonexistent in {}\".format(path))\n        maybe_download_and_extract(filename, path, url, extract=True)\n        del_file(os.path.join(path, filename))\n\n    # return images by the given tag.\n    # 1. image path list\n    folder_imgs = os.path.join(path, \"mirflickr\")\n    path_imgs = load_file_list(path=folder_imgs, regx='\\\\.jpg', printable=False)\n    path_imgs.sort(key=natural_keys)\n\n    # 2. tag path list\n    folder_tags = os.path.join(path, \"mirflickr\", \"meta\", \"tags\")\n    path_tags = load_file_list(path=folder_tags, regx='\\\\.txt', printable=False)\n    path_tags.sort(key=natural_keys)\n\n    # 3. select images\n    if tag is None:\n        logging.info(\"[Flickr25k] reading all images\")\n    else:\n        logging.info(\"[Flickr25k] reading images with tag: {}\".format(tag))\n    images_list = []\n    for idx, _v in enumerate(path_tags):\n        tags = read_file(os.path.join(folder_tags, path_tags[idx])).split('\\n')\n        # logging.info(idx+1, tags)\n        if tag is None or tag in tags:\n            images_list.append(path_imgs[idx])\n\n    images = visualize.read_images(images_list, folder_imgs, n_threads=n_threads, printable=printable)\n    return images",
    "doc": "Load Flickr25K dataset.\n\n    Returns a list of images by a given tag from Flick25k dataset,\n    it will download Flickr25k from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like 'dog', 'red', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    -----------\n    Get images with tag of sky\n\n    >>> images = tl.files.load_flickr25k_dataset(tag='sky')\n\n    Get all images\n\n    >>> images = tl.files.load_flickr25k_dataset(tag=None, n_threads=100, printable=True)"
  },
  {
    "code": "def load_flickr1M_dataset(tag='sky', size=10, path=\"data\", n_threads=50, printable=False):\n    \"\"\"Load Flick1M dataset.\n\n    Returns a list of images by a given tag from Flickr1M dataset,\n    it will download Flickr1M from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like 'dog', 'red', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    size : int\n        integer between 1 to 10. 1 means 100k images ... 5 means 500k images, 10 means all 1 million images. Default is 10.\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    ----------\n    Use 200k images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag='zebra', size=2)\n\n    Use 1 Million images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag='zebra')\n\n    \"\"\"\n    path = os.path.join(path, 'flickr1M')\n    logging.info(\"[Flickr1M] using {}% of images = {}\".format(size * 10, size * 100000))\n    images_zip = [\n        'images0.zip', 'images1.zip', 'images2.zip', 'images3.zip', 'images4.zip', 'images5.zip', 'images6.zip',\n        'images7.zip', 'images8.zip', 'images9.zip'\n    ]\n    tag_zip = 'tags.zip'\n    url = 'http://press.liacs.nl/mirflickr/mirflickr1m/'\n\n    # download dataset\n    for image_zip in images_zip[0:size]:\n        image_folder = image_zip.split(\".\")[0]\n        # logging.info(path+\"/\"+image_folder)\n        if folder_exists(os.path.join(path, image_folder)) is False:\n            # logging.info(image_zip)\n            logging.info(\"[Flickr1M] {} is missing in {}\".format(image_folder, path))\n            maybe_download_and_extract(image_zip, path, url, extract=True)\n            del_file(os.path.join(path, image_zip))\n            # os.system(\"mv {} {}\".format(os.path.join(path, 'images'), os.path.join(path, image_folder)))\n            shutil.move(os.path.join(path, 'images'), os.path.join(path, image_folder))\n        else:\n            logging.info(\"[Flickr1M] {} exists in {}\".format(image_folder, path))\n\n    # download tag\n    if folder_exists(os.path.join(path, \"tags\")) is False:\n        logging.info(\"[Flickr1M] tag files is nonexistent in {}\".format(path))\n        maybe_download_and_extract(tag_zip, path, url, extract=True)\n        del_file(os.path.join(path, tag_zip))\n    else:\n        logging.info(\"[Flickr1M] tags exists in {}\".format(path))\n\n    # 1. image path list\n    images_list = []\n    images_folder_list = []\n    for i in range(0, size):\n        images_folder_list += load_folder_list(path=os.path.join(path, 'images%d' % i))\n    images_folder_list.sort(key=lambda s: int(s.split('/')[-1]))  # folder/images/ddd\n\n    for folder in images_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx='\\\\.jpg', printable=False)\n        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.jpg\n        images_list.extend([os.path.join(folder, x) for x in tmp])\n\n    # 2. tag path list\n    tag_list = []\n    tag_folder_list = load_folder_list(os.path.join(path, \"tags\"))\n\n    # tag_folder_list.sort(key=lambda s: int(s.split(\"/\")[-1]))  # folder/images/ddd\n    tag_folder_list.sort(key=lambda s: int(os.path.basename(s)))\n\n    for folder in tag_folder_list[0:size * 10]:\n        tmp = load_file_list(path=folder, regx='\\\\.txt', printable=False)\n        tmp.sort(key=lambda s: int(s.split('.')[-2]))  # ddd.txt\n        tmp = [os.path.join(folder, s) for s in tmp]\n        tag_list += tmp\n\n    # 3. select images\n    logging.info(\"[Flickr1M] searching tag: {}\".format(tag))\n    select_images_list = []\n    for idx, _val in enumerate(tag_list):\n        tags = read_file(tag_list[idx]).split('\\n')\n        if tag in tags:\n            select_images_list.append(images_list[idx])\n\n    logging.info(\"[Flickr1M] reading images with tag: {}\".format(tag))\n    images = visualize.read_images(select_images_list, '', n_threads=n_threads, printable=printable)\n    return images",
    "doc": "Load Flick1M dataset.\n\n    Returns a list of images by a given tag from Flickr1M dataset,\n    it will download Flickr1M from `the official website <http://press.liacs.nl/mirflickr/mirdownload.html>`__\n    at the first time you use it.\n\n    Parameters\n    ------------\n    tag : str or None\n        What images to return.\n            - If you want to get images with tag, use string like 'dog', 'red', see `Flickr Search <https://www.flickr.com/search/>`__.\n            - If you want to get all images, set to ``None``.\n\n    size : int\n        integer between 1 to 10. 1 means 100k images ... 5 means 500k images, 10 means all 1 million images. Default is 10.\n    path : str\n        The path that the data is downloaded to, defaults is ``data/flickr25k/``.\n    n_threads : int\n        The number of thread to read image.\n    printable : boolean\n        Whether to print infomation when reading images, default is ``False``.\n\n    Examples\n    ----------\n    Use 200k images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag='zebra', size=2)\n\n    Use 1 Million images\n\n    >>> images = tl.files.load_flickr1M_dataset(tag='zebra')"
  },
  {
    "code": "def load_cyclegan_dataset(filename='summer2winter_yosemite', path='data'):\n    \"\"\"Load images from CycleGAN's database, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n\n    Parameters\n    ------------\n    filename : str\n        The dataset you want, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n    path : str\n        The path that the data is downloaded to, defaults is `data/cyclegan`\n\n    Examples\n    ---------\n    >>> im_train_A, im_train_B, im_test_A, im_test_B = load_cyclegan_dataset(filename='summer2winter_yosemite')\n\n    \"\"\"\n    path = os.path.join(path, 'cyclegan')\n    url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/'\n\n    if folder_exists(os.path.join(path, filename)) is False:\n        logging.info(\"[*] {} is nonexistent in {}\".format(filename, path))\n        maybe_download_and_extract(filename + '.zip', path, url, extract=True)\n        del_file(os.path.join(path, filename + '.zip'))\n\n    def load_image_from_folder(path):\n        path_imgs = load_file_list(path=path, regx='\\\\.jpg', printable=False)\n        return visualize.read_images(path_imgs, path=path, n_threads=10, printable=False)\n\n    im_train_A = load_image_from_folder(os.path.join(path, filename, \"trainA\"))\n    im_train_B = load_image_from_folder(os.path.join(path, filename, \"trainB\"))\n    im_test_A = load_image_from_folder(os.path.join(path, filename, \"testA\"))\n    im_test_B = load_image_from_folder(os.path.join(path, filename, \"testB\"))\n\n    def if_2d_to_3d(images):  # [h, w] --> [h, w, 3]\n        for i, _v in enumerate(images):\n            if len(images[i].shape) == 2:\n                images[i] = images[i][:, :, np.newaxis]\n                images[i] = np.tile(images[i], (1, 1, 3))\n        return images\n\n    im_train_A = if_2d_to_3d(im_train_A)\n    im_train_B = if_2d_to_3d(im_train_B)\n    im_test_A = if_2d_to_3d(im_test_A)\n    im_test_B = if_2d_to_3d(im_test_B)\n\n    return im_train_A, im_train_B, im_test_A, im_test_B",
    "doc": "Load images from CycleGAN's database, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n\n    Parameters\n    ------------\n    filename : str\n        The dataset you want, see `this link <https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/>`__.\n    path : str\n        The path that the data is downloaded to, defaults is `data/cyclegan`\n\n    Examples\n    ---------\n    >>> im_train_A, im_train_B, im_test_A, im_test_B = load_cyclegan_dataset(filename='summer2winter_yosemite')"
  },
  {
    "code": "def download_file_from_google_drive(ID, destination):\n    \"\"\"Download file from Google Drive.\n\n    See ``tl.files.load_celebA_dataset`` for example.\n\n    Parameters\n    --------------\n    ID : str\n        The driver ID.\n    destination : str\n        The destination for save file.\n\n    \"\"\"\n\n    def save_response_content(response, destination, chunk_size=32 * 1024):\n        total_size = int(response.headers.get('content-length', 0))\n        with open(destination, \"wb\") as f:\n            for chunk in tqdm(response.iter_content(chunk_size), total=total_size, unit='B', unit_scale=True,\n                              desc=destination):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n\n    def get_confirm_token(response):\n        for key, value in response.cookies.items():\n            if key.startswith('download_warning'):\n                return value\n        return None\n\n    URL = \"https://docs.google.com/uc?export=download\"\n    session = requests.Session()\n\n    response = session.get(URL, params={'id': ID}, stream=True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = {'id': ID, 'confirm': token}\n        response = session.get(URL, params=params, stream=True)\n    save_response_content(response, destination)",
    "doc": "Download file from Google Drive.\n\n    See ``tl.files.load_celebA_dataset`` for example.\n\n    Parameters\n    --------------\n    ID : str\n        The driver ID.\n    destination : str\n        The destination for save file."
  },
  {
    "code": "def load_celebA_dataset(path='data'):\n    \"\"\"Load CelebA dataset\n\n    Return a list of image path.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/celebA/``.\n\n    \"\"\"\n    data_dir = 'celebA'\n    filename, drive_id = \"img_align_celeba.zip\", \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n    save_path = os.path.join(path, filename)\n    image_path = os.path.join(path, data_dir)\n    if os.path.exists(image_path):\n        logging.info('[*] {} already exists'.format(save_path))\n    else:\n        exists_or_mkdir(path)\n        download_file_from_google_drive(drive_id, save_path)\n        zip_dir = ''\n        with zipfile.ZipFile(save_path) as zf:\n            zip_dir = zf.namelist()[0]\n            zf.extractall(path)\n        os.remove(save_path)\n        os.rename(os.path.join(path, zip_dir), image_path)\n\n    data_files = load_file_list(path=image_path, regx='\\\\.jpg', printable=False)\n    for i, _v in enumerate(data_files):\n        data_files[i] = os.path.join(image_path, data_files[i])\n    return data_files",
    "doc": "Load CelebA dataset\n\n    Return a list of image path.\n\n    Parameters\n    -----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/celebA/``."
  },
  {
    "code": "def save_npz(save_list=None, name='model.npz', sess=None):\n    \"\"\"Input parameters and the file name, save parameters into .npz file. Use tl.utils.load_npz() to restore.\n\n    Parameters\n    ----------\n    save_list : list of tensor\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n    sess : None or Session\n        Session may be required in some case.\n\n    Examples\n    --------\n    Save model to npz\n\n    >>> tl.files.save_npz(network.all_params, name='model.npz', sess=sess)\n\n    Load model from npz (Method 1)\n\n    >>> load_params = tl.files.load_npz(name='model.npz')\n    >>> tl.files.assign_params(sess, load_params, network)\n\n    Load model from npz (Method 2)\n\n    >>> tl.files.load_and_assign_npz(sess=sess, name='model.npz', network=network)\n\n    Notes\n    -----\n    If you got session issues, you can change the value.eval() to value.eval(session=sess)\n\n    References\n    ----------\n    `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__\n\n    \"\"\"\n    logging.info(\"[*] Saving TL params into %s\" % name)\n    if save_list is None:\n        save_list = []\n\n    save_list_var = []\n    if sess:\n        save_list_var = sess.run(save_list)\n    else:\n        try:\n            save_list_var.extend([v.eval() for v in save_list])\n        except Exception:\n            logging.info(\n                \" Fail to save model, Hint: pass the session into this function, tl.files.save_npz(network.all_params, name='model.npz', sess=sess)\"\n            )\n    np.savez(name, params=save_list_var)\n    save_list_var = None\n    del save_list_var\n    logging.info(\"[*] Saved\")",
    "doc": "Input parameters and the file name, save parameters into .npz file. Use tl.utils.load_npz() to restore.\n\n    Parameters\n    ----------\n    save_list : list of tensor\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n    sess : None or Session\n        Session may be required in some case.\n\n    Examples\n    --------\n    Save model to npz\n\n    >>> tl.files.save_npz(network.all_params, name='model.npz', sess=sess)\n\n    Load model from npz (Method 1)\n\n    >>> load_params = tl.files.load_npz(name='model.npz')\n    >>> tl.files.assign_params(sess, load_params, network)\n\n    Load model from npz (Method 2)\n\n    >>> tl.files.load_and_assign_npz(sess=sess, name='model.npz', network=network)\n\n    Notes\n    -----\n    If you got session issues, you can change the value.eval() to value.eval(session=sess)\n\n    References\n    ----------\n    `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__"
  },
  {
    "code": "def load_npz(path='', name='model.npz'):\n    \"\"\"Load the parameters of a Model saved by tl.files.save_npz().\n\n    Parameters\n    ----------\n    path : str\n        Folder path to `.npz` file.\n    name : str\n        The name of the `.npz` file.\n\n    Returns\n    --------\n    list of array\n        A list of parameters in order.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__\n\n    \"\"\"\n    d = np.load(os.path.join(path, name))\n    return d['params']",
    "doc": "Load the parameters of a Model saved by tl.files.save_npz().\n\n    Parameters\n    ----------\n    path : str\n        Folder path to `.npz` file.\n    name : str\n        The name of the `.npz` file.\n\n    Returns\n    --------\n    list of array\n        A list of parameters in order.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__"
  },
  {
    "code": "def assign_params(sess, params, network):\n    \"\"\"Assign the given parameters to the TensorLayer network.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    params : list of array\n        A list of parameters (array) in order.\n    network : :class:`Layer`\n        The network to be assigned.\n\n    Returns\n    --------\n    list of operations\n        A list of tf ops in order that assign params. Support sess.run(ops) manually.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Assign value to a TensorFlow variable <http://stackoverflow.com/questions/34220532/how-to-assign-value-to-a-tensorflow-variable>`__\n\n    \"\"\"\n    ops = []\n    for idx, param in enumerate(params):\n        ops.append(network.all_params[idx].assign(param))\n    if sess is not None:\n        sess.run(ops)\n    return ops",
    "doc": "Assign the given parameters to the TensorLayer network.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    params : list of array\n        A list of parameters (array) in order.\n    network : :class:`Layer`\n        The network to be assigned.\n\n    Returns\n    --------\n    list of operations\n        A list of tf ops in order that assign params. Support sess.run(ops) manually.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Assign value to a TensorFlow variable <http://stackoverflow.com/questions/34220532/how-to-assign-value-to-a-tensorflow-variable>`__"
  },
  {
    "code": "def load_and_assign_npz(sess=None, name=None, network=None):\n    \"\"\"Load model from npz and assign to a network.\n\n    Parameters\n    -------------\n    sess : Session\n        TensorFlow Session.\n    name : str\n        The name of the `.npz` file.\n    network : :class:`Layer`\n        The network to be assigned.\n\n    Returns\n    --------\n    False or network\n        Returns False, if the model is not exist.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    \"\"\"\n    if network is None:\n        raise ValueError(\"network is None.\")\n    if sess is None:\n        raise ValueError(\"session is None.\")\n    if not os.path.exists(name):\n        logging.error(\"file {} doesn't exist.\".format(name))\n        return False\n    else:\n        params = load_npz(name=name)\n        assign_params(sess, params, network)\n        logging.info(\"[*] Load {} SUCCESS!\".format(name))\n        return network",
    "doc": "Load model from npz and assign to a network.\n\n    Parameters\n    -------------\n    sess : Session\n        TensorFlow Session.\n    name : str\n        The name of the `.npz` file.\n    network : :class:`Layer`\n        The network to be assigned.\n\n    Returns\n    --------\n    False or network\n        Returns False, if the model is not exist.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``"
  },
  {
    "code": "def save_npz_dict(save_list=None, name='model.npz', sess=None):\n    \"\"\"Input parameters and the file name, save parameters as a dictionary into .npz file.\n\n    Use ``tl.files.load_and_assign_npz_dict()`` to restore.\n\n    Parameters\n    ----------\n    save_list : list of parameters\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n    sess : Session\n        TensorFlow Session.\n\n    \"\"\"\n    if sess is None:\n        raise ValueError(\"session is None.\")\n    if save_list is None:\n        save_list = []\n\n    save_list_names = [tensor.name for tensor in save_list]\n    save_list_var = sess.run(save_list)\n    save_var_dict = {save_list_names[idx]: val for idx, val in enumerate(save_list_var)}\n    np.savez(name, **save_var_dict)\n    save_list_var = None\n    save_var_dict = None\n    del save_list_var\n    del save_var_dict\n    logging.info(\"[*] Model saved in npz_dict %s\" % name)",
    "doc": "Input parameters and the file name, save parameters as a dictionary into .npz file.\n\n    Use ``tl.files.load_and_assign_npz_dict()`` to restore.\n\n    Parameters\n    ----------\n    save_list : list of parameters\n        A list of parameters (tensor) to be saved.\n    name : str\n        The name of the `.npz` file.\n    sess : Session\n        TensorFlow Session."
  },
  {
    "code": "def load_and_assign_npz_dict(name='model.npz', sess=None):\n    \"\"\"Restore the parameters saved by ``tl.files.save_npz_dict()``.\n\n    Parameters\n    ----------\n    name : str\n        The name of the `.npz` file.\n    sess : Session\n        TensorFlow Session.\n\n    \"\"\"\n    if sess is None:\n        raise ValueError(\"session is None.\")\n\n    if not os.path.exists(name):\n        logging.error(\"file {} doesn't exist.\".format(name))\n        return False\n\n    params = np.load(name)\n    if len(params.keys()) != len(set(params.keys())):\n        raise Exception(\"Duplication in model npz_dict %s\" % name)\n    ops = list()\n    for key in params.keys():\n        try:\n            # tensor = tf.get_default_graph().get_tensor_by_name(key)\n            # varlist = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=key)\n            varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=key)\n            if len(varlist) > 1:\n                raise Exception(\"[!] Multiple candidate variables to be assigned for name %s\" % key)\n            elif len(varlist) == 0:\n                raise KeyError\n            else:\n                ops.append(varlist[0].assign(params[key]))\n                logging.info(\"[*] params restored: %s\" % key)\n        except KeyError:\n            logging.info(\"[!] Warning: Tensor named %s not found in network.\" % key)\n\n    sess.run(ops)\n    logging.info(\"[*] Model restored from npz_dict %s\" % name)",
    "doc": "Restore the parameters saved by ``tl.files.save_npz_dict()``.\n\n    Parameters\n    ----------\n    name : str\n        The name of the `.npz` file.\n    sess : Session\n        TensorFlow Session."
  },
  {
    "code": "def save_ckpt(\n        sess=None, mode_name='model.ckpt', save_dir='checkpoint', var_list=None, global_step=None, printable=False\n):\n    \"\"\"Save parameters into `ckpt` file.\n\n    Parameters\n    ------------\n    sess : Session\n        TensorFlow Session.\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    global_step : int or None\n        Step number.\n    printable : boolean\n        Whether to print all parameters information.\n\n    See Also\n    --------\n    load_ckpt\n\n    \"\"\"\n    if sess is None:\n        raise ValueError(\"session is None.\")\n    if var_list is None:\n        var_list = []\n\n    ckpt_file = os.path.join(save_dir, mode_name)\n    if var_list == []:\n        var_list = tf.global_variables()\n\n    logging.info(\"[*] save %s n_params: %d\" % (ckpt_file, len(var_list)))\n\n    if printable:\n        for idx, v in enumerate(var_list):\n            logging.info(\"  param {:3}: {:15}   {}\".format(idx, v.name, str(v.get_shape())))\n\n    saver = tf.train.Saver(var_list)\n    saver.save(sess, ckpt_file, global_step=global_step)",
    "doc": "Save parameters into `ckpt` file.\n\n    Parameters\n    ------------\n    sess : Session\n        TensorFlow Session.\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    global_step : int or None\n        Step number.\n    printable : boolean\n        Whether to print all parameters information.\n\n    See Also\n    --------\n    load_ckpt"
  },
  {
    "code": "def load_ckpt(sess=None, mode_name='model.ckpt', save_dir='checkpoint', var_list=None, is_latest=True, printable=False):\n    \"\"\"Load parameters from `ckpt` file.\n\n    Parameters\n    ------------\n    sess : Session\n        TensorFlow Session.\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    is_latest : boolean\n        Whether to load the latest `ckpt`, if False, load the `ckpt` with the name of ```mode_name``.\n    printable : boolean\n        Whether to print all parameters information.\n\n    Examples\n    ----------\n    - Save all global parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name='model.ckpt', save_dir='model', printable=True)\n\n    - Save specific parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name='model.ckpt', var_list=net.all_params, save_dir='model', printable=True)\n\n    - Load latest ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, var_list=net.all_params, save_dir='model', printable=True)\n\n    - Load specific ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, mode_name='model.ckpt', var_list=net.all_params, save_dir='model', is_latest=False, printable=True)\n\n    \"\"\"\n    if sess is None:\n        raise ValueError(\"session is None.\")\n    if var_list is None:\n        var_list = []\n\n    if is_latest:\n        ckpt_file = tf.train.latest_checkpoint(save_dir)\n    else:\n        ckpt_file = os.path.join(save_dir, mode_name)\n\n    if not var_list:\n        var_list = tf.global_variables()\n\n    logging.info(\"[*] load %s n_params: %d\" % (ckpt_file, len(var_list)))\n\n    if printable:\n        for idx, v in enumerate(var_list):\n            logging.info(\"  param {:3}: {:15}   {}\".format(idx, v.name, str(v.get_shape())))\n\n    try:\n        saver = tf.train.Saver(var_list)\n        saver.restore(sess, ckpt_file)\n    except Exception as e:\n        logging.info(e)\n        logging.info(\"[*] load ckpt fail ...\")",
    "doc": "Load parameters from `ckpt` file.\n\n    Parameters\n    ------------\n    sess : Session\n        TensorFlow Session.\n    mode_name : str\n        The name of the model, default is ``model.ckpt``.\n    save_dir : str\n        The path / file directory to the `ckpt`, default is ``checkpoint``.\n    var_list : list of tensor\n        The parameters / variables (tensor) to be saved. If empty, save all global variables (default).\n    is_latest : boolean\n        Whether to load the latest `ckpt`, if False, load the `ckpt` with the name of ```mode_name``.\n    printable : boolean\n        Whether to print all parameters information.\n\n    Examples\n    ----------\n    - Save all global parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name='model.ckpt', save_dir='model', printable=True)\n\n    - Save specific parameters.\n\n    >>> tl.files.save_ckpt(sess=sess, mode_name='model.ckpt', var_list=net.all_params, save_dir='model', printable=True)\n\n    - Load latest ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, var_list=net.all_params, save_dir='model', printable=True)\n\n    - Load specific ckpt.\n\n    >>> tl.files.load_ckpt(sess=sess, mode_name='model.ckpt', var_list=net.all_params, save_dir='model', is_latest=False, printable=True)"
  },
  {
    "code": "def load_npy_to_any(path='', name='file.npy'):\n    \"\"\"Load `.npy` file.\n\n    Parameters\n    ------------\n    path : str\n        Path to the file (optional).\n    name : str\n        File name.\n\n    Examples\n    ---------\n    - see tl.files.save_any_to_npy()\n\n    \"\"\"\n    file_path = os.path.join(path, name)\n    try:\n        return np.load(file_path).item()\n    except Exception:\n        return np.load(file_path)\n    raise Exception(\"[!] Fail to load %s\" % file_path)",
    "doc": "Load `.npy` file.\n\n    Parameters\n    ------------\n    path : str\n        Path to the file (optional).\n    name : str\n        File name.\n\n    Examples\n    ---------\n    - see tl.files.save_any_to_npy()"
  },
  {
    "code": "def load_file_list(path=None, regx='\\.jpg', printable=True, keep_prefix=False):\n    r\"\"\"Return a file list in a folder by given a path and regular expression.\n\n    Parameters\n    ----------\n    path : str or None\n        A folder path, if `None`, use the current directory.\n    regx : str\n        The regx of file name.\n    printable : boolean\n        Whether to print the files infomation.\n    keep_prefix : boolean\n        Whether to keep path in the file name.\n\n    Examples\n    ----------\n    >>> file_list = tl.files.load_file_list(path=None, regx='w1pre_[0-9]+\\.(npz)')\n\n    \"\"\"\n    if path is None:\n        path = os.getcwd()\n    file_list = os.listdir(path)\n    return_list = []\n    for _, f in enumerate(file_list):\n        if re.search(regx, f):\n            return_list.append(f)\n    # return_list.sort()\n    if keep_prefix:\n        for i, f in enumerate(return_list):\n            return_list[i] = os.path.join(path, f)\n\n    if printable:\n        logging.info('Match file list = %s' % return_list)\n        logging.info('Number of files = %d' % len(return_list))\n    return return_list",
    "doc": "r\"\"\"Return a file list in a folder by given a path and regular expression.\n\n    Parameters\n    ----------\n    path : str or None\n        A folder path, if `None`, use the current directory.\n    regx : str\n        The regx of file name.\n    printable : boolean\n        Whether to print the files infomation.\n    keep_prefix : boolean\n        Whether to keep path in the file name.\n\n    Examples\n    ----------\n    >>> file_list = tl.files.load_file_list(path=None, regx='w1pre_[0-9]+\\.(npz)')"
  },
  {
    "code": "def load_folder_list(path=\"\"):\n    \"\"\"Return a folder list in a folder by given a folder path.\n\n    Parameters\n    ----------\n    path : str\n        A folder path.\n\n    \"\"\"\n    return [os.path.join(path, o) for o in os.listdir(path) if os.path.isdir(os.path.join(path, o))]",
    "doc": "Return a folder list in a folder by given a folder path.\n\n    Parameters\n    ----------\n    path : str\n        A folder path."
  },
  {
    "code": "def exists_or_mkdir(path, verbose=True):\n    \"\"\"Check a folder by given name, if not exist, create the folder and return False,\n    if directory exists, return True.\n\n    Parameters\n    ----------\n    path : str\n        A folder path.\n    verbose : boolean\n        If True (default), prints results.\n\n    Returns\n    --------\n    boolean\n        True if folder already exist, otherwise, returns False and create the folder.\n\n    Examples\n    --------\n    >>> tl.files.exists_or_mkdir(\"checkpoints/train\")\n\n    \"\"\"\n    if not os.path.exists(path):\n        if verbose:\n            logging.info(\"[*] creates %s ...\" % path)\n        os.makedirs(path)\n        return False\n    else:\n        if verbose:\n            logging.info(\"[!] %s exists ...\" % path)\n        return True",
    "doc": "Check a folder by given name, if not exist, create the folder and return False,\n    if directory exists, return True.\n\n    Parameters\n    ----------\n    path : str\n        A folder path.\n    verbose : boolean\n        If True (default), prints results.\n\n    Returns\n    --------\n    boolean\n        True if folder already exist, otherwise, returns False and create the folder.\n\n    Examples\n    --------\n    >>> tl.files.exists_or_mkdir(\"checkpoints/train\")"
  },
  {
    "code": "def maybe_download_and_extract(filename, working_directory, url_source, extract=False, expected_bytes=None):\n    \"\"\"Checks if file exists in working_directory otherwise tries to dowload the file,\n    and optionally also tries to extract the file if format is \".zip\" or \".tar\"\n\n    Parameters\n    -----------\n    filename : str\n        The name of the (to be) dowloaded file.\n    working_directory : str\n        A folder path to search for the file in and dowload the file to\n    url : str\n        The URL to download the file from\n    extract : boolean\n        If True, tries to uncompress the dowloaded file is \".tar.gz/.tar.bz2\" or \".zip\" file, default is False.\n    expected_bytes : int or None\n        If set tries to verify that the downloaded file is of the specified size, otherwise raises an Exception, defaults is None which corresponds to no check being performed.\n\n    Returns\n    ----------\n    str\n        File path of the dowloaded (uncompressed) file.\n\n    Examples\n    --------\n    >>> down_file = tl.files.maybe_download_and_extract(filename='train-images-idx3-ubyte.gz',\n    ...                                            working_directory='data/',\n    ...                                            url_source='http://yann.lecun.com/exdb/mnist/')\n    >>> tl.files.maybe_download_and_extract(filename='ADEChallengeData2016.zip',\n    ...                                             working_directory='data/',\n    ...                                             url_source='http://sceneparsing.csail.mit.edu/data/',\n    ...                                             extract=True)\n\n    \"\"\"\n\n    # We first define a download function, supporting both Python 2 and 3.\n    def _download(filename, working_directory, url_source):\n\n        progress_bar = progressbar.ProgressBar()\n\n        def _dlProgress(count, blockSize, totalSize, pbar=progress_bar):\n            if (totalSize != 0):\n\n                if not pbar.max_value:\n                    totalBlocks = math.ceil(float(totalSize) / float(blockSize))\n                    pbar.max_value = int(totalBlocks)\n\n                pbar.update(count, force=True)\n\n        filepath = os.path.join(working_directory, filename)\n\n        logging.info('Downloading %s...\\n' % filename)\n\n        urlretrieve(url_source + filename, filepath, reporthook=_dlProgress)\n\n    exists_or_mkdir(working_directory, verbose=False)\n    filepath = os.path.join(working_directory, filename)\n\n    if not os.path.exists(filepath):\n\n        _download(filename, working_directory, url_source)\n        statinfo = os.stat(filepath)\n        logging.info('Succesfully downloaded %s %s bytes.' % (filename, statinfo.st_size))  # , 'bytes.')\n        if (not (expected_bytes is None) and (expected_bytes != statinfo.st_size)):\n            raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n        if (extract):\n            if tarfile.is_tarfile(filepath):\n                logging.info('Trying to extract tar file')\n                tarfile.open(filepath, 'r').extractall(working_directory)\n                logging.info('... Success!')\n            elif zipfile.is_zipfile(filepath):\n                logging.info('Trying to extract zip file')\n                with zipfile.ZipFile(filepath) as zf:\n                    zf.extractall(working_directory)\n                logging.info('... Success!')\n            else:\n                logging.info(\"Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported\")\n    return filepath",
    "doc": "Checks if file exists in working_directory otherwise tries to dowload the file,\n    and optionally also tries to extract the file if format is \".zip\" or \".tar\"\n\n    Parameters\n    -----------\n    filename : str\n        The name of the (to be) dowloaded file.\n    working_directory : str\n        A folder path to search for the file in and dowload the file to\n    url : str\n        The URL to download the file from\n    extract : boolean\n        If True, tries to uncompress the dowloaded file is \".tar.gz/.tar.bz2\" or \".zip\" file, default is False.\n    expected_bytes : int or None\n        If set tries to verify that the downloaded file is of the specified size, otherwise raises an Exception, defaults is None which corresponds to no check being performed.\n\n    Returns\n    ----------\n    str\n        File path of the dowloaded (uncompressed) file.\n\n    Examples\n    --------\n    >>> down_file = tl.files.maybe_download_and_extract(filename='train-images-idx3-ubyte.gz',\n    ...                                            working_directory='data/',\n    ...                                            url_source='http://yann.lecun.com/exdb/mnist/')\n    >>> tl.files.maybe_download_and_extract(filename='ADEChallengeData2016.zip',\n    ...                                             working_directory='data/',\n    ...                                             url_source='http://sceneparsing.csail.mit.edu/data/',\n    ...                                             extract=True)"
  },
  {
    "code": "def natural_keys(text):\n    \"\"\"Sort list of string with number in human order.\n\n    Examples\n    ----------\n    >>> l = ['im1.jpg', 'im31.jpg', 'im11.jpg', 'im21.jpg', 'im03.jpg', 'im05.jpg']\n    >>> l.sort(key=tl.files.natural_keys)\n    ['im1.jpg', 'im03.jpg', 'im05', 'im11.jpg', 'im21.jpg', 'im31.jpg']\n    >>> l.sort() # that is what we dont want\n    ['im03.jpg', 'im05', 'im1.jpg', 'im11.jpg', 'im21.jpg', 'im31.jpg']\n\n    References\n    ----------\n    - `link <http://nedbatchelder.com/blog/200712/human_sorting.html>`__\n\n    \"\"\"\n\n    # - alist.sort(key=natural_keys) sorts in human order\n    # http://nedbatchelder.com/blog/200712/human_sorting.html\n    # (See Toothy's implementation in the comments)\n    def atoi(text):\n        return int(text) if text.isdigit() else text\n\n    return [atoi(c) for c in re.split('(\\d+)', text)]",
    "doc": "Sort list of string with number in human order.\n\n    Examples\n    ----------\n    >>> l = ['im1.jpg', 'im31.jpg', 'im11.jpg', 'im21.jpg', 'im03.jpg', 'im05.jpg']\n    >>> l.sort(key=tl.files.natural_keys)\n    ['im1.jpg', 'im03.jpg', 'im05', 'im11.jpg', 'im21.jpg', 'im31.jpg']\n    >>> l.sort() # that is what we dont want\n    ['im03.jpg', 'im05', 'im1.jpg', 'im11.jpg', 'im21.jpg', 'im31.jpg']\n\n    References\n    ----------\n    - `link <http://nedbatchelder.com/blog/200712/human_sorting.html>`__"
  },
  {
    "code": "def npz_to_W_pdf(path=None, regx='w1pre_[0-9]+\\.(npz)'):\n    r\"\"\"Convert the first weight matrix of `.npz` file to `.pdf` by using `tl.visualize.W()`.\n\n    Parameters\n    ----------\n    path : str\n        A folder path to `npz` files.\n    regx : str\n        Regx for the file name.\n\n    Examples\n    ---------\n    Convert the first weight matrix of w1_pre...npz file to w1_pre...pdf.\n\n    >>> tl.files.npz_to_W_pdf(path='/Users/.../npz_file/', regx='w1pre_[0-9]+\\.(npz)')\n\n    \"\"\"\n    file_list = load_file_list(path=path, regx=regx)\n    for f in file_list:\n        W = load_npz(path, f)[0]\n        logging.info(\"%s --> %s\" % (f, f.split('.')[0] + '.pdf'))\n        visualize.draw_weights(W, second=10, saveable=True, name=f.split('.')[0], fig_idx=2012)",
    "doc": "r\"\"\"Convert the first weight matrix of `.npz` file to `.pdf` by using `tl.visualize.W()`.\n\n    Parameters\n    ----------\n    path : str\n        A folder path to `npz` files.\n    regx : str\n        Regx for the file name.\n\n    Examples\n    ---------\n    Convert the first weight matrix of w1_pre...npz file to w1_pre...pdf.\n\n    >>> tl.files.npz_to_W_pdf(path='/Users/.../npz_file/', regx='w1pre_[0-9]+\\.(npz)')"
  },
  {
    "code": "def threading_data(data=None, fn=None, thread_count=None, **kwargs):\n    \"\"\"Process a batch of data by given function by threading.\n\n    Usually be used for data augmentation.\n\n    Parameters\n    -----------\n    data : numpy.array or others\n        The data to be processed.\n    thread_count : int\n        The number of threads to use.\n    fn : function\n        The function for data processing.\n    more args : the args for `fn`\n        Ssee Examples below.\n\n    Examples\n    --------\n    Process images.\n\n    >>> images, _, _, _ = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n    >>> images = tl.prepro.threading_data(images[0:32], tl.prepro.zoom, zoom_range=[0.5, 1])\n\n    Customized image preprocessing function.\n\n    >>> def distort_img(x):\n    >>>     x = tl.prepro.flip_axis(x, axis=0, is_random=True)\n    >>>     x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n    >>>     x = tl.prepro.crop(x, 100, 100, is_random=True)\n    >>>     return x\n    >>> images = tl.prepro.threading_data(images, distort_img)\n\n    Process images and masks together (Usually be used for image segmentation).\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], tl.prepro.zoom_multi, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, 'images.png')\n    >>> tl.vis.save_image(Y_, 'masks.png')\n\n    Process images and masks together by using ``thread_count``.\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data(X, tl.prepro.zoom_multi, 8, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, 'after.png')\n    >>> tl.vis.save_image(Y_, 'before.png')\n\n    Customized function for processing images and masks together.\n\n    >>> def distort_img(data):\n    >>>    x, y = data\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=0, is_random=True)\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=1, is_random=True)\n    >>>    x, y = tl.prepro.crop_multi([x, y], 100, 100, is_random=True)\n    >>>    return x, y\n\n    >>> X, Y --> [batch_size, row, col, channel]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], distort_img)\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n\n    Returns\n    -------\n    list or numpyarray\n        The processed results.\n\n    References\n    ----------\n    - `python queue <https://pymotw.com/2/Queue/index.html#module-Queue>`__\n    - `run with limited queue <http://effbot.org/librarybook/queue.htm>`__\n\n    \"\"\"\n\n    def apply_fn(results, i, data, kwargs):\n        results[i] = fn(data, **kwargs)\n\n    if thread_count is None:\n        results = [None] * len(data)\n        threads = []\n        # for i in range(len(data)):\n        #     t = threading.Thread(name='threading_and_return', target=apply_fn, args=(results, i, data[i], kwargs))\n        for i, d in enumerate(data):\n            t = threading.Thread(name='threading_and_return', target=apply_fn, args=(results, i, d, kwargs))\n            t.start()\n            threads.append(t)\n    else:\n        divs = np.linspace(0, len(data), thread_count + 1)\n        divs = np.round(divs).astype(int)\n        results = [None] * thread_count\n        threads = []\n        for i in range(thread_count):\n            t = threading.Thread(\n                name='threading_and_return', target=apply_fn, args=(results, i, data[divs[i]:divs[i + 1]], kwargs)\n            )\n            t.start()\n            threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    if thread_count is None:\n        try:\n            return np.asarray(results)\n        except Exception:\n            return results\n    else:\n        return np.concatenate(results)",
    "doc": "Process a batch of data by given function by threading.\n\n    Usually be used for data augmentation.\n\n    Parameters\n    -----------\n    data : numpy.array or others\n        The data to be processed.\n    thread_count : int\n        The number of threads to use.\n    fn : function\n        The function for data processing.\n    more args : the args for `fn`\n        Ssee Examples below.\n\n    Examples\n    --------\n    Process images.\n\n    >>> images, _, _, _ = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3))\n    >>> images = tl.prepro.threading_data(images[0:32], tl.prepro.zoom, zoom_range=[0.5, 1])\n\n    Customized image preprocessing function.\n\n    >>> def distort_img(x):\n    >>>     x = tl.prepro.flip_axis(x, axis=0, is_random=True)\n    >>>     x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n    >>>     x = tl.prepro.crop(x, 100, 100, is_random=True)\n    >>>     return x\n    >>> images = tl.prepro.threading_data(images, distort_img)\n\n    Process images and masks together (Usually be used for image segmentation).\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], tl.prepro.zoom_multi, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, 'images.png')\n    >>> tl.vis.save_image(Y_, 'masks.png')\n\n    Process images and masks together by using ``thread_count``.\n\n    >>> X, Y --> [batch_size, row, col, 1]\n    >>> data = tl.prepro.threading_data(X, tl.prepro.zoom_multi, 8, zoom_range=[0.5, 1], is_random=True)\n    data --> [batch_size, 2, row, col, 1]\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n    X_, Y_ --> [batch_size, row, col, 1]\n    >>> tl.vis.save_image(X_, 'after.png')\n    >>> tl.vis.save_image(Y_, 'before.png')\n\n    Customized function for processing images and masks together.\n\n    >>> def distort_img(data):\n    >>>    x, y = data\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=0, is_random=True)\n    >>>    x, y = tl.prepro.flip_axis_multi([x, y], axis=1, is_random=True)\n    >>>    x, y = tl.prepro.crop_multi([x, y], 100, 100, is_random=True)\n    >>>    return x, y\n\n    >>> X, Y --> [batch_size, row, col, channel]\n    >>> data = tl.prepro.threading_data([_ for _ in zip(X, Y)], distort_img)\n    >>> X_, Y_ = data.transpose((1,0,2,3,4))\n\n    Returns\n    -------\n    list or numpyarray\n        The processed results.\n\n    References\n    ----------\n    - `python queue <https://pymotw.com/2/Queue/index.html#module-Queue>`__\n    - `run with limited queue <http://effbot.org/librarybook/queue.htm>`__"
  },
  {
    "code": "def affine_rotation_matrix(angle=(-20, 20)):\n    \"\"\"Create an affine transform matrix for image rotation.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    angle : int/float or tuple of two int/float\n        Degree to rotate, usually -180 ~ 180.\n            - int/float, a fixed angle.\n            - tuple of 2 floats/ints, randomly sample a value as the angle between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n    if isinstance(angle, tuple):\n        theta = np.pi / 180 * np.random.uniform(angle[0], angle[1])\n    else:\n        theta = np.pi / 180 * angle\n    rotation_matrix = np.array([[np.cos(theta), np.sin(theta), 0], \\\n                                [-np.sin(theta), np.cos(theta), 0], \\\n                                [0, 0, 1]])\n    return rotation_matrix",
    "doc": "Create an affine transform matrix for image rotation.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    angle : int/float or tuple of two int/float\n        Degree to rotate, usually -180 ~ 180.\n            - int/float, a fixed angle.\n            - tuple of 2 floats/ints, randomly sample a value as the angle between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_horizontal_flip_matrix(prob=0.5):\n    \"\"\"Create an affine transformation matrix for image horizontal flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n    factor = np.random.uniform(0, 1)\n    if prob >= factor:\n        filp_matrix = np.array([[ -1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n    else:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix",
    "doc": "Create an affine transformation matrix for image horizontal flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_vertical_flip_matrix(prob=0.5):\n    \"\"\"Create an affine transformation for image vertical flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n    factor = np.random.uniform(0, 1)\n    if prob >= factor:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., -1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix\n    else:\n        filp_matrix = np.array([[ 1. , 0., 0. ], \\\n              [ 0., 1., 0. ], \\\n              [ 0., 0., 1. ]])\n        return filp_matrix",
    "doc": "Create an affine transformation for image vertical flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_shift_matrix(wrg=(-0.1, 0.1), hrg=(-0.1, 0.1), w=200, h=200):\n    \"\"\"Create an affine transform matrix for image shifting.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    wrg : float or tuple of floats\n        Range to shift on width axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    hrg : float or tuple of floats\n        Range to shift on height axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    w, h : int\n        The width and height of the image.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n    if isinstance(wrg, tuple):\n        tx = np.random.uniform(wrg[0], wrg[1]) * w\n    else:\n        tx = wrg * w\n    if isinstance(hrg, tuple):\n        ty = np.random.uniform(hrg[0], hrg[1]) * h\n    else:\n        ty = hrg * h\n    shift_matrix = np.array([[1, 0, tx], \\\n                        [0, 1, ty], \\\n                        [0, 0, 1]])\n    return shift_matrix",
    "doc": "Create an affine transform matrix for image shifting.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    wrg : float or tuple of floats\n        Range to shift on width axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    hrg : float or tuple of floats\n        Range to shift on height axis, -1 ~ 1.\n            - float, a fixed distance.\n            - tuple of 2 floats, randomly sample a value as the distance between these 2 values.\n    w, h : int\n        The width and height of the image.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_shear_matrix(x_shear=(-0.1, 0.1), y_shear=(-0.1, 0.1)):\n    \"\"\"Create affine transform matrix for image shearing.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    shear : tuple of two floats\n        Percentage of shears for width and height directions.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n    # if len(shear) != 2:\n    #     raise AssertionError(\n    #         \"shear should be tuple of 2 floats, or you want to use tl.prepro.shear rather than tl.prepro.shear2 ?\"\n    #     )\n    # if isinstance(shear, tuple):\n    #     shear = list(shear)\n    # if is_random:\n    #     shear[0] = np.random.uniform(-shear[0], shear[0])\n    #     shear[1] = np.random.uniform(-shear[1], shear[1])\n    if isinstance(x_shear, tuple):\n        x_shear = np.random.uniform(x_shear[0], x_shear[1])\n    if isinstance(y_shear, tuple):\n        y_shear = np.random.uniform(y_shear[0], y_shear[1])\n\n    shear_matrix = np.array([[1, x_shear, 0], \\\n                            [y_shear, 1, 0], \\\n                            [0, 0, 1]])\n    return shear_matrix",
    "doc": "Create affine transform matrix for image shearing.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    -----------\n    shear : tuple of two floats\n        Percentage of shears for width and height directions.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_zoom_matrix(zoom_range=(0.8, 1.1)):\n    \"\"\"Create an affine transform matrix for zooming/scaling an image's height and width.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n\n    if isinstance(zoom_range, (float, int)):\n        scale = zoom_range\n    elif isinstance(zoom_range, tuple):\n        scale = np.random.uniform(zoom_range[0], zoom_range[1])\n    else:\n        raise Exception(\"zoom_range: float or tuple of 2 floats\")\n\n    zoom_matrix = np.array([[scale, 0, 0], \\\n                            [0, scale, 0], \\\n                            [0, 0, 1]])\n    return zoom_matrix",
    "doc": "Create an affine transform matrix for zooming/scaling an image's height and width.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between these 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def affine_respective_zoom_matrix(w_range=0.8, h_range=1.1):\n    \"\"\"Get affine transform matrix for zooming/scaling that height and width are changed independently.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.\n\n    \"\"\"\n\n    if isinstance(h_range, (float, int)):\n        zy = h_range\n    elif isinstance(h_range, tuple):\n        zy = np.random.uniform(h_range[0], h_range[1])\n    else:\n        raise Exception(\"h_range: float or tuple of 2 floats\")\n\n    if isinstance(w_range, (float, int)):\n        zx = w_range\n    elif isinstance(w_range, tuple):\n        zx = np.random.uniform(w_range[0], w_range[1])\n    else:\n        raise Exception(\"w_range: float or tuple of 2 floats\")\n\n    zoom_matrix = np.array([[zx, 0, 0], \\\n                            [0, zy, 0], \\\n                            [0, 0, 1]])\n    return zoom_matrix",
    "doc": "Get affine transform matrix for zooming/scaling that height and width are changed independently.\n    OpenCV format, x is width.\n\n    Parameters\n    -----------\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix."
  },
  {
    "code": "def transform_matrix_offset_center(matrix, y, x):\n    \"\"\"Convert the matrix from Cartesian coordinates (the origin in the middle of image) to Image coordinates (the origin on the top-left of image).\n\n    Parameters\n    ----------\n    matrix : numpy.array\n        Transform matrix.\n    x and y : 2 int\n        Size of image.\n\n    Returns\n    -------\n    numpy.array\n        The transform matrix.\n\n    Examples\n    --------\n    - See ``tl.prepro.rotation``, ``tl.prepro.shear``, ``tl.prepro.zoom``.\n    \"\"\"\n    o_x = (x - 1) / 2.0\n    o_y = (y - 1) / 2.0\n    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n    return transform_matrix",
    "doc": "Convert the matrix from Cartesian coordinates (the origin in the middle of image) to Image coordinates (the origin on the top-left of image).\n\n    Parameters\n    ----------\n    matrix : numpy.array\n        Transform matrix.\n    x and y : 2 int\n        Size of image.\n\n    Returns\n    -------\n    numpy.array\n        The transform matrix.\n\n    Examples\n    --------\n    - See ``tl.prepro.rotation``, ``tl.prepro.shear``, ``tl.prepro.zoom``."
  },
  {
    "code": "def affine_transform(x, transform_matrix, channel_index=2, fill_mode='nearest', cval=0., order=1):\n    \"\"\"Return transformed images by given an affine matrix in Scipy format (x is height).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        Transform matrix (offset center), can be generated by ``transform_matrix_offset_center``\n    channel_index : int\n        Index of channel, default 2.\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n            - `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, h, w)\n    >>> result = tl.prepro.affine_transform(image, transform_matrix)\n\n    \"\"\"\n    # transform_matrix = transform_matrix_offset_center()\n    # asdihasid\n    # asd\n\n    x = np.rollaxis(x, channel_index, 0)\n    final_affine_matrix = transform_matrix[:2, :2]\n    final_offset = transform_matrix[:2, 2]\n    channel_images = [\n        ndi.interpolation.\n        affine_transform(x_channel, final_affine_matrix, final_offset, order=order, mode=fill_mode, cval=cval)\n        for x_channel in x\n    ]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_index + 1)\n    return x",
    "doc": "Return transformed images by given an affine matrix in Scipy format (x is height).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        Transform matrix (offset center), can be generated by ``transform_matrix_offset_center``\n    channel_index : int\n        Index of channel, default 2.\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n            - `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, h, w)\n    >>> result = tl.prepro.affine_transform(image, transform_matrix)"
  },
  {
    "code": "def affine_transform_cv2(x, transform_matrix, flags=None, border_mode='constant'):\n    \"\"\"Return transformed images by given an affine matrix in OpenCV format (x is width). (Powered by OpenCV2, faster than ``tl.prepro.affine_transform``)\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        A transform matrix, OpenCV format.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> result = tl.prepro.affine_transform_cv2(image, M_combined)\n    \"\"\"\n    rows, cols = x.shape[0], x.shape[1]\n    if flags is None:\n        flags = cv2.INTER_AREA\n    if border_mode is 'constant':\n        border_mode = cv2.BORDER_CONSTANT\n    elif border_mode is 'replicate':\n        border_mode = cv2.BORDER_REPLICATE\n    else:\n        raise Exception(\"unsupport border_mode, check cv.BORDER_ for more details.\")\n    return cv2.warpAffine(x, transform_matrix[0:2,:], \\\n            (cols,rows), flags=flags, borderMode=border_mode)",
    "doc": "Return transformed images by given an affine matrix in OpenCV format (x is width). (Powered by OpenCV2, faster than ``tl.prepro.affine_transform``)\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    transform_matrix : numpy.array\n        A transform matrix, OpenCV format.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Examples\n    --------\n    >>> M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)\n    >>> M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)\n    >>> M_combined = M_shear.dot(M_zoom)\n    >>> result = tl.prepro.affine_transform_cv2(image, M_combined)"
  },
  {
    "code": "def affine_transform_keypoints(coords_list, transform_matrix):\n    \"\"\"Transform keypoint coordinates according to a given affine transform matrix.\n    OpenCV format, x is width.\n\n    Note that, for pose estimation task, flipping requires maintaining the left and right body information.\n    We should not flip the left and right body, so please use ``tl.prepro.keypoint_random_flip``.\n\n    Parameters\n    -----------\n    coords_list : list of list of tuple/list\n        The coordinates\n        e.g., the keypoint coordinates of every person in an image.\n    transform_matrix : numpy.array\n        Transform matrix, OpenCV format.\n\n    Examples\n    ---------\n    >>> # 1. get all affine transform matrices\n    >>> M_rotate = tl.prepro.affine_rotation_matrix(angle=20)\n    >>> M_flip = tl.prepro.affine_horizontal_flip_matrix(prob=1)\n    >>> # 2. combine all affine transform matrices to one matrix\n    >>> M_combined = dot(M_flip).dot(M_rotate)\n    >>> # 3. transfrom the matrix from Cartesian coordinate (the origin in the middle of image)\n    >>> # to Image coordinate (the origin on the top-left of image)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, x=w, y=h)\n    >>> # 4. then we can transfrom the image once for all transformations\n    >>> result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # 76 times faster\n    >>> # 5. transform keypoint coordinates\n    >>> coords = [[(50, 100), (100, 100), (100, 50), (200, 200)], [(250, 50), (200, 50), (200, 100)]]\n    >>> coords_result = tl.prepro.affine_transform_keypoints(coords, transform_matrix)\n    \"\"\"\n    coords_result_list = []\n    for coords in coords_list:\n        coords = np.asarray(coords)\n        coords = coords.transpose([1, 0])\n        coords = np.insert(coords, 2, 1, axis=0)\n        # print(coords)\n        # print(transform_matrix)\n        coords_result = np.matmul(transform_matrix, coords)\n        coords_result = coords_result[0:2, :].transpose([1, 0])\n        coords_result_list.append(coords_result)\n    return coords_result_list",
    "doc": "Transform keypoint coordinates according to a given affine transform matrix.\n    OpenCV format, x is width.\n\n    Note that, for pose estimation task, flipping requires maintaining the left and right body information.\n    We should not flip the left and right body, so please use ``tl.prepro.keypoint_random_flip``.\n\n    Parameters\n    -----------\n    coords_list : list of list of tuple/list\n        The coordinates\n        e.g., the keypoint coordinates of every person in an image.\n    transform_matrix : numpy.array\n        Transform matrix, OpenCV format.\n\n    Examples\n    ---------\n    >>> # 1. get all affine transform matrices\n    >>> M_rotate = tl.prepro.affine_rotation_matrix(angle=20)\n    >>> M_flip = tl.prepro.affine_horizontal_flip_matrix(prob=1)\n    >>> # 2. combine all affine transform matrices to one matrix\n    >>> M_combined = dot(M_flip).dot(M_rotate)\n    >>> # 3. transfrom the matrix from Cartesian coordinate (the origin in the middle of image)\n    >>> # to Image coordinate (the origin on the top-left of image)\n    >>> transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, x=w, y=h)\n    >>> # 4. then we can transfrom the image once for all transformations\n    >>> result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # 76 times faster\n    >>> # 5. transform keypoint coordinates\n    >>> coords = [[(50, 100), (100, 100), (100, 50), (200, 200)], [(250, 50), (200, 50), (200, 100)]]\n    >>> coords_result = tl.prepro.affine_transform_keypoints(coords, transform_matrix)"
  },
  {
    "code": "def projective_transform_by_points(\n        x, src, dst, map_args=None, output_shape=None, order=1, mode='constant', cval=0.0, clip=True,\n        preserve_range=False\n):\n    \"\"\"Projective transform by given coordinates, usually 4 coordinates.\n\n    see `scikit-image <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    src : list or numpy\n        The original coordinates, usually 4 coordinates of (width, height).\n    dst : list or numpy\n        The coordinates after transformation, the number of coordinates is the same with src.\n    map_args : dictionary or None\n        Keyword arguments passed to inverse map.\n    output_shape : tuple of 2 int\n        Shape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n    mode : str\n        One of `constant` (default), `edge`, `symmetric`, `reflect` or `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    Assume X is an image from CIFAR-10, i.e. shape == (32, 32, 3)\n\n    >>> src = [[0,0],[0,32],[32,0],[32,32]]     # [w, h]\n    >>> dst = [[10,10],[0,32],[32,0],[32,32]]\n    >>> x = tl.prepro.projective_transform_by_points(X, src, dst)\n\n    References\n    -----------\n    - `scikit-image : geometric transformations <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__\n    - `scikit-image : examples <http://scikit-image.org/docs/dev/auto_examples/index.html>`__\n\n    \"\"\"\n    if map_args is None:\n        map_args = {}\n    # if type(src) is list:\n    if isinstance(src, list):  # convert to numpy\n        src = np.array(src)\n    # if type(dst) is list:\n    if isinstance(dst, list):\n        dst = np.array(dst)\n    if np.max(x) > 1:  # convert to [0, 1]\n        x = x / 255\n\n    m = transform.ProjectiveTransform()\n    m.estimate(dst, src)\n    warped = transform.warp(\n        x, m, map_args=map_args, output_shape=output_shape, order=order, mode=mode, cval=cval, clip=clip,\n        preserve_range=preserve_range\n    )\n    return warped",
    "doc": "Projective transform by given coordinates, usually 4 coordinates.\n\n    see `scikit-image <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    src : list or numpy\n        The original coordinates, usually 4 coordinates of (width, height).\n    dst : list or numpy\n        The coordinates after transformation, the number of coordinates is the same with src.\n    map_args : dictionary or None\n        Keyword arguments passed to inverse map.\n    output_shape : tuple of 2 int\n        Shape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5:\n            - 0 Nearest-neighbor\n            - 1 Bi-linear (default)\n            - 2 Bi-quadratic\n            - 3 Bi-cubic\n            - 4 Bi-quartic\n            - 5 Bi-quintic\n    mode : str\n        One of `constant` (default), `edge`, `symmetric`, `reflect` or `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    Assume X is an image from CIFAR-10, i.e. shape == (32, 32, 3)\n\n    >>> src = [[0,0],[0,32],[32,0],[32,32]]     # [w, h]\n    >>> dst = [[10,10],[0,32],[32,0],[32,32]]\n    >>> x = tl.prepro.projective_transform_by_points(X, src, dst)\n\n    References\n    -----------\n    - `scikit-image : geometric transformations <http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html>`__\n    - `scikit-image : examples <http://scikit-image.org/docs/dev/auto_examples/index.html>`__"
  },
  {
    "code": "def rotation(\n        x, rg=20, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode='nearest', cval=0., order=1\n):\n    \"\"\"Rotate an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n    is_random : boolean\n        If True, randomly rotate. Default is False\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=`constant`. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1]\n    >>> x = tl.prepro.rotation(x, rg=40, is_random=False)\n    >>> tl.vis.save_image(x, 'im.png')\n\n    \"\"\"\n    if is_random:\n        theta = np.pi / 180 * np.random.uniform(-rg, rg)\n    else:\n        theta = np.pi / 180 * rg\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0], [np.sin(theta), np.cos(theta), 0], [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x",
    "doc": "Rotate an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n    is_random : boolean\n        If True, randomly rotate. Default is False\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode=`constant`. Default is 0.0\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1]\n    >>> x = tl.prepro.rotation(x, rg=40, is_random=False)\n    >>> tl.vis.save_image(x, 'im.png')"
  },
  {
    "code": "def crop(x, wrg, hrg, is_random=False, row_index=0, col_index=1):\n    \"\"\"Randomly or centrally crop an image.\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : int\n        Size of width.\n    hrg : int\n        Size of height.\n    is_random : boolean,\n        If True, randomly crop, else central crop. Default is False.\n    row_index: int\n        index of row.\n    col_index: int\n        index of column.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    h, w = x.shape[row_index], x.shape[col_index]\n\n    if (h < hrg) or (w < wrg):\n        raise AssertionError(\"The size of cropping should smaller than or equal to the original image\")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg))\n        w_offset = int(np.random.uniform(0, w - wrg))\n        # tl.logging.info(h_offset, w_offset, x[h_offset: hrg+h_offset ,w_offset: wrg+w_offset].shape)\n        return x[h_offset:hrg + h_offset, w_offset:wrg + w_offset]\n    else:  # central crop\n        h_offset = int(np.floor((h - hrg) / 2.))\n        w_offset = int(np.floor((w - wrg) / 2.))\n        h_end = h_offset + hrg\n        w_end = w_offset + wrg\n        return x[h_offset:h_end, w_offset:w_end]",
    "doc": "Randomly or centrally crop an image.\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : int\n        Size of width.\n    hrg : int\n        Size of height.\n    is_random : boolean,\n        If True, randomly crop, else central crop. Default is False.\n    row_index: int\n        index of row.\n    col_index: int\n        index of column.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def crop_multi(x, wrg, hrg, is_random=False, row_index=0, col_index=1):\n    \"\"\"Randomly or centrally crop multiple images.\n\n    Parameters\n    ----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.crop``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n\n    if (h < hrg) or (w < wrg):\n        raise AssertionError(\"The size of cropping should smaller than or equal to the original image\")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg))\n        w_offset = int(np.random.uniform(0, w - wrg))\n        results = []\n        for data in x:\n            results.append(data[h_offset:hrg + h_offset, w_offset:wrg + w_offset])\n        return np.asarray(results)\n    else:\n        # central crop\n        h_offset = (h - hrg) / 2\n        w_offset = (w - wrg) / 2\n        results = []\n        for data in x:\n            results.append(data[h_offset:h - h_offset, w_offset:w - w_offset])\n        return np.asarray(results)",
    "doc": "Randomly or centrally crop multiple images.\n\n    Parameters\n    ----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.crop``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def flip_axis(x, axis=1, is_random=False):\n    \"\"\"Flip the axis of an image, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    axis : int\n        Which axis to flip.\n            - 0, flip up and down\n            - 1, flip left and right\n            - 2, flip channel\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            x = np.asarray(x).swapaxes(axis, 0)\n            x = x[::-1, ...]\n            x = x.swapaxes(0, axis)\n            return x\n        else:\n            return x\n    else:\n        x = np.asarray(x).swapaxes(axis, 0)\n        x = x[::-1, ...]\n        x = x.swapaxes(0, axis)\n        return x",
    "doc": "Flip the axis of an image, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    axis : int\n        Which axis to flip.\n            - 0, flip up and down\n            - 1, flip left and right\n            - 2, flip channel\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def flip_axis_multi(x, axis, is_random=False):\n    \"\"\"Flip the axises of multiple images together, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.flip_axis``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            # x = np.asarray(x).swapaxes(axis, 0)\n            # x = x[::-1, ...]\n            # x = x.swapaxes(0, axis)\n            # return x\n            results = []\n            for data in x:\n                data = np.asarray(data).swapaxes(axis, 0)\n                data = data[::-1, ...]\n                data = data.swapaxes(0, axis)\n                results.append(data)\n            return np.asarray(results)\n        else:\n            return np.asarray(x)\n    else:\n        # x = np.asarray(x).swapaxes(axis, 0)\n        # x = x[::-1, ...]\n        # x = x.swapaxes(0, axis)\n        # return x\n        results = []\n        for data in x:\n            data = np.asarray(data).swapaxes(axis, 0)\n            data = data[::-1, ...]\n            data = data.swapaxes(0, axis)\n            results.append(data)\n        return np.asarray(results)",
    "doc": "Flip the axises of multiple images together, such as flip left and right, up and down, randomly or non-randomly,\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.flip_axis``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def shift(\n        x, wrg=0.1, hrg=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode='nearest', cval=0.,\n        order=1\n):\n    \"\"\"Shift an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : float\n        Percentage of shift in axis x, usually -0.25 ~ 0.25.\n    hrg : float\n        Percentage of shift in axis y, usually -0.25 ~ 0.25.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    h, w = x.shape[row_index], x.shape[col_index]\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * h\n        ty = np.random.uniform(-wrg, wrg) * w\n    else:\n        tx, ty = hrg * h, wrg * w\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x",
    "doc": "Shift an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    wrg : float\n        Percentage of shift in axis x, usually -0.25 ~ 0.25.\n    hrg : float\n        Percentage of shift in axis y, usually -0.25 ~ 0.25.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def shift_multi(\n        x, wrg=0.1, hrg=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode='nearest', cval=0.,\n        order=1\n):\n    \"\"\"Shift images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n    h, w = x[0].shape[row_index], x[0].shape[col_index]\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * h\n        ty = np.random.uniform(-wrg, wrg) * w\n    else:\n        tx, ty = hrg * h, wrg * w\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    results = []\n    for data in x:\n        results.append(affine_transform(data, transform_matrix, channel_index, fill_mode, cval, order))\n    return np.asarray(results)",
    "doc": "Shift images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def shear(\n        x, intensity=0.1, is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode='nearest', cval=0.,\n        order=1\n):\n    \"\"\"Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Percentage of shear, usually -0.5 ~ 0.5 (is_random==True), 0 ~ 0.5 (is_random==False),\n        you can have a quick try by shear(X, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__\n\n    \"\"\"\n    if is_random:\n        shear = np.random.uniform(-intensity, intensity)\n    else:\n        shear = intensity\n    shear_matrix = np.array([[1, -np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x",
    "doc": "Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Percentage of shear, usually -0.5 ~ 0.5 (is_random==True), 0 ~ 0.5 (is_random==False),\n        you can have a quick try by shear(X, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__"
  },
  {
    "code": "def shear2(\n        x, shear=(0.1, 0.1), is_random=False, row_index=0, col_index=1, channel_index=2, fill_mode='nearest', cval=0.,\n        order=1\n):\n    \"\"\"Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    shear : tuple of two floats\n        Percentage of shear for height and width direction (0, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__\n\n    \"\"\"\n    if len(shear) != 2:\n        raise AssertionError(\n            \"shear should be tuple of 2 floats, or you want to use tl.prepro.shear rather than tl.prepro.shear2 ?\"\n        )\n    if isinstance(shear, tuple):\n        shear = list(shear)\n    if is_random:\n        shear[0] = np.random.uniform(-shear[0], shear[0])\n        shear[1] = np.random.uniform(-shear[1], shear[1])\n\n    shear_matrix = np.array([[1, shear[0], 0], \\\n                            [shear[1], 1, 0], \\\n                            [0, 0, 1]])\n\n    h, w = x.shape[row_index], x.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    x = affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x",
    "doc": "Shear an image randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    shear : tuple of two floats\n        Percentage of shear for height and width direction (0, 1).\n    is_random : boolean\n        If True, randomly shear. Default is False.\n    row_index col_index and channel_index : int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    fill_mode : str\n        Method to fill missing pixel, default `nearest`, more options `constant`, `reflect` or `wrap`, see `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n    cval : float\n        Value used for points outside the boundaries of the input if mode='constant'. Default is 0.0.\n    order : int\n        The order of interpolation. The order has to be in the range 0-5. See ``tl.prepro.affine_transform`` and `scipy ndimage affine_transform <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.affine_transform.html>`__\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `Affine transformation <https://uk.mathworks.com/discovery/affine-transformation.html>`__"
  },
  {
    "code": "def swirl(\n        x, center=None, strength=1, radius=100, rotation=0, output_shape=None, order=1, mode='constant', cval=0,\n        clip=True, preserve_range=False, is_random=False\n):\n    \"\"\"Swirl an image randomly or non-randomly, see `scikit-image swirl API <http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.swirl>`__\n    and `example <http://scikit-image.org/docs/dev/auto_examples/plot_swirl.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    center : tuple or 2 int or None\n        Center coordinate of transformation (optional).\n    strength : float\n        The amount of swirling applied.\n    radius : float\n        The extent of the swirl in pixels. The effect dies out rapidly beyond radius.\n    rotation : float\n        Additional rotation applied to the image, usually [0, 360], relates to center.\n    output_shape : tuple of 2 int or None\n        Shape of the output image generated (height, width). By default the shape of the input image is preserved.\n    order : int, optional\n        The order of the spline interpolation, default is 1. The order has to be in the range 0-5. See skimage.transform.warp for detail.\n    mode : str\n        One of `constant` (default), `edge`, `symmetric` `reflect` and `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode, with `constant` used as the default. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n    is_random : boolean,\n        If True, random swirl. Default is False.\n            - random center = [(0 ~ x.shape[0]), (0 ~ x.shape[1])]\n            - random strength = [0, strength]\n            - random radius = [1e-10, radius]\n            - random rotation = [-rotation, rotation]\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1] greyscale\n    >>> x = tl.prepro.swirl(x, strength=4, radius=100)\n\n    \"\"\"\n    if radius == 0:\n        raise AssertionError(\"Invalid radius value\")\n\n    rotation = np.pi / 180 * rotation\n    if is_random:\n        center_h = int(np.random.uniform(0, x.shape[0]))\n        center_w = int(np.random.uniform(0, x.shape[1]))\n        center = (center_h, center_w)\n        strength = np.random.uniform(0, strength)\n        radius = np.random.uniform(1e-10, radius)\n        rotation = np.random.uniform(-rotation, rotation)\n\n    max_v = np.max(x)\n    if max_v > 1:  # Note: the input of this fn should be [-1, 1], rescale is required.\n        x = x / max_v\n    swirled = skimage.transform.swirl(\n        x, center=center, strength=strength, radius=radius, rotation=rotation, output_shape=output_shape, order=order,\n        mode=mode, cval=cval, clip=clip, preserve_range=preserve_range\n    )\n    if max_v > 1:\n        swirled = swirled * max_v\n    return swirled",
    "doc": "Swirl an image randomly or non-randomly, see `scikit-image swirl API <http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.swirl>`__\n    and `example <http://scikit-image.org/docs/dev/auto_examples/plot_swirl.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    center : tuple or 2 int or None\n        Center coordinate of transformation (optional).\n    strength : float\n        The amount of swirling applied.\n    radius : float\n        The extent of the swirl in pixels. The effect dies out rapidly beyond radius.\n    rotation : float\n        Additional rotation applied to the image, usually [0, 360], relates to center.\n    output_shape : tuple of 2 int or None\n        Shape of the output image generated (height, width). By default the shape of the input image is preserved.\n    order : int, optional\n        The order of the spline interpolation, default is 1. The order has to be in the range 0-5. See skimage.transform.warp for detail.\n    mode : str\n        One of `constant` (default), `edge`, `symmetric` `reflect` and `wrap`.\n        Points outside the boundaries of the input are filled according to the given mode, with `constant` used as the default. Modes match the behaviour of numpy.pad.\n    cval : float\n        Used in conjunction with mode `constant`, the value outside the image boundaries.\n    clip : boolean\n        Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.\n    preserve_range : boolean\n        Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float.\n    is_random : boolean,\n        If True, random swirl. Default is False.\n            - random center = [(0 ~ x.shape[0]), (0 ~ x.shape[1])]\n            - random strength = [0, strength]\n            - random radius = [1e-10, radius]\n            - random rotation = [-rotation, rotation]\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x --> [row, col, 1] greyscale\n    >>> x = tl.prepro.swirl(x, strength=4, radius=100)"
  },
  {
    "code": "def elastic_transform(x, alpha, sigma, mode=\"constant\", cval=0, is_random=False):\n    \"\"\"Elastic transformation for image as described in `[Simard2003] <http://deeplearning.cs.cmu.edu/pdfs/Simard.pdf>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        A greyscale image.\n    alpha : float\n        Alpha value for elastic transformation.\n    sigma : float or sequence of float\n        The smaller the sigma, the more transformation. Standard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.\n    mode : str\n        See `scipy.ndimage.filters.gaussian_filter <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.gaussian_filter.html>`__. Default is `constant`.\n    cval : float,\n        Used in conjunction with `mode` of `constant`, the value outside the image boundaries.\n    is_random : boolean\n        Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x = tl.prepro.elastic_transform(x, alpha=x.shape[1]*3, sigma=x.shape[1]*0.07)\n\n    References\n    ------------\n    - `Github <https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a>`__.\n    - `Kaggle <https://www.kaggle.com/pscion/ultrasound-nerve-segmentation/elastic-transform-for-data-augmentation-0878921a>`__\n\n    \"\"\"\n    if is_random is False:\n        random_state = np.random.RandomState(None)\n    else:\n        random_state = np.random.RandomState(int(time.time()))\n    #\n    is_3d = False\n    if len(x.shape) == 3 and x.shape[-1] == 1:\n        x = x[:, :, 0]\n        is_3d = True\n    elif len(x.shape) == 3 and x.shape[-1] != 1:\n        raise Exception(\"Only support greyscale image\")\n\n    if len(x.shape) != 2:\n        raise AssertionError(\"input should be grey-scale image\")\n\n    shape = x.shape\n\n    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n\n    x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n    indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n    if is_3d:\n        return map_coordinates(x, indices, order=1).reshape((shape[0], shape[1], 1))\n    else:\n        return map_coordinates(x, indices, order=1).reshape(shape)",
    "doc": "Elastic transformation for image as described in `[Simard2003] <http://deeplearning.cs.cmu.edu/pdfs/Simard.pdf>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        A greyscale image.\n    alpha : float\n        Alpha value for elastic transformation.\n    sigma : float or sequence of float\n        The smaller the sigma, the more transformation. Standard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.\n    mode : str\n        See `scipy.ndimage.filters.gaussian_filter <https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.gaussian_filter.html>`__. Default is `constant`.\n    cval : float,\n        Used in conjunction with `mode` of `constant`, the value outside the image boundaries.\n    is_random : boolean\n        Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    >>> x = tl.prepro.elastic_transform(x, alpha=x.shape[1]*3, sigma=x.shape[1]*0.07)\n\n    References\n    ------------\n    - `Github <https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a>`__.\n    - `Kaggle <https://www.kaggle.com/pscion/ultrasound-nerve-segmentation/elastic-transform-for-data-augmentation-0878921a>`__"
  },
  {
    "code": "def zoom(x, zoom_range=(0.9, 1.1), flags=None, border_mode='constant'):\n    \"\"\"Zooming/Scaling a single image that height and width are changed together.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    zoom_matrix = affine_zoom_matrix(zoom_range=zoom_range)\n    h, w = x.shape[0], x.shape[1]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    x = affine_transform_cv2(x, transform_matrix, flags=flags, border_mode=border_mode)\n    return x",
    "doc": "Zooming/Scaling a single image that height and width are changed together.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    zoom_range : float or tuple of 2 floats\n        The zooming/scaling ratio, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def respective_zoom(x, h_range=(0.9, 1.1), w_range=(0.9, 1.1), flags=None, border_mode='constant'):\n    \"\"\"Zooming/Scaling a single image that height and width are changed independently.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    zoom_matrix = affine_respective_zoom_matrix(h_range=h_range, w_range=w_range)\n    h, w = x.shape[0], x.shape[1]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    x = affine_transform_cv2(\n        x, transform_matrix, flags=flags, border_mode=border_mode\n    )  #affine_transform(x, transform_matrix, channel_index, fill_mode, cval, order)\n    return x",
    "doc": "Zooming/Scaling a single image that height and width are changed independently.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    h_range : float or tuple of 2 floats\n        The zooming/scaling ratio of height, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    w_range : float or tuple of 2 floats\n        The zooming/scaling ratio of width, greater than 1 means larger.\n            - float, a fixed ratio.\n            - tuple of 2 floats, randomly sample a value as the ratio between 2 values.\n    border_mode : str\n        - `constant`, pad the image with a constant value (i.e. black or 0)\n        - `replicate`, the row or column at the very edge of the original is replicated to the extra border.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def zoom_multi(x, zoom_range=(0.9, 1.1), flags=None, border_mode='constant'):\n    \"\"\"Zoom in and out of images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.zoom``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n\n    zoom_matrix = affine_zoom_matrix(zoom_range=zoom_range)\n    results = []\n    for img in x:\n        h, w = x.shape[0], x.shape[1]\n        transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n        results.append(affine_transform_cv2(x, transform_matrix, flags=flags, border_mode=border_mode))\n    return result",
    "doc": "Zoom in and out of images with the same arguments, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.zoom``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def brightness(x, gamma=1, gain=1, is_random=False):\n    \"\"\"Change the brightness of a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Non negative real number. Default value is 1.\n            - Small than 1 means brighter.\n            - If `is_random` is True, gamma in a range of (1-gamma, 1+gamma).\n    gain : float\n        The constant multiplier. Default value is 1.\n    is_random : boolean\n        If True, randomly change brightness. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `skimage.exposure.adjust_gamma <http://scikit-image.org/docs/dev/api/skimage.exposure.html>`__\n    - `chinese blog <http://www.cnblogs.com/denny402/p/5124402.html>`__\n\n    \"\"\"\n    if is_random:\n        gamma = np.random.uniform(1 - gamma, 1 + gamma)\n    x = exposure.adjust_gamma(x, gamma, gain)\n    return x",
    "doc": "Change the brightness of a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Non negative real number. Default value is 1.\n            - Small than 1 means brighter.\n            - If `is_random` is True, gamma in a range of (1-gamma, 1+gamma).\n    gain : float\n        The constant multiplier. Default value is 1.\n    is_random : boolean\n        If True, randomly change brightness. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    -----------\n    - `skimage.exposure.adjust_gamma <http://scikit-image.org/docs/dev/api/skimage.exposure.html>`__\n    - `chinese blog <http://www.cnblogs.com/denny402/p/5124402.html>`__"
  },
  {
    "code": "def brightness_multi(x, gamma=1, gain=1, is_random=False):\n    \"\"\"Change the brightness of multiply images, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpyarray\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.brightness``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n    if is_random:\n        gamma = np.random.uniform(1 - gamma, 1 + gamma)\n\n    results = []\n    for data in x:\n        results.append(exposure.adjust_gamma(data, gamma, gain))\n    return np.asarray(results)",
    "doc": "Change the brightness of multiply images, randomly or non-randomly.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpyarray\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.brightness``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def illumination(x, gamma=1., contrast=1., saturation=1., is_random=False):\n    \"\"\"Perform illumination augmentation for a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Change brightness (the same with ``tl.prepro.brightness``)\n            - if is_random=False, one float number, small than one means brighter, greater than one means darker.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    contrast : float\n        Change contrast.\n            - if is_random=False, one float number, small than one means blur.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    saturation : float\n        Change saturation.\n            - if is_random=False, one float number, small than one means unsaturation.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    is_random : boolean\n        If True, randomly change illumination. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random\n\n    >>> x = tl.prepro.illumination(x, gamma=(0.5, 5.0), contrast=(0.3, 1.0), saturation=(0.7, 1.0), is_random=True)\n\n    Non-random\n\n    >>> x = tl.prepro.illumination(x, 0.5, 0.6, 0.8, is_random=False)\n\n    \"\"\"\n    if is_random:\n        if not (len(gamma) == len(contrast) == len(saturation) == 2):\n            raise AssertionError(\"if is_random = True, the arguments are (min, max)\")\n\n        ## random change brightness  # small --> brighter\n        illum_settings = np.random.randint(0, 3)  # 0-brighter, 1-darker, 2 keep normal\n\n        if illum_settings == 0:  # brighter\n            gamma = np.random.uniform(gamma[0], 1.0)  # (.5, 1.0)\n        elif illum_settings == 1:  # darker\n            gamma = np.random.uniform(1.0, gamma[1])  # (1.0, 5.0)\n        else:\n            gamma = 1\n        im_ = brightness(x, gamma=gamma, gain=1, is_random=False)\n\n        # tl.logging.info(\"using contrast and saturation\")\n        image = PIL.Image.fromarray(im_)  # array -> PIL\n        contrast_adjust = PIL.ImageEnhance.Contrast(image)\n        image = contrast_adjust.enhance(np.random.uniform(contrast[0], contrast[1]))  #0.3,0.9))\n\n        saturation_adjust = PIL.ImageEnhance.Color(image)\n        image = saturation_adjust.enhance(np.random.uniform(saturation[0], saturation[1]))  # (0.7,1.0))\n        im_ = np.array(image)  # PIL -> array\n    else:\n        im_ = brightness(x, gamma=gamma, gain=1, is_random=False)\n        image = PIL.Image.fromarray(im_)  # array -> PIL\n        contrast_adjust = PIL.ImageEnhance.Contrast(image)\n        image = contrast_adjust.enhance(contrast)\n\n        saturation_adjust = PIL.ImageEnhance.Color(image)\n        image = saturation_adjust.enhance(saturation)\n        im_ = np.array(image)  # PIL -> array\n    return np.asarray(im_)",
    "doc": "Perform illumination augmentation for a single image, randomly or non-randomly.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    gamma : float\n        Change brightness (the same with ``tl.prepro.brightness``)\n            - if is_random=False, one float number, small than one means brighter, greater than one means darker.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    contrast : float\n        Change contrast.\n            - if is_random=False, one float number, small than one means blur.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    saturation : float\n        Change saturation.\n            - if is_random=False, one float number, small than one means unsaturation.\n            - if is_random=True, tuple of two float numbers, (min, max).\n    is_random : boolean\n        If True, randomly change illumination. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random\n\n    >>> x = tl.prepro.illumination(x, gamma=(0.5, 5.0), contrast=(0.3, 1.0), saturation=(0.7, 1.0), is_random=True)\n\n    Non-random\n\n    >>> x = tl.prepro.illumination(x, 0.5, 0.6, 0.8, is_random=False)"
  },
  {
    "code": "def rgb_to_hsv(rgb):\n    \"\"\"Input RGB image [0~255] return HSV image [0~1].\n\n    Parameters\n    ------------\n    rgb : numpy.array\n        An image with values between 0 and 255.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    # Translated from source of colorsys.rgb_to_hsv\n    # r,g,b should be a numpy arrays with values between 0 and 255\n    # rgb_to_hsv returns an array of floats between 0.0 and 1.0.\n    rgb = rgb.astype('float')\n    hsv = np.zeros_like(rgb)\n    # in case an RGBA array was passed, just copy the A channel\n    hsv[..., 3:] = rgb[..., 3:]\n    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n    maxc = np.max(rgb[..., :3], axis=-1)\n    minc = np.min(rgb[..., :3], axis=-1)\n    hsv[..., 2] = maxc\n    mask = maxc != minc\n    hsv[mask, 1] = (maxc - minc)[mask] / maxc[mask]\n    rc = np.zeros_like(r)\n    gc = np.zeros_like(g)\n    bc = np.zeros_like(b)\n    rc[mask] = (maxc - r)[mask] / (maxc - minc)[mask]\n    gc[mask] = (maxc - g)[mask] / (maxc - minc)[mask]\n    bc[mask] = (maxc - b)[mask] / (maxc - minc)[mask]\n    hsv[..., 0] = np.select([r == maxc, g == maxc], [bc - gc, 2.0 + rc - bc], default=4.0 + gc - rc)\n    hsv[..., 0] = (hsv[..., 0] / 6.0) % 1.0\n    return hsv",
    "doc": "Input RGB image [0~255] return HSV image [0~1].\n\n    Parameters\n    ------------\n    rgb : numpy.array\n        An image with values between 0 and 255.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def hsv_to_rgb(hsv):\n    \"\"\"Input HSV image [0~1] return RGB image [0~255].\n\n    Parameters\n    -------------\n    hsv : numpy.array\n        An image with values between 0.0 and 1.0\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n    \"\"\"\n    # Translated from source of colorsys.hsv_to_rgb\n    # h,s should be a numpy arrays with values between 0.0 and 1.0\n    # v should be a numpy array with values between 0.0 and 255.0\n    # hsv_to_rgb returns an array of uints between 0 and 255.\n    rgb = np.empty_like(hsv)\n    rgb[..., 3:] = hsv[..., 3:]\n    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n    i = (h * 6.0).astype('uint8')\n    f = (h * 6.0) - i\n    p = v * (1.0 - s)\n    q = v * (1.0 - s * f)\n    t = v * (1.0 - s * (1.0 - f))\n    i = i % 6\n    conditions = [s == 0.0, i == 1, i == 2, i == 3, i == 4, i == 5]\n    rgb[..., 0] = np.select(conditions, [v, q, p, p, t, v], default=v)\n    rgb[..., 1] = np.select(conditions, [v, v, v, q, p, p], default=t)\n    rgb[..., 2] = np.select(conditions, [v, p, t, v, v, q], default=p)\n    return rgb.astype('uint8')",
    "doc": "Input HSV image [0~1] return RGB image [0~255].\n\n    Parameters\n    -------------\n    hsv : numpy.array\n        An image with values between 0.0 and 1.0\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def adjust_hue(im, hout=0.66, is_offset=True, is_clip=True, is_random=False):\n    \"\"\"Adjust hue of an RGB image.\n\n    This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type.\n    For TF, see `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.and `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with values between 0 and 255.\n    hout : float\n        The scale value for adjusting hue.\n            - If is_offset is False, set all hue values to this value. 0 is red; 0.33 is green; 0.66 is blue.\n            - If is_offset is True, add this value as the offset to the hue channel.\n    is_offset : boolean\n        Whether `hout` is added on HSV as offset or not. Default is True.\n    is_clip : boolean\n        If HSV value smaller than 0, set to 0. Default is True.\n    is_random : boolean\n        If True, randomly change hue. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random, add a random value between -0.2 and 0.2 as the offset to every hue values.\n\n    >>> im_hue = tl.prepro.adjust_hue(image, hout=0.2, is_offset=True, is_random=False)\n\n    Non-random, make all hue to green.\n\n    >>> im_green = tl.prepro.adjust_hue(image, hout=0.66, is_offset=False, is_random=False)\n\n    References\n    -----------\n    - `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n    - `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.\n    - `StackOverflow: Changing image hue with python PIL <https://stackoverflow.com/questions/7274221/changing-image-hue-with-python-pil>`__.\n\n    \"\"\"\n    hsv = rgb_to_hsv(im)\n    if is_random:\n        hout = np.random.uniform(-hout, hout)\n\n    if is_offset:\n        hsv[..., 0] += hout\n    else:\n        hsv[..., 0] = hout\n\n    if is_clip:\n        hsv[..., 0] = np.clip(hsv[..., 0], 0, np.inf)  # Hao : can remove green dots\n\n    rgb = hsv_to_rgb(hsv)\n    return rgb",
    "doc": "Adjust hue of an RGB image.\n\n    This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type.\n    For TF, see `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.and `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with values between 0 and 255.\n    hout : float\n        The scale value for adjusting hue.\n            - If is_offset is False, set all hue values to this value. 0 is red; 0.33 is green; 0.66 is blue.\n            - If is_offset is True, add this value as the offset to the hue channel.\n    is_offset : boolean\n        Whether `hout` is added on HSV as offset or not. Default is True.\n    is_clip : boolean\n        If HSV value smaller than 0, set to 0. Default is True.\n    is_random : boolean\n        If True, randomly change hue. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ---------\n    Random, add a random value between -0.2 and 0.2 as the offset to every hue values.\n\n    >>> im_hue = tl.prepro.adjust_hue(image, hout=0.2, is_offset=True, is_random=False)\n\n    Non-random, make all hue to green.\n\n    >>> im_green = tl.prepro.adjust_hue(image, hout=0.66, is_offset=False, is_random=False)\n\n    References\n    -----------\n    - `tf.image.random_hue <https://www.tensorflow.org/api_docs/python/tf/image/random_hue>`__.\n    - `tf.image.adjust_hue <https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue>`__.\n    - `StackOverflow: Changing image hue with python PIL <https://stackoverflow.com/questions/7274221/changing-image-hue-with-python-pil>`__."
  },
  {
    "code": "def imresize(x, size=None, interp='bicubic', mode=None):\n    \"\"\"Resize an image by given output size and method.\n\n    Warning, this function will rescale the value to [0, 255].\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    size : list of 2 int or None\n        For height and width.\n    interp : str\n        Interpolation method for re-sizing (`nearest`, `lanczos`, `bilinear`, `bicubic` (default) or `cubic`).\n    mode : str\n        The PIL image mode (`P`, `L`, etc.) to convert image before resizing.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    ------------\n    - `scipy.misc.imresize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html>`__\n\n    \"\"\"\n    if size is None:\n        size = [100, 100]\n\n    if x.shape[-1] == 1:\n        # greyscale\n        x = scipy.misc.imresize(x[:, :, 0], size, interp=interp, mode=mode)\n        return x[:, :, np.newaxis]\n    else:\n        # rgb, bgr, rgba\n        return scipy.misc.imresize(x, size, interp=interp, mode=mode)",
    "doc": "Resize an image by given output size and method.\n\n    Warning, this function will rescale the value to [0, 255].\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    size : list of 2 int or None\n        For height and width.\n    interp : str\n        Interpolation method for re-sizing (`nearest`, `lanczos`, `bilinear`, `bicubic` (default) or `cubic`).\n    mode : str\n        The PIL image mode (`P`, `L`, etc.) to convert image before resizing.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    References\n    ------------\n    - `scipy.misc.imresize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html>`__"
  },
  {
    "code": "def pixel_value_scale(im, val=0.9, clip=None, is_random=False):\n    \"\"\"Scales each value in the pixels of the image.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image.\n    val : float\n        The scale value for changing pixel value.\n            - If is_random=False, multiply this value with all pixels.\n            - If is_random=True, multiply a value between [1-val, 1+val] with all pixels.\n    clip : tuple of 2 numbers\n        The minimum and maximum value.\n    is_random : boolean\n        If True, see ``val``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ----------\n    Random\n\n    >>> im = pixel_value_scale(im, 0.1, [0, 255], is_random=True)\n\n    Non-random\n\n    >>> im = pixel_value_scale(im, 0.9, [0, 255], is_random=False)\n\n    \"\"\"\n\n    clip = clip if clip is not None else (-np.inf, np.inf)\n\n    if is_random:\n        scale = 1 + np.random.uniform(-val, val)\n        im = im * scale\n    else:\n        im = im * val\n\n    if len(clip) == 2:\n        im = np.clip(im, clip[0], clip[1])\n    else:\n        raise Exception(\"clip : tuple of 2 numbers\")\n\n    return im",
    "doc": "Scales each value in the pixels of the image.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image.\n    val : float\n        The scale value for changing pixel value.\n            - If is_random=False, multiply this value with all pixels.\n            - If is_random=True, multiply a value between [1-val, 1+val] with all pixels.\n    clip : tuple of 2 numbers\n        The minimum and maximum value.\n    is_random : boolean\n        If True, see ``val``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    ----------\n    Random\n\n    >>> im = pixel_value_scale(im, 0.1, [0, 255], is_random=True)\n\n    Non-random\n\n    >>> im = pixel_value_scale(im, 0.9, [0, 255], is_random=False)"
  },
  {
    "code": "def samplewise_norm(\n        x, rescale=None, samplewise_center=False, samplewise_std_normalization=False, channel_index=2, epsilon=1e-7\n):\n    \"\"\"Normalize an image by rescale, samplewise centering and samplewise centering in order.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rescale : float\n        Rescaling factor. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation)\n    samplewise_center : boolean\n        If True, set each sample mean to 0.\n    samplewise_std_normalization : boolean\n        If True, divide each input by its std.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> x = samplewise_norm(x, samplewise_center=True, samplewise_std_normalization=True)\n    >>> print(x.shape, np.mean(x), np.std(x))\n    (160, 176, 1), 0.0, 1.0\n\n    Notes\n    ------\n    When samplewise_center and samplewise_std_normalization are True.\n    - For greyscale image, every pixels are subtracted and divided by the mean and std of whole image.\n    - For RGB image, every pixels are subtracted and divided by the mean and std of this pixel i.e. the mean and std of a pixel is 0 and 1.\n\n    \"\"\"\n    if rescale:\n        x *= rescale\n\n    if x.shape[channel_index] == 1:\n        # greyscale\n        if samplewise_center:\n            x = x - np.mean(x)\n        if samplewise_std_normalization:\n            x = x / np.std(x)\n        return x\n    elif x.shape[channel_index] == 3:\n        # rgb\n        if samplewise_center:\n            x = x - np.mean(x, axis=channel_index, keepdims=True)\n        if samplewise_std_normalization:\n            x = x / (np.std(x, axis=channel_index, keepdims=True) + epsilon)\n        return x\n    else:\n        raise Exception(\"Unsupported channels %d\" % x.shape[channel_index])",
    "doc": "Normalize an image by rescale, samplewise centering and samplewise centering in order.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    rescale : float\n        Rescaling factor. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation)\n    samplewise_center : boolean\n        If True, set each sample mean to 0.\n    samplewise_std_normalization : boolean\n        If True, divide each input by its std.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    Examples\n    --------\n    >>> x = samplewise_norm(x, samplewise_center=True, samplewise_std_normalization=True)\n    >>> print(x.shape, np.mean(x), np.std(x))\n    (160, 176, 1), 0.0, 1.0\n\n    Notes\n    ------\n    When samplewise_center and samplewise_std_normalization are True.\n    - For greyscale image, every pixels are subtracted and divided by the mean and std of whole image.\n    - For RGB image, every pixels are subtracted and divided by the mean and std of this pixel i.e. the mean and std of a pixel is 0 and 1."
  },
  {
    "code": "def featurewise_norm(x, mean=None, std=None, epsilon=1e-7):\n    \"\"\"Normalize every pixels by the same given mean and std, which are usually\n    compute from all examples.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    mean : float\n        Value for subtraction.\n    std : float\n        Value for division.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    if mean:\n        x = x - mean\n    if std:\n        x = x / (std + epsilon)\n    return x",
    "doc": "Normalize every pixels by the same given mean and std, which are usually\n    compute from all examples.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    mean : float\n        Value for subtraction.\n    std : float\n        Value for division.\n    epsilon : float\n        A small position value for dividing standard deviation.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def get_zca_whitening_principal_components_img(X):\n    \"\"\"Return the ZCA whitening principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        Batch of images with dimension of [n_example, row, col, channel] (default).\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    flatX = np.reshape(X, (X.shape[0], X.shape[1] * X.shape[2] * X.shape[3]))\n    tl.logging.info(\"zca : computing sigma ..\")\n    sigma = np.dot(flatX.T, flatX) / flatX.shape[0]\n    tl.logging.info(\"zca : computing U, S and V ..\")\n    U, S, _ = linalg.svd(sigma)  # USV\n    tl.logging.info(\"zca : computing principal components ..\")\n    principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n    return principal_components",
    "doc": "Return the ZCA whitening principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        Batch of images with dimension of [n_example, row, col, channel] (default).\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def zca_whitening(x, principal_components):\n    \"\"\"Apply ZCA whitening on an image by given principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    principal_components : matrix\n        Matrix from ``get_zca_whitening_principal_components_img``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    flatx = np.reshape(x, (x.size))\n    # tl.logging.info(principal_components.shape, x.shape)  # ((28160, 28160), (160, 176, 1))\n    # flatx = np.reshape(x, (x.shape))\n    # flatx = np.reshape(x, (x.shape[0], ))\n    # tl.logging.info(flatx.shape)  # (160, 176, 1)\n    whitex = np.dot(flatx, principal_components)\n    x = np.reshape(whitex, (x.shape[0], x.shape[1], x.shape[2]))\n    return x",
    "doc": "Apply ZCA whitening on an image by given principal components matrix.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    principal_components : matrix\n        Matrix from ``get_zca_whitening_principal_components_img``.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def channel_shift(x, intensity, is_random=False, channel_index=2):\n    \"\"\"Shift the channels of an image, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Intensity of shifting.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    channel_index : int\n        Index of channel. Default is 2.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    if is_random:\n        factor = np.random.uniform(-intensity, intensity)\n    else:\n        factor = intensity\n    x = np.rollaxis(x, channel_index, 0)\n    min_x, max_x = np.min(x), np.max(x)\n    channel_images = [np.clip(x_channel + factor, min_x, max_x) for x_channel in x]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_index + 1)\n    return x",
    "doc": "Shift the channels of an image, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    intensity : float\n        Intensity of shifting.\n    is_random : boolean\n        If True, randomly shift. Default is False.\n    channel_index : int\n        Index of channel. Default is 2.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def channel_shift_multi(x, intensity, is_random=False, channel_index=2):\n    \"\"\"Shift the channels of images with the same arguments, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.channel_shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images.\n\n    \"\"\"\n    if is_random:\n        factor = np.random.uniform(-intensity, intensity)\n    else:\n        factor = intensity\n\n    results = []\n    for data in x:\n        data = np.rollaxis(data, channel_index, 0)\n        min_x, max_x = np.min(data), np.max(data)\n        channel_images = [np.clip(x_channel + factor, min_x, max_x) for x_channel in x]\n        data = np.stack(channel_images, axis=0)\n        data = np.rollaxis(x, 0, channel_index + 1)\n        results.append(data)\n    return np.asarray(results)",
    "doc": "Shift the channels of images with the same arguments, randomly or non-randomly, see `numpy.rollaxis <https://docs.scipy.org/doc/numpy/reference/generated/numpy.rollaxis.html>`__.\n    Usually be used for image segmentation which x=[X, Y], X and Y should be matched.\n\n    Parameters\n    -----------\n    x : list of numpy.array\n        List of images with dimension of [n_images, row, col, channel] (default).\n    others : args\n        See ``tl.prepro.channel_shift``.\n\n    Returns\n    -------\n    numpy.array\n        A list of processed images."
  },
  {
    "code": "def drop(x, keep=0.5):\n    \"\"\"Randomly set some pixels to zero by a given keeping probability.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] or [row, col].\n    keep : float\n        The keeping probability (0, 1), the lower more values will be set to zero.\n\n    Returns\n    -------\n    numpy.array\n        A processed image.\n\n    \"\"\"\n    if len(x.shape) == 3:\n        if x.shape[-1] == 3:  # color\n            img_size = x.shape\n            mask = np.random.binomial(n=1, p=keep, size=x.shape[:-1])\n            for i in range(3):\n                x[:, :, i] = np.multiply(x[:, :, i], mask)\n        elif x.shape[-1] == 1:  # greyscale image\n            img_size = x.shape\n            x = np.multiply(x, np.random.binomial(n=1, p=keep, size=img_size))\n        else:\n            raise Exception(\"Unsupported shape {}\".format(x.shape))\n    elif len(x.shape) == 2 or 1:  # greyscale matrix (image) or vector\n        img_size = x.shape\n        x = np.multiply(x, np.random.binomial(n=1, p=keep, size=img_size))\n    else:\n        raise Exception(\"Unsupported shape {}\".format(x.shape))\n    return x",
    "doc": "Randomly set some pixels to zero by a given keeping probability.\n\n    Parameters\n    -----------\n    x : numpy.array\n        An image with dimension of [row, col, channel] or [row, col].\n    keep : float\n        The keeping probability (0, 1), the lower more values will be set to zero.\n\n    Returns\n    -------\n    numpy.array\n        A processed image."
  },
  {
    "code": "def array_to_img(x, dim_ordering=(0, 1, 2), scale=True):\n    \"\"\"Converts a numpy array to PIL image object (uint8 format).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of 3 and channels of 1 or 3.\n    dim_ordering : tuple of 3 int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    scale : boolean\n        If True, converts image to [0, 255] from any range of value like [-1, 2]. Default is True.\n\n    Returns\n    -------\n    PIL.image\n        An image.\n\n    References\n    -----------\n    `PIL Image.fromarray <http://pillow.readthedocs.io/en/3.1.x/reference/Image.html?highlight=fromarray>`__\n\n    \"\"\"\n    # if dim_ordering == 'default':\n    #     dim_ordering = K.image_dim_ordering()\n    # if dim_ordering == 'th':  # theano\n    #     x = x.transpose(1, 2, 0)\n\n    x = x.transpose(dim_ordering)\n\n    if scale:\n        x += max(-np.min(x), 0)\n        x_max = np.max(x)\n        if x_max != 0:\n            # tl.logging.info(x_max)\n            # x /= x_max\n            x = x / x_max\n        x *= 255\n\n    if x.shape[2] == 3:\n        # RGB\n        return PIL.Image.fromarray(x.astype('uint8'), 'RGB')\n\n    elif x.shape[2] == 1:\n        # grayscale\n        return PIL.Image.fromarray(x[:, :, 0].astype('uint8'), 'L')\n\n    else:\n        raise Exception('Unsupported channel number: ', x.shape[2])",
    "doc": "Converts a numpy array to PIL image object (uint8 format).\n\n    Parameters\n    ----------\n    x : numpy.array\n        An image with dimension of 3 and channels of 1 or 3.\n    dim_ordering : tuple of 3 int\n        Index of row, col and channel, default (0, 1, 2), for theano (1, 2, 0).\n    scale : boolean\n        If True, converts image to [0, 255] from any range of value like [-1, 2]. Default is True.\n\n    Returns\n    -------\n    PIL.image\n        An image.\n\n    References\n    -----------\n    `PIL Image.fromarray <http://pillow.readthedocs.io/en/3.1.x/reference/Image.html?highlight=fromarray>`__"
  },
  {
    "code": "def find_contours(x, level=0.8, fully_connected='low', positive_orientation='low'):\n    \"\"\"Find iso-valued contours in a 2D array for a given level value, returns list of (n, 2)-ndarrays\n    see `skimage.measure.find_contours <http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours>`__.\n\n    Parameters\n    ------------\n    x : 2D ndarray of double.\n        Input data in which to find contours.\n    level : float\n        Value along which to find contours in the array.\n    fully_connected : str\n        Either `low` or `high`. Indicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)\n    positive_orientation : str\n        Either `low` or `high`. Indicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If `low` then contours will wind counter-clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour.\n\n    Returns\n    --------\n    list of (n,2)-ndarrays\n        Each contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.\n\n    \"\"\"\n    return skimage.measure.find_contours(\n        x, level, fully_connected=fully_connected, positive_orientation=positive_orientation\n    )",
    "doc": "Find iso-valued contours in a 2D array for a given level value, returns list of (n, 2)-ndarrays\n    see `skimage.measure.find_contours <http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours>`__.\n\n    Parameters\n    ------------\n    x : 2D ndarray of double.\n        Input data in which to find contours.\n    level : float\n        Value along which to find contours in the array.\n    fully_connected : str\n        Either `low` or `high`. Indicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)\n    positive_orientation : str\n        Either `low` or `high`. Indicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If `low` then contours will wind counter-clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour.\n\n    Returns\n    --------\n    list of (n,2)-ndarrays\n        Each contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour."
  },
  {
    "code": "def pt2map(list_points=None, size=(100, 100), val=1):\n    \"\"\"Inputs a list of points, return a 2D image.\n\n    Parameters\n    --------------\n    list_points : list of 2 int\n        [[x, y], [x, y]..] for point coordinates.\n    size : tuple of 2 int\n        (w, h) for output size.\n    val : float or int\n        For the contour value.\n\n    Returns\n    -------\n    numpy.array\n        An image.\n\n    \"\"\"\n    if list_points is None:\n        raise Exception(\"list_points : list of 2 int\")\n    i_m = np.zeros(size)\n    if len(list_points) == 0:\n        return i_m\n    for xx in list_points:\n        for x in xx:\n            # tl.logging.info(x)\n            i_m[int(np.round(x[0]))][int(np.round(x[1]))] = val\n    return i_m",
    "doc": "Inputs a list of points, return a 2D image.\n\n    Parameters\n    --------------\n    list_points : list of 2 int\n        [[x, y], [x, y]..] for point coordinates.\n    size : tuple of 2 int\n        (w, h) for output size.\n    val : float or int\n        For the contour value.\n\n    Returns\n    -------\n    numpy.array\n        An image."
  },
  {
    "code": "def binary_dilation(x, radius=3):\n    \"\"\"Return fast binary morphological dilation of an image.\n    see `skimage.morphology.binary_dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image.\n\n    \"\"\"\n    mask = disk(radius)\n    x = _binary_dilation(x, selem=mask)\n\n    return x",
    "doc": "Return fast binary morphological dilation of an image.\n    see `skimage.morphology.binary_dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image."
  },
  {
    "code": "def dilation(x, radius=3):\n    \"\"\"Return greyscale morphological dilation of an image,\n    see `skimage.morphology.dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        An greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image.\n\n    \"\"\"\n    mask = disk(radius)\n    x = dilation(x, selem=mask)\n\n    return x",
    "doc": "Return greyscale morphological dilation of an image,\n    see `skimage.morphology.dilation <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.dilation>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        An greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image."
  },
  {
    "code": "def binary_erosion(x, radius=3):\n    \"\"\"Return binary morphological erosion of an image,\n    see `skimage.morphology.binary_erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image.\n\n    \"\"\"\n    mask = disk(radius)\n    x = _binary_erosion(x, selem=mask)\n    return x",
    "doc": "Return binary morphological erosion of an image,\n    see `skimage.morphology.binary_erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.binary_erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A binary image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed binary image."
  },
  {
    "code": "def erosion(x, radius=3):\n    \"\"\"Return greyscale morphological erosion of an image,\n    see `skimage.morphology.erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image.\n\n    \"\"\"\n    mask = disk(radius)\n    x = _erosion(x, selem=mask)\n    return x",
    "doc": "Return greyscale morphological erosion of an image,\n    see `skimage.morphology.erosion <http://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.erosion>`__.\n\n    Parameters\n    -----------\n    x : 2D array\n        A greyscale image.\n    radius : int\n        For the radius of mask.\n\n    Returns\n    -------\n    numpy.array\n        A processed greyscale image."
  },
  {
    "code": "def obj_box_coords_rescale(coords=None, shape=None):\n    \"\"\"Scale down a list of coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n\n    Parameters\n    ------------\n    coords : list of list of 4 ints or None\n        For coordinates of more than one images .e.g.[[x, y, w, h], [x, y, w, h], ...].\n    shape : list of 2 int or None\n        \u3010height, width].\n\n    Returns\n    -------\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n\n    Examples\n    ---------\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50], [10, 10, 20, 20]], shape=[100, 100])\n    >>> print(coords)\n      [[0.3, 0.4, 0.5, 0.5], [0.1, 0.1, 0.2, 0.2]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[50, 100])\n    >>> print(coords)\n      [[0.3, 0.8, 0.5, 1.0]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[100, 200])\n    >>> print(coords)\n      [[0.15, 0.4, 0.25, 0.5]]\n\n    Returns\n    -------\n    list of 4 numbers\n        New coordinates.\n\n    \"\"\"\n    if coords is None:\n        coords = []\n    if shape is None:\n        shape = [100, 200]\n\n    imh, imw = shape[0], shape[1]\n    imh = imh * 1.0  # * 1.0 for python2 : force division to be float point\n    imw = imw * 1.0\n    coords_new = list()\n    for coord in coords:\n\n        if len(coord) != 4:\n            raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n        x = coord[0] / imw\n        y = coord[1] / imh\n        w = coord[2] / imw\n        h = coord[3] / imh\n        coords_new.append([x, y, w, h])\n    return coords_new",
    "doc": "Scale down a list of coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n\n    Parameters\n    ------------\n    coords : list of list of 4 ints or None\n        For coordinates of more than one images .e.g.[[x, y, w, h], [x, y, w, h], ...].\n    shape : list of 2 int or None\n        \u3010height, width].\n\n    Returns\n    -------\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n\n    Examples\n    ---------\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50], [10, 10, 20, 20]], shape=[100, 100])\n    >>> print(coords)\n      [[0.3, 0.4, 0.5, 0.5], [0.1, 0.1, 0.2, 0.2]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[50, 100])\n    >>> print(coords)\n      [[0.3, 0.8, 0.5, 1.0]]\n    >>> coords = obj_box_coords_rescale(coords=[[30, 40, 50, 50]], shape=[100, 200])\n    >>> print(coords)\n      [[0.15, 0.4, 0.25, 0.5]]\n\n    Returns\n    -------\n    list of 4 numbers\n        New coordinates."
  },
  {
    "code": "def obj_box_coord_rescale(coord=None, shape=None):\n    \"\"\"Scale down one coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n    It is the reverse process of ``obj_box_coord_scale_to_pixelunit``.\n\n    Parameters\n    ------------\n    coords : list of 4 int or None\n        One coordinates of one image e.g. [x, y, w, h].\n    shape : list of 2 int or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = tl.prepro.obj_box_coord_rescale(coord=[30, 40, 50, 50], shape=[100, 100])\n      [0.3, 0.4, 0.5, 0.5]\n\n    \"\"\"\n    if coord is None:\n        coord = []\n    if shape is None:\n        shape = [100, 200]\n\n    return obj_box_coords_rescale(coords=[coord], shape=shape)[0]",
    "doc": "Scale down one coordinates from pixel unit to the ratio of image size i.e. in the range of [0, 1].\n    It is the reverse process of ``obj_box_coord_scale_to_pixelunit``.\n\n    Parameters\n    ------------\n    coords : list of 4 int or None\n        One coordinates of one image e.g. [x, y, w, h].\n    shape : list of 2 int or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = tl.prepro.obj_box_coord_rescale(coord=[30, 40, 50, 50], shape=[100, 100])\n      [0.3, 0.4, 0.5, 0.5]"
  },
  {
    "code": "def obj_box_coord_scale_to_pixelunit(coord, shape=None):\n    \"\"\"Convert one coordinate [x, y, w (or x2), h (or y2)] in ratio format to image coordinate format.\n    It is the reverse process of ``obj_box_coord_rescale``.\n\n    Parameters\n    -----------\n    coord : list of 4 float\n        One coordinate of one image [x, y, w (or x2), h (or y2)] in ratio format, i.e value range [0~1].\n    shape : tuple of 2 or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([0.2, 0.3, 0.5, 0.7], shape=(100, 200, 3))\n      [40, 30, 100, 70]\n\n    \"\"\"\n    if shape is None:\n        shape = [100, 100]\n\n    imh, imw = shape[0:2]\n    x = int(coord[0] * imw)\n    x2 = int(coord[2] * imw)\n    y = int(coord[1] * imh)\n    y2 = int(coord[3] * imh)\n    return [x, y, x2, y2]",
    "doc": "Convert one coordinate [x, y, w (or x2), h (or y2)] in ratio format to image coordinate format.\n    It is the reverse process of ``obj_box_coord_rescale``.\n\n    Parameters\n    -----------\n    coord : list of 4 float\n        One coordinate of one image [x, y, w (or x2), h (or y2)] in ratio format, i.e value range [0~1].\n    shape : tuple of 2 or None\n        For [height, width].\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([0.2, 0.3, 0.5, 0.7], shape=(100, 200, 3))\n      [40, 30, 100, 70]"
  },
  {
    "code": "def obj_box_coord_centroid_to_upleft_butright(coord, to_int=False):\n    \"\"\"Convert one coordinate [x_center, y_center, w, h] to [x1, y1, x2, y2] in up-left and botton-right format.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n    to_int : boolean\n        Whether to convert output as integer.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = obj_box_coord_centroid_to_upleft_butright([30, 40, 20, 20])\n      [20, 30, 40, 50]\n\n    \"\"\"\n    if len(coord) != 4:\n        raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n    x_center, y_center, w, h = coord\n    x = x_center - w / 2.\n    y = y_center - h / 2.\n    x2 = x + w\n    y2 = y + h\n    if to_int:\n        return [int(x), int(y), int(x2), int(y2)]\n    else:\n        return [x, y, x2, y2]",
    "doc": "Convert one coordinate [x_center, y_center, w, h] to [x1, y1, x2, y2] in up-left and botton-right format.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n    to_int : boolean\n        Whether to convert output as integer.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    Examples\n    ---------\n    >>> coord = obj_box_coord_centroid_to_upleft_butright([30, 40, 20, 20])\n      [20, 30, 40, 50]"
  },
  {
    "code": "def obj_box_coord_upleft_butright_to_centroid(coord):\n    \"\"\"Convert one coordinate [x1, y1, x2, y2] to [x_center, y_center, w, h].\n    It is the reverse process of ``obj_box_coord_centroid_to_upleft_butright``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    \"\"\"\n    if len(coord) != 4:\n        raise AssertionError(\"coordinate should be 4 values : [x1, y1, x2, y2]\")\n    x1, y1, x2, y2 = coord\n    w = x2 - x1\n    h = y2 - y1\n    x_c = x1 + w / 2.\n    y_c = y1 + h / 2.\n    return [x_c, y_c, w, h]",
    "doc": "Convert one coordinate [x1, y1, x2, y2] to [x_center, y_center, w, h].\n    It is the reverse process of ``obj_box_coord_centroid_to_upleft_butright``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box."
  },
  {
    "code": "def obj_box_coord_centroid_to_upleft(coord):\n    \"\"\"Convert one coordinate [x_center, y_center, w, h] to [x, y, w, h].\n    It is the reverse process of ``obj_box_coord_upleft_to_centroid``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box.\n\n    \"\"\"\n    if len(coord) != 4:\n        raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n    x_center, y_center, w, h = coord\n    x = x_center - w / 2.\n    y = y_center - h / 2.\n    return [x, y, w, h]",
    "doc": "Convert one coordinate [x_center, y_center, w, h] to [x, y, w, h].\n    It is the reverse process of ``obj_box_coord_upleft_to_centroid``.\n\n    Parameters\n    ------------\n    coord : list of 4 int/float\n        One coordinate.\n\n    Returns\n    -------\n    list of 4 numbers\n        New bounding box."
  },
  {
    "code": "def parse_darknet_ann_str_to_list(annotations):\n    r\"\"\"Input string format of class, x, y, w, h, return list of list format.\n\n    Parameters\n    -----------\n    annotations : str\n        The annotations in darkent format \"class, x, y, w, h ....\" seperated by \"\\\\n\".\n\n    Returns\n    -------\n    list of list of 4 numbers\n        List of bounding box.\n\n    \"\"\"\n    annotations = annotations.split(\"\\n\")\n    ann = []\n    for a in annotations:\n        a = a.split()\n        if len(a) == 5:\n            for i, _v in enumerate(a):\n                if i == 0:\n                    a[i] = int(a[i])\n                else:\n                    a[i] = float(a[i])\n            ann.append(a)\n    return ann",
    "doc": "r\"\"\"Input string format of class, x, y, w, h, return list of list format.\n\n    Parameters\n    -----------\n    annotations : str\n        The annotations in darkent format \"class, x, y, w, h ....\" seperated by \"\\\\n\".\n\n    Returns\n    -------\n    list of list of 4 numbers\n        List of bounding box."
  },
  {
    "code": "def parse_darknet_ann_list_to_cls_box(annotations):\n    \"\"\"Parse darknet annotation format into two lists for class and bounding box.\n\n    Input list of [[class, x, y, w, h], ...], return two list of [class ...] and [[x, y, w, h], ...].\n\n    Parameters\n    ------------\n    annotations : list of list\n        A list of class and bounding boxes of images e.g. [[class, x, y, w, h], ...]\n\n    Returns\n    -------\n    list of int\n        List of class labels.\n\n    list of list of 4 numbers\n        List of bounding box.\n\n    \"\"\"\n    class_list = []\n    bbox_list = []\n    for ann in annotations:\n        class_list.append(ann[0])\n        bbox_list.append(ann[1:])\n    return class_list, bbox_list",
    "doc": "Parse darknet annotation format into two lists for class and bounding box.\n\n    Input list of [[class, x, y, w, h], ...], return two list of [class ...] and [[x, y, w, h], ...].\n\n    Parameters\n    ------------\n    annotations : list of list\n        A list of class and bounding boxes of images e.g. [[class, x, y, w, h], ...]\n\n    Returns\n    -------\n    list of int\n        List of class labels.\n\n    list of list of 4 numbers\n        List of bounding box."
  },
  {
    "code": "def obj_box_horizontal_flip(im, coords=None, is_rescale=False, is_center=False, is_random=False):\n    \"\"\"Left-right flip the image and coordinates for object detection.\n\n    Parameters\n    ----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100])    # as an image with shape width=100, height=80\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3], [0.1, 0.5, 0.2, 0.3]], is_rescale=True, is_center=True, is_random=False)\n    >>> print(coords)\n      [[0.8, 0.4, 0.3, 0.3], [0.9, 0.5, 0.2, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3]], is_rescale=True, is_center=False, is_random=False)\n    >>> print(coords)\n      [[0.5, 0.4, 0.3, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=True, is_random=False)\n    >>> print(coords)\n      [[80, 40, 30, 30]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=False, is_random=False)\n    >>> print(coords)\n      [[50, 40, 30, 30]]\n\n    \"\"\"\n    if coords is None:\n        coords = []\n\n    def _flip(im, coords):\n        im = flip_axis(im, axis=1, is_random=False)\n        coords_new = list()\n\n        for coord in coords:\n\n            if len(coord) != 4:\n                raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n            if is_rescale:\n                if is_center:\n                    # x_center' = 1 - x\n                    x = 1. - coord[0]\n                else:\n                    # x_center' = 1 - x - w\n                    x = 1. - coord[0] - coord[2]\n            else:\n                if is_center:\n                    # x' = im.width - x\n                    x = im.shape[1] - coord[0]\n                else:\n                    # x' = im.width - x - w\n                    x = im.shape[1] - coord[0] - coord[2]\n            coords_new.append([x, coord[1], coord[2], coord[3]])\n        return im, coords_new\n\n    if is_random:\n        factor = np.random.uniform(-1, 1)\n        if factor > 0:\n            return _flip(im, coords)\n        else:\n            return im, coords\n    else:\n        return _flip(im, coords)",
    "doc": "Left-right flip the image and coordinates for object detection.\n\n    Parameters\n    ----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    is_random : boolean\n        If True, randomly flip. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100])    # as an image with shape width=100, height=80\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3], [0.1, 0.5, 0.2, 0.3]], is_rescale=True, is_center=True, is_random=False)\n    >>> print(coords)\n      [[0.8, 0.4, 0.3, 0.3], [0.9, 0.5, 0.2, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[0.2, 0.4, 0.3, 0.3]], is_rescale=True, is_center=False, is_random=False)\n    >>> print(coords)\n      [[0.5, 0.4, 0.3, 0.3]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=True, is_random=False)\n    >>> print(coords)\n      [[80, 40, 30, 30]]\n    >>> im, coords = obj_box_left_right_flip(im, coords=[[20, 40, 30, 30]], is_rescale=False, is_center=False, is_random=False)\n    >>> print(coords)\n      [[50, 40, 30, 30]]"
  },
  {
    "code": "def obj_box_imresize(im, coords=None, size=None, interp='bicubic', mode=None, is_rescale=False):\n    \"\"\"Resize an image, and compute the new bounding box coordinates.\n\n    Parameters\n    -------------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    size interp and mode : args\n        See ``tl.prepro.imresize``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1], then return the original coordinates. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100, 3])    # as an image with shape width=100, height=80\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30], [10, 20, 20, 20]], size=[160, 200], is_rescale=False)\n    >>> print(coords)\n      [[40, 80, 60, 60], [20, 40, 40, 40]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[40, 100], is_rescale=False)\n    >>> print(coords)\n      [[20, 20, 30, 15]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[60, 150], is_rescale=False)\n    >>> print(coords)\n      [[30, 30, 45, 22]]\n    >>> im2, coords = obj_box_imresize(im, coords=[[0.2, 0.4, 0.3, 0.3]], size=[160, 200], is_rescale=True)\n    >>> print(coords, im2.shape)\n      [[0.2, 0.4, 0.3, 0.3]] (160, 200, 3)\n\n    \"\"\"\n    if coords is None:\n        coords = []\n    if size is None:\n        size = [100, 100]\n\n    imh, imw = im.shape[0:2]\n    imh = imh * 1.0  # * 1.0 for python2 : force division to be float point\n    imw = imw * 1.0\n    im = imresize(im, size=size, interp=interp, mode=mode)\n\n    if is_rescale is False:\n        coords_new = list()\n\n        for coord in coords:\n\n            if len(coord) != 4:\n                raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n            # x' = x * (imw'/imw)\n            x = int(coord[0] * (size[1] / imw))\n            # y' = y * (imh'/imh)\n            # tl.logging.info('>>', coord[1], size[0], imh)\n            y = int(coord[1] * (size[0] / imh))\n            # w' = w * (imw'/imw)\n            w = int(coord[2] * (size[1] / imw))\n            # h' = h * (imh'/imh)\n            h = int(coord[3] * (size[0] / imh))\n            coords_new.append([x, y, w, h])\n        return im, coords_new\n    else:\n        return im, coords",
    "doc": "Resize an image, and compute the new bounding box coordinates.\n\n    Parameters\n    -------------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    size interp and mode : args\n        See ``tl.prepro.imresize``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1], then return the original coordinates. Default is False.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    Examples\n    --------\n    >>> im = np.zeros([80, 100, 3])    # as an image with shape width=100, height=80\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30], [10, 20, 20, 20]], size=[160, 200], is_rescale=False)\n    >>> print(coords)\n      [[40, 80, 60, 60], [20, 40, 40, 40]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[40, 100], is_rescale=False)\n    >>> print(coords)\n      [[20, 20, 30, 15]]\n    >>> _, coords = obj_box_imresize(im, coords=[[20, 40, 30, 30]], size=[60, 150], is_rescale=False)\n    >>> print(coords)\n      [[30, 30, 45, 22]]\n    >>> im2, coords = obj_box_imresize(im, coords=[[0.2, 0.4, 0.3, 0.3]], size=[160, 200], is_rescale=True)\n    >>> print(coords, im2.shape)\n      [[0.2, 0.4, 0.3, 0.3]] (160, 200, 3)"
  },
  {
    "code": "def obj_box_crop(\n        im, classes=None, coords=None, wrg=100, hrg=100, is_rescale=False, is_center=False, is_random=False,\n        thresh_wh=0.02, thresh_wh2=12.\n):\n    \"\"\"Randomly or centrally crop an image, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg hrg and is_random : args\n        See ``tl.prepro.crop``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean, default False\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    \"\"\"\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    h, w = im.shape[0], im.shape[1]\n\n    if (h <= hrg) or (w <= wrg):\n        raise AssertionError(\"The size of cropping should smaller than the original image\")\n\n    if is_random:\n        h_offset = int(np.random.uniform(0, h - hrg) - 1)\n        w_offset = int(np.random.uniform(0, w - wrg) - 1)\n        h_end = hrg + h_offset\n        w_end = wrg + w_offset\n        im_new = im[h_offset:h_end, w_offset:w_end]\n    else:  # central crop\n        h_offset = int(np.floor((h - hrg) / 2.))\n        w_offset = int(np.floor((w - wrg) / 2.))\n        h_end = h_offset + hrg\n        w_end = w_offset + wrg\n        im_new = im[h_offset:h_end, w_offset:w_end]\n\n    #              w\n    #   _____________________________\n    #   |  h/w offset               |\n    #   |       -------             |\n    # h |       |     |             |\n    #   |       |     |             |\n    #   |       -------             |\n    #   |            h/w end        |\n    #   |___________________________|\n\n    def _get_coord(coord):\n        \"\"\"Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        \"\"\"\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        ##======= pixel unit format and upleft, w, h ==========##\n\n        # x = np.clip( coord[0] - w_offset, 0, w_end - w_offset)\n        # y = np.clip( coord[1] - h_offset, 0, h_end - h_offset)\n        # w = np.clip( coord[2]           , 0, w_end - w_offset)\n        # h = np.clip( coord[3]           , 0, h_end - h_offset)\n\n        x = coord[0] - w_offset\n        y = coord[1] - h_offset\n        w = coord[2]\n        h = coord[3]\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info('xx', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info('yy', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        ## convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new",
    "doc": "Randomly or centrally crop an image, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg hrg and is_random : args\n        See ``tl.prepro.crop``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean, default False\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes."
  },
  {
    "code": "def obj_box_shift(\n        im, classes=None, coords=None, wrg=0.1, hrg=0.1, row_index=0, col_index=1, channel_index=2, fill_mode='nearest',\n        cval=0., order=1, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02, thresh_wh2=12.\n):\n    \"\"\"Shift an image randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg, hrg row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.shift``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    \"\"\"\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    imh, imw = im.shape[row_index], im.shape[col_index]\n\n    if (hrg >= 1.0) and (hrg <= 0.) and (wrg >= 1.0) and (wrg <= 0.):\n        raise AssertionError(\"shift range should be (0, 1)\")\n\n    if is_random:\n        tx = np.random.uniform(-hrg, hrg) * imh\n        ty = np.random.uniform(-wrg, wrg) * imw\n    else:\n        tx, ty = hrg * imh, wrg * imw\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    im_new = affine_transform(im, transform_matrix, channel_index, fill_mode, cval, order)\n\n    # modified from obj_box_crop\n    def _get_coord(coord):\n        \"\"\"Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        \"\"\"\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        ##======= pixel unit format and upleft, w, h ==========##\n        x = coord[0] - ty  # only change this\n        y = coord[1] - tx  # only change this\n        w = coord[2]\n        h = coord[3]\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info('xx', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info('yy', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        ## convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new",
    "doc": "Shift an image randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...]\n    wrg, hrg row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.shift``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes."
  },
  {
    "code": "def obj_box_zoom(\n        im, classes=None, coords=None, zoom_range=(0.9,\n                                                   1.1), row_index=0, col_index=1, channel_index=2, fill_mode='nearest',\n        cval=0., order=1, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02, thresh_wh2=12.\n):\n    \"\"\"Zoom in and out of a single image, randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    zoom_range row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.zoom``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid. (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes.\n\n    \"\"\"\n    if classes is None:\n        classes = []\n    if coords is None:\n        coords = []\n\n    if len(zoom_range) != 2:\n        raise Exception('zoom_range should be a tuple or list of two floats. ' 'Received arg: ', zoom_range)\n    if is_random:\n        if zoom_range[0] == 1 and zoom_range[1] == 1:\n            zx, zy = 1, 1\n            tl.logging.info(\" random_zoom : not zoom in/out\")\n        else:\n            zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n    else:\n        zx, zy = zoom_range\n    # tl.logging.info(zx, zy)\n    zoom_matrix = np.array([[zx, 0, 0], [0, zy, 0], [0, 0, 1]])\n\n    h, w = im.shape[row_index], im.shape[col_index]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    im_new = affine_transform(im, transform_matrix, channel_index, fill_mode, cval, order)\n\n    # modified from obj_box_crop\n    def _get_coord(coord):\n        \"\"\"Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,\n        before getting the new coordinates.\n        Boxes outsides the cropped image will be removed.\n\n        \"\"\"\n        if is_center:\n            coord = obj_box_coord_centroid_to_upleft(coord)\n\n        # ======= pixel unit format and upleft, w, h ==========\n        x = (coord[0] - im.shape[1] / 2) / zy + im.shape[1] / 2  # only change this\n        y = (coord[1] - im.shape[0] / 2) / zx + im.shape[0] / 2  # only change this\n        w = coord[2] / zy  # only change this\n        h = coord[3] / zx  # only change thisS\n\n        if x < 0:\n            if x + w <= 0:\n                return None\n            w = w + x\n            x = 0\n        elif x > im_new.shape[1]:  # object outside the cropped image\n            return None\n\n        if y < 0:\n            if y + h <= 0:\n                return None\n            h = h + y\n            y = 0\n        elif y > im_new.shape[0]:  # object outside the cropped image\n            return None\n\n        if (x is not None) and (x + w > im_new.shape[1]):  # box outside the cropped image\n            w = im_new.shape[1] - x\n\n        if (y is not None) and (y + h > im_new.shape[0]):  # box outside the cropped image\n            h = im_new.shape[0] - y\n\n        if (w / (h + 1.) > thresh_wh2) or (h / (w + 1.) > thresh_wh2):  # object shape strange: too narrow\n            # tl.logging.info('xx', w, h)\n            return None\n\n        if (w / (im_new.shape[1] * 1.) < thresh_wh) or (h / (im_new.shape[0] * 1.) <\n                                                        thresh_wh):  # object shape strange: too narrow\n            # tl.logging.info('yy', w, im_new.shape[1], h, im_new.shape[0])\n            return None\n\n        coord = [x, y, w, h]\n\n        # convert back if input format is center.\n        if is_center:\n            coord = obj_box_coord_upleft_to_centroid(coord)\n\n        return coord\n\n    coords_new = list()\n    classes_new = list()\n    for i, _ in enumerate(coords):\n        coord = coords[i]\n\n        if len(coord) != 4:\n            raise AssertionError(\"coordinate should be 4 values : [x, y, w, h]\")\n\n        if is_rescale:\n            # for scaled coord, upscaled before process and scale back in the end.\n            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)\n            coord = _get_coord(coord)\n            if coord is not None:\n                coord = obj_box_coord_rescale(coord, im_new.shape)\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n        else:\n            coord = _get_coord(coord)\n            if coord is not None:\n                coords_new.append(coord)\n                classes_new.append(classes[i])\n    return im_new, classes_new, coords_new",
    "doc": "Zoom in and out of a single image, randomly or non-randomly, and compute the new bounding box coordinates.\n    Objects outside the cropped image will be removed.\n\n    Parameters\n    -----------\n    im : numpy.array\n        An image with dimension of [row, col, channel] (default).\n    classes : list of int or None\n        Class IDs.\n    coords : list of list of 4 int/float or None\n        Coordinates [[x, y, w, h], [x, y, w, h], ...].\n    zoom_range row_index col_index channel_index is_random fill_mode cval and order : see ``tl.prepro.zoom``.\n    is_rescale : boolean\n        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.\n    is_center : boolean\n        Set to True, if the x and y of coordinates are the centroid. (i.e. darknet format). Default is False.\n    thresh_wh : float\n        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.\n    thresh_wh2 : float\n        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.\n\n    Returns\n    -------\n    numpy.array\n        A processed image\n    list of int\n        A list of classes\n    list of list of 4 numbers\n        A list of new bounding boxes."
  },
  {
    "code": "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.):\n    \"\"\"Pads each sequence to the same length:\n    the length of the longest sequence.\n    If maxlen is provided, any sequence longer\n    than maxlen is truncated to maxlen.\n    Truncation happens off either the beginning (default) or\n    the end of the sequence.\n    Supports post-padding and pre-padding (default).\n\n    Parameters\n    ----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    maxlen : int\n        Maximum length.\n    dtype : numpy.dtype or str\n        Data type to cast the resulting sequence.\n    padding : str\n        Either 'pre' or 'post', pad either before or after each sequence.\n    truncating : str\n        Either 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence\n    value : float\n        Value to pad the sequences to the desired value.\n\n    Returns\n    ----------\n    x : numpy.array\n        With dimensions (number_of_sequences, maxlen)\n\n    Examples\n    ----------\n    >>> sequences = [[1,1,1,1,1],[2,2,2],[3,3]]\n    >>> sequences = pad_sequences(sequences, maxlen=None, dtype='int32',\n    ...                  padding='post', truncating='pre', value=0.)\n    [[1 1 1 1 1]\n     [2 2 2 0 0]\n     [3 3 0 0 0]]\n\n    \"\"\"\n    lengths = [len(s) for s in sequences]\n\n    nb_samples = len(sequences)\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = tuple()\n    for s in sequences:\n        if len(s) > 0:\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n    for idx, s in enumerate(sequences):\n        if len(s) == 0:\n            continue  # empty list was found\n        if truncating == 'pre':\n            trunc = s[-maxlen:]\n        elif truncating == 'post':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\n                'Shape of sample %s of sequence at position %s is different from expected shape %s' %\n                (trunc.shape[1:], idx, sample_shape)\n            )\n\n        if padding == 'post':\n            x[idx, :len(trunc)] = trunc\n        elif padding == 'pre':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError('Padding type \"%s\" not understood' % padding)\n    return x.tolist()",
    "doc": "Pads each sequence to the same length:\n    the length of the longest sequence.\n    If maxlen is provided, any sequence longer\n    than maxlen is truncated to maxlen.\n    Truncation happens off either the beginning (default) or\n    the end of the sequence.\n    Supports post-padding and pre-padding (default).\n\n    Parameters\n    ----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    maxlen : int\n        Maximum length.\n    dtype : numpy.dtype or str\n        Data type to cast the resulting sequence.\n    padding : str\n        Either 'pre' or 'post', pad either before or after each sequence.\n    truncating : str\n        Either 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence\n    value : float\n        Value to pad the sequences to the desired value.\n\n    Returns\n    ----------\n    x : numpy.array\n        With dimensions (number_of_sequences, maxlen)\n\n    Examples\n    ----------\n    >>> sequences = [[1,1,1,1,1],[2,2,2],[3,3]]\n    >>> sequences = pad_sequences(sequences, maxlen=None, dtype='int32',\n    ...                  padding='post', truncating='pre', value=0.)\n    [[1 1 1 1 1]\n     [2 2 2 0 0]\n     [3 3 0 0 0]]"
  },
  {
    "code": "def remove_pad_sequences(sequences, pad_id=0):\n    \"\"\"Remove padding.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ----------\n    >>> sequences = [[2,3,4,0,0], [5,1,2,3,4,0,0,0], [4,5,0,2,4,0,0,0]]\n    >>> print(remove_pad_sequences(sequences, pad_id=0))\n    [[2, 3, 4], [5, 1, 2, 3, 4], [4, 5, 0, 2, 4]]\n\n    \"\"\"\n    sequences_out = copy.deepcopy(sequences)\n\n    for i, _ in enumerate(sequences):\n        # for j in range(len(sequences[i])):\n        #     if sequences[i][j] == pad_id:\n        #         sequences_out[i] = sequences_out[i][:j]\n        #         break\n        for j in range(1, len(sequences[i])):\n            if sequences[i][-j] != pad_id:\n                sequences_out[i] = sequences_out[i][0:-j + 1]\n                break\n\n    return sequences_out",
    "doc": "Remove padding.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ----------\n    >>> sequences = [[2,3,4,0,0], [5,1,2,3,4,0,0,0], [4,5,0,2,4,0,0,0]]\n    >>> print(remove_pad_sequences(sequences, pad_id=0))\n    [[2, 3, 4], [5, 1, 2, 3, 4], [4, 5, 0, 2, 4]]"
  },
  {
    "code": "def process_sequences(sequences, end_id=0, pad_val=0, is_shorten=True, remain_end_id=False):\n    \"\"\"Set all tokens(ids) after END token to the padding value, and then shorten (option) it to the maximum sequence length in this batch.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The special token for END.\n    pad_val : int\n        Replace the `end_id` and the IDs after `end_id` to this value.\n    is_shorten : boolean\n        Shorten the sequences. Default is True.\n    remain_end_id : boolean\n        Keep an `end_id` in the end. Default is False.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 3, 5, 3, 2, 2, 2, 2],  <-- end_id is 2\n    ...                  [5, 3, 9, 4, 9, 2, 2, 3]]  <-- end_id is 2\n    >>> sentences_ids = precess_sequences(sentences_ids, end_id=vocab.end_id, pad_val=0, is_shorten=True)\n    [[4, 3, 5, 3, 0], [5, 3, 9, 4, 9]]\n\n    \"\"\"\n    max_length = 0\n    for _, seq in enumerate(sequences):\n        is_end = False\n        for i_w, n in enumerate(seq):\n            if n == end_id and is_end == False:  # 1st time to see end_id\n                is_end = True\n                if max_length < i_w:\n                    max_length = i_w\n                if remain_end_id is False:\n                    seq[i_w] = pad_val  # set end_id to pad_val\n            elif is_end ==True:\n                seq[i_w] = pad_val\n\n    if remain_end_id is True:\n        max_length += 1\n    if is_shorten:\n        for i, seq in enumerate(sequences):\n            sequences[i] = seq[:max_length]\n    return sequences",
    "doc": "Set all tokens(ids) after END token to the padding value, and then shorten (option) it to the maximum sequence length in this batch.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The special token for END.\n    pad_val : int\n        Replace the `end_id` and the IDs after `end_id` to this value.\n    is_shorten : boolean\n        Shorten the sequences. Default is True.\n    remain_end_id : boolean\n        Keep an `end_id` in the end. Default is False.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 3, 5, 3, 2, 2, 2, 2],  <-- end_id is 2\n    ...                  [5, 3, 9, 4, 9, 2, 2, 3]]  <-- end_id is 2\n    >>> sentences_ids = precess_sequences(sentences_ids, end_id=vocab.end_id, pad_val=0, is_shorten=True)\n    [[4, 3, 5, 3, 0], [5, 3, 9, 4, 9]]"
  },
  {
    "code": "def sequences_add_start_id(sequences, start_id=0, remove_last=False):\n    \"\"\"Add special start token(id) in the beginning of each sequence.\n\n    Parameters\n    ------------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    start_id : int\n        The start ID.\n    remove_last : boolean\n        Remove the last value of each sequences. Usually be used for removing the end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4,3,5,3,2,2,2,2], [5,3,9,4,9,2,2,3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2)\n    [[2, 4, 3, 5, 3, 2, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2, 3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2, remove_last=True)\n    [[2, 4, 3, 5, 3, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2]]\n\n    For Seq2seq\n\n    >>> input = [a, b, c]\n    >>> target = [x, y, z]\n    >>> decode_seq = [start_id, a, b] <-- sequences_add_start_id(input, start_id, True)\n\n    \"\"\"\n    sequences_out = [[] for _ in range(len(sequences))]  #[[]] * len(sequences)\n    for i, _ in enumerate(sequences):\n        if remove_last:\n            sequences_out[i] = [start_id] + sequences[i][:-1]\n        else:\n            sequences_out[i] = [start_id] + sequences[i]\n    return sequences_out",
    "doc": "Add special start token(id) in the beginning of each sequence.\n\n    Parameters\n    ------------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    start_id : int\n        The start ID.\n    remove_last : boolean\n        Remove the last value of each sequences. Usually be used for removing the end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4,3,5,3,2,2,2,2], [5,3,9,4,9,2,2,3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2)\n    [[2, 4, 3, 5, 3, 2, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2, 3]]\n    >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2, remove_last=True)\n    [[2, 4, 3, 5, 3, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2]]\n\n    For Seq2seq\n\n    >>> input = [a, b, c]\n    >>> target = [x, y, z]\n    >>> decode_seq = [start_id, a, b] <-- sequences_add_start_id(input, start_id, True)"
  },
  {
    "code": "def sequences_add_end_id(sequences, end_id=888):\n    \"\"\"Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,3],[4,5,6,7]]\n    >>> print(sequences_add_end_id(sequences, end_id=999))\n    [[1, 2, 3, 999], [4, 5, 6, 999]]\n\n    \"\"\"\n    sequences_out = [[] for _ in range(len(sequences))]  #[[]] * len(sequences)\n    for i, _ in enumerate(sequences):\n        sequences_out[i] = sequences[i] + [end_id]\n    return sequences_out",
    "doc": "Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,3],[4,5,6,7]]\n    >>> print(sequences_add_end_id(sequences, end_id=999))\n    [[1, 2, 3, 999], [4, 5, 6, 999]]"
  },
  {
    "code": "def sequences_add_end_id_after_pad(sequences, end_id=888, pad_id=0):\n    \"\"\"Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,0,0], [1,2,3,0], [1,2,3,4]]\n    >>> print(sequences_add_end_id_after_pad(sequences, end_id=99, pad_id=0))\n    [[1, 2, 99, 0], [1, 2, 3, 99], [1, 2, 3, 4]]\n\n    \"\"\"\n    # sequences_out = [[] for _ in range(len(sequences))]#[[]] * len(sequences)\n\n    sequences_out = copy.deepcopy(sequences)\n    # # add a pad to all\n    # for i in range(len(sequences)):\n    #     for j in range(len(sequences[i])):\n    #         sequences_out[i].append(pad_id)\n    # # pad -- > end\n    # max_len = 0\n\n    for i, v in enumerate(sequences):\n        for j, _v2 in enumerate(v):\n            if sequences[i][j] == pad_id:\n                sequences_out[i][j] = end_id\n                # if j > max_len:\n                #     max_len = j\n                break\n\n    # # remove pad if too long\n    # for i in range(len(sequences)):\n    #     for j in range(len(sequences[i])):\n    #         sequences_out[i] = sequences_out[i][:max_len+1]\n    return sequences_out",
    "doc": "Add special end token(id) in the end of each sequence.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    end_id : int\n        The end ID.\n    pad_id : int\n        The pad ID.\n\n    Returns\n    ----------\n    list of list of int\n        The processed sequences.\n\n    Examples\n    ---------\n    >>> sequences = [[1,2,0,0], [1,2,3,0], [1,2,3,4]]\n    >>> print(sequences_add_end_id_after_pad(sequences, end_id=99, pad_id=0))\n    [[1, 2, 99, 0], [1, 2, 3, 99], [1, 2, 3, 4]]"
  },
  {
    "code": "def sequences_get_mask(sequences, pad_val=0):\n    \"\"\"Return mask for sequences.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_val : int\n        The pad value.\n\n    Returns\n    ----------\n    list of list of int\n        The mask.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 0, 5, 3, 0, 0],\n    ...                  [5, 3, 9, 4, 9, 0]]\n    >>> mask = sequences_get_mask(sentences_ids, pad_val=0)\n    [[1 1 1 1 0 0]\n     [1 1 1 1 1 0]]\n\n    \"\"\"\n    mask = np.ones_like(sequences)\n    for i, seq in enumerate(sequences):\n        for i_w in reversed(range(len(seq))):\n            if seq[i_w] == pad_val:\n                mask[i, i_w] = 0\n            else:\n                break  # <-- exit the for loop, prepcess next sequence\n    return mask",
    "doc": "Return mask for sequences.\n\n    Parameters\n    -----------\n    sequences : list of list of int\n        All sequences where each row is a sequence.\n    pad_val : int\n        The pad value.\n\n    Returns\n    ----------\n    list of list of int\n        The mask.\n\n    Examples\n    ---------\n    >>> sentences_ids = [[4, 0, 5, 3, 0, 0],\n    ...                  [5, 3, 9, 4, 9, 0]]\n    >>> mask = sequences_get_mask(sentences_ids, pad_val=0)\n    [[1 1 1 1 0 0]\n     [1 1 1 1 1 0]]"
  },
  {
    "code": "def keypoint_random_crop(image, annos, mask=None, size=(368, 368)):\n    \"\"\"Randomly crop an image and corresponding keypoints without influence scales, given by ``keypoint_random_resize_shortestedge``.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annotation, mask\n\n    \"\"\"\n\n    _target_height = size[0]\n    _target_width = size[1]\n    target_size = (_target_width, _target_height)\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    height, width, _ = np.shape(image)\n\n    for _ in range(50):\n        x = random.randrange(0, width - target_size[0]) if width > target_size[0] else 0\n        y = random.randrange(0, height - target_size[1]) if height > target_size[1] else 0\n\n        # check whether any face is inside the box to generate a reasonably-balanced datasets\n        for joint in annos:\n            if x <= joint[0][0] < x + target_size[0] and y <= joint[0][1] < y + target_size[1]:\n                break\n\n    def pose_crop(image, annos, mask, x, y, w, h):  # TODO : speed up with affine transform\n        # adjust image\n        target_size = (w, h)\n\n        img = image\n        resized = img[y:y + target_size[1], x:x + target_size[0], :]\n        resized_mask = mask[y:y + target_size[1], x:x + target_size[0]]\n        # adjust meta data\n        adjust_joint_list = []\n        for joint in annos:\n            adjust_joint = []\n            for point in joint:\n                if point[0] < -10 or point[1] < -10:\n                    adjust_joint.append((-1000, -1000))\n                    continue\n                new_x, new_y = point[0] - x, point[1] - y\n                # should not crop outside the image\n                if new_x > w - 1 or new_y > h - 1:\n                    adjust_joint.append((-1000, -1000))\n                    continue\n                adjust_joint.append((new_x, new_y))\n            adjust_joint_list.append(adjust_joint)\n\n        return resized, adjust_joint_list, resized_mask\n\n    return pose_crop(image, annos, mask, x, y, target_size[0], target_size[1])",
    "doc": "Randomly crop an image and corresponding keypoints without influence scales, given by ``keypoint_random_resize_shortestedge``.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annotation, mask"
  },
  {
    "code": "def keypoint_resize_random_crop(image, annos, mask=None, size=(368, 368)):\n    \"\"\"Reszie the image to make either its width or height equals to the given sizes.\n    Then randomly crop image without influence scales.\n    Resize the image match with the minimum size before cropping, this API will change the zoom scale of object.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size (height, width) of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n\n    def resize_image(image, annos, mask, target_width, target_height):\n        \"\"\"Reszie image\n\n        Parameters\n        -----------\n        image : 3 channel image\n            The given image.\n        annos : list of list of floats\n            Keypoints of people\n        mask : single channel image or None\n            The mask if available.\n        target_width : int\n            Expected width of returned image.\n        target_height : int\n            Expected height of returned image.\n\n        Returns\n        ----------\n        preprocessed input image, annos, mask\n\n        \"\"\"\n        y, x, _ = np.shape(image)\n\n        ratio_y = target_height / y\n        ratio_x = target_width / x\n\n        new_joints = []\n        # update meta\n        for people in annos:\n            new_keypoints = []\n            for keypoints in people:\n                if keypoints[0] < 0 or keypoints[1] < 0:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n                pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))\n                if pts[0] > target_width - 1 or pts[1] > target_height - 1:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n\n                new_keypoints.append(pts)\n            new_joints.append(new_keypoints)\n        annos = new_joints\n\n        new_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)\n        if mask is not None:\n            new_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_AREA)\n            return new_image, annos, new_mask\n        else:\n            return new_image, annos, None\n\n    _target_height = size[0]\n    _target_width = size[1]\n    if len(np.shape(image)) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    height, width, _ = np.shape(image)\n    # print(\"the size of original img is:\", height, width)\n    if height <= width:\n        ratio = _target_height / height\n        new_width = int(ratio * width)\n        if height == width:\n            new_width = _target_height\n\n        image, annos, mask = resize_image(image, annos, mask, new_width, _target_height)\n\n        # for i in annos:\n        #     if len(i) is not 19:\n        #         print('Joints of person is not 19 ERROR FROM RESIZE')\n\n        if new_width > _target_width:\n            crop_range_x = np.random.randint(0, new_width - _target_width)\n        else:\n            crop_range_x = 0\n        image = image[:, crop_range_x:crop_range_x + _target_width, :]\n        if mask is not None:\n            mask = mask[:, crop_range_x:crop_range_x + _target_width]\n        # joint_list= []\n        new_joints = []\n        #annos-pepople-joints (must be 19 or [])\n        for people in annos:\n            # print(\"number of keypoints is\", np.shape(people))\n            new_keypoints = []\n            for keypoints in people:\n                if keypoints[0] < -10 or keypoints[1] < -10:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n                top = crop_range_x + _target_width - 1\n                if keypoints[0] >= crop_range_x and keypoints[0] <= top:\n                    # pts = (keypoints[0]-crop_range_x, keypoints[1])\n                    pts = (int(keypoints[0] - crop_range_x), int(keypoints[1]))\n                else:\n                    pts = (-1000, -1000)\n                new_keypoints.append(pts)\n\n            new_joints.append(new_keypoints)\n            # if len(new_keypoints) != 19:\n            #     print('1:The Length of joints list should be 0 or 19 but actually:', len(new_keypoints))\n        annos = new_joints\n\n    if height > width:\n        ratio = _target_width / width\n        new_height = int(ratio * height)\n        image, annos, mask = resize_image(image, annos, mask, _target_width, new_height)\n\n        # for i in annos:\n        #     if len(i) is not 19:\n        #         print('Joints of person is not 19 ERROR')\n\n        if new_height > _target_height:\n            crop_range_y = np.random.randint(0, new_height - _target_height)\n\n        else:\n            crop_range_y = 0\n        image = image[crop_range_y:crop_range_y + _target_width, :, :]\n        if mask is not None:\n            mask = mask[crop_range_y:crop_range_y + _target_width, :]\n        new_joints = []\n\n        for people in annos:  # TODO : speed up with affine transform\n            new_keypoints = []\n            for keypoints in people:\n\n                # case orginal points are not usable\n                if keypoints[0] < 0 or keypoints[1] < 0:\n                    new_keypoints.append((-1000, -1000))\n                    continue\n                # y axis coordinate change\n                bot = crop_range_y + _target_height - 1\n                if keypoints[1] >= crop_range_y and keypoints[1] <= bot:\n                    # pts = (keypoints[0], keypoints[1]-crop_range_y)\n                    pts = (int(keypoints[0]), int(keypoints[1] - crop_range_y))\n                    # if pts[0]>367 or pts[1]>367:\n                    #     print('Error2')\n                else:\n                    pts = (-1000, -1000)\n\n                new_keypoints.append(pts)\n\n            new_joints.append(new_keypoints)\n            # if len(new_keypoints) != 19:\n            #     print('2:The Length of joints list should be 0 or 19 but actually:', len(new_keypoints))\n\n        annos = new_joints\n\n    # mask = cv2.resize(mask, (46, 46), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        return image, annos, mask\n    else:\n        return image, annos, None",
    "doc": "Reszie the image to make either its width or height equals to the given sizes.\n    Then randomly crop image without influence scales.\n    Resize the image match with the minimum size before cropping, this API will change the zoom scale of object.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    size : tuple of int\n        The size (height, width) of returned image.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask"
  },
  {
    "code": "def keypoint_random_rotate(image, annos, mask=None, rg=15.):\n    \"\"\"Rotate an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n\n    def _rotate_coord(shape, newxy, point, angle):\n        angle = -1 * angle / 180.0 * math.pi\n        ox, oy = shape\n        px, py = point\n        ox /= 2\n        oy /= 2\n        qx = math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n        qy = math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n        new_x, new_y = newxy\n        qx += ox - new_x\n        qy += oy - new_y\n        return int(qx + 0.5), int(qy + 0.5)\n\n    def _largest_rotated_rect(w, h, angle):\n        \"\"\"\n        Get largest rectangle after rotation.\n        http://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n        \"\"\"\n        angle = angle / 180.0 * math.pi\n        if w <= 0 or h <= 0:\n            return 0, 0\n\n        width_is_longer = w >= h\n        side_long, side_short = (w, h) if width_is_longer else (h, w)\n\n        # since the solutions for angle, -angle and 180-angle are all the same,\n        # if suffices to look at the first quadrant and the absolute values of sin,cos:\n        sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n        if side_short <= 2. * sin_a * cos_a * side_long:\n            # half constrained case: two crop corners touch the longer side,\n            #   the other two corners are on the mid-line parallel to the longer line\n            x = 0.5 * side_short\n            wr, hr = (x / sin_a, x / cos_a) if width_is_longer else (x / cos_a, x / sin_a)\n        else:\n            # fully constrained case: crop touches all 4 sides\n            cos_2a = cos_a * cos_a - sin_a * sin_a\n            wr, hr = (w * cos_a - h * sin_a) / cos_2a, (h * cos_a - w * sin_a) / cos_2a\n        return int(np.round(wr)), int(np.round(hr))\n\n    img_shape = np.shape(image)\n    height = img_shape[0]\n    width = img_shape[1]\n    deg = np.random.uniform(-rg, rg)\n\n    img = image\n    center = (img.shape[1] * 0.5, img.shape[0] * 0.5)  # x, y\n    rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n    ret = cv2.warpAffine(img, rot_m, img.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n    if img.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    neww, newh = _largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n    neww = min(neww, ret.shape[1])\n    newh = min(newh, ret.shape[0])\n    newx = int(center[0] - neww * 0.5)\n    newy = int(center[1] - newh * 0.5)\n    # print(ret.shape, deg, newx, newy, neww, newh)\n    img = ret[newy:newy + newh, newx:newx + neww]\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n\n            x, y = _rotate_coord((width, height), (newx, newy), point, deg)\n\n            if x > neww - 1 or y > newh - 1:\n                adjust_joint.append((-1000, -1000))\n                continue\n            if x < 0 or y < 0:\n                adjust_joint.append((-1000, -1000))\n                continue\n\n            adjust_joint.append((x, y))\n        adjust_joint_list.append(adjust_joint)\n    joint_list = adjust_joint_list\n\n    if mask is not None:\n        msk = mask\n        center = (msk.shape[1] * 0.5, msk.shape[0] * 0.5)  # x, y\n        rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n        ret = cv2.warpAffine(msk, rot_m, msk.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n        if msk.ndim == 3 and msk.ndim == 2:\n            ret = ret[:, :, np.newaxis]\n        neww, newh = _largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n        neww = min(neww, ret.shape[1])\n        newh = min(newh, ret.shape[0])\n        newx = int(center[0] - neww * 0.5)\n        newy = int(center[1] - newh * 0.5)\n        # print(ret.shape, deg, newx, newy, neww, newh)\n        msk = ret[newy:newy + newh, newx:newx + neww]\n        return img, joint_list, msk\n    else:\n        return img, joint_list, None",
    "doc": "Rotate an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    rg : int or float\n        Degree to rotate, usually 0 ~ 180.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask"
  },
  {
    "code": "def keypoint_random_flip(\n        image, annos, mask=None, prob=0.5, flip_list=(0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16, 18)\n):\n    \"\"\"Flip an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    prob : float, 0 to 1\n        The probability to flip the image, if 1, always flip the image.\n    flip_list : tuple of int\n        Denotes how the keypoints number be changed after flipping which is required for pose estimation task.\n        The left and right body should be maintained rather than switch.\n        (Default COCO format).\n        Set to an empty tuple if you don't need to maintain left and right information.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n\n    _prob = np.random.uniform(0, 1.0)\n    if _prob < prob:\n        return image, annos, mask\n\n    _, width, _ = np.shape(image)\n    image = cv2.flip(image, 1)\n    mask = cv2.flip(mask, 1)\n    new_joints = []\n    for people in annos:  # TODO : speed up with affine transform\n        new_keypoints = []\n        for k in flip_list:\n            point = people[k]\n            if point[0] < 0 or point[1] < 0:\n                new_keypoints.append((-1000, -1000))\n                continue\n            if point[0] > image.shape[1] - 1 or point[1] > image.shape[0] - 1:\n                new_keypoints.append((-1000, -1000))\n                continue\n            if (width - point[0]) > image.shape[1] - 1:\n                new_keypoints.append((-1000, -1000))\n                continue\n            new_keypoints.append((width - point[0], point[1]))\n        new_joints.append(new_keypoints)\n    annos = new_joints\n\n    return image, annos, mask",
    "doc": "Flip an image and corresponding keypoints.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    prob : float, 0 to 1\n        The probability to flip the image, if 1, always flip the image.\n    flip_list : tuple of int\n        Denotes how the keypoints number be changed after flipping which is required for pose estimation task.\n        The left and right body should be maintained rather than switch.\n        (Default COCO format).\n        Set to an empty tuple if you don't need to maintain left and right information.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask"
  },
  {
    "code": "def keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    \"\"\"Randomly resize an image and corresponding keypoints.\n    The height and width of image will be changed independently, so the scale will be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None",
    "doc": "Randomly resize an image and corresponding keypoints.\n    The height and width of image will be changed independently, so the scale will be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask"
  },
  {
    "code": "def Vgg19(rgb):\n    \"\"\"\n    Build the VGG 19 Model\n\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]\n    \"\"\"\n    start_time = time.time()\n    print(\"build model started\")\n    rgb_scaled = rgb * 255.0\n    # Convert RGB to BGR\n    red, green, blue = tf.split(rgb_scaled, 3, 3)\n\n    if red.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    if green.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    if blue.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    bgr = tf.concat([\n        blue - VGG_MEAN[0],\n        green - VGG_MEAN[1],\n        red - VGG_MEAN[2],\n    ], axis=3)\n\n    if bgr.get_shape().as_list()[1:] != [224, 224, 3]:\n        raise Exception(\"image size unmatch\")\n    # input layer\n    net_in = InputLayer(bgr, name='input')\n    # conv1\n    net = Conv2dLayer(net_in, act=tf.nn.relu, shape=[3, 3, 3, 64], strides=[1, 1, 1, 1], padding='SAME', name='conv1_1')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 64, 64], strides=[1, 1, 1, 1], padding='SAME', name='conv1_2')\n    net = PoolLayer(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=tf.nn.max_pool, name='pool1')\n    # conv2\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 64, 128], strides=[1, 1, 1, 1], padding='SAME', name='conv2_1')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 128, 128], strides=[1, 1, 1, 1], padding='SAME', name='conv2_2')\n    net = PoolLayer(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=tf.nn.max_pool, name='pool2')\n    # conv3\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 128, 256], strides=[1, 1, 1, 1], padding='SAME', name='conv3_1')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding='SAME', name='conv3_2')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding='SAME', name='conv3_3')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding='SAME', name='conv3_4')\n    net = PoolLayer(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=tf.nn.max_pool, name='pool3')\n    # conv4\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 256, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv4_1')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv4_2')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv4_3')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv4_4')\n    net = PoolLayer(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=tf.nn.max_pool, name='pool4')\n    # conv5\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv5_1')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv5_2')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv5_3')\n    net = Conv2dLayer(net, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding='SAME', name='conv5_4')\n    net = PoolLayer(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', pool=tf.nn.max_pool, name='pool5')\n    # fc 6~8\n    net = FlattenLayer(net, name='flatten')\n    net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc6')\n    net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc7')\n    net = DenseLayer(net, n_units=1000, act=None, name='fc8')\n    print(\"build model finished: %fs\" % (time.time() - start_time))\n    return net",
    "doc": "Build the VGG 19 Model\n\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]"
  },
  {
    "code": "def Vgg19_simple_api(rgb):\n    \"\"\"\n    Build the VGG 19 Model\n\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]\n    \"\"\"\n    start_time = time.time()\n    print(\"build model started\")\n    rgb_scaled = rgb * 255.0\n    # Convert RGB to BGR\n    red, green, blue = tf.split(rgb_scaled, 3, 3)\n\n    if red.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    if green.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    if blue.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(\"image size unmatch\")\n\n    bgr = tf.concat([\n        blue - VGG_MEAN[0],\n        green - VGG_MEAN[1],\n        red - VGG_MEAN[2],\n    ], axis=3)\n\n    if bgr.get_shape().as_list()[1:] != [224, 224, 3]:\n        raise Exception(\"image size unmatch\")\n\n    # input layer\n    net_in = InputLayer(bgr, name='input')\n    # conv1\n    net = Conv2d(net_in, 64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv1_1')\n    net = Conv2d(net, n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv1_2')\n    net = MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool1')\n    # conv2\n    net = Conv2d(net, n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv2_1')\n    net = Conv2d(net, n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv2_2')\n    net = MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool2')\n    # conv3\n    net = Conv2d(net, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3_1')\n    net = Conv2d(net, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3_2')\n    net = Conv2d(net, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3_3')\n    net = Conv2d(net, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3_4')\n    net = MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool3')\n    # conv4\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4_1')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4_2')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4_3')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4_4')\n    net = MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool4')\n    # conv5\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv5_1')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv5_2')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv5_3')\n    net = Conv2d(net, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv5_4')\n    net = MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool5')\n    # fc 6~8\n    net = FlattenLayer(net, name='flatten')\n    net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc6')\n    net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc7')\n    net = DenseLayer(net, n_units=1000, act=None, name='fc8')\n    print(\"build model finished: %fs\" % (time.time() - start_time))\n    return net",
    "doc": "Build the VGG 19 Model\n\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]"
  },
  {
    "code": "def prepro(I):\n    \"\"\"Prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.\"\"\"\n    I = I[35:195]\n    I = I[::2, ::2, 0]\n    I[I == 144] = 0\n    I[I == 109] = 0\n    I[I != 0] = 1\n    return I.astype(np.float).ravel()",
    "doc": "Prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector."
  },
  {
    "code": "def discount_episode_rewards(rewards=None, gamma=0.99, mode=0):\n    \"\"\"Take 1D float array of rewards and compute discounted rewards for an\n    episode. When encount a non-zero value, consider as the end a of an episode.\n\n    Parameters\n    ----------\n    rewards : list\n        List of rewards\n    gamma : float\n        Discounted factor\n    mode : int\n        Mode for computing the discount rewards.\n            - If mode == 0, reset the discount process when encount a non-zero reward (Ping-pong game).\n            - If mode == 1, would not reset the discount process.\n\n    Returns\n    --------\n    list of float\n        The discounted rewards.\n\n    Examples\n    ----------\n    >>> rewards = np.asarray([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1])\n    >>> gamma = 0.9\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma)\n    >>> print(discount_rewards)\n    [ 0.72899997  0.81        0.89999998  1.          0.72899997  0.81\n    0.89999998  1.          0.72899997  0.81        0.89999998  1.        ]\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma, mode=1)\n    >>> print(discount_rewards)\n    [ 1.52110755  1.69011939  1.87791049  2.08656716  1.20729685  1.34144104\n    1.49048996  1.65610003  0.72899997  0.81        0.89999998  1.        ]\n\n    \"\"\"\n    if rewards is None:\n        raise Exception(\"rewards should be a list\")\n    discounted_r = np.zeros_like(rewards, dtype=np.float32)\n    running_add = 0\n    for t in reversed(xrange(0, rewards.size)):\n        if mode == 0:\n            if rewards[t] != 0: running_add = 0\n\n        running_add = running_add * gamma + rewards[t]\n        discounted_r[t] = running_add\n    return discounted_r",
    "doc": "Take 1D float array of rewards and compute discounted rewards for an\n    episode. When encount a non-zero value, consider as the end a of an episode.\n\n    Parameters\n    ----------\n    rewards : list\n        List of rewards\n    gamma : float\n        Discounted factor\n    mode : int\n        Mode for computing the discount rewards.\n            - If mode == 0, reset the discount process when encount a non-zero reward (Ping-pong game).\n            - If mode == 1, would not reset the discount process.\n\n    Returns\n    --------\n    list of float\n        The discounted rewards.\n\n    Examples\n    ----------\n    >>> rewards = np.asarray([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1])\n    >>> gamma = 0.9\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma)\n    >>> print(discount_rewards)\n    [ 0.72899997  0.81        0.89999998  1.          0.72899997  0.81\n    0.89999998  1.          0.72899997  0.81        0.89999998  1.        ]\n    >>> discount_rewards = tl.rein.discount_episode_rewards(rewards, gamma, mode=1)\n    >>> print(discount_rewards)\n    [ 1.52110755  1.69011939  1.87791049  2.08656716  1.20729685  1.34144104\n    1.49048996  1.65610003  0.72899997  0.81        0.89999998  1.        ]"
  },
  {
    "code": "def cross_entropy_reward_loss(logits, actions, rewards, name=None):\n    \"\"\"Calculate the loss for Policy Gradient Network.\n\n    Parameters\n    ----------\n    logits : tensor\n        The network outputs without softmax. This function implements softmax inside.\n    actions : tensor or placeholder\n        The agent actions.\n    rewards : tensor or placeholder\n        The rewards.\n\n    Returns\n    --------\n    Tensor\n        The TensorFlow loss function.\n\n    Examples\n    ----------\n    >>> states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])\n    >>> network = InputLayer(states_batch_pl, name='input')\n    >>> network = DenseLayer(network, n_units=H, act=tf.nn.relu, name='relu1')\n    >>> network = DenseLayer(network, n_units=3, name='out')\n    >>> probs = network.outputs\n    >>> sampling_prob = tf.nn.softmax(probs)\n    >>> actions_batch_pl = tf.placeholder(tf.int32, shape=[None])\n    >>> discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])\n    >>> loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl, discount_rewards_batch_pl)\n    >>> train_op = tf.train.RMSPropOptimizer(learning_rate, decay_rate).minimize(loss)\n\n    \"\"\"\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=actions, logits=logits, name=name)\n\n    return tf.reduce_sum(tf.multiply(cross_entropy, rewards))",
    "doc": "Calculate the loss for Policy Gradient Network.\n\n    Parameters\n    ----------\n    logits : tensor\n        The network outputs without softmax. This function implements softmax inside.\n    actions : tensor or placeholder\n        The agent actions.\n    rewards : tensor or placeholder\n        The rewards.\n\n    Returns\n    --------\n    Tensor\n        The TensorFlow loss function.\n\n    Examples\n    ----------\n    >>> states_batch_pl = tf.placeholder(tf.float32, shape=[None, D])\n    >>> network = InputLayer(states_batch_pl, name='input')\n    >>> network = DenseLayer(network, n_units=H, act=tf.nn.relu, name='relu1')\n    >>> network = DenseLayer(network, n_units=3, name='out')\n    >>> probs = network.outputs\n    >>> sampling_prob = tf.nn.softmax(probs)\n    >>> actions_batch_pl = tf.placeholder(tf.int32, shape=[None])\n    >>> discount_rewards_batch_pl = tf.placeholder(tf.float32, shape=[None])\n    >>> loss = tl.rein.cross_entropy_reward_loss(probs, actions_batch_pl, discount_rewards_batch_pl)\n    >>> train_op = tf.train.RMSPropOptimizer(learning_rate, decay_rate).minimize(loss)"
  },
  {
    "code": "def log_weight(probs, weights, name='log_weight'):\n    \"\"\"Log weight.\n\n    Parameters\n    -----------\n    probs : tensor\n        If it is a network output, usually we should scale it to [0, 1] via softmax.\n    weights : tensor\n        The weights.\n\n    Returns\n    --------\n    Tensor\n        The Tensor after appling the log weighted expression.\n\n    \"\"\"\n    with tf.variable_scope(name):\n        exp_v = tf.reduce_mean(tf.log(probs) * weights)\n        return exp_v",
    "doc": "Log weight.\n\n    Parameters\n    -----------\n    probs : tensor\n        If it is a network output, usually we should scale it to [0, 1] via softmax.\n    weights : tensor\n        The weights.\n\n    Returns\n    --------\n    Tensor\n        The Tensor after appling the log weighted expression."
  },
  {
    "code": "def choice_action_by_probs(probs=(0.5, 0.5), action_list=None):\n    \"\"\"Choice and return an an action by given the action probability distribution.\n\n    Parameters\n    ------------\n    probs : list of float.\n        The probability distribution of all actions.\n    action_list : None or a list of int or others\n        A list of action in integer, string or others. If None, returns an integer range between 0 and len(probs)-1.\n\n    Returns\n    --------\n    float int or str\n        The chosen action.\n\n    Examples\n    ----------\n    >>> for _ in range(5):\n    >>>     a = choice_action_by_probs([0.2, 0.4, 0.4])\n    >>>     print(a)\n    0\n    1\n    1\n    2\n    1\n    >>> for _ in range(3):\n    >>>     a = choice_action_by_probs([0.5, 0.5], ['a', 'b'])\n    >>>     print(a)\n    a\n    b\n    b\n\n    \"\"\"\n    if action_list is None:\n        n_action = len(probs)\n        action_list = np.arange(n_action)\n    else:\n        if len(action_list) != len(probs):\n            raise Exception(\"number of actions should equal to number of probabilities.\")\n    return np.random.choice(action_list, p=probs)",
    "doc": "Choice and return an an action by given the action probability distribution.\n\n    Parameters\n    ------------\n    probs : list of float.\n        The probability distribution of all actions.\n    action_list : None or a list of int or others\n        A list of action in integer, string or others. If None, returns an integer range between 0 and len(probs)-1.\n\n    Returns\n    --------\n    float int or str\n        The chosen action.\n\n    Examples\n    ----------\n    >>> for _ in range(5):\n    >>>     a = choice_action_by_probs([0.2, 0.4, 0.4])\n    >>>     print(a)\n    0\n    1\n    1\n    2\n    1\n    >>> for _ in range(3):\n    >>>     a = choice_action_by_probs([0.5, 0.5], ['a', 'b'])\n    >>>     print(a)\n    a\n    b\n    b"
  },
  {
    "code": "def cross_entropy(output, target, name=None):\n    \"\"\"Softmax cross-entropy operation, returns the TensorFlow expression of cross-entropy for two distributions,\n    it implements softmax internally. See ``tf.nn.sparse_softmax_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss.\n\n    Examples\n    --------\n    >>> ce = tl.cost.cross_entropy(y_logits, y_target_logits, 'my_loss')\n\n    References\n    -----------\n    - About cross-entropy: `<https://en.wikipedia.org/wiki/Cross_entropy>`__.\n    - The code is borrowed from: `<https://en.wikipedia.org/wiki/Cross_entropy>`__.\n\n    \"\"\"\n    if name is None:\n        raise Exception(\"Please give a unique name to tl.cost.cross_entropy for TF1.0+\")\n    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output), name=name)",
    "doc": "Softmax cross-entropy operation, returns the TensorFlow expression of cross-entropy for two distributions,\n    it implements softmax internally. See ``tf.nn.sparse_softmax_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss.\n\n    Examples\n    --------\n    >>> ce = tl.cost.cross_entropy(y_logits, y_target_logits, 'my_loss')\n\n    References\n    -----------\n    - About cross-entropy: `<https://en.wikipedia.org/wiki/Cross_entropy>`__.\n    - The code is borrowed from: `<https://en.wikipedia.org/wiki/Cross_entropy>`__."
  },
  {
    "code": "def sigmoid_cross_entropy(output, target, name=None):\n    \"\"\"Sigmoid cross-entropy operation, see ``tf.nn.sigmoid_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss.\n\n    \"\"\"\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output), name=name)",
    "doc": "Sigmoid cross-entropy operation, see ``tf.nn.sigmoid_cross_entropy_with_logits``.\n\n    Parameters\n    ----------\n    output : Tensor\n        A batch of distribution with shape: [batch_size, num of classes].\n    target : Tensor\n        A batch of index with shape: [batch_size, ].\n    name : string\n        Name of this loss."
  },
  {
    "code": "def binary_cross_entropy(output, target, epsilon=1e-8, name='bce_loss'):\n    \"\"\"Binary cross entropy operation.\n\n    Parameters\n    ----------\n    output : Tensor\n        Tensor with type of `float32` or `float64`.\n    target : Tensor\n        The target distribution, format the same with `output`.\n    epsilon : float\n        A small value to avoid output to be zero.\n    name : str\n        An optional name to attach to this function.\n\n    References\n    -----------\n    - `ericjang-DRAW <https://github.com/ericjang/draw/blob/master/draw.py#L73>`__\n\n    \"\"\"\n    #     with ops.op_scope([output, target], name, \"bce_loss\") as name:\n    #         output = ops.convert_to_tensor(output, name=\"preds\")\n    #         target = ops.convert_to_tensor(targets, name=\"target\")\n\n    # with tf.name_scope(name):\n    return tf.reduce_mean(\n        tf.reduce_sum(-(target * tf.log(output + epsilon) + (1. - target) * tf.log(1. - output + epsilon)), axis=1),\n        name=name\n    )",
    "doc": "Binary cross entropy operation.\n\n    Parameters\n    ----------\n    output : Tensor\n        Tensor with type of `float32` or `float64`.\n    target : Tensor\n        The target distribution, format the same with `output`.\n    epsilon : float\n        A small value to avoid output to be zero.\n    name : str\n        An optional name to attach to this function.\n\n    References\n    -----------\n    - `ericjang-DRAW <https://github.com/ericjang/draw/blob/master/draw.py#L73>`__"
  },
  {
    "code": "def mean_squared_error(output, target, is_mean=False, name=\"mean_squared_error\"):\n    \"\"\"Return the TensorFlow expression of mean-square-error (L2) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    name : str\n        An optional name to attach to this function.\n\n    References\n    ------------\n    - `Wiki Mean Squared Error <https://en.wikipedia.org/wiki/Mean_squared_error>`__\n\n    \"\"\"\n    # with tf.name_scope(name):\n    if output.get_shape().ndims == 2:  # [batch_size, n_feature]\n        if is_mean:\n            mse = tf.reduce_mean(tf.reduce_mean(tf.squared_difference(output, target), 1), name=name)\n        else:\n            mse = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(output, target), 1), name=name)\n    elif output.get_shape().ndims == 3:  # [batch_size, w, h]\n        if is_mean:\n            mse = tf.reduce_mean(tf.reduce_mean(tf.squared_difference(output, target), [1, 2]), name=name)\n        else:\n            mse = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(output, target), [1, 2]), name=name)\n    elif output.get_shape().ndims == 4:  # [batch_size, w, h, c]\n        if is_mean:\n            mse = tf.reduce_mean(tf.reduce_mean(tf.squared_difference(output, target), [1, 2, 3]), name=name)\n        else:\n            mse = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(output, target), [1, 2, 3]), name=name)\n    else:\n        raise Exception(\"Unknow dimension\")\n    return mse",
    "doc": "Return the TensorFlow expression of mean-square-error (L2) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    name : str\n        An optional name to attach to this function.\n\n    References\n    ------------\n    - `Wiki Mean Squared Error <https://en.wikipedia.org/wiki/Mean_squared_error>`__"
  },
  {
    "code": "def normalized_mean_square_error(output, target, name=\"normalized_mean_squared_error_loss\"):\n    \"\"\"Return the TensorFlow expression of normalized mean-square-error of two distributions.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    name : str\n        An optional name to attach to this function.\n\n    \"\"\"\n    # with tf.name_scope(\"normalized_mean_squared_error_loss\"):\n    if output.get_shape().ndims == 2:  # [batch_size, n_feature]\n        nmse_a = tf.sqrt(tf.reduce_sum(tf.squared_difference(output, target), axis=1))\n        nmse_b = tf.sqrt(tf.reduce_sum(tf.square(target), axis=1))\n    elif output.get_shape().ndims == 3:  # [batch_size, w, h]\n        nmse_a = tf.sqrt(tf.reduce_sum(tf.squared_difference(output, target), axis=[1, 2]))\n        nmse_b = tf.sqrt(tf.reduce_sum(tf.square(target), axis=[1, 2]))\n    elif output.get_shape().ndims == 4:  # [batch_size, w, h, c]\n        nmse_a = tf.sqrt(tf.reduce_sum(tf.squared_difference(output, target), axis=[1, 2, 3]))\n        nmse_b = tf.sqrt(tf.reduce_sum(tf.square(target), axis=[1, 2, 3]))\n    nmse = tf.reduce_mean(nmse_a / nmse_b, name=name)\n    return nmse",
    "doc": "Return the TensorFlow expression of normalized mean-square-error of two distributions.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    name : str\n        An optional name to attach to this function."
  },
  {
    "code": "def absolute_difference_error(output, target, is_mean=False, name=\"absolute_difference_error_loss\"):\n    \"\"\"Return the TensorFlow expression of absolute difference error (L1) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    name : str\n        An optional name to attach to this function.\n\n    \"\"\"\n    # with tf.name_scope(\"absolute_difference_error_loss\"):\n    if output.get_shape().ndims == 2:  # [batch_size, n_feature]\n        if is_mean:\n            loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), 1), name=name)\n        else:\n            loss = tf.reduce_mean(tf.reduce_sum(tf.abs(output - target), 1), name=name)\n    elif output.get_shape().ndims == 3:  # [batch_size, w, h]\n        if is_mean:\n            loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), [1, 2]), name=name)\n        else:\n            loss = tf.reduce_mean(tf.reduce_sum(tf.abs(output - target), [1, 2]), name=name)\n    elif output.get_shape().ndims == 4:  # [batch_size, w, h, c]\n        if is_mean:\n            loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), [1, 2, 3]), name=name)\n        else:\n            loss = tf.reduce_mean(tf.reduce_sum(tf.abs(output - target), [1, 2, 3]), name=name)\n    else:\n        raise Exception(\"Unknow dimension\")\n    return loss",
    "doc": "Return the TensorFlow expression of absolute difference error (L1) of two batch of data.\n\n    Parameters\n    ----------\n    output : Tensor\n        2D, 3D or 4D tensor i.e. [batch_size, n_feature], [batch_size, height, width] or [batch_size, height, width, channel].\n    target : Tensor\n        The target distribution, format the same with `output`.\n    is_mean : boolean\n        Whether compute the mean or sum for each example.\n            - If True, use ``tf.reduce_mean`` to compute the loss between one target and predict data.\n            - If False, use ``tf.reduce_sum`` (default).\n    name : str\n        An optional name to attach to this function."
  },
  {
    "code": "def dice_coe(output, target, loss_type='jaccard', axis=(1, 2, 3), smooth=1e-5):\n    \"\"\"Soft dice (S\u00f8rensen or Jaccard) coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation\n    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.\n\n    Parameters\n    -----------\n    output : Tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : Tensor\n        The target distribution, format the same with `output`.\n    loss_type : str\n        ``jaccard`` or ``sorensen``, default is ``jaccard``.\n    axis : tuple of int\n        All dimensions are reduced, default ``[1,2,3]``.\n    smooth : float\n        This small value will be added to the numerator and denominator.\n            - If both output and target are empty, it makes sure dice is 1.\n            - If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``, then if smooth is very small, dice close to 0 (even the image values lower than the threshold), so in this case, higher smooth can have a higher dice.\n\n    Examples\n    ---------\n    >>> outputs = tl.act.pixel_wise_softmax(network.outputs)\n    >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\u00f8rensen\u2013Dice_coefficient>`__\n\n    \"\"\"\n    inse = tf.reduce_sum(output * target, axis=axis)\n    if loss_type == 'jaccard':\n        l = tf.reduce_sum(output * output, axis=axis)\n        r = tf.reduce_sum(target * target, axis=axis)\n    elif loss_type == 'sorensen':\n        l = tf.reduce_sum(output, axis=axis)\n        r = tf.reduce_sum(target, axis=axis)\n    else:\n        raise Exception(\"Unknow loss_type\")\n    # old axis=[0,1,2,3]\n    # dice = 2 * (inse) / (l + r)\n    # epsilon = 1e-5\n    # dice = tf.clip_by_value(dice, 0, 1.0-epsilon) # if all empty, dice = 1\n    # new haodong\n    dice = (2. * inse + smooth) / (l + r + smooth)\n    ##\n    dice = tf.reduce_mean(dice, name='dice_coe')\n    return dice",
    "doc": "Soft dice (S\u00f8rensen or Jaccard) coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation\n    i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.\n\n    Parameters\n    -----------\n    output : Tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : Tensor\n        The target distribution, format the same with `output`.\n    loss_type : str\n        ``jaccard`` or ``sorensen``, default is ``jaccard``.\n    axis : tuple of int\n        All dimensions are reduced, default ``[1,2,3]``.\n    smooth : float\n        This small value will be added to the numerator and denominator.\n            - If both output and target are empty, it makes sure dice is 1.\n            - If either output or target are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``, then if smooth is very small, dice close to 0 (even the image values lower than the threshold), so in this case, higher smooth can have a higher dice.\n\n    Examples\n    ---------\n    >>> outputs = tl.act.pixel_wise_softmax(network.outputs)\n    >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\u00f8rensen\u2013Dice_coefficient>`__"
  },
  {
    "code": "def dice_hard_coe(output, target, threshold=0.5, axis=(1, 2, 3), smooth=1e-5):\n    \"\"\"Non-differentiable S\u00f8rensen\u2013Dice coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation i.e. labels are binary.\n    The coefficient between 0 to 1, 1 if totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\u00f8rensen\u2013Dice_coefficient>`__\n\n    \"\"\"\n    output = tf.cast(output > threshold, dtype=tf.float32)\n    target = tf.cast(target > threshold, dtype=tf.float32)\n    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)\n    l = tf.reduce_sum(output, axis=axis)\n    r = tf.reduce_sum(target, axis=axis)\n    # old axis=[0,1,2,3]\n    # hard_dice = 2 * (inse) / (l + r)\n    # epsilon = 1e-5\n    # hard_dice = tf.clip_by_value(hard_dice, 0, 1.0-epsilon)\n    # new haodong\n    hard_dice = (2. * inse + smooth) / (l + r + smooth)\n    ##\n    hard_dice = tf.reduce_mean(hard_dice, name='hard_dice')\n    return hard_dice",
    "doc": "Non-differentiable S\u00f8rensen\u2013Dice coefficient for comparing the similarity\n    of two batch of data, usually be used for binary image segmentation i.e. labels are binary.\n    The coefficient between 0 to 1, 1 if totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    References\n    -----------\n    - `Wiki-Dice <https://en.wikipedia.org/wiki/S\u00f8rensen\u2013Dice_coefficient>`__"
  },
  {
    "code": "def iou_coe(output, target, threshold=0.5, axis=(1, 2, 3), smooth=1e-5):\n    \"\"\"Non-differentiable Intersection over Union (IoU) for comparing the\n    similarity of two batch of data, usually be used for evaluating binary image segmentation.\n    The coefficient between 0 to 1, and 1 means totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A batch of distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    Notes\n    ------\n    - IoU cannot be used as training loss, people usually use dice coefficient for training, IoU and hard-dice for evaluating.\n\n    \"\"\"\n    pre = tf.cast(output > threshold, dtype=tf.float32)\n    truth = tf.cast(target > threshold, dtype=tf.float32)\n    inse = tf.reduce_sum(tf.multiply(pre, truth), axis=axis)  # AND\n    union = tf.reduce_sum(tf.cast(tf.add(pre, truth) >= 1, dtype=tf.float32), axis=axis)  # OR\n    # old axis=[0,1,2,3]\n    # epsilon = 1e-5\n    # batch_iou = inse / (union + epsilon)\n    # new haodong\n    batch_iou = (inse + smooth) / (union + smooth)\n    iou = tf.reduce_mean(batch_iou, name='iou_coe')\n    return iou",
    "doc": "Non-differentiable Intersection over Union (IoU) for comparing the\n    similarity of two batch of data, usually be used for evaluating binary image segmentation.\n    The coefficient between 0 to 1, and 1 means totally match.\n\n    Parameters\n    -----------\n    output : tensor\n        A batch of distribution with shape: [batch_size, ....], (any dimensions).\n    target : tensor\n        The target distribution, format the same with `output`.\n    threshold : float\n        The threshold value to be true.\n    axis : tuple of integer\n        All dimensions are reduced, default ``(1,2,3)``.\n    smooth : float\n        This small value will be added to the numerator and denominator, see ``dice_coe``.\n\n    Notes\n    ------\n    - IoU cannot be used as training loss, people usually use dice coefficient for training, IoU and hard-dice for evaluating."
  },
  {
    "code": "def cross_entropy_seq(logits, target_seqs, batch_size=None):  # , batch_size=1, num_steps=None):\n    \"\"\"Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for fixed length RNN outputs, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.\n\n    Parameters\n    ----------\n    logits : Tensor\n        2D tensor with shape of `[batch_size * n_steps, n_classes]`.\n    target_seqs : Tensor\n        The target sequence, 2D tensor `[batch_size, n_steps]`, if the number of step is dynamic, please use ``tl.cost.cross_entropy_seq_with_mask`` instead.\n    batch_size : None or int.\n        Whether to divide the cost by batch size.\n            - If integer, the return cost will be divided by `batch_size`.\n            - If None (default), the return cost will not be divided by anything.\n\n    Examples\n    --------\n    >>> see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.for more details\n    >>> input_data = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> targets = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> # build the network\n    >>> print(net.outputs)\n    (batch_size * n_steps, n_classes)\n    >>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)\n\n    \"\"\"\n    sequence_loss_by_example_fn = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n\n    loss = sequence_loss_by_example_fn(\n        [logits], [tf.reshape(target_seqs, [-1])], [tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)]\n    )\n    # [tf.ones([batch_size * num_steps])])\n    cost = tf.reduce_sum(loss)  # / batch_size\n    if batch_size is not None:\n        cost = cost / batch_size\n    return cost",
    "doc": "Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for fixed length RNN outputs, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.\n\n    Parameters\n    ----------\n    logits : Tensor\n        2D tensor with shape of `[batch_size * n_steps, n_classes]`.\n    target_seqs : Tensor\n        The target sequence, 2D tensor `[batch_size, n_steps]`, if the number of step is dynamic, please use ``tl.cost.cross_entropy_seq_with_mask`` instead.\n    batch_size : None or int.\n        Whether to divide the cost by batch size.\n            - If integer, the return cost will be divided by `batch_size`.\n            - If None (default), the return cost will not be divided by anything.\n\n    Examples\n    --------\n    >>> see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.for more details\n    >>> input_data = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> targets = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> # build the network\n    >>> print(net.outputs)\n    (batch_size * n_steps, n_classes)\n    >>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)"
  },
  {
    "code": "def cross_entropy_seq_with_mask(logits, target_seqs, input_mask, return_details=False, name=None):\n    \"\"\"Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for Dynamic RNN with Synced sequence input and output.\n\n    Parameters\n    -----------\n    logits : Tensor\n        2D tensor with shape of [batch_size * ?, n_classes], `?` means dynamic IDs for each example.\n        - Can be get from `DynamicRNNLayer` by setting ``return_seq_2d`` to `True`.\n    target_seqs : Tensor\n        int of tensor, like word ID. [batch_size, ?], `?` means dynamic IDs for each example.\n    input_mask : Tensor\n        The mask to compute loss, it has the same size with `target_seqs`, normally 0 or 1.\n    return_details : boolean\n        Whether to return detailed losses.\n            - If False (default), only returns the loss.\n            - If True, returns the loss, losses, weights and targets (see source code).\n\n    Examples\n    --------\n    >>> batch_size = 64\n    >>> vocab_size = 10000\n    >>> embedding_size = 256\n    >>> input_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"input\")\n    >>> target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target\")\n    >>> input_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"mask\")\n    >>> net = tl.layers.EmbeddingInputlayer(\n    ...         inputs = input_seqs,\n    ...         vocabulary_size = vocab_size,\n    ...         embedding_size = embedding_size,\n    ...         name = 'seq_embedding')\n    >>> net = tl.layers.DynamicRNNLayer(net,\n    ...         cell_fn = tf.contrib.rnn.BasicLSTMCell,\n    ...         n_hidden = embedding_size,\n    ...         dropout = (0.7 if is_train else None),\n    ...         sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),\n    ...         return_seq_2d = True,\n    ...         name = 'dynamicrnn')\n    >>> print(net.outputs)\n    (?, 256)\n    >>> net = tl.layers.DenseLayer(net, n_units=vocab_size, name=\"output\")\n    >>> print(net.outputs)\n    (?, 10000)\n    >>> loss = tl.cost.cross_entropy_seq_with_mask(net.outputs, target_seqs, input_mask)\n\n    \"\"\"\n    targets = tf.reshape(target_seqs, [-1])  # to one vector\n    weights = tf.to_float(tf.reshape(input_mask, [-1]))  # to one vector like targets\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name) * weights\n    # losses = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name)) # for TF1.0 and others\n\n    loss = tf.divide(\n        tf.reduce_sum(losses),  # loss from mask. reduce_sum before element-wise mul with mask !!\n        tf.reduce_sum(weights),\n        name=\"seq_loss_with_mask\"\n    )\n\n    if return_details:\n        return loss, losses, weights, targets\n    else:\n        return loss",
    "doc": "Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for Dynamic RNN with Synced sequence input and output.\n\n    Parameters\n    -----------\n    logits : Tensor\n        2D tensor with shape of [batch_size * ?, n_classes], `?` means dynamic IDs for each example.\n        - Can be get from `DynamicRNNLayer` by setting ``return_seq_2d`` to `True`.\n    target_seqs : Tensor\n        int of tensor, like word ID. [batch_size, ?], `?` means dynamic IDs for each example.\n    input_mask : Tensor\n        The mask to compute loss, it has the same size with `target_seqs`, normally 0 or 1.\n    return_details : boolean\n        Whether to return detailed losses.\n            - If False (default), only returns the loss.\n            - If True, returns the loss, losses, weights and targets (see source code).\n\n    Examples\n    --------\n    >>> batch_size = 64\n    >>> vocab_size = 10000\n    >>> embedding_size = 256\n    >>> input_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"input\")\n    >>> target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target\")\n    >>> input_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"mask\")\n    >>> net = tl.layers.EmbeddingInputlayer(\n    ...         inputs = input_seqs,\n    ...         vocabulary_size = vocab_size,\n    ...         embedding_size = embedding_size,\n    ...         name = 'seq_embedding')\n    >>> net = tl.layers.DynamicRNNLayer(net,\n    ...         cell_fn = tf.contrib.rnn.BasicLSTMCell,\n    ...         n_hidden = embedding_size,\n    ...         dropout = (0.7 if is_train else None),\n    ...         sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),\n    ...         return_seq_2d = True,\n    ...         name = 'dynamicrnn')\n    >>> print(net.outputs)\n    (?, 256)\n    >>> net = tl.layers.DenseLayer(net, n_units=vocab_size, name=\"output\")\n    >>> print(net.outputs)\n    (?, 10000)\n    >>> loss = tl.cost.cross_entropy_seq_with_mask(net.outputs, target_seqs, input_mask)"
  },
  {
    "code": "def cosine_similarity(v1, v2):\n    \"\"\"Cosine similarity [-1, 1].\n\n    Parameters\n    ----------\n    v1, v2 : Tensor\n        Tensor with the same shape [batch_size, n_feature].\n\n    References\n    ----------\n    - `Wiki <https://en.wikipedia.org/wiki/Cosine_similarity>`__.\n\n    \"\"\"\n\n    return tf.reduce_sum(tf.multiply(v1, v2), 1) / \\\n        (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) *\n         tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))",
    "doc": "Cosine similarity [-1, 1].\n\n    Parameters\n    ----------\n    v1, v2 : Tensor\n        Tensor with the same shape [batch_size, n_feature].\n\n    References\n    ----------\n    - `Wiki <https://en.wikipedia.org/wiki/Cosine_similarity>`__."
  },
  {
    "code": "def li_regularizer(scale, scope=None):\n    \"\"\"Li regularization removes the neurons of previous layer. The `i` represents `inputs`.\n    Returns a function that can be used to apply group li regularization to weights.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: str\n        An optional scope name for this function.\n\n    Returns\n    --------\n    A function with signature `li(weights, name=None)` that apply Li regularization.\n\n    Raises\n    ------\n    ValueError : if scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    \"\"\"\n    if isinstance(scale, numbers.Integral):\n        raise ValueError('scale cannot be an integer: %s' % scale)\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError('Setting a scale less than 0 on a regularizer: %g' % scale)\n        if scale >= 1.:\n            raise ValueError('Setting a scale greater than 1 on a regularizer: %g' % scale)\n        if scale == 0.:\n            tl.logging.info('Scale of 0 disables regularizer.')\n            return lambda _, name=None: None\n\n    def li(weights):\n        \"\"\"Applies li regularization to weights.\"\"\"\n        with tf.name_scope('li_regularizer') as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name='scale')\n            # if tf.__version__ <= '0.12':\n            #     standard_ops_fn = standard_ops.mul\n            # else:\n            standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.sqrt(standard_ops.reduce_sum(tf.square(weights), 1))),\n                name=scope\n            )\n\n    return li",
    "doc": "Li regularization removes the neurons of previous layer. The `i` represents `inputs`.\n    Returns a function that can be used to apply group li regularization to weights.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: str\n        An optional scope name for this function.\n\n    Returns\n    --------\n    A function with signature `li(weights, name=None)` that apply Li regularization.\n\n    Raises\n    ------\n    ValueError : if scale is outside of the range [0.0, 1.0] or if scale is not a float."
  },
  {
    "code": "def maxnorm_regularizer(scale=1.0):\n    \"\"\"Max-norm regularization returns a function that can be used to apply max-norm regularization to weights.\n\n    More about max-norm, see `wiki-max norm <https://en.wikipedia.org/wiki/Matrix_norm#Max_norm>`_.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    --------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    \"\"\"\n    if isinstance(scale, numbers.Integral):\n        raise ValueError('scale cannot be an integer: %s' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError('Setting a scale less than 0 on a regularizer: %g' % scale)\n        # if scale >= 1.:\n        #   raise ValueError('Setting a scale greater than 1 on a regularizer: %g' %\n        #                    scale)\n        if scale == 0.:\n            tl.logging.info('Scale of 0 disables regularizer.')\n            return lambda _, name=None: None\n\n    def mn(weights, name='max_regularizer'):\n        \"\"\"Applies max-norm regularization to weights.\"\"\"\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name='scale')\n            #   if tf.__version__ <= '0.12':\n            #       standard_ops_fn = standard_ops.mul\n            #   else:\n            standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(my_scale, standard_ops.reduce_max(standard_ops.abs(weights)), name=scope)\n\n    return mn",
    "doc": "Max-norm regularization returns a function that can be used to apply max-norm regularization to weights.\n\n    More about max-norm, see `wiki-max norm <https://en.wikipedia.org/wiki/Matrix_norm#Max_norm>`_.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    --------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float."
  },
  {
    "code": "def maxnorm_o_regularizer(scale):\n    \"\"\"Max-norm output regularization removes the neurons of current layer.\n    Returns a function that can be used to apply max-norm regularization to each column of weight matrix.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn_o(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    ---------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float.\n\n    \"\"\"\n    if isinstance(scale, numbers.Integral):\n        raise ValueError('scale cannot be an integer: %s' % scale)\n\n    if isinstance(scale, numbers.Real):\n        if scale < 0.:\n            raise ValueError('Setting a scale less than 0 on a regularizer: %g' % scale)\n        # if scale >= 1.:\n        #   raise ValueError('Setting a scale greater than 1 on a regularizer: %g' %\n        #                    scale)\n        if scale == 0.:\n            tl.logging.info('Scale of 0 disables regularizer.')\n            return lambda _, name=None: None\n\n    def mn_o(weights, name='maxnorm_o_regularizer'):\n        \"\"\"Applies max-norm regularization to weights.\"\"\"\n        with tf.name_scope(name) as scope:\n            my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name='scale')\n            if tf.__version__ <= '0.12':\n                standard_ops_fn = standard_ops.mul\n            else:\n                standard_ops_fn = standard_ops.multiply\n            return standard_ops_fn(\n                my_scale, standard_ops.reduce_sum(standard_ops.reduce_max(standard_ops.abs(weights), 0)), name=scope\n            )\n\n    return mn_o",
    "doc": "Max-norm output regularization removes the neurons of current layer.\n    Returns a function that can be used to apply max-norm regularization to each column of weight matrix.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n\n    Returns\n    ---------\n    A function with signature `mn_o(weights, name=None)` that apply Lo regularization.\n\n    Raises\n    ---------\n    ValueError : If scale is outside of the range [0.0, 1.0] or if scale is not a float."
  },
  {
    "code": "def ramp(x, v_min=0, v_max=1, name=None):\n    \"\"\"Ramp activation function.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    v_min : float\n        cap input to v_min as a lower bound.\n    v_max : float\n        cap input to v_max as a upper bound.\n    name : str\n        The function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    \"\"\"\n    return tf.clip_by_value(x, clip_value_min=v_min, clip_value_max=v_max, name=name)",
    "doc": "Ramp activation function.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    v_min : float\n        cap input to v_min as a lower bound.\n    v_max : float\n        cap input to v_max as a upper bound.\n    name : str\n        The function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``."
  },
  {
    "code": "def leaky_relu(x, alpha=0.2, name=\"leaky_relu\"):\n    \"\"\"leaky_relu can be used through its shortcut: :func:`tl.act.lrelu`.\n\n    This function is a modified version of ReLU, introducing a nonzero gradient for negative input. Introduced by the paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x >= 0: ``f(x) = x``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.lrelu(x, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    \"\"\"\n    if not (0 < alpha <= 1):\n        raise ValueError(\"`alpha` value must be in [0, 1]`\")\n\n    with tf.name_scope(name, \"leaky_relu\") as name_scope:\n        x = tf.convert_to_tensor(x, name=\"features\")\n        return tf.maximum(x, alpha * x, name=name_scope)",
    "doc": "leaky_relu can be used through its shortcut: :func:`tl.act.lrelu`.\n\n    This function is a modified version of ReLU, introducing a nonzero gradient for negative input. Introduced by the paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x >= 0: ``f(x) = x``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.lrelu(x, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__"
  },
  {
    "code": "def leaky_relu6(x, alpha=0.2, name=\"leaky_relu6\"):\n    \"\"\":func:`leaky_relu6` can be used through its shortcut: :func:`tl.act.lrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.leaky_relu6(x, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n    \"\"\"\n    if not isinstance(alpha, tf.Tensor) and not (0 < alpha <= 1):\n        raise ValueError(\"`alpha` value must be in [0, 1]`\")\n\n    with tf.name_scope(name, \"leaky_relu6\") as name_scope:\n        x = tf.convert_to_tensor(x, name=\"features\")\n        return tf.minimum(tf.maximum(x, alpha * x), 6, name=name_scope)",
    "doc": ":func:`leaky_relu6` can be used through its shortcut: :func:`tl.act.lrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha : float\n        Slope.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.leaky_relu6(x, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__"
  },
  {
    "code": "def leaky_twice_relu6(x, alpha_low=0.2, alpha_high=0.2, name=\"leaky_relu6\"):\n    \"\"\":func:`leaky_twice_relu6` can be used through its shortcut: :func:`:func:`tl.act.ltrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    This function push further the logic by adding `leaky` behaviour both below zero and above six.\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6 + (alpha_high * (x-6))``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha_low : float\n        Slope for x < 0: ``f(x) = alpha_low * x``.\n    alpha_high : float\n        Slope for x < 6: ``f(x) = 6 (alpha_high * (x-6))``.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.leaky_twice_relu6(x, 0.2, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    \"\"\"\n    if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1):\n        raise ValueError(\"`alpha_high` value must be in [0, 1]`\")\n\n    if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):\n        raise ValueError(\"`alpha_low` value must be in [0, 1]`\")\n\n    with tf.name_scope(name, \"leaky_twice_relu6\") as name_scope:\n        x = tf.convert_to_tensor(x, name=\"features\")\n\n        x_is_above_0 = tf.minimum(x, 6 * (1 - alpha_high) + alpha_high * x)\n        x_is_below_0 = tf.minimum(alpha_low * x, 0)\n\n        return tf.maximum(x_is_above_0, x_is_below_0, name=name_scope)",
    "doc": ":func:`leaky_twice_relu6` can be used through its shortcut: :func:`:func:`tl.act.ltrelu6`.\n\n    This activation function is a modified version :func:`leaky_relu` introduced by the following paper:\n    `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n\n    This activation function also follows the behaviour of the activation function :func:`tf.nn.relu6` introduced by the following paper:\n    `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__\n\n    This function push further the logic by adding `leaky` behaviour both below zero and above six.\n\n    The function return the following results:\n      - When x < 0: ``f(x) = alpha_low * x``.\n      - When x in [0, 6]: ``f(x) = x``.\n      - When x > 6: ``f(x) = 6 + (alpha_high * (x-6))``.\n\n    Parameters\n    ----------\n    x : Tensor\n        Support input type ``float``, ``double``, ``int32``, ``int64``, ``uint8``, ``int16``, or ``int8``.\n    alpha_low : float\n        Slope for x < 0: ``f(x) = alpha_low * x``.\n    alpha_high : float\n        Slope for x < 6: ``f(x) = 6 (alpha_high * (x-6))``.\n    name : str\n        The function name (optional).\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> net = tl.layers.DenseLayer(net, 100, act=lambda x : tl.act.leaky_twice_relu6(x, 0.2, 0.2), name='dense')\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    References\n    ----------\n    - `Rectifier Nonlinearities Improve Neural Network Acoustic Models [A. L. Maas et al., 2013] <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`__\n    - `Convolutional Deep Belief Networks on CIFAR-10 [A. Krizhevsky, 2010] <http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf>`__"
  },
  {
    "code": "def swish(x, name='swish'):\n    \"\"\"Swish function.\n\n     See `Swish: a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941>`__.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    name: str\n        function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    \"\"\"\n    with tf.name_scope(name):\n        x = tf.nn.sigmoid(x) * x\n    return x",
    "doc": "Swish function.\n\n     See `Swish: a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941>`__.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n    name: str\n        function name (optional).\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``."
  },
  {
    "code": "def pixel_wise_softmax(x, name='pixel_wise_softmax'):\n    \"\"\"Return the softmax outputs of images, every pixels have multiple label, the sum of a pixel is 1.\n\n    Usually be used for image segmentation.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n            - For 2d image, 4D tensor (batch_size, height, weight, channel), where channel >= 2.\n            - For 3d image, 5D tensor (batch_size, depth, height, weight, channel), where channel >= 2.\n    name : str\n        function name (optional)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    Examples\n    --------\n    >>> outputs = pixel_wise_softmax(network.outputs)\n    >>> dice_loss = 1 - dice_coe(outputs, y_, epsilon=1e-5)\n\n    References\n    ----------\n    - `tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`__\n\n    \"\"\"\n    with tf.name_scope(name):\n        return tf.nn.softmax(x)",
    "doc": "Return the softmax outputs of images, every pixels have multiple label, the sum of a pixel is 1.\n\n    Usually be used for image segmentation.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n            - For 2d image, 4D tensor (batch_size, height, weight, channel), where channel >= 2.\n            - For 3d image, 5D tensor (batch_size, depth, height, weight, channel), where channel >= 2.\n    name : str\n        function name (optional)\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    Examples\n    --------\n    >>> outputs = pixel_wise_softmax(network.outputs)\n    >>> dice_loss = 1 - dice_coe(outputs, y_, epsilon=1e-5)\n\n    References\n    ----------\n    - `tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`__"
  },
  {
    "code": "def _conv_linear(args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n    \"\"\"convolution:\n\n    Parameters\n    ----------\n    args : tensor\n        4D Tensor or a list of 4D, batch x n, Tensors.\n    filter_size : tuple of int\n        Filter height and width.\n    num_features : int\n        Nnumber of features.\n    bias_start : float\n        Starting value to initialize the bias; 0 by default.\n    scope : VariableScope\n        For the created subgraph; defaults to \"Linear\".\n\n    Returns\n    --------\n    - A 4D Tensor with shape [batch h w num_features]\n\n    Raises\n    -------\n    - ValueError : if some of the arguments has unspecified or wrong shape.\n\n    \"\"\"\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size_depth = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 4:\n            raise ValueError(\"Linear is expecting 4D arguments: %s\" % str(shapes))\n        if not shape[3]:\n            raise ValueError(\"Linear expects shape[4] of arguments: %s\" % str(shapes))\n        else:\n            total_arg_size_depth += shape[3]\n\n    dtype = [a.dtype for a in args][0]\n\n    # Now the computation.\n    with tf.variable_scope(scope or \"Conv\"):\n        matrix = tf.get_variable(\n            \"Matrix\", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype\n        )\n        if len(args) == 1:\n            res = tf.nn.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding='SAME')\n        else:\n            res = tf.nn.conv2d(tf.concat(args, 3), matrix, strides=[1, 1, 1, 1], padding='SAME')\n        if not bias:\n            return res\n        bias_term = tf.get_variable(\n            \"Bias\", [num_features], dtype=dtype, initializer=tf.constant_initializer(bias_start, dtype=dtype)\n        )\n    return res + bias_term",
    "doc": "convolution:\n\n    Parameters\n    ----------\n    args : tensor\n        4D Tensor or a list of 4D, batch x n, Tensors.\n    filter_size : tuple of int\n        Filter height and width.\n    num_features : int\n        Nnumber of features.\n    bias_start : float\n        Starting value to initialize the bias; 0 by default.\n    scope : VariableScope\n        For the created subgraph; defaults to \"Linear\".\n\n    Returns\n    --------\n    - A 4D Tensor with shape [batch h w num_features]\n\n    Raises\n    -------\n    - ValueError : if some of the arguments has unspecified or wrong shape."
  },
  {
    "code": "def advanced_indexing_op(inputs, index):\n    \"\"\"Advanced Indexing for Sequences, returns the outputs by given sequence lengths.\n    When return the last output :class:`DynamicRNNLayer` uses it to get the last outputs with the sequence lengths.\n\n    Parameters\n    -----------\n    inputs : tensor for data\n        With shape of [batch_size, n_step(max), n_features]\n    index : tensor for indexing\n        Sequence length in Dynamic RNN. [batch_size]\n\n    Examples\n    ---------\n    >>> import numpy as np\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> batch_size, max_length, n_features = 3, 5, 2\n    >>> z = np.random.uniform(low=-1, high=1, size=[batch_size, max_length, n_features]).astype(np.float32)\n    >>> b_z = tf.constant(z)\n    >>> sl = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n    >>> o = advanced_indexing_op(b_z, sl)\n    >>>\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>>\n    >>> order = np.asarray([1,1,2])\n    >>> print(\"real\",z[0][order[0]-1], z[1][order[1]-1], z[2][order[2]-1])\n    >>> y = sess.run([o], feed_dict={sl:order})\n    >>> print(\"given\",order)\n    >>> print(\"out\", y)\n    real [-0.93021595  0.53820813] [-0.92548317 -0.77135968] [ 0.89952248  0.19149846]\n    given [1 1 2]\n    out [array([[-0.93021595,  0.53820813],\n                [-0.92548317, -0.77135968],\n                [ 0.89952248,  0.19149846]], dtype=float32)]\n\n    References\n    -----------\n    - Modified from TFlearn (the original code is used for fixed length rnn), `references <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__.\n\n    \"\"\"\n    batch_size = tf.shape(inputs)[0]\n    # max_length = int(inputs.get_shape()[1])    # for fixed length rnn, length is given\n    max_length = tf.shape(inputs)[1]  # for dynamic_rnn, length is unknown\n    dim_size = int(inputs.get_shape()[2])\n    index = tf.range(0, batch_size) * max_length + (index - 1)\n    flat = tf.reshape(inputs, [-1, dim_size])\n    relevant = tf.gather(flat, index)\n    return relevant",
    "doc": "Advanced Indexing for Sequences, returns the outputs by given sequence lengths.\n    When return the last output :class:`DynamicRNNLayer` uses it to get the last outputs with the sequence lengths.\n\n    Parameters\n    -----------\n    inputs : tensor for data\n        With shape of [batch_size, n_step(max), n_features]\n    index : tensor for indexing\n        Sequence length in Dynamic RNN. [batch_size]\n\n    Examples\n    ---------\n    >>> import numpy as np\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> batch_size, max_length, n_features = 3, 5, 2\n    >>> z = np.random.uniform(low=-1, high=1, size=[batch_size, max_length, n_features]).astype(np.float32)\n    >>> b_z = tf.constant(z)\n    >>> sl = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n    >>> o = advanced_indexing_op(b_z, sl)\n    >>>\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>>\n    >>> order = np.asarray([1,1,2])\n    >>> print(\"real\",z[0][order[0]-1], z[1][order[1]-1], z[2][order[2]-1])\n    >>> y = sess.run([o], feed_dict={sl:order})\n    >>> print(\"given\",order)\n    >>> print(\"out\", y)\n    real [-0.93021595  0.53820813] [-0.92548317 -0.77135968] [ 0.89952248  0.19149846]\n    given [1 1 2]\n    out [array([[-0.93021595,  0.53820813],\n                [-0.92548317, -0.77135968],\n                [ 0.89952248,  0.19149846]], dtype=float32)]\n\n    References\n    -----------\n    - Modified from TFlearn (the original code is used for fixed length rnn), `references <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__."
  },
  {
    "code": "def retrieve_seq_length_op(data):\n    \"\"\"An op to compute the length of a sequence from input shape of [batch_size, n_step(max), n_features],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max), n_features] with zero padding on right hand side.\n\n    Examples\n    ---------\n    >>> data = [[[1],[2],[0],[0],[0]],\n    ...         [[1],[2],[3],[0],[0]],\n    ...         [[1],[2],[6],[1],[0]]]\n    >>> data = np.asarray(data)\n    >>> print(data.shape)\n    (3, 5, 1)\n    >>> data = tf.constant(data)\n    >>> sl = retrieve_seq_length_op(data)\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>> y = sl.eval()\n    [2 3 4]\n\n    Multiple features\n    >>> data = [[[1,2],[2,2],[1,2],[1,2],[0,0]],\n    ...         [[2,3],[2,4],[3,2],[0,0],[0,0]],\n    ...         [[3,3],[2,2],[5,3],[1,2],[0,0]]]\n    >>> print(sl)\n    [4 3 4]\n\n    References\n    ------------\n    Borrow from `TFlearn <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__.\n\n    \"\"\"\n    with tf.name_scope('GetLength'):\n        used = tf.sign(tf.reduce_max(tf.abs(data), 2))\n        length = tf.reduce_sum(used, 1)\n\n        return tf.cast(length, tf.int32)",
    "doc": "An op to compute the length of a sequence from input shape of [batch_size, n_step(max), n_features],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max), n_features] with zero padding on right hand side.\n\n    Examples\n    ---------\n    >>> data = [[[1],[2],[0],[0],[0]],\n    ...         [[1],[2],[3],[0],[0]],\n    ...         [[1],[2],[6],[1],[0]]]\n    >>> data = np.asarray(data)\n    >>> print(data.shape)\n    (3, 5, 1)\n    >>> data = tf.constant(data)\n    >>> sl = retrieve_seq_length_op(data)\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>> y = sl.eval()\n    [2 3 4]\n\n    Multiple features\n    >>> data = [[[1,2],[2,2],[1,2],[1,2],[0,0]],\n    ...         [[2,3],[2,4],[3,2],[0,0],[0,0]],\n    ...         [[3,3],[2,2],[5,3],[1,2],[0,0]]]\n    >>> print(sl)\n    [4 3 4]\n\n    References\n    ------------\n    Borrow from `TFlearn <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__."
  },
  {
    "code": "def retrieve_seq_length_op2(data):\n    \"\"\"An op to compute the length of a sequence, from input shape of [batch_size, n_step(max)],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max)] with zero padding on right hand side.\n\n    Examples\n    --------\n    >>> data = [[1,2,0,0,0],\n    ...         [1,2,3,0,0],\n    ...         [1,2,6,1,0]]\n    >>> o = retrieve_seq_length_op2(data)\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>> print(o.eval())\n    [2 3 4]\n\n    \"\"\"\n    return tf.reduce_sum(tf.cast(tf.greater(data, tf.zeros_like(data)), tf.int32), 1)",
    "doc": "An op to compute the length of a sequence, from input shape of [batch_size, n_step(max)],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max)] with zero padding on right hand side.\n\n    Examples\n    --------\n    >>> data = [[1,2,0,0,0],\n    ...         [1,2,3,0,0],\n    ...         [1,2,6,1,0]]\n    >>> o = retrieve_seq_length_op2(data)\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>> print(o.eval())\n    [2 3 4]"
  },
  {
    "code": "def retrieve_seq_length_op3(data, pad_val=0):  # HangSheng: return tensor for sequence length, if input is tf.string\n    \"\"\"Return tensor for sequence length, if input is ``tf.string``.\"\"\"\n    data_shape_size = data.get_shape().ndims\n    if data_shape_size == 3:\n        return tf.reduce_sum(tf.cast(tf.reduce_any(tf.not_equal(data, pad_val), axis=2), dtype=tf.int32), 1)\n    elif data_shape_size == 2:\n        return tf.reduce_sum(tf.cast(tf.not_equal(data, pad_val), dtype=tf.int32), 1)\n    elif data_shape_size == 1:\n        raise ValueError(\"retrieve_seq_length_op3: data has wrong shape!\")\n    else:\n        raise ValueError(\n            \"retrieve_seq_length_op3: handling data_shape_size %s hasn't been implemented!\" % (data_shape_size)\n        )",
    "doc": "Return tensor for sequence length, if input is ``tf.string``."
  },
  {
    "code": "def zero_state(self, batch_size, dtype=LayersConfig.tf_dtype):\n        \"\"\"Return zero-filled state tensor(s).\n        Args:\n          batch_size: int, float, or unit Tensor representing the batch size.\n        Returns:\n          tensor of shape '[batch_size x shape[0] x shape[1] x num_features]\n          filled with zeros\n\n        \"\"\"\n        shape = self.shape\n        num_features = self.num_features\n        # TODO : TypeError: 'NoneType' object is not subscriptable\n        zeros = tf.zeros([batch_size, shape[0], shape[1], num_features * 2], dtype=dtype)\n        return zeros",
    "doc": "Return zero-filled state tensor(s).\n        Args:\n          batch_size: int, float, or unit Tensor representing the batch size.\n        Returns:\n          tensor of shape '[batch_size x shape[0] x shape[1] x num_features]\n          filled with zeros"
  },
  {
    "code": "def state_size(self):\n        \"\"\"State size of the LSTMStateTuple.\"\"\"\n        return (LSTMStateTuple(self._num_units, self._num_units) if self._state_is_tuple else 2 * self._num_units)",
    "doc": "State size of the LSTMStateTuple."
  },
  {
    "code": "def _to_bc_h_w(self, x, x_shape):\n        \"\"\"(b, h, w, c) -> (b*c, h, w)\"\"\"\n        x = tf.transpose(x, [0, 3, 1, 2])\n        x = tf.reshape(x, (-1, x_shape[1], x_shape[2]))\n        return x",
    "doc": "(b, h, w, c) -> (b*c, h, w)"
  },
  {
    "code": "def _to_b_h_w_n_c(self, x, x_shape):\n        \"\"\"(b*c, h, w, n) -> (b, h, w, n, c)\"\"\"\n        x = tf.reshape(x, (-1, x_shape[4], x_shape[1], x_shape[2], x_shape[3]))\n        x = tf.transpose(x, [0, 2, 3, 4, 1])\n        return x",
    "doc": "(b*c, h, w, n) -> (b, h, w, n, c)"
  },
  {
    "code": "def _tf_repeat(self, a, repeats):\n        \"\"\"Tensorflow version of np.repeat for 1D\"\"\"\n        # https://github.com/tensorflow/tensorflow/issues/8521\n\n        if len(a.get_shape()) != 1:\n            raise AssertionError(\"This is not a 1D Tensor\")\n\n        a = tf.expand_dims(a, -1)\n        a = tf.tile(a, [1, repeats])\n        a = self.tf_flatten(a)\n        return a",
    "doc": "Tensorflow version of np.repeat for 1D"
  },
  {
    "code": "def _tf_batch_map_coordinates(self, inputs, coords):\n        \"\"\"Batch version of tf_map_coordinates\n\n        Only supports 2D feature maps\n\n        Parameters\n        ----------\n        inputs : ``tf.Tensor``\n            shape = (b*c, h, w)\n        coords : ``tf.Tensor``\n            shape = (b*c, h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b*c, h, w, n)\n\n        \"\"\"\n        input_shape = inputs.get_shape()\n        coords_shape = coords.get_shape()\n        batch_channel = tf.shape(inputs)[0]\n        input_h = int(input_shape[1])\n        input_w = int(input_shape[2])\n        kernel_n = int(coords_shape[3])\n        n_coords = input_h * input_w * kernel_n\n\n        coords_lt = tf.cast(tf.floor(coords), 'int32')\n        coords_rb = tf.cast(tf.ceil(coords), 'int32')\n        coords_lb = tf.stack([coords_lt[:, :, :, :, 0], coords_rb[:, :, :, :, 1]], axis=-1)\n        coords_rt = tf.stack([coords_rb[:, :, :, :, 0], coords_lt[:, :, :, :, 1]], axis=-1)\n\n        idx = self._tf_repeat(tf.range(batch_channel), n_coords)\n\n        vals_lt = self._get_vals_by_coords(inputs, coords_lt, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_rb = self._get_vals_by_coords(inputs, coords_rb, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_lb = self._get_vals_by_coords(inputs, coords_lb, idx, (batch_channel, input_h, input_w, kernel_n))\n        vals_rt = self._get_vals_by_coords(inputs, coords_rt, idx, (batch_channel, input_h, input_w, kernel_n))\n\n        coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n\n        vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, :, :, :, 0]\n        vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, :, :, :, 0]\n        mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, :, :, :, 1]\n\n        return mapped_vals",
    "doc": "Batch version of tf_map_coordinates\n\n        Only supports 2D feature maps\n\n        Parameters\n        ----------\n        inputs : ``tf.Tensor``\n            shape = (b*c, h, w)\n        coords : ``tf.Tensor``\n            shape = (b*c, h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b*c, h, w, n)"
  },
  {
    "code": "def _tf_batch_map_offsets(self, inputs, offsets, grid_offset):\n        \"\"\"Batch map offsets into input\n\n        Parameters\n        ------------\n        inputs : ``tf.Tensor``\n            shape = (b, h, w, c)\n        offsets: ``tf.Tensor``\n            shape = (b, h, w, 2*n)\n        grid_offset: `tf.Tensor``\n            Offset grids shape = (h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b, h, w, c)\n\n        \"\"\"\n        input_shape = inputs.get_shape()\n        batch_size = tf.shape(inputs)[0]\n        kernel_n = int(int(offsets.get_shape()[3]) / 2)\n        input_h = input_shape[1]\n        input_w = input_shape[2]\n        channel = input_shape[3]\n\n        # inputs (b, h, w, c) --> (b*c, h, w)\n        inputs = self._to_bc_h_w(inputs, input_shape)\n\n        # offsets (b, h, w, 2*n) --> (b, h, w, n, 2)\n        offsets = tf.reshape(offsets, (batch_size, input_h, input_w, kernel_n, 2))\n        # offsets (b, h, w, n, 2) --> (b*c, h, w, n, 2)\n        # offsets = tf.tile(offsets, [channel, 1, 1, 1, 1])\n\n        coords = tf.expand_dims(grid_offset, 0)  # grid_offset --> (1, h, w, n, 2)\n        coords = tf.tile(coords, [batch_size, 1, 1, 1, 1]) + offsets  # grid_offset --> (b, h, w, n, 2)\n\n        # clip out of bound\n        coords = tf.stack(\n            [\n                tf.clip_by_value(coords[:, :, :, :, 0], 0.0, tf.cast(input_h - 1, 'float32')),\n                tf.clip_by_value(coords[:, :, :, :, 1], 0.0, tf.cast(input_w - 1, 'float32'))\n            ], axis=-1\n        )\n        coords = tf.tile(coords, [channel, 1, 1, 1, 1])\n\n        mapped_vals = self._tf_batch_map_coordinates(inputs, coords)\n        # (b*c, h, w, n) --> (b, h, w, n, c)\n        mapped_vals = self._to_b_h_w_n_c(mapped_vals, [batch_size, input_h, input_w, kernel_n, channel])\n\n        return mapped_vals",
    "doc": "Batch map offsets into input\n\n        Parameters\n        ------------\n        inputs : ``tf.Tensor``\n            shape = (b, h, w, c)\n        offsets: ``tf.Tensor``\n            shape = (b, h, w, 2*n)\n        grid_offset: `tf.Tensor``\n            Offset grids shape = (h, w, n, 2)\n\n        Returns\n        -------\n        ``tf.Tensor``\n            A Tensor with the shape as (b, h, w, c)"
  },
  {
    "code": "def minibatches(inputs=None, targets=None, batch_size=None, allow_dynamic_batch_size=False, shuffle=False):\n    \"\"\"Generate a generator that input a group of example in numpy.array and\n    their labels, return the examples and labels by the given batch size.\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every row is a example.\n    batch_size : int\n        The batch size.\n    allow_dynamic_batch_size: boolean\n        Allow the use of the last data batch in case the number of examples is not a multiple of batch_size, this may result in unexpected behaviour if other functions expect a fixed-sized batch-size.\n    shuffle : boolean\n        Indicating whether to use a shuffling queue, shuffle the dataset before return.\n\n    Examples\n    --------\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.minibatches(inputs=X, targets=y, batch_size=2, shuffle=False):\n    >>>     print(batch)\n    (array([['a', 'a'], ['b', 'b']], dtype='<U1'), array([0, 1]))\n    (array([['c', 'c'], ['d', 'd']], dtype='<U1'), array([2, 3]))\n    (array([['e', 'e'], ['f', 'f']], dtype='<U1'), array([4, 5]))\n\n    Notes\n    -----\n    If you have two inputs and one label and want to shuffle them together, e.g. X1 (1000, 100), X2 (1000, 80) and Y (1000, 1), you can stack them together (`np.hstack((X1, X2))`)\n    into (1000, 180) and feed to ``inputs``. After getting a batch, you can split it back into X1 and X2.\n\n    \"\"\"\n    if len(inputs) != len(targets):\n        raise AssertionError(\"The length of inputs and targets should be equal\")\n\n    if shuffle:\n        indices = np.arange(len(inputs))\n        np.random.shuffle(indices)\n\n    # for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n    # chulei: handling the case where the number of samples is not a multiple of batch_size, avoiding wasting samples\n    for start_idx in range(0, len(inputs), batch_size):\n        end_idx = start_idx + batch_size\n        if end_idx > len(inputs):\n            if allow_dynamic_batch_size:\n                end_idx = len(inputs)\n            else:\n                break\n        if shuffle:\n            excerpt = indices[start_idx:end_idx]\n        else:\n            excerpt = slice(start_idx, end_idx)\n        if (isinstance(inputs, list) or isinstance(targets, list)) and (shuffle ==True):\n            # zsdonghao: for list indexing when shuffle==True\n            yield [inputs[i] for i in excerpt], [targets[i] for i in excerpt]\n        else:\n            yield inputs[excerpt], targets[excerpt]",
    "doc": "Generate a generator that input a group of example in numpy.array and\n    their labels, return the examples and labels by the given batch size.\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every row is a example.\n    batch_size : int\n        The batch size.\n    allow_dynamic_batch_size: boolean\n        Allow the use of the last data batch in case the number of examples is not a multiple of batch_size, this may result in unexpected behaviour if other functions expect a fixed-sized batch-size.\n    shuffle : boolean\n        Indicating whether to use a shuffling queue, shuffle the dataset before return.\n\n    Examples\n    --------\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.minibatches(inputs=X, targets=y, batch_size=2, shuffle=False):\n    >>>     print(batch)\n    (array([['a', 'a'], ['b', 'b']], dtype='<U1'), array([0, 1]))\n    (array([['c', 'c'], ['d', 'd']], dtype='<U1'), array([2, 3]))\n    (array([['e', 'e'], ['f', 'f']], dtype='<U1'), array([4, 5]))\n\n    Notes\n    -----\n    If you have two inputs and one label and want to shuffle them together, e.g. X1 (1000, 100), X2 (1000, 80) and Y (1000, 1), you can stack them together (`np.hstack((X1, X2))`)\n    into (1000, 180) and feed to ``inputs``. After getting a batch, you can split it back into X1 and X2."
  },
  {
    "code": "def seq_minibatches(inputs, targets, batch_size, seq_length, stride=1):\n    \"\"\"Generate a generator that return a batch of sequence inputs and targets.\n    If `batch_size=100` and `seq_length=5`, one return will have 500 rows (examples).\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every element is a example.\n    batch_size : int\n        The batch size.\n    seq_length : int\n        The sequence length.\n    stride : int\n        The stride step, default is 1.\n\n    Examples\n    --------\n    Synced sequence input and output.\n\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> y = np.asarray([0, 1, 2, 3, 4, 5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=y, batch_size=2, seq_length=2, stride=1):\n    >>>     print(batch)\n    (array([['a', 'a'], ['b', 'b'], ['b', 'b'], ['c', 'c']], dtype='<U1'), array([0, 1, 1, 2]))\n    (array([['c', 'c'], ['d', 'd'], ['d', 'd'], ['e', 'e']], dtype='<U1'), array([2, 3, 3, 4]))\n\n    Many to One\n\n    >>> return_last = True\n    >>> num_steps = 2\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> Y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=Y, batch_size=2, seq_length=num_steps, stride=1):\n    >>>     x, y = batch\n    >>>     if return_last:\n    >>>         tmp_y = y.reshape((-1, num_steps) + y.shape[1:])\n    >>>     y = tmp_y[:, -1]\n    >>>     print(x, y)\n    [['a' 'a']\n    ['b' 'b']\n    ['b' 'b']\n    ['c' 'c']] [1 2]\n    [['c' 'c']\n    ['d' 'd']\n    ['d' 'd']\n    ['e' 'e']] [3 4]\n\n    \"\"\"\n    if len(inputs) != len(targets):\n        raise AssertionError(\"The length of inputs and targets should be equal\")\n\n    n_loads = (batch_size * stride) + (seq_length - stride)\n\n    for start_idx in range(0, len(inputs) - n_loads + 1, (batch_size * stride)):\n        seq_inputs = np.zeros((batch_size, seq_length) + inputs.shape[1:], dtype=inputs.dtype)\n        seq_targets = np.zeros((batch_size, seq_length) + targets.shape[1:], dtype=targets.dtype)\n        for b_idx in xrange(batch_size):\n            start_seq_idx = start_idx + (b_idx * stride)\n            end_seq_idx = start_seq_idx + seq_length\n            seq_inputs[b_idx] = inputs[start_seq_idx:end_seq_idx]\n            seq_targets[b_idx] = targets[start_seq_idx:end_seq_idx]\n        flatten_inputs = seq_inputs.reshape((-1, ) + inputs.shape[1:])\n        flatten_targets = seq_targets.reshape((-1, ) + targets.shape[1:])\n        yield flatten_inputs, flatten_targets",
    "doc": "Generate a generator that return a batch of sequence inputs and targets.\n    If `batch_size=100` and `seq_length=5`, one return will have 500 rows (examples).\n\n    Parameters\n    ----------\n    inputs : numpy.array\n        The input features, every row is a example.\n    targets : numpy.array\n        The labels of inputs, every element is a example.\n    batch_size : int\n        The batch size.\n    seq_length : int\n        The sequence length.\n    stride : int\n        The stride step, default is 1.\n\n    Examples\n    --------\n    Synced sequence input and output.\n\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> y = np.asarray([0, 1, 2, 3, 4, 5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=y, batch_size=2, seq_length=2, stride=1):\n    >>>     print(batch)\n    (array([['a', 'a'], ['b', 'b'], ['b', 'b'], ['c', 'c']], dtype='<U1'), array([0, 1, 1, 2]))\n    (array([['c', 'c'], ['d', 'd'], ['d', 'd'], ['e', 'e']], dtype='<U1'), array([2, 3, 3, 4]))\n\n    Many to One\n\n    >>> return_last = True\n    >>> num_steps = 2\n    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])\n    >>> Y = np.asarray([0,1,2,3,4,5])\n    >>> for batch in tl.iterate.seq_minibatches(inputs=X, targets=Y, batch_size=2, seq_length=num_steps, stride=1):\n    >>>     x, y = batch\n    >>>     if return_last:\n    >>>         tmp_y = y.reshape((-1, num_steps) + y.shape[1:])\n    >>>     y = tmp_y[:, -1]\n    >>>     print(x, y)\n    [['a' 'a']\n    ['b' 'b']\n    ['b' 'b']\n    ['c' 'c']] [1 2]\n    [['c' 'c']\n    ['d' 'd']\n    ['d' 'd']\n    ['e' 'e']] [3 4]"
  },
  {
    "code": "def seq_minibatches2(inputs, targets, batch_size, num_steps):\n    \"\"\"Generate a generator that iterates on two list of words. Yields (Returns) the source contexts and\n    the target context by the given batch_size and num_steps (sequence_length).\n    In TensorFlow's tutorial, this generates the `batch_size` pointers into the raw PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    inputs : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    targets : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    batch_size : int\n        The batch size.\n    num_steps : int\n        The number of unrolls. i.e. sequence length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> X = [i for i in range(20)]\n    >>> Y = [i for i in range(20,40)]\n    >>> for batch in tl.iterate.seq_minibatches2(X, Y, batch_size=2, num_steps=3):\n    ...     x, y = batch\n    ...     print(x, y)\n\n    [[  0.   1.   2.]\n    [ 10.  11.  12.]]\n    [[ 20.  21.  22.]\n    [ 30.  31.  32.]]\n\n    [[  3.   4.   5.]\n    [ 13.  14.  15.]]\n    [[ 23.  24.  25.]\n    [ 33.  34.  35.]]\n\n    [[  6.   7.   8.]\n    [ 16.  17.  18.]]\n    [[ 26.  27.  28.]\n    [ 36.  37.  38.]]\n\n    Notes\n    -----\n    - Hint, if the input data are images, you can modify the source code `data = np.zeros([batch_size, batch_len)` to `data = np.zeros([batch_size, batch_len, inputs.shape[1], inputs.shape[2], inputs.shape[3]])`.\n    \"\"\"\n    if len(inputs) != len(targets):\n        raise AssertionError(\"The length of inputs and targets should be equal\")\n\n    data_len = len(inputs)\n    batch_len = data_len // batch_size\n    # data = np.zeros([batch_size, batch_len])\n    data = np.zeros((batch_size, batch_len) + inputs.shape[1:], dtype=inputs.dtype)\n    data2 = np.zeros([batch_size, batch_len])\n\n    for i in range(batch_size):\n        data[i] = inputs[batch_len * i:batch_len * (i + 1)]\n        data2[i] = targets[batch_len * i:batch_len * (i + 1)]\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    if epoch_size == 0:\n        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps:(i + 1) * num_steps]\n        x2 = data2[:, i * num_steps:(i + 1) * num_steps]\n        yield (x, x2)",
    "doc": "Generate a generator that iterates on two list of words. Yields (Returns) the source contexts and\n    the target context by the given batch_size and num_steps (sequence_length).\n    In TensorFlow's tutorial, this generates the `batch_size` pointers into the raw PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    inputs : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    targets : list of data\n        The context in list format; note that context usually be represented by splitting by space, and then convert to unique word IDs.\n    batch_size : int\n        The batch size.\n    num_steps : int\n        The number of unrolls. i.e. sequence length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> X = [i for i in range(20)]\n    >>> Y = [i for i in range(20,40)]\n    >>> for batch in tl.iterate.seq_minibatches2(X, Y, batch_size=2, num_steps=3):\n    ...     x, y = batch\n    ...     print(x, y)\n\n    [[  0.   1.   2.]\n    [ 10.  11.  12.]]\n    [[ 20.  21.  22.]\n    [ 30.  31.  32.]]\n\n    [[  3.   4.   5.]\n    [ 13.  14.  15.]]\n    [[ 23.  24.  25.]\n    [ 33.  34.  35.]]\n\n    [[  6.   7.   8.]\n    [ 16.  17.  18.]]\n    [[ 26.  27.  28.]\n    [ 36.  37.  38.]]\n\n    Notes\n    -----\n    - Hint, if the input data are images, you can modify the source code `data = np.zeros([batch_size, batch_len)` to `data = np.zeros([batch_size, batch_len, inputs.shape[1], inputs.shape[2], inputs.shape[3]])`."
  },
  {
    "code": "def ptb_iterator(raw_data, batch_size, num_steps):\n    \"\"\"Generate a generator that iterates on a list of words, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.\n    Yields the source contexts and the target context by the given batch_size and num_steps (sequence_length).\n\n    In TensorFlow's tutorial, this generates `batch_size` pointers into the raw\n    PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    raw_data : a list\n            the context in list format; note that context usually be\n            represented by splitting by space, and then convert to unique\n            word IDs.\n    batch_size : int\n            the batch size.\n    num_steps : int\n            the number of unrolls. i.e. sequence_length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> train_data = [i for i in range(20)]\n    >>> for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):\n    >>>     x, y = batch\n    >>>     print(x, y)\n    [[ 0  1  2] <---x                       1st subset/ iteration\n     [10 11 12]]\n    [[ 1  2  3] <---y\n     [11 12 13]]\n\n    [[ 3  4  5]  <--- 1st batch input       2nd subset/ iteration\n     [13 14 15]] <--- 2nd batch input\n    [[ 4  5  6]  <--- 1st batch target\n     [14 15 16]] <--- 2nd batch target\n\n    [[ 6  7  8]                             3rd subset/ iteration\n     [16 17 18]]\n    [[ 7  8  9]\n     [17 18 19]]\n    \"\"\"\n    raw_data = np.array(raw_data, dtype=np.int32)\n\n    data_len = len(raw_data)\n    batch_len = data_len // batch_size\n    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n    for i in range(batch_size):\n        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    if epoch_size == 0:\n        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps:(i + 1) * num_steps]\n        y = data[:, i * num_steps + 1:(i + 1) * num_steps + 1]\n        yield (x, y)",
    "doc": "Generate a generator that iterates on a list of words, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.\n    Yields the source contexts and the target context by the given batch_size and num_steps (sequence_length).\n\n    In TensorFlow's tutorial, this generates `batch_size` pointers into the raw\n    PTB data, and allows minibatch iteration along these pointers.\n\n    Parameters\n    ----------\n    raw_data : a list\n            the context in list format; note that context usually be\n            represented by splitting by space, and then convert to unique\n            word IDs.\n    batch_size : int\n            the batch size.\n    num_steps : int\n            the number of unrolls. i.e. sequence_length\n\n    Yields\n    ------\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n    Raises\n    ------\n    ValueError : if batch_size or num_steps are too high.\n\n    Examples\n    --------\n    >>> train_data = [i for i in range(20)]\n    >>> for batch in tl.iterate.ptb_iterator(train_data, batch_size=2, num_steps=3):\n    >>>     x, y = batch\n    >>>     print(x, y)\n    [[ 0  1  2] <---x                       1st subset/ iteration\n     [10 11 12]]\n    [[ 1  2  3] <---y\n     [11 12 13]]\n\n    [[ 3  4  5]  <--- 1st batch input       2nd subset/ iteration\n     [13 14 15]] <--- 2nd batch input\n    [[ 4  5  6]  <--- 1st batch target\n     [14 15 16]] <--- 2nd batch target\n\n    [[ 6  7  8]                             3rd subset/ iteration\n     [16 17 18]]\n    [[ 7  8  9]\n     [17 18 19]]"
  },
  {
    "code": "def deconv2d_bilinear_upsampling_initializer(shape):\n    \"\"\"Returns the initializer that can be passed to DeConv2dLayer for initializing the\n    weights in correspondence to channel-wise bilinear up-sampling.\n    Used in segmentation approaches such as [FCN](https://arxiv.org/abs/1605.06211)\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The shape of the filters, [height, width, output_channels, in_channels].\n        It must match the shape passed to DeConv2dLayer.\n\n    Returns\n    -------\n    ``tf.constant_initializer``\n        A constant initializer with weights set to correspond to per channel bilinear upsampling\n        when passed as W_int in DeConv2dLayer\n\n    Examples\n    --------\n    - Upsampling by a factor of 2, ie e.g 100->200\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> rescale_factor = 2\n    >>> imsize = 128\n    >>> num_channels = 3\n    >>> filter_shape = (5, 5)\n    >>> filter_size = (2 * rescale_factor - rescale_factor % 2) #Corresponding bilinear filter size\n    >>> num_in_channels = 3\n    >>> num_out_channels = 3\n    >>> deconv_filter_shape = (filter_size, filter_size, num_out_channels, num_in_channels)\n    >>> x = tf.placeholder(tf.float32, (1, imsize, imsize, num_channels))\n    >>> net = tl.layers.InputLayer(x, name='input_layer')\n    >>> bilinear_init = deconv2d_bilinear_upsampling_initializer(shape=filter_shape)\n    >>> net = tl.layers.DeConv2dLayer(net,\n    ...                    shape=filter_shape,\n    ...                    output_shape=(1, imsize*rescale_factor, imsize*rescale_factor, num_out_channels),\n    ...                    strides=(1, rescale_factor, rescale_factor, 1),\n    ...                    W_init=bilinear_init,\n    ...                    padding='SAME',\n    ...                    act=None, name='g/h1/decon2d')\n\n    \"\"\"\n    if shape[0] != shape[1]:\n        raise Exception('deconv2d_bilinear_upsampling_initializer only supports symmetrical filter sizes')\n\n    if shape[3] < shape[2]:\n        raise Exception(\n            'deconv2d_bilinear_upsampling_initializer behaviour is not defined for num_in_channels < num_out_channels '\n        )\n\n    filter_size = shape[0]\n    num_out_channels = shape[2]\n    num_in_channels = shape[3]\n\n    # Create bilinear filter kernel as numpy array\n    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n    scale_factor = (filter_size + 1) // 2\n    if filter_size % 2 == 1:\n        center = scale_factor - 1\n    else:\n        center = scale_factor - 0.5\n    for x in range(filter_size):\n        for y in range(filter_size):\n            bilinear_kernel[x, y] = (1 - abs(x - center) / scale_factor) * (1 - abs(y - center) / scale_factor)\n    weights = np.zeros((filter_size, filter_size, num_out_channels, num_in_channels))\n    for i in range(num_out_channels):\n        weights[:, :, i, i] = bilinear_kernel\n\n    # assign numpy array to constant_initalizer and pass to get_variable\n    return tf.constant_initializer(value=weights, dtype=LayersConfig.tf_dtype)",
    "doc": "Returns the initializer that can be passed to DeConv2dLayer for initializing the\n    weights in correspondence to channel-wise bilinear up-sampling.\n    Used in segmentation approaches such as [FCN](https://arxiv.org/abs/1605.06211)\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The shape of the filters, [height, width, output_channels, in_channels].\n        It must match the shape passed to DeConv2dLayer.\n\n    Returns\n    -------\n    ``tf.constant_initializer``\n        A constant initializer with weights set to correspond to per channel bilinear upsampling\n        when passed as W_int in DeConv2dLayer\n\n    Examples\n    --------\n    - Upsampling by a factor of 2, ie e.g 100->200\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> rescale_factor = 2\n    >>> imsize = 128\n    >>> num_channels = 3\n    >>> filter_shape = (5, 5)\n    >>> filter_size = (2 * rescale_factor - rescale_factor % 2) #Corresponding bilinear filter size\n    >>> num_in_channels = 3\n    >>> num_out_channels = 3\n    >>> deconv_filter_shape = (filter_size, filter_size, num_out_channels, num_in_channels)\n    >>> x = tf.placeholder(tf.float32, (1, imsize, imsize, num_channels))\n    >>> net = tl.layers.InputLayer(x, name='input_layer')\n    >>> bilinear_init = deconv2d_bilinear_upsampling_initializer(shape=filter_shape)\n    >>> net = tl.layers.DeConv2dLayer(net,\n    ...                    shape=filter_shape,\n    ...                    output_shape=(1, imsize*rescale_factor, imsize*rescale_factor, num_out_channels),\n    ...                    strides=(1, rescale_factor, rescale_factor, 1),\n    ...                    W_init=bilinear_init,\n    ...                    padding='SAME',\n    ...                    act=None, name='g/h1/decon2d')"
  },
  {
    "code": "def save_model(self, network=None, model_name='model', **kwargs):\n        \"\"\"Save model architecture and parameters into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        network : TensorLayer layer\n            TensorLayer layer instance.\n        model_name : str\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        Save model architecture and parameters into database.\n        >>> db.save_model(net, accuracy=0.8, loss=2.3, name='second_model')\n\n        Load one model with parameters from database (run this in other script)\n        >>> net = db.find_top_model(sess=sess, accuracy=0.8, loss=2.3)\n\n        Find and load the latest model.\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", pymongo.DESCENDING)])\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", -1)])\n\n        Find and load the oldest model.\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", pymongo.ASCENDING)])\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", 1)])\n\n        Get model information\n        >>> net._accuracy\n        ... 0.8\n\n        Returns\n        ---------\n        boolean : True for success, False for fail.\n        \"\"\"\n        kwargs.update({'model_name': model_name})\n        self._fill_project_info(kwargs)  # put project_name into kwargs\n\n        params = network.get_all_params()\n\n        s = time.time()\n\n        kwargs.update({'architecture': network.all_graphs, 'time': datetime.utcnow()})\n\n        try:\n            params_id = self.model_fs.put(self._serialization(params))\n            kwargs.update({'params_id': params_id, 'time': datetime.utcnow()})\n            self.db.Model.insert_one(kwargs)\n            print(\"[Database] Save model: SUCCESS, took: {}s\".format(round(time.time() - s, 2)))\n            return True\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(\"{}  {}  {}  {}  {}\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            print(\"[Database] Save model: FAIL\")\n            return False",
    "doc": "Save model architecture and parameters into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        network : TensorLayer layer\n            TensorLayer layer instance.\n        model_name : str\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        Save model architecture and parameters into database.\n        >>> db.save_model(net, accuracy=0.8, loss=2.3, name='second_model')\n\n        Load one model with parameters from database (run this in other script)\n        >>> net = db.find_top_model(sess=sess, accuracy=0.8, loss=2.3)\n\n        Find and load the latest model.\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", pymongo.DESCENDING)])\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", -1)])\n\n        Find and load the oldest model.\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", pymongo.ASCENDING)])\n        >>> net = db.find_top_model(sess=sess, sort=[(\"time\", 1)])\n\n        Get model information\n        >>> net._accuracy\n        ... 0.8\n\n        Returns\n        ---------\n        boolean : True for success, False for fail."
  },
  {
    "code": "def find_top_model(self, sess, sort=None, model_name='model', **kwargs):\n        \"\"\"Finds and returns a model architecture and its parameters from the database which matches the requirement.\n\n        Parameters\n        ----------\n        sess : Session\n            TensorFlow session.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        model_name : str or None\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        - see ``save_model``.\n\n        Returns\n        ---------\n        network : TensorLayer layer\n            Note that, the returned network contains all information of the document (record), e.g. if you saved accuracy in the document, you can get the accuracy by using ``net._accuracy``.\n        \"\"\"\n        # print(kwargs)   # {}\n        kwargs.update({'model_name': model_name})\n        self._fill_project_info(kwargs)\n\n        s = time.time()\n\n        d = self.db.Model.find_one(filter=kwargs, sort=sort)\n\n        _temp_file_name = '_find_one_model_ztemp_file'\n        if d is not None:\n            params_id = d['params_id']\n            graphs = d['architecture']\n            _datetime = d['time']\n            exists_or_mkdir(_temp_file_name, False)\n            with open(os.path.join(_temp_file_name, 'graph.pkl'), 'wb') as file:\n                pickle.dump(graphs, file, protocol=pickle.HIGHEST_PROTOCOL)\n        else:\n            print(\"[Database] FAIL! Cannot find model: {}\".format(kwargs))\n            return False\n        try:\n            params = self._deserialization(self.model_fs.get(params_id).read())\n            np.savez(os.path.join(_temp_file_name, 'params.npz'), params=params)\n\n            network = load_graph_and_params(name=_temp_file_name, sess=sess)\n            del_folder(_temp_file_name)\n\n            pc = self.db.Model.find(kwargs)\n            print(\n                \"[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s\".\n                format(kwargs, sort, _datetime, round(time.time() - s, 2))\n            )\n\n            # put all informations of model into the TL layer\n            for key in d:\n                network.__dict__.update({\"_%s\" % key: d[key]})\n\n            # check whether more parameters match the requirement\n            params_id_list = pc.distinct('params_id')\n            n_params = len(params_id_list)\n            if n_params != 1:\n                print(\"     Note that there are {} models match the kwargs\".format(n_params))\n            return network\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(\"{}  {}  {}  {}  {}\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            return False",
    "doc": "Finds and returns a model architecture and its parameters from the database which matches the requirement.\n\n        Parameters\n        ----------\n        sess : Session\n            TensorFlow session.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        model_name : str or None\n            The name/key of model.\n        kwargs : other events\n            Other events, such as name, accuracy, loss, step number and etc (optinal).\n\n        Examples\n        ---------\n        - see ``save_model``.\n\n        Returns\n        ---------\n        network : TensorLayer layer\n            Note that, the returned network contains all information of the document (record), e.g. if you saved accuracy in the document, you can get the accuracy by using ``net._accuracy``."
  },
  {
    "code": "def delete_model(self, **kwargs):\n        \"\"\"Delete model.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n        \"\"\"\n        self._fill_project_info(kwargs)\n        self.db.Model.delete_many(kwargs)\n        logging.info(\"[Database] Delete Model SUCCESS\")",
    "doc": "Delete model.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log."
  },
  {
    "code": "def save_dataset(self, dataset=None, dataset_name=None, **kwargs):\n        \"\"\"Saves one dataset into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        dataset : any type\n            The dataset you want to store.\n        dataset_name : str\n            The name of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ----------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset('mnist')\n\n        Returns\n        ---------\n        boolean : Return True if save success, otherwise, return False.\n        \"\"\"\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(\"dataset_name is None, please give a dataset name\")\n        kwargs.update({'dataset_name': dataset_name})\n\n        s = time.time()\n        try:\n            dataset_id = self.dataset_fs.put(self._serialization(dataset))\n            kwargs.update({'dataset_id': dataset_id, 'time': datetime.utcnow()})\n            self.db.Dataset.insert_one(kwargs)\n            # print(\"[Database] Save params: {} SUCCESS, took: {}s\".format(file_name, round(time.time()-s, 2)))\n            print(\"[Database] Save dataset: SUCCESS, took: {}s\".format(round(time.time() - s, 2)))\n            return True\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(\"{}  {}  {}  {}  {}\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            print(\"[Database] Save dataset: FAIL\")\n            return False",
    "doc": "Saves one dataset into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        dataset : any type\n            The dataset you want to store.\n        dataset_name : str\n            The name of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ----------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset('mnist')\n\n        Returns\n        ---------\n        boolean : Return True if save success, otherwise, return False."
  },
  {
    "code": "def find_top_dataset(self, dataset_name=None, sort=None, **kwargs):\n        \"\"\"Finds and returns a dataset from the database which matches the requirement.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name of dataset.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ---------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset('mnist')\n        >>> datasets = db.find_datasets('mnist')\n\n        Returns\n        --------\n        dataset : the dataset or False\n            Return False if nothing found.\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(\"dataset_name is None, please give a dataset name\")\n        kwargs.update({'dataset_name': dataset_name})\n\n        s = time.time()\n\n        d = self.db.Dataset.find_one(filter=kwargs, sort=sort)\n\n        if d is not None:\n            dataset_id = d['dataset_id']\n        else:\n            print(\"[Database] FAIL! Cannot find dataset: {}\".format(kwargs))\n            return False\n        try:\n            dataset = self._deserialization(self.dataset_fs.get(dataset_id).read())\n            pc = self.db.Dataset.find(kwargs)\n            print(\"[Database] Find one dataset SUCCESS, {} took: {}s\".format(kwargs, round(time.time() - s, 2)))\n\n            # check whether more datasets match the requirement\n            dataset_id_list = pc.distinct('dataset_id')\n            n_dataset = len(dataset_id_list)\n            if n_dataset != 1:\n                print(\"     Note that there are {} datasets match the requirement\".format(n_dataset))\n            return dataset\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(\"{}  {}  {}  {}  {}\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            return False",
    "doc": "Finds and returns a dataset from the database which matches the requirement.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name of dataset.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ---------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset('mnist')\n        >>> datasets = db.find_datasets('mnist')\n\n        Returns\n        --------\n        dataset : the dataset or False\n            Return False if nothing found."
  },
  {
    "code": "def find_datasets(self, dataset_name=None, **kwargs):\n        \"\"\"Finds and returns all datasets from the database which matches the requirement.\n        In some case, the data in a dataset can be stored separately for better management.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name/key of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optional).\n\n        Returns\n        --------\n        params : the parameters, return False if nothing found.\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        if dataset_name is None:\n            raise Exception(\"dataset_name is None, please give a dataset name\")\n        kwargs.update({'dataset_name': dataset_name})\n\n        s = time.time()\n        pc = self.db.Dataset.find(kwargs)\n\n        if pc is not None:\n            dataset_id_list = pc.distinct('dataset_id')\n            dataset_list = []\n            for dataset_id in dataset_id_list:  # you may have multiple Buckets files\n                tmp = self.dataset_fs.get(dataset_id).read()\n                dataset_list.append(self._deserialization(tmp))\n        else:\n            print(\"[Database] FAIL! Cannot find any dataset: {}\".format(kwargs))\n            return False\n\n        print(\"[Database] Find {} datasets SUCCESS, took: {}s\".format(len(dataset_list), round(time.time() - s, 2)))\n        return dataset_list",
    "doc": "Finds and returns all datasets from the database which matches the requirement.\n        In some case, the data in a dataset can be stored separately for better management.\n\n        Parameters\n        ----------\n        dataset_name : str\n            The name/key of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optional).\n\n        Returns\n        --------\n        params : the parameters, return False if nothing found."
  },
  {
    "code": "def delete_datasets(self, **kwargs):\n        \"\"\"Delete datasets.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        self.db.Dataset.delete_many(kwargs)\n        logging.info(\"[Database] Delete Dataset SUCCESS\")",
    "doc": "Delete datasets.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log."
  },
  {
    "code": "def save_training_log(self, **kwargs):\n        \"\"\"Saves the training log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_training_log(accuracy=0.33, loss=0.98)\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        kwargs.update({'time': datetime.utcnow()})\n        _result = self.db.TrainLog.insert_one(kwargs)\n        _log = self._print_dict(kwargs)\n        logging.info(\"[Database] train log: \" + _log)",
    "doc": "Saves the training log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_training_log(accuracy=0.33, loss=0.98)"
  },
  {
    "code": "def save_validation_log(self, **kwargs):\n        \"\"\"Saves the validation log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_validation_log(accuracy=0.33, loss=0.98)\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        kwargs.update({'time': datetime.utcnow()})\n        _result = self.db.ValidLog.insert_one(kwargs)\n        _log = self._print_dict(kwargs)\n        logging.info(\"[Database] valid log: \" + _log)",
    "doc": "Saves the validation log, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Events, such as accuracy, loss, step number and etc.\n\n        Examples\n        ---------\n        >>> db.save_validation_log(accuracy=0.33, loss=0.98)"
  },
  {
    "code": "def delete_training_log(self, **kwargs):\n        \"\"\"Deletes training log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        Save training log\n        >>> db.save_training_log(accuracy=0.33)\n        >>> db.save_training_log(accuracy=0.44)\n\n        Delete logs that match the requirement\n        >>> db.delete_training_log(accuracy=0.33)\n\n        Delete all logs\n        >>> db.delete_training_log()\n        \"\"\"\n        self._fill_project_info(kwargs)\n        self.db.TrainLog.delete_many(kwargs)\n        logging.info(\"[Database] Delete TrainLog SUCCESS\")",
    "doc": "Deletes training log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        Save training log\n        >>> db.save_training_log(accuracy=0.33)\n        >>> db.save_training_log(accuracy=0.44)\n\n        Delete logs that match the requirement\n        >>> db.delete_training_log(accuracy=0.33)\n\n        Delete all logs\n        >>> db.delete_training_log()"
  },
  {
    "code": "def delete_validation_log(self, **kwargs):\n        \"\"\"Deletes validation log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        - see ``save_training_log``.\n        \"\"\"\n        self._fill_project_info(kwargs)\n        self.db.ValidLog.delete_many(kwargs)\n        logging.info(\"[Database] Delete ValidLog SUCCESS\")",
    "doc": "Deletes validation log.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        - see ``save_training_log``."
  },
  {
    "code": "def create_task(self, task_name=None, script=None, hyper_parameters=None, saved_result_keys=None, **kwargs):\n        \"\"\"Uploads a task to the database, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        script : str\n            File name of the python script.\n        hyper_parameters : dictionary\n            The hyper parameters pass into the script.\n        saved_result_keys : list of str\n            The keys of the task results to keep in the database when the task finishes.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        -----------\n        Uploads a task\n        >>> db.create_task(task_name='mnist', script='example/tutorial_mnist_simple.py', description='simple tutorial')\n\n        Finds and runs the latest task\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", pymongo.DESCENDING)])\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", -1)])\n\n        Finds and runs the oldest task\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", pymongo.ASCENDING)])\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", 1)])\n\n        \"\"\"\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(\"task_name should be string\")\n        if not isinstance(script, str):  # is None:\n            raise Exception(\"script should be string\")\n        if hyper_parameters is None:\n            hyper_parameters = {}\n        if saved_result_keys is None:\n            saved_result_keys = []\n\n        self._fill_project_info(kwargs)\n        kwargs.update({'time': datetime.utcnow()})\n        kwargs.update({'hyper_parameters': hyper_parameters})\n        kwargs.update({'saved_result_keys': saved_result_keys})\n\n        _script = open(script, 'rb').read()\n\n        kwargs.update({'status': 'pending', 'script': _script, 'result': {}})\n        self.db.Task.insert_one(kwargs)\n        logging.info(\"[Database] Saved Task - task_name: {} script: {}\".format(task_name, script))",
    "doc": "Uploads a task to the database, timestamp will be added automatically.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        script : str\n            File name of the python script.\n        hyper_parameters : dictionary\n            The hyper parameters pass into the script.\n        saved_result_keys : list of str\n            The keys of the task results to keep in the database when the task finishes.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        -----------\n        Uploads a task\n        >>> db.create_task(task_name='mnist', script='example/tutorial_mnist_simple.py', description='simple tutorial')\n\n        Finds and runs the latest task\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", pymongo.DESCENDING)])\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", -1)])\n\n        Finds and runs the oldest task\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", pymongo.ASCENDING)])\n        >>> db.run_top_task(sess=sess, sort=[(\"time\", 1)])"
  },
  {
    "code": "def run_top_task(self, task_name=None, sort=None, **kwargs):\n        \"\"\"Finds and runs a pending task that in the first of the sorting list.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Monitors the database and pull tasks to run\n        >>> while True:\n        >>>     print(\"waiting task from distributor\")\n        >>>     db.run_top_task(task_name='mnist', sort=[(\"time\", -1)])\n        >>>     time.sleep(1)\n\n        Returns\n        --------\n        boolean : True for success, False for fail.\n        \"\"\"\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(\"task_name should be string\")\n        self._fill_project_info(kwargs)\n        kwargs.update({'status': 'pending'})\n\n        # find task and set status to running\n        task = self.db.Task.find_one_and_update(kwargs, {'$set': {'status': 'running'}}, sort=sort)\n\n        try:\n            # get task info e.g. hyper parameters, python script\n            if task is None:\n                logging.info(\"[Database] Find Task FAIL: key: {} sort: {}\".format(task_name, sort))\n                return False\n            else:\n                logging.info(\"[Database] Find Task SUCCESS: key: {} sort: {}\".format(task_name, sort))\n            _datetime = task['time']\n            _script = task['script']\n            _id = task['_id']\n            _hyper_parameters = task['hyper_parameters']\n            _saved_result_keys = task['saved_result_keys']\n            logging.info(\"  hyper parameters:\")\n            for key in _hyper_parameters:\n                globals()[key] = _hyper_parameters[key]\n                logging.info(\"    {}: {}\".format(key, _hyper_parameters[key]))\n            # run task\n            s = time.time()\n            logging.info(\"[Database] Start Task: key: {} sort: {} push time: {}\".format(task_name, sort, _datetime))\n            _script = _script.decode('utf-8')\n            with tf.Graph().as_default():  # as graph: # clear all TF graphs\n                exec(_script, globals())\n\n            # set status to finished\n            _ = self.db.Task.find_one_and_update({'_id': _id}, {'$set': {'status': 'finished'}})\n\n            # return results\n            __result = {}\n            for _key in _saved_result_keys:\n                logging.info(\"  result: {}={} {}\".format(_key, globals()[_key], type(globals()[_key])))\n                __result.update({\"%s\" % _key: globals()[_key]})\n            _ = self.db.Task.find_one_and_update(\n                {\n                    '_id': _id\n                }, {'$set': {\n                    'result': __result\n                }}, return_document=pymongo.ReturnDocument.AFTER\n            )\n            logging.info(\n                \"[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s\".\n                format(task_name, sort, _datetime,\n                       time.time() - s)\n            )\n            return True\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            logging.info(\"{}  {}  {}  {}  {}\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))\n            logging.info(\"[Database] Fail to run task\")\n            # if fail, set status back to pending\n            _ = self.db.Task.find_one_and_update({'_id': _id}, {'$set': {'status': 'pending'}})\n            return False",
    "doc": "Finds and runs a pending task that in the first of the sorting list.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        sort : List of tuple\n            PyMongo sort comment, search \"PyMongo find one sorting\" and `collection level operations <http://api.mongodb.com/python/current/api/pymongo/collection.html>`__ for more details.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Monitors the database and pull tasks to run\n        >>> while True:\n        >>>     print(\"waiting task from distributor\")\n        >>>     db.run_top_task(task_name='mnist', sort=[(\"time\", -1)])\n        >>>     time.sleep(1)\n\n        Returns\n        --------\n        boolean : True for success, False for fail."
  },
  {
    "code": "def delete_tasks(self, **kwargs):\n        \"\"\"Delete tasks.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        >>> db.delete_tasks()\n\n        \"\"\"\n\n        self._fill_project_info(kwargs)\n        self.db.Task.delete_many(kwargs)\n        logging.info(\"[Database] Delete Task SUCCESS\")",
    "doc": "Delete tasks.\n\n        Parameters\n        -----------\n        kwargs : logging information\n            Find items to delete, leave it empty to delete all log.\n\n        Examples\n        ---------\n        >>> db.delete_tasks()"
  },
  {
    "code": "def check_unfinished_task(self, task_name=None, **kwargs):\n        \"\"\"Finds and runs a pending task.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Wait until all tasks finish in user's local console\n\n        >>> while not db.check_unfinished_task():\n        >>>     time.sleep(1)\n        >>> print(\"all tasks finished\")\n        >>> sess = tf.InteractiveSession()\n        >>> net = db.find_top_model(sess=sess, sort=[(\"test_accuracy\", -1)])\n        >>> print(\"the best accuracy {} is from model {}\".format(net._test_accuracy, net._name))\n\n        Returns\n        --------\n        boolean : True for success, False for fail.\n\n        \"\"\"\n\n        if not isinstance(task_name, str):  # is None:\n            raise Exception(\"task_name should be string\")\n        self._fill_project_info(kwargs)\n\n        kwargs.update({'$or': [{'status': 'pending'}, {'status': 'running'}]})\n\n        # ## find task\n        # task = self.db.Task.find_one(kwargs)\n        task = self.db.Task.find(kwargs)\n\n        task_id_list = task.distinct('_id')\n        n_task = len(task_id_list)\n\n        if n_task == 0:\n            logging.info(\"[Database] No unfinished task - task_name: {}\".format(task_name))\n            return False\n        else:\n\n            logging.info(\"[Database] Find {} unfinished task - task_name: {}\".format(n_task, task_name))\n            return True",
    "doc": "Finds and runs a pending task.\n\n        Parameters\n        -----------\n        task_name : str\n            The task name.\n        kwargs : other parameters\n            Users customized parameters such as description, version number.\n\n        Examples\n        ---------\n        Wait until all tasks finish in user's local console\n\n        >>> while not db.check_unfinished_task():\n        >>>     time.sleep(1)\n        >>> print(\"all tasks finished\")\n        >>> sess = tf.InteractiveSession()\n        >>> net = db.find_top_model(sess=sess, sort=[(\"test_accuracy\", -1)])\n        >>> print(\"the best accuracy {} is from model {}\".format(net._test_accuracy, net._name))\n\n        Returns\n        --------\n        boolean : True for success, False for fail."
  },
  {
    "code": "def augment_with_ngrams(unigrams, unigram_vocab_size, n_buckets, n=2):\n    \"\"\"Augment unigram features with hashed n-gram features.\"\"\"\n\n    def get_ngrams(n):\n        return list(zip(*[unigrams[i:] for i in range(n)]))\n\n    def hash_ngram(ngram):\n        bytes_ = array.array('L', ngram).tobytes()\n        hash_ = int(hashlib.sha256(bytes_).hexdigest(), 16)\n        return unigram_vocab_size + hash_ % n_buckets\n\n    return unigrams + [hash_ngram(ngram) for i in range(2, n + 1) for ngram in get_ngrams(i)]",
    "doc": "Augment unigram features with hashed n-gram features."
  },
  {
    "code": "def load_and_preprocess_imdb_data(n_gram=None):\n    \"\"\"Load IMDb data and augment with hashed n-gram features.\"\"\"\n    X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(nb_words=VOCAB_SIZE)\n\n    if n_gram is not None:\n        X_train = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_train])\n        X_test = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_test])\n\n    return X_train, y_train, X_test, y_test",
    "doc": "Load IMDb data and augment with hashed n-gram features."
  },
  {
    "code": "def read_image(image, path=''):\n    \"\"\"Read one image.\n\n    Parameters\n    -----------\n    image : str\n        The image file name.\n    path : str\n        The image folder path.\n\n    Returns\n    -------\n    numpy.array\n        The image.\n\n    \"\"\"\n    return imageio.imread(os.path.join(path, image))",
    "doc": "Read one image.\n\n    Parameters\n    -----------\n    image : str\n        The image file name.\n    path : str\n        The image folder path.\n\n    Returns\n    -------\n    numpy.array\n        The image."
  },
  {
    "code": "def read_images(img_list, path='', n_threads=10, printable=True):\n    \"\"\"Returns all images in list by given path and name of each image file.\n\n    Parameters\n    -------------\n    img_list : list of str\n        The image file names.\n    path : str\n        The image folder path.\n    n_threads : int\n        The number of threads to read image.\n    printable : boolean\n        Whether to print information when reading images.\n\n    Returns\n    -------\n    list of numpy.array\n        The images.\n\n    \"\"\"\n    imgs = []\n    for idx in range(0, len(img_list), n_threads):\n        b_imgs_list = img_list[idx:idx + n_threads]\n        b_imgs = tl.prepro.threading_data(b_imgs_list, fn=read_image, path=path)\n        # tl.logging.info(b_imgs.shape)\n        imgs.extend(b_imgs)\n        if printable:\n            tl.logging.info('read %d from %s' % (len(imgs), path))\n    return imgs",
    "doc": "Returns all images in list by given path and name of each image file.\n\n    Parameters\n    -------------\n    img_list : list of str\n        The image file names.\n    path : str\n        The image folder path.\n    n_threads : int\n        The number of threads to read image.\n    printable : boolean\n        Whether to print information when reading images.\n\n    Returns\n    -------\n    list of numpy.array\n        The images."
  },
  {
    "code": "def save_image(image, image_path='_temp.png'):\n    \"\"\"Save a image.\n\n    Parameters\n    -----------\n    image : numpy array\n        [w, h, c]\n    image_path : str\n        path\n\n    \"\"\"\n    try:  # RGB\n        imageio.imwrite(image_path, image)\n    except Exception:  # Greyscale\n        imageio.imwrite(image_path, image[:, :, 0])",
    "doc": "Save a image.\n\n    Parameters\n    -----------\n    image : numpy array\n        [w, h, c]\n    image_path : str\n        path"
  },
  {
    "code": "def save_images(images, size, image_path='_temp.png'):\n    \"\"\"Save multiple images into one single image.\n\n    Parameters\n    -----------\n    images : numpy array\n        (batch, w, h, c)\n    size : list of 2 ints\n        row and column number.\n        number of images should be equal or less than size[0] * size[1]\n    image_path : str\n        save path\n\n    Examples\n    ---------\n    >>> import numpy as np\n    >>> import tensorlayer as tl\n    >>> images = np.random.rand(64, 100, 100, 3)\n    >>> tl.visualize.save_images(images, [8, 8], 'temp.png')\n\n    \"\"\"\n    if len(images.shape) == 3:  # Greyscale [batch, h, w] --> [batch, h, w, 1]\n        images = images[:, :, :, np.newaxis]\n\n    def merge(images, size):\n        h, w = images.shape[1], images.shape[2]\n        img = np.zeros((h * size[0], w * size[1], 3), dtype=images.dtype)\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w, :] = image\n        return img\n\n    def imsave(images, size, path):\n        if np.max(images) <= 1 and (-1 <= np.min(images) < 0):\n            images = ((images + 1) * 127.5).astype(np.uint8)\n        elif np.max(images) <= 1 and np.min(images) >= 0:\n            images = (images * 255).astype(np.uint8)\n\n        return imageio.imwrite(path, merge(images, size))\n\n    if len(images) > size[0] * size[1]:\n        raise AssertionError(\"number of images should be equal or less than size[0] * size[1] {}\".format(len(images)))\n\n    return imsave(images, size, image_path)",
    "doc": "Save multiple images into one single image.\n\n    Parameters\n    -----------\n    images : numpy array\n        (batch, w, h, c)\n    size : list of 2 ints\n        row and column number.\n        number of images should be equal or less than size[0] * size[1]\n    image_path : str\n        save path\n\n    Examples\n    ---------\n    >>> import numpy as np\n    >>> import tensorlayer as tl\n    >>> images = np.random.rand(64, 100, 100, 3)\n    >>> tl.visualize.save_images(images, [8, 8], 'temp.png')"
  },
  {
    "code": "def draw_boxes_and_labels_to_image(\n        image, classes, coords, scores, classes_list, is_center=True, is_rescale=True, save_name=None\n):\n    \"\"\"Draw bboxes and class labels on image. Return or save the image with bboxes, example in the docs of ``tl.prepro``.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    classes : list of int\n        A list of class ID (int).\n    coords : list of int\n        A list of list for coordinates.\n            - Should be [x, y, x2, y2] (up-left and botton-right format)\n            - If [x_center, y_center, w, h] (set is_center to True).\n    scores : list of float\n        A list of score (float). (Optional)\n    classes_list : list of str\n        for converting ID to string on image.\n    is_center : boolean\n        Whether the coordinates is [x_center, y_center, w, h]\n            - If coordinates are [x_center, y_center, w, h], set it to True for converting it to [x, y, x2, y2] (up-left and botton-right) internally.\n            - If coordinates are [x1, x2, y1, y2], set it to False.\n    is_rescale : boolean\n        Whether to rescale the coordinates from pixel-unit format to ratio format.\n            - If True, the input coordinates are the portion of width and high, this API will scale the coordinates to pixel unit internally.\n            - If False, feed the coordinates with pixel unit format.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    -------\n    numpy.array\n        The saved image.\n\n    References\n    -----------\n    - OpenCV rectangle and putText.\n    - `scikit-image <http://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.rectangle>`__.\n\n    \"\"\"\n    if len(coords) != len(classes):\n        raise AssertionError(\"number of coordinates and classes are equal\")\n\n    if len(scores) > 0 and len(scores) != len(classes):\n        raise AssertionError(\"number of scores and classes are equal\")\n\n    # don't change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy\n    image = image.copy()\n\n    imh, imw = image.shape[0:2]\n    thick = int((imh + imw) // 430)\n\n    for i, _v in enumerate(coords):\n        if is_center:\n            x, y, x2, y2 = tl.prepro.obj_box_coord_centroid_to_upleft_butright(coords[i])\n        else:\n            x, y, x2, y2 = coords[i]\n\n        if is_rescale:  # scale back to pixel unit if the coords are the portion of width and high\n            x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([x, y, x2, y2], (imh, imw))\n\n        cv2.rectangle(\n            image,\n            (int(x), int(y)),\n            (int(x2), int(y2)),  # up-left and botton-right\n            [0, 255, 0],\n            thick\n        )\n\n        cv2.putText(\n            image,\n            classes_list[classes[i]] + ((\" %.2f\" % (scores[i])) if (len(scores) != 0) else \" \"),\n            (int(x), int(y)),  # button left\n            0,\n            1.5e-3 * imh,  # bigger = larger font\n            [0, 0, 256],  # self.meta['colors'][max_indx],\n            int(thick / 2) + 1\n        )  # bold\n\n    if save_name is not None:\n        # cv2.imwrite('_my.png', image)\n        save_image(image, save_name)\n    # if len(coords) == 0:\n    #     tl.logging.info(\"draw_boxes_and_labels_to_image: no bboxes exist, cannot draw !\")\n    return image",
    "doc": "Draw bboxes and class labels on image. Return or save the image with bboxes, example in the docs of ``tl.prepro``.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    classes : list of int\n        A list of class ID (int).\n    coords : list of int\n        A list of list for coordinates.\n            - Should be [x, y, x2, y2] (up-left and botton-right format)\n            - If [x_center, y_center, w, h] (set is_center to True).\n    scores : list of float\n        A list of score (float). (Optional)\n    classes_list : list of str\n        for converting ID to string on image.\n    is_center : boolean\n        Whether the coordinates is [x_center, y_center, w, h]\n            - If coordinates are [x_center, y_center, w, h], set it to True for converting it to [x, y, x2, y2] (up-left and botton-right) internally.\n            - If coordinates are [x1, x2, y1, y2], set it to False.\n    is_rescale : boolean\n        Whether to rescale the coordinates from pixel-unit format to ratio format.\n            - If True, the input coordinates are the portion of width and high, this API will scale the coordinates to pixel unit internally.\n            - If False, feed the coordinates with pixel unit format.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    -------\n    numpy.array\n        The saved image.\n\n    References\n    -----------\n    - OpenCV rectangle and putText.\n    - `scikit-image <http://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.rectangle>`__."
  },
  {
    "code": "def draw_mpii_pose_to_image(image, poses, save_name='image.png'):\n    \"\"\"Draw people(s) into image using MPII dataset format as input, return or save the result image.\n\n    This is an experimental API, can be changed in the future.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    poses : list of dict\n        The people(s) annotation in MPII format, see ``tl.files.load_mpii_pose_dataset``.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    --------\n    numpy.array\n        The saved image.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], 'image.png')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__\n    \"\"\"\n    # import skimage\n    # don't change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy\n    image = image.copy()\n\n    imh, imw = image.shape[0:2]\n    thick = int((imh + imw) // 430)\n    # radius = int(image.shape[1] / 500) + 1\n    radius = int(thick * 1.5)\n\n    if image.max() < 1:\n        image = image * 255\n\n    for people in poses:\n        # Pose Keyponts\n        joint_pos = people['joint_pos']\n        # draw sketch\n        # joint id (0 - r ankle, 1 - r knee, 2 - r hip, 3 - l hip, 4 - l knee,\n        #           5 - l ankle, 6 - pelvis, 7 - thorax, 8 - upper neck,\n        #           9 - head top, 10 - r wrist, 11 - r elbow, 12 - r shoulder,\n        #           13 - l shoulder, 14 - l elbow, 15 - l wrist)\n        #\n        #               9\n        #               8\n        #         12 ** 7 ** 13\n        #        *      *      *\n        #       11      *       14\n        #      *        *         *\n        #     10    2 * 6 * 3     15\n        #           *       *\n        #           1       4\n        #           *       *\n        #           0       5\n\n        lines = [\n            [(0, 1), [100, 255, 100]],\n            [(1, 2), [50, 255, 50]],\n            [(2, 6), [0, 255, 0]],  # right leg\n            [(3, 4), [100, 100, 255]],\n            [(4, 5), [50, 50, 255]],\n            [(6, 3), [0, 0, 255]],  # left leg\n            [(6, 7), [255, 255, 100]],\n            [(7, 8), [255, 150, 50]],  # body\n            [(8, 9), [255, 200, 100]],  # head\n            [(10, 11), [255, 100, 255]],\n            [(11, 12), [255, 50, 255]],\n            [(12, 8), [255, 0, 255]],  # right hand\n            [(8, 13), [0, 255, 255]],\n            [(13, 14), [100, 255, 255]],\n            [(14, 15), [200, 255, 255]]  # left hand\n        ]\n        for line in lines:\n            start, end = line[0]\n            if (start in joint_pos) and (end in joint_pos):\n                cv2.line(\n                    image,\n                    (int(joint_pos[start][0]), int(joint_pos[start][1])),\n                    (int(joint_pos[end][0]), int(joint_pos[end][1])),  # up-left and botton-right\n                    line[1],\n                    thick\n                )\n                # rr, cc, val = skimage.draw.line_aa(int(joint_pos[start][1]), int(joint_pos[start][0]), int(joint_pos[end][1]), int(joint_pos[end][0]))\n                # image[rr, cc] = line[1]\n        # draw circles\n        for pos in joint_pos.items():\n            _, pos_loc = pos  # pos_id, pos_loc\n            pos_loc = (int(pos_loc[0]), int(pos_loc[1]))\n            cv2.circle(image, center=pos_loc, radius=radius, color=(200, 200, 200), thickness=-1)\n            # rr, cc = skimage.draw.circle(int(pos_loc[1]), int(pos_loc[0]), radius)\n            # image[rr, cc] = [0, 255, 0]\n\n        # Head\n        head_rect = people['head_rect']\n        if head_rect:  # if head exists\n            cv2.rectangle(\n                image,\n                (int(head_rect[0]), int(head_rect[1])),\n                (int(head_rect[2]), int(head_rect[3])),  # up-left and botton-right\n                [0, 180, 0],\n                thick\n            )\n\n    if save_name is not None:\n        # cv2.imwrite(save_name, image)\n        save_image(image, save_name)\n    return image",
    "doc": "Draw people(s) into image using MPII dataset format as input, return or save the result image.\n\n    This is an experimental API, can be changed in the future.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    poses : list of dict\n        The people(s) annotation in MPII format, see ``tl.files.load_mpii_pose_dataset``.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    --------\n    numpy.array\n        The saved image.\n\n    Examples\n    --------\n    >>> import pprint\n    >>> import tensorlayer as tl\n    >>> img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()\n    >>> image = tl.vis.read_image(img_train_list[0])\n    >>> tl.vis.draw_mpii_pose_to_image(image, ann_train_list[0], 'image.png')\n    >>> pprint.pprint(ann_train_list[0])\n\n    References\n    -----------\n    - `MPII Keyponts and ID <http://human-pose.mpi-inf.mpg.de/#download>`__"
  },
  {
    "code": "def frame(I=None, second=5, saveable=True, name='frame', cmap=None, fig_idx=12836):\n    \"\"\"Display a frame. Make sure OpenAI Gym render() is disable before using it.\n\n    Parameters\n    ----------\n    I : numpy.array\n        The image.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    cmap : None or str\n        'gray' for greyscale, None for default, etc.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> env = gym.make(\"Pong-v0\")\n    >>> observation = env.reset()\n    >>> tl.visualize.frame(observation)\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    if saveable is False:\n        plt.ion()\n    plt.figure(fig_idx)  # show all feature images\n\n    if len(I.shape) and I.shape[-1] == 1:  # (10,10,1) --> (10,10)\n        I = I[:, :, 0]\n\n    plt.imshow(I, cmap)\n    plt.title(name)\n    # plt.gca().xaxis.set_major_locator(plt.NullLocator())    # distable tick\n    # plt.gca().yaxis.set_major_locator(plt.NullLocator())\n\n    if saveable:\n        plt.savefig(name + '.pdf', format='pdf')\n    else:\n        plt.draw()\n        plt.pause(second)",
    "doc": "Display a frame. Make sure OpenAI Gym render() is disable before using it.\n\n    Parameters\n    ----------\n    I : numpy.array\n        The image.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    cmap : None or str\n        'gray' for greyscale, None for default, etc.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> env = gym.make(\"Pong-v0\")\n    >>> observation = env.reset()\n    >>> tl.visualize.frame(observation)"
  },
  {
    "code": "def CNN2d(CNN=None, second=10, saveable=True, name='cnn', fig_idx=3119362):\n    \"\"\"Display a group of RGB or Greyscale CNN masks.\n\n    Parameters\n    ----------\n    CNN : numpy.array\n        The image. e.g: 64 5x5 RGB images can be (5, 5, 3, 64).\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        The matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.CNN2d(network.all_params[0].eval(), second=10, saveable=True, name='cnn1_mnist', fig_idx=2012)\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    # tl.logging.info(CNN.shape)    # (5, 5, 3, 64)\n    # exit()\n    n_mask = CNN.shape[3]\n    n_row = CNN.shape[0]\n    n_col = CNN.shape[1]\n    n_color = CNN.shape[2]\n    row = int(np.sqrt(n_mask))\n    col = int(np.ceil(n_mask / row))\n    plt.ion()  # active mode\n    fig = plt.figure(fig_idx)\n    count = 1\n    for _ir in range(1, row + 1):\n        for _ic in range(1, col + 1):\n            if count > n_mask:\n                break\n            fig.add_subplot(col, row, count)\n            # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5\n            # exit()\n            # plt.imshow(\n            #         np.reshape(CNN[count-1,:,:,:], (n_row, n_col)),\n            #         cmap='gray', interpolation=\"nearest\")     # theano\n            if n_color == 1:\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)), cmap='gray', interpolation=\"nearest\")\n            elif n_color == 3:\n                plt.imshow(\n                    np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)), cmap='gray', interpolation=\"nearest\"\n                )\n            else:\n                raise Exception(\"Unknown n_color\")\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            count = count + 1\n    if saveable:\n        plt.savefig(name + '.pdf', format='pdf')\n    else:\n        plt.draw()\n        plt.pause(second)",
    "doc": "Display a group of RGB or Greyscale CNN masks.\n\n    Parameters\n    ----------\n    CNN : numpy.array\n        The image. e.g: 64 5x5 RGB images can be (5, 5, 3, 64).\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        The matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.CNN2d(network.all_params[0].eval(), second=10, saveable=True, name='cnn1_mnist', fig_idx=2012)"
  },
  {
    "code": "def tsne_embedding(embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='tsne', fig_idx=9862):\n    \"\"\"Visualize the embeddings by using t-SNE.\n\n    Parameters\n    ----------\n    embeddings : numpy.array\n        The embedding matrix.\n    reverse_dictionary : dictionary\n        id_to_word, mapping id to unique word.\n    plot_only : int\n        The number of examples to plot, choice the most common words.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> see 'tutorial_word2vec_basic.py'\n    >>> final_embeddings = normalized_embeddings.eval()\n    >>> tl.visualize.tsne_embedding(final_embeddings, labels, reverse_dictionary,\n    ...                   plot_only=500, second=5, saveable=False, name='tsne')\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    def plot_with_labels(low_dim_embs, labels, figsize=(18, 18), second=5, saveable=True, name='tsne', fig_idx=9862):\n\n        if low_dim_embs.shape[0] < len(labels):\n            raise AssertionError(\"More labels than embeddings\")\n\n        if saveable is False:\n            plt.ion()\n            plt.figure(fig_idx)\n\n        plt.figure(figsize=figsize)  # in inches\n\n        for i, label in enumerate(labels):\n            x, y = low_dim_embs[i, :]\n            plt.scatter(x, y)\n            plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n\n        if saveable:\n            plt.savefig(name + '.pdf', format='pdf')\n        else:\n            plt.draw()\n            plt.pause(second)\n\n    try:\n        from sklearn.manifold import TSNE\n        from six.moves import xrange\n\n        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n        # plot_only = 500\n        low_dim_embs = tsne.fit_transform(embeddings[:plot_only, :])\n        labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n        plot_with_labels(low_dim_embs, labels, second=second, saveable=saveable, name=name, fig_idx=fig_idx)\n\n    except ImportError:\n        _err = \"Please install sklearn and matplotlib to visualize embeddings.\"\n        tl.logging.error(_err)\n        raise ImportError(_err)",
    "doc": "Visualize the embeddings by using t-SNE.\n\n    Parameters\n    ----------\n    embeddings : numpy.array\n        The embedding matrix.\n    reverse_dictionary : dictionary\n        id_to_word, mapping id to unique word.\n    plot_only : int\n        The number of examples to plot, choice the most common words.\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    name : str\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> see 'tutorial_word2vec_basic.py'\n    >>> final_embeddings = normalized_embeddings.eval()\n    >>> tl.visualize.tsne_embedding(final_embeddings, labels, reverse_dictionary,\n    ...                   plot_only=500, second=5, saveable=False, name='tsne')"
  },
  {
    "code": "def draw_weights(W=None, second=10, saveable=True, shape=None, name='mnist', fig_idx=2396512):\n    \"\"\"Visualize every columns of the weight matrix to a group of Greyscale img.\n\n    Parameters\n    ----------\n    W : numpy.array\n        The weight matrix\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    shape : a list with 2 int or None\n        The shape of feature image, MNIST is [28, 80].\n    name : a string\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.draw_weights(network.all_params[0].eval(), second=10, saveable=True, name='weight_of_1st_layer', fig_idx=2012)\n\n    \"\"\"\n    if shape is None:\n        shape = [28, 28]\n\n    import matplotlib.pyplot as plt\n    if saveable is False:\n        plt.ion()\n    fig = plt.figure(fig_idx)  # show all feature images\n    n_units = W.shape[1]\n\n    num_r = int(np.sqrt(n_units))  # \u6bcf\u884c\u663e\u793a\u7684\u4e2a\u6570   \u82e525\u4e2ahidden unit -> \u6bcf\u884c\u663e\u793a5\u4e2a\n    num_c = int(np.ceil(n_units / num_r))\n    count = int(1)\n    for _row in range(1, num_r + 1):\n        for _col in range(1, num_c + 1):\n            if count > n_units:\n                break\n            fig.add_subplot(num_r, num_c, count)\n            # ------------------------------------------------------------\n            # plt.imshow(np.reshape(W[:,count-1],(28,28)), cmap='gray')\n            # ------------------------------------------------------------\n            feature = W[:, count - 1] / np.sqrt((W[:, count - 1]**2).sum())\n            # feature[feature<0.0001] = 0   # value threshold\n            # if count == 1 or count == 2:\n            #     print(np.mean(feature))\n            # if np.std(feature) < 0.03:      # condition threshold\n            #     feature = np.zeros_like(feature)\n            # if np.mean(feature) < -0.015:      # condition threshold\n            #     feature = np.zeros_like(feature)\n            plt.imshow(\n                np.reshape(feature, (shape[0], shape[1])), cmap='gray', interpolation=\"nearest\"\n            )  # , vmin=np.min(feature), vmax=np.max(feature))\n            # plt.title(name)\n            # ------------------------------------------------------------\n            # plt.imshow(np.reshape(W[:,count-1] ,(np.sqrt(size),np.sqrt(size))), cmap='gray', interpolation=\"nearest\")\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n            count = count + 1\n    if saveable:\n        plt.savefig(name + '.pdf', format='pdf')\n    else:\n        plt.draw()\n        plt.pause(second)",
    "doc": "Visualize every columns of the weight matrix to a group of Greyscale img.\n\n    Parameters\n    ----------\n    W : numpy.array\n        The weight matrix\n    second : int\n        The display second(s) for the image(s), if saveable is False.\n    saveable : boolean\n        Save or plot the figure.\n    shape : a list with 2 int or None\n        The shape of feature image, MNIST is [28, 80].\n    name : a string\n        A name to save the image, if saveable is True.\n    fig_idx : int\n        matplotlib figure index.\n\n    Examples\n    --------\n    >>> tl.visualize.draw_weights(network.all_params[0].eval(), second=10, saveable=True, name='weight_of_1st_layer', fig_idx=2012)"
  },
  {
    "code": "def data_to_tfrecord(images, labels, filename):\n    \"\"\"Save data into TFRecord.\"\"\"\n    if os.path.isfile(filename):\n        print(\"%s exists\" % filename)\n        return\n    print(\"Converting data into %s ...\" % filename)\n    # cwd = os.getcwd()\n    writer = tf.python_io.TFRecordWriter(filename)\n    for index, img in enumerate(images):\n        img_raw = img.tobytes()\n        # Visualize a image\n        # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236)\n        label = int(labels[index])\n        # print(label)\n        # Convert the bytes back to image as follow:\n        # image = Image.frombytes('RGB', (32, 32), img_raw)\n        # image = np.fromstring(img_raw, np.float32)\n        # image = image.reshape([32, 32, 3])\n        # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236)\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature={\n                    \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n                }\n            )\n        )\n        writer.write(example.SerializeToString())  # Serialize To String\n    writer.close()",
    "doc": "Save data into TFRecord."
  },
  {
    "code": "def read_and_decode(filename, is_train=None):\n    \"\"\"Return tensor to read from TFRecord.\"\"\"\n    filename_queue = tf.train.string_input_producer([filename])\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(\n        serialized_example, features={\n            'label': tf.FixedLenFeature([], tf.int64),\n            'img_raw': tf.FixedLenFeature([], tf.string),\n        }\n    )\n    # You can do more image distortion here for training data\n    img = tf.decode_raw(features['img_raw'], tf.float32)\n    img = tf.reshape(img, [32, 32, 3])\n    # img = tf.cast(img, tf.float32) #* (1. / 255) - 0.5\n    if is_train ==True:\n        # 1. Randomly crop a [height, width] section of the image.\n        img = tf.random_crop(img, [24, 24, 3])\n\n        # 2. Randomly flip the image horizontally.\n        img = tf.image.random_flip_left_right(img)\n\n        # 3. Randomly change brightness.\n        img = tf.image.random_brightness(img, max_delta=63)\n\n        # 4. Randomly change contrast.\n        img = tf.image.random_contrast(img, lower=0.2, upper=1.8)\n\n        # 5. Subtract off the mean and divide by the variance of the pixels.\n        img = tf.image.per_image_standardization(img)\n\n    elif is_train == False:\n        # 1. Crop the central [height, width] of the image.\n        img = tf.image.resize_image_with_crop_or_pad(img, 24, 24)\n\n        # 2. Subtract off the mean and divide by the variance of the pixels.\n        img = tf.image.per_image_standardization(img)\n\n    elif is_train == None:\n        img = img\n\n    label = tf.cast(features['label'], tf.int32)\n    return img, label",
    "doc": "Return tensor to read from TFRecord."
  },
  {
    "code": "def print_params(self, details=True, session=None):\n        \"\"\"Print all info of parameters in the network\"\"\"\n        for i, p in enumerate(self.all_params):\n            if details:\n                try:\n                    val = p.eval(session=session)\n                    logging.info(\n                        \"  param {:3}: {:20} {:15}    {} (mean: {:<18}, median: {:<18}, std: {:<18})   \".\n                        format(i, p.name, str(val.shape), p.dtype.name, val.mean(), np.median(val), val.std())\n                    )\n                except Exception as e:\n                    logging.info(str(e))\n                    raise Exception(\n                        \"Hint: print params details after tl.layers.initialize_global_variables(sess) \"\n                        \"or use network.print_params(False).\"\n                    )\n            else:\n                logging.info(\"  param {:3}: {:20} {:15}    {}\".format(i, p.name, str(p.get_shape()), p.dtype.name))\n        logging.info(\"  num of params: %d\" % self.count_params())",
    "doc": "Print all info of parameters in the network"
  },
  {
    "code": "def print_layers(self):\n        \"\"\"Print all info of layers in the network.\"\"\"\n        for i, layer in enumerate(self.all_layers):\n            # logging.info(\"  layer %d: %s\" % (i, str(layer)))\n            logging.info(\n                \"  layer {:3}: {:20} {:15}    {}\".format(i, layer.name, str(layer.get_shape()), layer.dtype.name)\n            )",
    "doc": "Print all info of layers in the network."
  },
  {
    "code": "def count_params(self):\n        \"\"\"Returns the number of parameters in the network.\"\"\"\n        n_params = 0\n        for _i, p in enumerate(self.all_params):\n            n = 1\n            # for s in p.eval().shape:\n            for s in p.get_shape():\n                try:\n                    s = int(s)\n                except Exception:\n                    s = 1\n                if s:\n                    n = n * s\n            n_params = n_params + n\n        return n_params",
    "doc": "Returns the number of parameters in the network."
  },
  {
    "code": "def get_all_params(self, session=None):\n        \"\"\"Return the parameters in a list of array.\"\"\"\n        _params = []\n        for p in self.all_params:\n            if session is None:\n                _params.append(p.eval())\n            else:\n                _params.append(session.run(p))\n        return _params",
    "doc": "Return the parameters in a list of array."
  },
  {
    "code": "def _get_init_args(self, skip=4):\n        \"\"\"Get all arguments of current layer for saving the graph.\"\"\"\n        stack = inspect.stack()\n\n        if len(stack) < skip + 1:\n            raise ValueError(\"The length of the inspection stack is shorter than the requested start position.\")\n\n        args, _, _, values = inspect.getargvalues(stack[skip][0])\n\n        params = {}\n\n        for arg in args:\n\n            # some args dont need to be saved into the graph. e.g. the input placeholder\n            if values[arg] is not None and arg not in ['self', 'prev_layer', 'inputs']:\n\n                val = values[arg]\n\n                # change function (e.g. act) into dictionary of module path and function name\n                if inspect.isfunction(val):\n                    params[arg] = {\"module_path\": val.__module__, \"func_name\": val.__name__}\n                # ignore more args e.g. TF class\n                elif arg.endswith('init'):\n                    continue\n                # for other data type, save them directly\n                else:\n                    params[arg] = val\n\n        return params",
    "doc": "Get all arguments of current layer for saving the graph."
  },
  {
    "code": "def roi_pooling(input, rois, pool_height, pool_width):\n    \"\"\"\n      returns a tensorflow operation for computing the Region of Interest Pooling\n    \n      @arg input: feature maps on which to perform the pooling operation\n      @arg rois: list of regions of interest in the format (feature map index, upper left, bottom right)\n      @arg pool_width: size of the pooling sections\n    \"\"\"\n    # TODO(maciek): ops scope\n    out = roi_pooling_module.roi_pooling(input, rois, pool_height=pool_height, pool_width=pool_width)\n    output, argmax_output = out[0], out[1]\n    return output",
    "doc": "returns a tensorflow operation for computing the Region of Interest Pooling\n    \n      @arg input: feature maps on which to perform the pooling operation\n      @arg rois: list of regions of interest in the format (feature map index, upper left, bottom right)\n      @arg pool_width: size of the pooling sections"
  },
  {
    "code": "def _int64_feature(value):\n    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto,\n    e.g, An integer label.\n    \"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))",
    "doc": "Wrapper for inserting an int64 Feature into a SequenceExample proto,\n    e.g, An integer label."
  },
  {
    "code": "def _bytes_feature(value):\n    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto,\n    e.g, an image in byte\n    \"\"\"\n    # return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))",
    "doc": "Wrapper for inserting a bytes Feature into a SequenceExample proto,\n    e.g, an image in byte"
  },
  {
    "code": "def _int64_feature_list(values):\n    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto,\n    e.g, sentence in list of ints\n    \"\"\"\n    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])",
    "doc": "Wrapper for inserting an int64 FeatureList into a SequenceExample proto,\n    e.g, sentence in list of ints"
  },
  {
    "code": "def _bytes_feature_list(values):\n    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto,\n    e.g, sentence in list of bytes\n    \"\"\"\n    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])",
    "doc": "Wrapper for inserting a bytes FeatureList into a SequenceExample proto,\n    e.g, sentence in list of bytes"
  },
  {
    "code": "def distort_image(image, thread_id):\n    \"\"\"Perform random distortions on an image.\n    Args:\n        image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n        thread_id: Preprocessing thread id used to select the ordering of color\n        distortions. There should be a multiple of 2 preprocessing threads.\n    Returns:````\n        distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n        [0, 1].\n    \"\"\"\n    # Randomly flip horizontally.\n    with tf.name_scope(\"flip_horizontal\"):  # , values=[image]): # DH MOdify\n        # with tf.name_scope(\"flip_horizontal\", values=[image]):\n        image = tf.image.random_flip_left_right(image)\n    # Randomly distort the colors based on thread id.\n    color_ordering = thread_id % 2\n    with tf.name_scope(\"distort_color\"):  # , values=[image]): # DH MOdify\n        # with tf.name_scope(\"distort_color\", values=[image]): # DH MOdify\n        if color_ordering == 0:\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.032)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        elif color_ordering == 1:\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.032)\n        # The random_* ops do not necessarily clamp.\n        image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image",
    "doc": "Perform random distortions on an image.\n    Args:\n        image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n        thread_id: Preprocessing thread id used to select the ordering of color\n        distortions. There should be a multiple of 2 preprocessing threads.\n    Returns:````\n        distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n        [0, 1]."
  },
  {
    "code": "def prefetch_input_data(\n        reader, file_pattern, is_training, batch_size, values_per_shard, input_queue_capacity_factor=16,\n        num_reader_threads=1, shard_queue_name=\"filename_queue\", value_queue_name=\"input_queue\"\n):\n    \"\"\"Prefetches string values from disk into an input queue.\n\n    In training the capacity of the queue is important because a larger queue\n    means better mixing of training examples between shards. The minimum number of\n    values kept in the queue is values_per_shard * input_queue_capacity_factor,\n    where input_queue_memory factor should be chosen to trade-off better mixing\n    with memory usage.\n\n    Args:\n        reader: Instance of tf.ReaderBase.\n        file_pattern: Comma-separated list of file patterns (e.g.\n            /tmp/train_data-?????-of-00100).\n        is_training: Boolean; whether prefetching for training or eval.\n        batch_size: Model batch size used to determine queue capacity.\n        values_per_shard: Approximate number of values per shard.\n        input_queue_capacity_factor: Minimum number of values to keep in the queue\n        in multiples of values_per_shard. See comments above.\n        num_reader_threads: Number of reader threads to fill the queue.\n        shard_queue_name: Name for the shards filename queue.\n        value_queue_name: Name for the values input queue.\n\n    Returns:\n        A Queue containing prefetched string values.\n    \"\"\"\n    data_files = []\n    for pattern in file_pattern.split(\",\"):\n        data_files.extend(tf.gfile.Glob(pattern))\n    if not data_files:\n        tl.logging.fatal(\"Found no input files matching %s\", file_pattern)\n    else:\n        tl.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)\n\n    if is_training:\n        print(\"   is_training == True : RandomShuffleQueue\")\n        filename_queue = tf.train.string_input_producer(data_files, shuffle=True, capacity=16, name=shard_queue_name)\n        min_queue_examples = values_per_shard * input_queue_capacity_factor\n        capacity = min_queue_examples + 100 * batch_size\n        values_queue = tf.RandomShuffleQueue(\n            capacity=capacity, min_after_dequeue=min_queue_examples, dtypes=[tf.string],\n            name=\"random_\" + value_queue_name\n        )\n    else:\n        print(\"   is_training == False : FIFOQueue\")\n        filename_queue = tf.train.string_input_producer(data_files, shuffle=False, capacity=1, name=shard_queue_name)\n        capacity = values_per_shard + 3 * batch_size\n        values_queue = tf.FIFOQueue(capacity=capacity, dtypes=[tf.string], name=\"fifo_\" + value_queue_name)\n\n    enqueue_ops = []\n    for _ in range(num_reader_threads):\n        _, value = reader.read(filename_queue)\n        enqueue_ops.append(values_queue.enqueue([value]))\n    tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(values_queue, enqueue_ops))\n\n    tf.summary.scalar(\n        \"queue/%s/fraction_of_%d_full\" % (values_queue.name, capacity),\n        tf.cast(values_queue.size(), tf.float32) * (1. / capacity)\n    )\n\n    return values_queue",
    "doc": "Prefetches string values from disk into an input queue.\n\n    In training the capacity of the queue is important because a larger queue\n    means better mixing of training examples between shards. The minimum number of\n    values kept in the queue is values_per_shard * input_queue_capacity_factor,\n    where input_queue_memory factor should be chosen to trade-off better mixing\n    with memory usage.\n\n    Args:\n        reader: Instance of tf.ReaderBase.\n        file_pattern: Comma-separated list of file patterns (e.g.\n            /tmp/train_data-?????-of-00100).\n        is_training: Boolean; whether prefetching for training or eval.\n        batch_size: Model batch size used to determine queue capacity.\n        values_per_shard: Approximate number of values per shard.\n        input_queue_capacity_factor: Minimum number of values to keep in the queue\n        in multiples of values_per_shard. See comments above.\n        num_reader_threads: Number of reader threads to fill the queue.\n        shard_queue_name: Name for the shards filename queue.\n        value_queue_name: Name for the values input queue.\n\n    Returns:\n        A Queue containing prefetched string values."
  },
  {
    "code": "def batch_with_dynamic_pad(images_and_captions, batch_size, queue_capacity, add_summaries=True):\n    \"\"\"Batches input images and captions.\n\n    This function splits the caption into an input sequence and a target sequence,\n    where the target sequence is the input sequence right-shifted by 1. Input and\n    target sequences are batched and padded up to the maximum length of sequences\n    in the batch. A mask is created to distinguish real words from padding words.\n\n    Example:\n        Actual captions in the batch ('-' denotes padded character):\n        [\n            [ 1 2 5 4 5 ],\n            [ 1 2 3 4 - ],\n            [ 1 2 3 - - ],\n        ]\n\n        input_seqs:\n        [\n            [ 1 2 3 4 ],\n            [ 1 2 3 - ],\n            [ 1 2 - - ],\n        ]\n\n        target_seqs:\n        [\n            [ 2 3 4 5 ],\n            [ 2 3 4 - ],\n            [ 2 3 - - ],\n        ]\n\n        mask:\n        [\n            [ 1 1 1 1 ],\n            [ 1 1 1 0 ],\n            [ 1 1 0 0 ],\n        ]\n\n    Args:\n        images_and_captions: A list of pairs [image, caption], where image is a\n        Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n        any length. Each pair will be processed and added to the queue in a\n        separate thread.\n        batch_size: Batch size.\n        queue_capacity: Queue capacity.\n        add_summaries: If true, add caption length summaries.\n\n    Returns:\n        images: A Tensor of shape [batch_size, height, width, channels].\n        input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n    \"\"\"\n    enqueue_list = []\n    for image, caption in images_and_captions:\n        caption_length = tf.shape(caption)[0]\n        input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)\n\n        input_seq = tf.slice(caption, [0], input_length)\n        target_seq = tf.slice(caption, [1], input_length)\n        indicator = tf.ones(input_length, dtype=tf.int32)\n        enqueue_list.append([image, input_seq, target_seq, indicator])\n\n    images, input_seqs, target_seqs, mask = tf.train.batch_join(\n        enqueue_list, batch_size=batch_size, capacity=queue_capacity, dynamic_pad=True, name=\"batch_and_pad\"\n    )\n\n    if add_summaries:\n        lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n        tf.summary.scalar(\"caption_length/batch_min\", tf.reduce_min(lengths))\n        tf.summary.scalar(\"caption_length/batch_max\", tf.reduce_max(lengths))\n        tf.summary.scalar(\"caption_length/batch_mean\", tf.reduce_mean(lengths))\n\n    return images, input_seqs, target_seqs, mask",
    "doc": "Batches input images and captions.\n\n    This function splits the caption into an input sequence and a target sequence,\n    where the target sequence is the input sequence right-shifted by 1. Input and\n    target sequences are batched and padded up to the maximum length of sequences\n    in the batch. A mask is created to distinguish real words from padding words.\n\n    Example:\n        Actual captions in the batch ('-' denotes padded character):\n        [\n            [ 1 2 5 4 5 ],\n            [ 1 2 3 4 - ],\n            [ 1 2 3 - - ],\n        ]\n\n        input_seqs:\n        [\n            [ 1 2 3 4 ],\n            [ 1 2 3 - ],\n            [ 1 2 - - ],\n        ]\n\n        target_seqs:\n        [\n            [ 2 3 4 5 ],\n            [ 2 3 4 - ],\n            [ 2 3 - - ],\n        ]\n\n        mask:\n        [\n            [ 1 1 1 1 ],\n            [ 1 1 1 0 ],\n            [ 1 1 0 0 ],\n        ]\n\n    Args:\n        images_and_captions: A list of pairs [image, caption], where image is a\n        Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n        any length. Each pair will be processed and added to the queue in a\n        separate thread.\n        batch_size: Batch size.\n        queue_capacity: Queue capacity.\n        add_summaries: If true, add caption length summaries.\n\n    Returns:\n        images: A Tensor of shape [batch_size, height, width, channels].\n        input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n        mask: An int32 0/1 Tensor of shape [batch_size, padded_length]."
  },
  {
    "code": "def _to_channel_first_bias(b):\n    \"\"\"Reshape [c] to [c, 1, 1].\"\"\"\n    channel_size = int(b.shape[0])\n    new_shape = (channel_size, 1, 1)\n    # new_shape = [-1, 1, 1]  # doesn't work with tensorRT\n    return tf.reshape(b, new_shape)",
    "doc": "Reshape [c] to [c, 1, 1]."
  },
  {
    "code": "def _bias_scale(x, b, data_format):\n    \"\"\"The multiplication counter part of tf.nn.bias_add.\"\"\"\n    if data_format == 'NHWC':\n        return x * b\n    elif data_format == 'NCHW':\n        return x * _to_channel_first_bias(b)\n    else:\n        raise ValueError('invalid data_format: %s' % data_format)",
    "doc": "The multiplication counter part of tf.nn.bias_add."
  },
  {
    "code": "def _bias_add(x, b, data_format):\n    \"\"\"Alternative implementation of tf.nn.bias_add which is compatiable with tensorRT.\"\"\"\n    if data_format == 'NHWC':\n        return tf.add(x, b)\n    elif data_format == 'NCHW':\n        return tf.add(x, _to_channel_first_bias(b))\n    else:\n        raise ValueError('invalid data_format: %s' % data_format)",
    "doc": "Alternative implementation of tf.nn.bias_add which is compatiable with tensorRT."
  },
  {
    "code": "def batch_normalization(x, mean, variance, offset, scale, variance_epsilon, data_format, name=None):\n    \"\"\"Data Format aware version of tf.nn.batch_normalization.\"\"\"\n    with ops.name_scope(name, 'batchnorm', [x, mean, variance, scale, offset]):\n        inv = math_ops.rsqrt(variance + variance_epsilon)\n        if scale is not None:\n            inv *= scale\n\n        a = math_ops.cast(inv, x.dtype)\n        b = math_ops.cast(offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\n        # Return a * x + b with customized data_format.\n        # Currently TF doesn't have bias_scale, and tensorRT has bug in converting tf.nn.bias_add\n        # So we reimplemted them to allow make the model work with tensorRT.\n        # See https://github.com/tensorlayer/openpose-plus/issues/75 for more details.\n        df = {'channels_first': 'NCHW', 'channels_last': 'NHWC'}\n        return _bias_add(_bias_scale(x, a, df[data_format]), b, df[data_format])",
    "doc": "Data Format aware version of tf.nn.batch_normalization."
  },
  {
    "code": "def compute_alpha(x):\n    \"\"\"Computing the scale parameter.\"\"\"\n    threshold = _compute_threshold(x)\n    alpha1_temp1 = tf.where(tf.greater(x, threshold), x, tf.zeros_like(x, tf.float32))\n    alpha1_temp2 = tf.where(tf.less(x, -threshold), x, tf.zeros_like(x, tf.float32))\n    alpha_array = tf.add(alpha1_temp1, alpha1_temp2, name=None)\n    alpha_array_abs = tf.abs(alpha_array)\n    alpha_array_abs1 = tf.where(\n        tf.greater(alpha_array_abs, 0), tf.ones_like(alpha_array_abs, tf.float32),\n        tf.zeros_like(alpha_array_abs, tf.float32)\n    )\n    alpha_sum = tf.reduce_sum(alpha_array_abs)\n    n = tf.reduce_sum(alpha_array_abs1)\n    alpha = tf.div(alpha_sum, n)\n    return alpha",
    "doc": "Computing the scale parameter."
  },
  {
    "code": "def flatten_reshape(variable, name='flatten'):\n    \"\"\"Reshapes a high-dimension vector input.\n\n    [batch_size, mask_row, mask_col, n_mask] ---> [batch_size, mask_row x mask_col x n_mask]\n\n    Parameters\n    ----------\n    variable : TensorFlow variable or tensor\n        The variable or tensor to be flatten.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    Tensor\n        Flatten Tensor\n\n    Examples\n    --------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> x = tf.placeholder(tf.float32, [None, 128, 128, 3])\n    >>> # Convolution Layer with 32 filters and a kernel size of 5\n    >>> network = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n    >>> # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n    >>> network = tf.layers.max_pooling2d(network, 2, 2)\n    >>> print(network.get_shape()[:].as_list())\n    >>> [None, 62, 62, 32]\n    >>> network = tl.layers.flatten_reshape(network)\n    >>> print(network.get_shape()[:].as_list()[1:])\n    >>> [None, 123008]\n    \"\"\"\n    dim = 1\n    for d in variable.get_shape()[1:].as_list():\n        dim *= d\n    return tf.reshape(variable, shape=[-1, dim], name=name)",
    "doc": "Reshapes a high-dimension vector input.\n\n    [batch_size, mask_row, mask_col, n_mask] ---> [batch_size, mask_row x mask_col x n_mask]\n\n    Parameters\n    ----------\n    variable : TensorFlow variable or tensor\n        The variable or tensor to be flatten.\n    name : str\n        A unique layer name.\n\n    Returns\n    -------\n    Tensor\n        Flatten Tensor\n\n    Examples\n    --------\n    >>> import tensorflow as tf\n    >>> import tensorlayer as tl\n    >>> x = tf.placeholder(tf.float32, [None, 128, 128, 3])\n    >>> # Convolution Layer with 32 filters and a kernel size of 5\n    >>> network = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n    >>> # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n    >>> network = tf.layers.max_pooling2d(network, 2, 2)\n    >>> print(network.get_shape()[:].as_list())\n    >>> [None, 62, 62, 32]\n    >>> network = tl.layers.flatten_reshape(network)\n    >>> print(network.get_shape()[:].as_list()[1:])\n    >>> [None, 123008]"
  },
  {
    "code": "def get_layers_with_name(net, name=\"\", verbose=False):\n    \"\"\"Get a list of layers' output in a network by a given name scope.\n\n    Parameters\n    -----------\n    net : :class:`Layer`\n        The last layer of the network.\n    name : str\n        Get the layers' output that contain this name.\n    verbose : boolean\n        If True, print information of all the layers' output\n\n    Returns\n    --------\n    list of Tensor\n        A list of layers' output (TensorFlow tensor)\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> layers = tl.layers.get_layers_with_name(net, \"CNN\", True)\n\n    \"\"\"\n    logging.info(\"  [*] geting layers with %s\" % name)\n\n    layers = []\n    i = 0\n\n    for layer in net.all_layers:\n        # logging.info(type(layer.name))\n        if name in layer.name:\n            layers.append(layer)\n\n            if verbose:\n                logging.info(\"  got {:3}: {:15}   {}\".format(i, layer.name, str(layer.get_shape())))\n                i = i + 1\n\n    return layers",
    "doc": "Get a list of layers' output in a network by a given name scope.\n\n    Parameters\n    -----------\n    net : :class:`Layer`\n        The last layer of the network.\n    name : str\n        Get the layers' output that contain this name.\n    verbose : boolean\n        If True, print information of all the layers' output\n\n    Returns\n    --------\n    list of Tensor\n        A list of layers' output (TensorFlow tensor)\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> layers = tl.layers.get_layers_with_name(net, \"CNN\", True)"
  },
  {
    "code": "def get_variables_with_name(name=None, train_only=True, verbose=False):\n    \"\"\"Get a list of TensorFlow variables by a given name scope.\n\n    Parameters\n    ----------\n    name : str\n        Get the variables that contain this name.\n    train_only : boolean\n        If Ture, only get the trainable variables.\n    verbose : boolean\n        If True, print the information of all variables.\n\n    Returns\n    -------\n    list of Tensor\n        A list of TensorFlow variables\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> dense_vars = tl.layers.get_variables_with_name('dense', True, True)\n\n    \"\"\"\n    if name is None:\n        raise Exception(\"please input a name\")\n\n    logging.info(\"  [*] geting variables with %s\" % name)\n\n    # tvar = tf.trainable_variables() if train_only else tf.all_variables()\n    if train_only:\n        t_vars = tf.trainable_variables()\n\n    else:\n        t_vars = tf.global_variables()\n\n    d_vars = [var for var in t_vars if name in var.name]\n\n    if verbose:\n        for idx, v in enumerate(d_vars):\n            logging.info(\"  got {:3}: {:15}   {}\".format(idx, v.name, str(v.get_shape())))\n\n    return d_vars",
    "doc": "Get a list of TensorFlow variables by a given name scope.\n\n    Parameters\n    ----------\n    name : str\n        Get the variables that contain this name.\n    train_only : boolean\n        If Ture, only get the trainable variables.\n    verbose : boolean\n        If True, print the information of all variables.\n\n    Returns\n    -------\n    list of Tensor\n        A list of TensorFlow variables\n\n    Examples\n    --------\n    >>> import tensorlayer as tl\n    >>> dense_vars = tl.layers.get_variables_with_name('dense', True, True)"
  },
  {
    "code": "def initialize_rnn_state(state, feed_dict=None):\n    \"\"\"Returns the initialized RNN state.\n    The inputs are `LSTMStateTuple` or `State` of `RNNCells`, and an optional `feed_dict`.\n\n    Parameters\n    ----------\n    state : RNN state.\n        The TensorFlow's RNN state.\n    feed_dict : dictionary\n        Initial RNN state; if None, returns zero state.\n\n    Returns\n    -------\n    RNN state\n        The TensorFlow's RNN state.\n\n    \"\"\"\n    if isinstance(state, LSTMStateTuple):\n        c = state.c.eval(feed_dict=feed_dict)\n        h = state.h.eval(feed_dict=feed_dict)\n        return c, h\n    else:\n        new_state = state.eval(feed_dict=feed_dict)\n        return new_state",
    "doc": "Returns the initialized RNN state.\n    The inputs are `LSTMStateTuple` or `State` of `RNNCells`, and an optional `feed_dict`.\n\n    Parameters\n    ----------\n    state : RNN state.\n        The TensorFlow's RNN state.\n    feed_dict : dictionary\n        Initial RNN state; if None, returns zero state.\n\n    Returns\n    -------\n    RNN state\n        The TensorFlow's RNN state."
  },
  {
    "code": "def list_remove_repeat(x):\n    \"\"\"Remove the repeated items in a list, and return the processed list.\n    You may need it to create merged layer like Concat, Elementwise and etc.\n\n    Parameters\n    ----------\n    x : list\n        Input\n\n    Returns\n    -------\n    list\n        A list that after removing it's repeated items\n\n    Examples\n    -------\n    >>> l = [2, 3, 4, 2, 3]\n    >>> l = list_remove_repeat(l)\n    [2, 3, 4]\n\n    \"\"\"\n    y = []\n    for i in x:\n        if i not in y:\n            y.append(i)\n\n    return y",
    "doc": "Remove the repeated items in a list, and return the processed list.\n    You may need it to create merged layer like Concat, Elementwise and etc.\n\n    Parameters\n    ----------\n    x : list\n        Input\n\n    Returns\n    -------\n    list\n        A list that after removing it's repeated items\n\n    Examples\n    -------\n    >>> l = [2, 3, 4, 2, 3]\n    >>> l = list_remove_repeat(l)\n    [2, 3, 4]"
  },
  {
    "code": "def merge_networks(layers=None):\n    \"\"\"Merge all parameters, layers and dropout probabilities to a :class:`Layer`.\n    The output of return network is the first network in the list.\n\n    Parameters\n    ----------\n    layers : list of :class:`Layer`\n        Merge all parameters, layers and dropout probabilities to the first layer in the list.\n\n    Returns\n    --------\n    :class:`Layer`\n        The network after merging all parameters, layers and dropout probabilities to the first network in the list.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> n1 = ...\n    >>> n2 = ...\n    >>> n1 = tl.layers.merge_networks([n1, n2])\n\n    \"\"\"\n    if layers is None:\n        raise Exception(\"layers should be a list of TensorLayer's Layers.\")\n    layer = layers[0]\n\n    all_params = []\n    all_layers = []\n    all_drop = {}\n\n    for l in layers:\n        all_params.extend(l.all_params)\n        all_layers.extend(l.all_layers)\n        all_drop.update(l.all_drop)\n\n    layer.all_params = list(all_params)\n    layer.all_layers = list(all_layers)\n    layer.all_drop = dict(all_drop)\n\n    layer.all_layers = list_remove_repeat(layer.all_layers)\n    layer.all_params = list_remove_repeat(layer.all_params)\n\n    return layer",
    "doc": "Merge all parameters, layers and dropout probabilities to a :class:`Layer`.\n    The output of return network is the first network in the list.\n\n    Parameters\n    ----------\n    layers : list of :class:`Layer`\n        Merge all parameters, layers and dropout probabilities to the first layer in the list.\n\n    Returns\n    --------\n    :class:`Layer`\n        The network after merging all parameters, layers and dropout probabilities to the first network in the list.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> n1 = ...\n    >>> n2 = ...\n    >>> n1 = tl.layers.merge_networks([n1, n2])"
  },
  {
    "code": "def print_all_variables(train_only=False):\n    \"\"\"Print information of trainable or all variables,\n    without ``tl.layers.initialize_global_variables(sess)``.\n\n    Parameters\n    ----------\n    train_only : boolean\n        Whether print trainable variables only.\n            - If True, print the trainable variables.\n            - If False, print all variables.\n\n    \"\"\"\n    # tvar = tf.trainable_variables() if train_only else tf.all_variables()\n    if train_only:\n        t_vars = tf.trainable_variables()\n        logging.info(\"  [*] printing trainable variables\")\n\n    else:\n        t_vars = tf.global_variables()\n        logging.info(\"  [*] printing global variables\")\n\n    for idx, v in enumerate(t_vars):\n        logging.info(\"  var {:3}: {:15}   {}\".format(idx, str(v.get_shape()), v.name))",
    "doc": "Print information of trainable or all variables,\n    without ``tl.layers.initialize_global_variables(sess)``.\n\n    Parameters\n    ----------\n    train_only : boolean\n        Whether print trainable variables only.\n            - If True, print the trainable variables.\n            - If False, print all variables."
  },
  {
    "code": "def ternary_operation(x):\n    \"\"\"Ternary operation use threshold computed with weights.\"\"\"\n    g = tf.get_default_graph()\n    with g.gradient_override_map({\"Sign\": \"Identity\"}):\n        threshold = _compute_threshold(x)\n        x = tf.sign(tf.add(tf.sign(tf.add(x, threshold)), tf.sign(tf.add(x, -threshold))))\n        return x",
    "doc": "Ternary operation use threshold computed with weights."
  },
  {
    "code": "def _compute_threshold(x):\n    \"\"\"\n    ref: https://github.com/XJTUWYD/TWN\n    Computing the threshold.\n    \"\"\"\n    x_sum = tf.reduce_sum(tf.abs(x), reduction_indices=None, keepdims=False, name=None)\n    threshold = tf.div(x_sum, tf.cast(tf.size(x), tf.float32), name=None)\n    threshold = tf.multiply(0.7, threshold, name=None)\n    return threshold",
    "doc": "ref: https://github.com/XJTUWYD/TWN\n    Computing the threshold."
  },
  {
    "code": "def freeze_graph(graph_path, checkpoint_path, output_path, end_node_names, is_binary_graph):\n    \"\"\"Reimplementation of the TensorFlow official freeze_graph function to freeze the graph and checkpoint together:\n\n    Parameters\n    -----------\n    graph_path : string\n        the path where your graph file save.\n    checkpoint_output_path : string\n        the path where your checkpoint save.\n    output_path : string\n        the path where you want to save the output proto buff\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    is_binary_graph : boolean\n        declare your file whether is a binary graph\n\n    References\n    ----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n    - `tensorflow freeze_graph <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py>`\n    \"\"\"\n    _freeze_graph(\n        input_graph=graph_path, input_saver='', input_binary=is_binary_graph, input_checkpoint=checkpoint_path,\n        output_graph=output_path, output_node_names=end_node_names, restore_op_name='save/restore_all',\n        filename_tensor_name='save/Const:0', clear_devices=True, initializer_nodes=None\n    )",
    "doc": "Reimplementation of the TensorFlow official freeze_graph function to freeze the graph and checkpoint together:\n\n    Parameters\n    -----------\n    graph_path : string\n        the path where your graph file save.\n    checkpoint_output_path : string\n        the path where your checkpoint save.\n    output_path : string\n        the path where you want to save the output proto buff\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    is_binary_graph : boolean\n        declare your file whether is a binary graph\n\n    References\n    ----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n    - `tensorflow freeze_graph <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py>`"
  },
  {
    "code": "def convert_model_to_onnx(frozen_graph_path, end_node_names, onnx_output_path):\n    \"\"\"Reimplementation of the TensorFlow-onnx official tutorial convert the proto buff to onnx file:\n\n    Parameters\n    -----------\n    frozen_graph_path : string\n        the path where your frozen graph file save.\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    onnx_output_path : string\n        the path where you want to save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`\n    \"\"\"\n    with tf.gfile.GFile(frozen_graph_path, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        onnx_model = tensorflow_graph_to_onnx_model(graph_def, end_node_names, opset=6)\n        file = open(onnx_output_path, \"wb\")\n        file.write(onnx_model.SerializeToString())\n        file.close()",
    "doc": "Reimplementation of the TensorFlow-onnx official tutorial convert the proto buff to onnx file:\n\n    Parameters\n    -----------\n    frozen_graph_path : string\n        the path where your frozen graph file save.\n    end_node_names : string\n        the name of the end node in your graph you want to get in your proto buff\n    onnx_output_path : string\n        the path where you want to save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`"
  },
  {
    "code": "def convert_onnx_to_model(onnx_input_path):\n    \"\"\"Reimplementation of the TensorFlow-onnx official tutorial convert the onnx file to specific: model\n\n    Parameters\n    -----------\n    onnx_input_path : string\n    the path where you save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__\n    \"\"\"\n    model = onnx.load(onnx_input_path)\n    tf_rep = prepare(model)\n    # Image Path\n    img = np.load(\"./assets/image.npz\")\n    output = tf_rep.run(img.reshape([1, 784]))\n    print(\"The digit is classified as \", np.argmax(output))",
    "doc": "Reimplementation of the TensorFlow-onnx official tutorial convert the onnx file to specific: model\n\n    Parameters\n    -----------\n    onnx_input_path : string\n    the path where you save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__"
  },
  {
    "code": "def _add_deprecated_function_notice_to_docstring(doc, date, instructions):\n    \"\"\"Adds a deprecation notice to a docstring for deprecated functions.\"\"\"\n\n    if instructions:\n        deprecation_message = \"\"\"\n            .. warning::\n                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.\n                *Instructions for updating:* %s.\n        \"\"\" % (('in a future version' if date is None else ('after %s' % date)), instructions)\n\n    else:\n        deprecation_message = \"\"\"\n            .. warning::\n                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.\n        \"\"\" % (('in a future version' if date is None else ('after %s' % date)))\n\n    main_text = [deprecation_message]\n\n    return _add_notice_to_docstring(doc, 'DEPRECATED FUNCTION', main_text)",
    "doc": "Adds a deprecation notice to a docstring for deprecated functions."
  },
  {
    "code": "def _add_notice_to_docstring(doc, no_doc_str, notice):\n    \"\"\"Adds a deprecation notice to a docstring.\"\"\"\n    if not doc:\n        lines = [no_doc_str]\n\n    else:\n        lines = _normalize_docstring(doc).splitlines()\n\n    notice = [''] + notice\n\n    if len(lines) > 1:\n        # Make sure that we keep our distance from the main body\n        if lines[1].strip():\n            notice.append('')\n\n        lines[1:1] = notice\n    else:\n        lines += notice\n\n    return '\\n'.join(lines)",
    "doc": "Adds a deprecation notice to a docstring."
  },
  {
    "code": "def alphas(shape, alpha_value, name=None):\n    \"\"\"Creates a tensor with all elements set to `alpha_value`.\n    This operation returns a tensor of type `dtype` with shape `shape` and all\n    elements set to alpha.\n\n    Parameters\n    ----------\n    shape: A list of integers, a tuple of integers, or a 1-D `Tensor` of type `int32`.\n        The shape of the desired tensor\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n\n    Returns\n    -------\n    A `Tensor` with all elements set to alpha.\n\n    Examples\n    --------\n    >>> tl.alphas([2, 3], tf.int32)  # [[alpha, alpha, alpha], [alpha, alpha, alpha]]\n    \"\"\"\n    with ops.name_scope(name, \"alphas\", [shape]) as name:\n\n        alpha_tensor = convert_to_tensor(alpha_value)\n        alpha_dtype = dtypes.as_dtype(alpha_tensor.dtype).base_dtype\n\n        if not isinstance(shape, ops.Tensor):\n            try:\n                shape = constant_op._tensor_shape_tensor_conversion_function(tensor_shape.TensorShape(shape))\n            except (TypeError, ValueError):\n                shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\n\n        if not shape._shape_tuple():\n            shape = reshape(shape, [-1])  # Ensure it's a vector\n\n        try:\n            output = constant(alpha_value, shape=shape, dtype=alpha_dtype, name=name)\n\n        except (TypeError, ValueError):\n            output = fill(shape, constant(alpha_value, dtype=alpha_dtype), name=name)\n\n        if output.dtype.base_dtype != alpha_dtype:\n            raise AssertionError(\"Dtypes do not corresponds: %s and %s\" % (output.dtype.base_dtype, alpha_dtype))\n\n        return output",
    "doc": "Creates a tensor with all elements set to `alpha_value`.\n    This operation returns a tensor of type `dtype` with shape `shape` and all\n    elements set to alpha.\n\n    Parameters\n    ----------\n    shape: A list of integers, a tuple of integers, or a 1-D `Tensor` of type `int32`.\n        The shape of the desired tensor\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n\n    Returns\n    -------\n    A `Tensor` with all elements set to alpha.\n\n    Examples\n    --------\n    >>> tl.alphas([2, 3], tf.int32)  # [[alpha, alpha, alpha], [alpha, alpha, alpha]]"
  },
  {
    "code": "def alphas_like(tensor, alpha_value, name=None, optimize=True):\n    \"\"\"Creates a tensor with all elements set to `alpha_value`.\n    Given a single tensor (`tensor`), this operation returns a tensor of the same\n    type and shape as `tensor` with all elements set to `alpha_value`.\n\n    Parameters\n    ----------\n    tensor: tf.Tensor\n        The Tensorflow Tensor that will be used as a template.\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n    optimize: bool\n        if true, attempt to statically determine the shape of 'tensor' and encode it as a constant.\n\n    Returns\n    -------\n    A `Tensor` with all elements set to `alpha_value`.\n\n    Examples\n    --------\n    >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n    >>> tl.alphas_like(tensor, 0.5)  # [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\n    \"\"\"\n    with ops.name_scope(name, \"alphas_like\", [tensor]) as name:\n        tensor = ops.convert_to_tensor(tensor, name=\"tensor\")\n\n        if context.in_eager_mode():  # and dtype is not None and dtype != tensor.dtype:\n            ret = alphas(shape_internal(tensor, optimize=optimize), alpha_value=alpha_value, name=name)\n\n        else:  # if context.in_graph_mode():\n\n            # For now, variant types must be created via zeros_like; as we need to\n            # pass the input variant object to the proper zeros callback.\n\n            if (optimize and tensor.shape.is_fully_defined()):\n                # We can produce a zeros tensor independent of the value of 'tensor',\n                # since the shape is known statically.\n                ret = alphas(tensor.shape, alpha_value=alpha_value, name=name)\n\n            # elif dtype is not None and dtype != tensor.dtype and dtype != dtypes.variant:\n            else:\n                ret = alphas(shape_internal(tensor, optimize=optimize), alpha_value=alpha_value, name=name)\n\n            ret.set_shape(tensor.get_shape())\n\n        return ret",
    "doc": "Creates a tensor with all elements set to `alpha_value`.\n    Given a single tensor (`tensor`), this operation returns a tensor of the same\n    type and shape as `tensor` with all elements set to `alpha_value`.\n\n    Parameters\n    ----------\n    tensor: tf.Tensor\n        The Tensorflow Tensor that will be used as a template.\n    alpha_value: `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, int32`, `int64`\n        The value used to fill the resulting `Tensor`.\n    name: str\n        A name for the operation (optional).\n    optimize: bool\n        if true, attempt to statically determine the shape of 'tensor' and encode it as a constant.\n\n    Returns\n    -------\n    A `Tensor` with all elements set to `alpha_value`.\n\n    Examples\n    --------\n    >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n    >>> tl.alphas_like(tensor, 0.5)  # [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]"
  },
  {
    "code": "def example1():\n    \"\"\" Example 1: Applying transformation one-by-one is very SLOW ! \"\"\"\n    st = time.time()\n    for _ in range(100):  # Try 100 times and compute the averaged speed\n        xx = tl.prepro.rotation(image, rg=-20, is_random=False)\n        xx = tl.prepro.flip_axis(xx, axis=1, is_random=False)\n        xx = tl.prepro.shear2(xx, shear=(0., -0.2), is_random=False)\n        xx = tl.prepro.zoom(xx, zoom_range=1 / 0.8)\n        xx = tl.prepro.shift(xx, wrg=-0.1, hrg=0, is_random=False)\n    print(\"apply transforms one-by-one took %fs for each image\" % ((time.time() - st) / 100))\n    tl.vis.save_image(xx, '_result_slow.png')",
    "doc": "Example 1: Applying transformation one-by-one is very SLOW !"
  },
  {
    "code": "def example2():\n    \"\"\" Example 2: Applying all transforms in one is very FAST ! \"\"\"\n    st = time.time()\n    for _ in range(100):  # Repeat 100 times and compute the averaged speed\n        transform_matrix = create_transformation_matrix()\n        result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # Transform the image using a single operation\n    print(\"apply all transforms once took %fs for each image\" % ((time.time() - st) / 100))  # usually 50x faster\n    tl.vis.save_image(result, '_result_fast.png')",
    "doc": "Example 2: Applying all transforms in one is very FAST !"
  },
  {
    "code": "def example3():\n    \"\"\" Example 3: Using TF dataset API to load and process image for training \"\"\"\n    n_data = 100\n    imgs_file_list = ['tiger.jpeg'] * n_data\n    train_targets = [np.ones(1)] * n_data\n\n    def generator():\n        if len(imgs_file_list) != len(train_targets):\n            raise RuntimeError('len(imgs_file_list) != len(train_targets)')\n        for _input, _target in zip(imgs_file_list, train_targets):\n            yield _input, _target\n\n    def _data_aug_fn(image):\n        transform_matrix = create_transformation_matrix()\n        result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # Transform the image using a single operation\n        return result\n\n    def _map_fn(image_path, target):\n        image = tf.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)  # Get RGB with 0~1\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = tf.py_func(_data_aug_fn, [image], [tf.float32])\n        # image = tf.reshape(image, (h, w, 3))\n        target = tf.reshape(target, ())\n        return image, target\n\n    n_epoch = 10\n    batch_size = 5\n    dataset = tf.data.Dataset().from_generator(generator, output_types=(tf.string, tf.int64))\n    dataset = dataset.shuffle(buffer_size=4096)  # shuffle before loading images\n    dataset = dataset.repeat(n_epoch)\n    dataset = dataset.map(_map_fn, num_parallel_calls=multiprocessing.cpu_count())\n    dataset = dataset.batch(batch_size)  # TODO: consider using tf.contrib.map_and_batch\n    dataset = dataset.prefetch(1)  # prefetch 1 batch\n    iterator = dataset.make_one_shot_iterator()\n    one_element = iterator.get_next()\n    sess = tf.Session()\n    # feed `one_element` into a network, for demo, we simply get the data as follows\n    n_step = round(n_epoch * n_data / batch_size)\n    st = time.time()\n    for _ in range(n_step):\n        _images, _targets = sess.run(one_element)\n    print(\"dataset APIs took %fs for each image\" % ((time.time() - st) / batch_size / n_step))",
    "doc": "Example 3: Using TF dataset API to load and process image for training"
  },
  {
    "code": "def example4():\n    \"\"\" Example 4: Transforming coordinates using affine matrix. \"\"\"\n    transform_matrix = create_transformation_matrix()\n    result = tl.prepro.affine_transform_cv2(image, transform_matrix)  # 76 times faster\n    # Transform keypoint coordinates\n    coords = [[(50, 100), (100, 100), (100, 50), (200, 200)], [(250, 50), (200, 50), (200, 100)]]\n    coords_result = tl.prepro.affine_transform_keypoints(coords, transform_matrix)\n\n    def imwrite(image, coords_list, name):\n        coords_list_ = []\n        for coords in coords_list:\n            coords = np.array(coords, np.int32)\n            coords = coords.reshape((-1, 1, 2))\n            coords_list_.append(coords)\n        image = cv2.polylines(image, coords_list_, True, (0, 255, 255), 3)\n        cv2.imwrite(name, image[..., ::-1])\n\n    imwrite(image, coords, '_with_keypoints_origin.png')\n    imwrite(result, coords_result, '_with_keypoints_result.png')",
    "doc": "Example 4: Transforming coordinates using affine matrix."
  },
  {
    "code": "def distort_fn(x, is_train=False):\n    \"\"\"\n    The images are processed as follows:\n    .. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n    .. They are approximately whitened to make the model insensitive to dynamic range.\n    For training, we additionally apply a series of random distortions to\n    artificially increase the data set size:\n    .. Randomly flip the image from left to right.\n    .. Randomly distort the image brightness.\n    \"\"\"\n    # print('begin',x.shape, np.min(x), np.max(x))\n    x = tl.prepro.crop(x, 24, 24, is_random=is_train)\n    # print('after crop',x.shape, np.min(x), np.max(x))\n    if is_train:\n        # x = tl.prepro.zoom(x, zoom_range=(0.9, 1.0), is_random=True)\n        # print('after zoom', x.shape, np.min(x), np.max(x))\n        x = tl.prepro.flip_axis(x, axis=1, is_random=True)\n        # print('after flip',x.shape, np.min(x), np.max(x))\n        x = tl.prepro.brightness(x, gamma=0.1, gain=1, is_random=True)\n        # print('after brightness',x.shape, np.min(x), np.max(x))\n        # tmp = np.max(x)\n        # x += np.random.uniform(-20, 20)\n        # x /= tmp\n    # normalize the image\n    x = (x - np.mean(x)) / max(np.std(x), 1e-5)  # avoid values divided by 0\n    # print('after norm', x.shape, np.min(x), np.max(x), np.mean(x))\n    return x",
    "doc": "The images are processed as follows:\n    .. They are cropped to 24 x 24 pixels, centrally for evaluation or randomly for training.\n    .. They are approximately whitened to make the model insensitive to dynamic range.\n    For training, we additionally apply a series of random distortions to\n    artificially increase the data set size:\n    .. Randomly flip the image from left to right.\n    .. Randomly distort the image brightness."
  },
  {
    "code": "def fit(\n        sess, network, train_op, cost, X_train, y_train, x, y_, acc=None, batch_size=100, n_epoch=100, print_freq=5,\n        X_val=None, y_val=None, eval_train=True, tensorboard_dir=None, tensorboard_epoch_freq=5,\n        tensorboard_weight_histograms=True, tensorboard_graph_vis=True\n):\n    \"\"\"Training a given non time-series network by the given cost function, training data, batch_size, n_epoch etc.\n\n    - MNIST example click `here <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_.\n    - In order to control the training details, the authors HIGHLY recommend ``tl.iterate`` see two MNIST examples `1 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_, `2 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    network : TensorLayer layer\n        the network to be trained.\n    train_op : TensorFlow optimizer\n        The optimizer for training e.g. tf.train.AdamOptimizer.\n    X_train : numpy.array\n        The input of training data\n    y_train : numpy.array\n        The target of training data\n    x : placeholder\n        For inputs.\n    y_ : placeholder\n        For targets.\n    acc : TensorFlow expression or None\n        Metric for accuracy or others. If None, would not print the information.\n    batch_size : int\n        The batch size for training and evaluating.\n    n_epoch : int\n        The number of training epochs.\n    print_freq : int\n        Print the training information every ``print_freq`` epochs.\n    X_val : numpy.array or None\n        The input of validation data. If None, would not perform validation.\n    y_val : numpy.array or None\n        The target of validation data. If None, would not perform validation.\n    eval_train : boolean\n        Whether to evaluate the model during training.\n        If X_val and y_val are not None, it reflects whether to evaluate the model on training data.\n    tensorboard_dir : string\n        path to log dir, if set, summary data will be stored to the tensorboard_dir/ directory for visualization with tensorboard. (default None)\n        Also runs `tl.layers.initialize_global_variables(sess)` internally in fit() to setup the summary nodes.\n    tensorboard_epoch_freq : int\n        How many epochs between storing tensorboard checkpoint for visualization to log/ directory (default 5).\n    tensorboard_weight_histograms : boolean\n        If True updates tensorboard data in the logs/ directory for visualization\n        of the weight histograms every tensorboard_epoch_freq epoch (default True).\n    tensorboard_graph_vis : boolean\n        If True stores the graph in the tensorboard summaries saved to log/ (default True).\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,\n    ...            acc=acc, batch_size=500, n_epoch=200, print_freq=5,\n    ...            X_val=X_val, y_val=y_val, eval_train=False)\n    >>> tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,\n    ...            acc=acc, batch_size=500, n_epoch=200, print_freq=5,\n    ...            X_val=X_val, y_val=y_val, eval_train=False,\n    ...            tensorboard=True, tensorboard_weight_histograms=True, tensorboard_graph_vis=True)\n\n    Notes\n    --------\n    If tensorboard_dir not None, the `global_variables_initializer` will be run inside the fit function\n    in order to initialize the automatically generated summary nodes used for tensorboard visualization,\n    thus `tf.global_variables_initializer().run()` before the `fit()` call will be undefined.\n\n    \"\"\"\n    if X_train.shape[0] < batch_size:\n        raise AssertionError(\"Number of training examples should be bigger than the batch size\")\n\n    if tensorboard_dir is not None:\n        tl.logging.info(\"Setting up tensorboard ...\")\n        #Set up tensorboard summaries and saver\n        tl.files.exists_or_mkdir(tensorboard_dir)\n\n        #Only write summaries for more recent TensorFlow versions\n        if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n            if tensorboard_graph_vis:\n                train_writer = tf.summary.FileWriter(tensorboard_dir + '/train', sess.graph)\n                val_writer = tf.summary.FileWriter(tensorboard_dir + '/validation', sess.graph)\n            else:\n                train_writer = tf.summary.FileWriter(tensorboard_dir + '/train')\n                val_writer = tf.summary.FileWriter(tensorboard_dir + '/validation')\n\n        #Set up summary nodes\n        if (tensorboard_weight_histograms):\n            for param in network.all_params:\n                if hasattr(tf, 'summary') and hasattr(tf.summary, 'histogram'):\n                    tl.logging.info('Param name %s' % param.name)\n                    tf.summary.histogram(param.name, param)\n\n        if hasattr(tf, 'summary') and hasattr(tf.summary, 'histogram'):\n            tf.summary.scalar('cost', cost)\n\n        merged = tf.summary.merge_all()\n\n        #Initalize all variables and summaries\n        tl.layers.initialize_global_variables(sess)\n        tl.logging.info(\"Finished! use `tensorboard --logdir=%s/` to start tensorboard\" % tensorboard_dir)\n\n    tl.logging.info(\"Start training the network ...\")\n    start_time_begin = time.time()\n    tensorboard_train_index, tensorboard_val_index = 0, 0\n    for epoch in range(n_epoch):\n        start_time = time.time()\n        loss_ep = 0\n        n_step = 0\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n            feed_dict = {x: X_train_a, y_: y_train_a}\n            feed_dict.update(network.all_drop)  # enable noise layers\n            loss, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n            loss_ep += loss\n            n_step += 1\n        loss_ep = loss_ep / n_step\n\n        if tensorboard_dir is not None and hasattr(tf, 'summary'):\n            if epoch + 1 == 1 or (epoch + 1) % tensorboard_epoch_freq == 0:\n                for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n                    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n                    feed_dict = {x: X_train_a, y_: y_train_a}\n                    feed_dict.update(dp_dict)\n                    result = sess.run(merged, feed_dict=feed_dict)\n                    train_writer.add_summary(result, tensorboard_train_index)\n                    tensorboard_train_index += 1\n                if (X_val is not None) and (y_val is not None):\n                    for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n                        dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n                        feed_dict = {x: X_val_a, y_: y_val_a}\n                        feed_dict.update(dp_dict)\n                        result = sess.run(merged, feed_dict=feed_dict)\n                        val_writer.add_summary(result, tensorboard_val_index)\n                        tensorboard_val_index += 1\n\n        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n            if (X_val is not None) and (y_val is not None):\n                tl.logging.info(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n                if eval_train is True:\n                    train_loss, train_acc, n_batch = 0, 0, 0\n                    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n                        dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n                        feed_dict = {x: X_train_a, y_: y_train_a}\n                        feed_dict.update(dp_dict)\n                        if acc is not None:\n                            err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n                            train_acc += ac\n                        else:\n                            err = sess.run(cost, feed_dict=feed_dict)\n                        train_loss += err\n                        n_batch += 1\n                    tl.logging.info(\"   train loss: %f\" % (train_loss / n_batch))\n                    if acc is not None:\n                        tl.logging.info(\"   train acc: %f\" % (train_acc / n_batch))\n                val_loss, val_acc, n_batch = 0, 0, 0\n                for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n                    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n                    feed_dict = {x: X_val_a, y_: y_val_a}\n                    feed_dict.update(dp_dict)\n                    if acc is not None:\n                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n                        val_acc += ac\n                    else:\n                        err = sess.run(cost, feed_dict=feed_dict)\n                    val_loss += err\n                    n_batch += 1\n\n                tl.logging.info(\"   val loss: %f\" % (val_loss / n_batch))\n\n                if acc is not None:\n                    tl.logging.info(\"   val acc: %f\" % (val_acc / n_batch))\n            else:\n                tl.logging.info(\n                    \"Epoch %d of %d took %fs, loss %f\" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep)\n                )\n    tl.logging.info(\"Total training time: %fs\" % (time.time() - start_time_begin))",
    "doc": "Training a given non time-series network by the given cost function, training data, batch_size, n_epoch etc.\n\n    - MNIST example click `here <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_.\n    - In order to control the training details, the authors HIGHLY recommend ``tl.iterate`` see two MNIST examples `1 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_, `2 <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py>`_.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    network : TensorLayer layer\n        the network to be trained.\n    train_op : TensorFlow optimizer\n        The optimizer for training e.g. tf.train.AdamOptimizer.\n    X_train : numpy.array\n        The input of training data\n    y_train : numpy.array\n        The target of training data\n    x : placeholder\n        For inputs.\n    y_ : placeholder\n        For targets.\n    acc : TensorFlow expression or None\n        Metric for accuracy or others. If None, would not print the information.\n    batch_size : int\n        The batch size for training and evaluating.\n    n_epoch : int\n        The number of training epochs.\n    print_freq : int\n        Print the training information every ``print_freq`` epochs.\n    X_val : numpy.array or None\n        The input of validation data. If None, would not perform validation.\n    y_val : numpy.array or None\n        The target of validation data. If None, would not perform validation.\n    eval_train : boolean\n        Whether to evaluate the model during training.\n        If X_val and y_val are not None, it reflects whether to evaluate the model on training data.\n    tensorboard_dir : string\n        path to log dir, if set, summary data will be stored to the tensorboard_dir/ directory for visualization with tensorboard. (default None)\n        Also runs `tl.layers.initialize_global_variables(sess)` internally in fit() to setup the summary nodes.\n    tensorboard_epoch_freq : int\n        How many epochs between storing tensorboard checkpoint for visualization to log/ directory (default 5).\n    tensorboard_weight_histograms : boolean\n        If True updates tensorboard data in the logs/ directory for visualization\n        of the weight histograms every tensorboard_epoch_freq epoch (default True).\n    tensorboard_graph_vis : boolean\n        If True stores the graph in the tensorboard summaries saved to log/ (default True).\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,\n    ...            acc=acc, batch_size=500, n_epoch=200, print_freq=5,\n    ...            X_val=X_val, y_val=y_val, eval_train=False)\n    >>> tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,\n    ...            acc=acc, batch_size=500, n_epoch=200, print_freq=5,\n    ...            X_val=X_val, y_val=y_val, eval_train=False,\n    ...            tensorboard=True, tensorboard_weight_histograms=True, tensorboard_graph_vis=True)\n\n    Notes\n    --------\n    If tensorboard_dir not None, the `global_variables_initializer` will be run inside the fit function\n    in order to initialize the automatically generated summary nodes used for tensorboard visualization,\n    thus `tf.global_variables_initializer().run()` before the `fit()` call will be undefined."
  },
  {
    "code": "def predict(sess, network, X, x, y_op, batch_size=None):\n    \"\"\"\n    Return the predict results of given non time-series network.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    network : TensorLayer layer\n        The network.\n    X : numpy.array\n        The inputs.\n    x : placeholder\n        For inputs.\n    y_op : placeholder\n        The argmax expression of softmax outputs.\n    batch_size : int or None\n        The batch size for prediction, when dataset is large, we should use minibatche for prediction;\n        if dataset is small, we can set it to None.\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> y = network.outputs\n    >>> y_op = tf.argmax(tf.nn.softmax(y), 1)\n    >>> print(tl.utils.predict(sess, network, X_test, x, y_op))\n\n    \"\"\"\n    if batch_size is None:\n        dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n        feed_dict = {\n            x: X,\n        }\n        feed_dict.update(dp_dict)\n        return sess.run(y_op, feed_dict=feed_dict)\n    else:\n        result = None\n        for X_a, _ in tl.iterate.minibatches(X, X, batch_size, shuffle=False):\n            dp_dict = dict_to_one(network.all_drop)\n            feed_dict = {\n                x: X_a,\n            }\n            feed_dict.update(dp_dict)\n            result_a = sess.run(y_op, feed_dict=feed_dict)\n            if result is None:\n                result = result_a\n            else:\n                result = np.concatenate((result, result_a))\n        if result is None:\n            if len(X) % batch_size != 0:\n                dp_dict = dict_to_one(network.all_drop)\n                feed_dict = {\n                    x: X[-(len(X) % batch_size):, :],\n                }\n                feed_dict.update(dp_dict)\n                result_a = sess.run(y_op, feed_dict=feed_dict)\n                result = result_a\n        else:\n            if len(X) != len(result) and len(X) % batch_size != 0:\n                dp_dict = dict_to_one(network.all_drop)\n                feed_dict = {\n                    x: X[-(len(X) % batch_size):, :],\n                }\n                feed_dict.update(dp_dict)\n                result_a = sess.run(y_op, feed_dict=feed_dict)\n                result = np.concatenate((result, result_a))\n        return result",
    "doc": "Return the predict results of given non time-series network.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    network : TensorLayer layer\n        The network.\n    X : numpy.array\n        The inputs.\n    x : placeholder\n        For inputs.\n    y_op : placeholder\n        The argmax expression of softmax outputs.\n    batch_size : int or None\n        The batch size for prediction, when dataset is large, we should use minibatche for prediction;\n        if dataset is small, we can set it to None.\n\n    Examples\n    --------\n    See `tutorial_mnist_simple.py <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mnist_simple.py>`_\n\n    >>> y = network.outputs\n    >>> y_op = tf.argmax(tf.nn.softmax(y), 1)\n    >>> print(tl.utils.predict(sess, network, X_test, x, y_op))"
  },
  {
    "code": "def evaluation(y_test=None, y_predict=None, n_classes=None):\n    \"\"\"\n    Input the predicted results, targets results and\n    the number of class, return the confusion matrix, F1-score of each class,\n    accuracy and macro F1-score.\n\n    Parameters\n    ----------\n    y_test : list\n        The target results\n    y_predict : list\n        The predicted results\n    n_classes : int\n        The number of classes\n\n    Examples\n    --------\n    >>> c_mat, f1, acc, f1_macro = tl.utils.evaluation(y_test, y_predict, n_classes)\n\n    \"\"\"\n    c_mat = confusion_matrix(y_test, y_predict, labels=[x for x in range(n_classes)])\n    f1 = f1_score(y_test, y_predict, average=None, labels=[x for x in range(n_classes)])\n    f1_macro = f1_score(y_test, y_predict, average='macro')\n    acc = accuracy_score(y_test, y_predict)\n    tl.logging.info('confusion matrix: \\n%s' % c_mat)\n    tl.logging.info('f1-score        : %s' % f1)\n    tl.logging.info('f1-score(macro) : %f' % f1_macro)  # same output with > f1_score(y_true, y_pred, average='macro')\n    tl.logging.info('accuracy-score  : %f' % acc)\n    return c_mat, f1, acc, f1_macro",
    "doc": "Input the predicted results, targets results and\n    the number of class, return the confusion matrix, F1-score of each class,\n    accuracy and macro F1-score.\n\n    Parameters\n    ----------\n    y_test : list\n        The target results\n    y_predict : list\n        The predicted results\n    n_classes : int\n        The number of classes\n\n    Examples\n    --------\n    >>> c_mat, f1, acc, f1_macro = tl.utils.evaluation(y_test, y_predict, n_classes)"
  },
  {
    "code": "def class_balancing_oversample(X_train=None, y_train=None, printable=True):\n    \"\"\"Input the features and labels, return the features and labels after oversampling.\n\n    Parameters\n    ----------\n    X_train : numpy.array\n        The inputs.\n    y_train : numpy.array\n        The targets.\n\n    Examples\n    --------\n    One X\n\n    >>> X_train, y_train = class_balancing_oversample(X_train, y_train, printable=True)\n\n    Two X\n\n    >>> X, y = tl.utils.class_balancing_oversample(X_train=np.hstack((X1, X2)), y_train=y, printable=False)\n    >>> X1 = X[:, 0:5]\n    >>> X2 = X[:, 5:]\n\n    \"\"\"\n    # ======== Classes balancing\n    if printable:\n        tl.logging.info(\"Classes balancing for training examples...\")\n\n    c = Counter(y_train)\n\n    if printable:\n        tl.logging.info('the occurrence number of each stage: %s' % c.most_common())\n        tl.logging.info('the least stage is Label %s have %s instances' % c.most_common()[-1])\n        tl.logging.info('the most stage is  Label %s have %s instances' % c.most_common(1)[0])\n\n    most_num = c.most_common(1)[0][1]\n\n    if printable:\n        tl.logging.info('most num is %d, all classes tend to be this num' % most_num)\n\n    locations = {}\n    number = {}\n\n    for lab, num in c.most_common():  # find the index from y_train\n        number[lab] = num\n        locations[lab] = np.where(np.array(y_train) == lab)[0]\n    if printable:\n        tl.logging.info('convert list(np.array) to dict format')\n    X = {}  # convert list to dict\n    for lab, num in number.items():\n        X[lab] = X_train[locations[lab]]\n\n    # oversampling\n    if printable:\n        tl.logging.info('start oversampling')\n    for key in X:\n        temp = X[key]\n        while True:\n            if len(X[key]) >= most_num:\n                break\n            X[key] = np.vstack((X[key], temp))\n    if printable:\n        tl.logging.info('first features of label 0 > %d' % len(X[0][0]))\n        tl.logging.info('the occurrence num of each stage after oversampling')\n    for key in X:\n        tl.logging.info(\"%s %d\" % (key, len(X[key])))\n    if printable:\n        tl.logging.info('make each stage have same num of instances')\n    for key in X:\n        X[key] = X[key][0:most_num, :]\n        tl.logging.info(\"%s %d\" % (key, len(X[key])))\n\n    # convert dict to list\n    if printable:\n        tl.logging.info('convert from dict to list format')\n    y_train = []\n    X_train = np.empty(shape=(0, len(X[0][0])))\n    for key in X:\n        X_train = np.vstack((X_train, X[key]))\n        y_train.extend([key for i in range(len(X[key]))])\n    # tl.logging.info(len(X_train), len(y_train))\n    c = Counter(y_train)\n    if printable:\n        tl.logging.info('the occurrence number of each stage after oversampling: %s' % c.most_common())\n    # ================ End of Classes balancing\n    return X_train, y_train",
    "doc": "Input the features and labels, return the features and labels after oversampling.\n\n    Parameters\n    ----------\n    X_train : numpy.array\n        The inputs.\n    y_train : numpy.array\n        The targets.\n\n    Examples\n    --------\n    One X\n\n    >>> X_train, y_train = class_balancing_oversample(X_train, y_train, printable=True)\n\n    Two X\n\n    >>> X, y = tl.utils.class_balancing_oversample(X_train=np.hstack((X1, X2)), y_train=y, printable=False)\n    >>> X1 = X[:, 0:5]\n    >>> X2 = X[:, 5:]"
  },
  {
    "code": "def get_random_int(min_v=0, max_v=10, number=5, seed=None):\n    \"\"\"Return a list of random integer by the given range and quantity.\n\n    Parameters\n    -----------\n    min_v : number\n        The minimum value.\n    max_v : number\n        The maximum value.\n    number : int\n        Number of value.\n    seed : int or None\n        The seed for random.\n\n    Examples\n    ---------\n    >>> r = get_random_int(min_v=0, max_v=10, number=5)\n    [10, 2, 3, 3, 7]\n\n    \"\"\"\n    rnd = random.Random()\n    if seed:\n        rnd = random.Random(seed)\n    # return [random.randint(min,max) for p in range(0, number)]\n    return [rnd.randint(min_v, max_v) for p in range(0, number)]",
    "doc": "Return a list of random integer by the given range and quantity.\n\n    Parameters\n    -----------\n    min_v : number\n        The minimum value.\n    max_v : number\n        The maximum value.\n    number : int\n        Number of value.\n    seed : int or None\n        The seed for random.\n\n    Examples\n    ---------\n    >>> r = get_random_int(min_v=0, max_v=10, number=5)\n    [10, 2, 3, 3, 7]"
  },
  {
    "code": "def list_string_to_dict(string):\n    \"\"\"Inputs ``['a', 'b', 'c']``, returns ``{'a': 0, 'b': 1, 'c': 2}``.\"\"\"\n    dictionary = {}\n    for idx, c in enumerate(string):\n        dictionary.update({c: idx})\n    return dictionary",
    "doc": "Inputs ``['a', 'b', 'c']``, returns ``{'a': 0, 'b': 1, 'c': 2}``."
  },
  {
    "code": "def exit_tensorflow(sess=None, port=6006):\n    \"\"\"Close TensorFlow session, TensorBoard and Nvidia-process if available.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    tb_port : int\n        TensorBoard port you want to close, `6006` as default.\n\n    \"\"\"\n    text = \"[TL] Close tensorboard and nvidia-process if available\"\n    text2 = \"[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on \"\n\n    if sess is not None:\n        sess.close()\n\n    if _platform == \"linux\" or _platform == \"linux2\":\n        tl.logging.info('linux: %s' % text)\n        os.system('nvidia-smi')\n        os.system('fuser ' + port + '/tcp -k')  # kill tensorboard 6006\n        os.system(\"nvidia-smi | grep python |awk '{print $3}'|xargs kill\")  # kill all nvidia-smi python process\n        _exit()\n\n    elif _platform == \"darwin\":\n        tl.logging.info('OS X: %s' % text)\n        subprocess.Popen(\n            \"lsof -i tcp:\" + str(port) + \"  | grep -v PID | awk '{print $2}' | xargs kill\", shell=True\n        )  # kill tensorboard\n    elif _platform == \"win32\":\n        raise NotImplementedError(\"this function is not supported on the Windows platform\")\n\n    else:\n        tl.logging.info(text2 + _platform)",
    "doc": "Close TensorFlow session, TensorBoard and Nvidia-process if available.\n\n    Parameters\n    ----------\n    sess : Session\n        TensorFlow Session.\n    tb_port : int\n        TensorBoard port you want to close, `6006` as default."
  },
  {
    "code": "def open_tensorboard(log_dir='/tmp/tensorflow', port=6006):\n    \"\"\"Open Tensorboard.\n\n    Parameters\n    ----------\n    log_dir : str\n        Directory where your tensorboard logs are saved\n    port : int\n        TensorBoard port you want to open, 6006 is tensorboard default\n\n    \"\"\"\n    text = \"[TL] Open tensorboard, go to localhost:\" + str(port) + \" to access\"\n    text2 = \" not yet supported by this function (tl.ops.open_tb)\"\n\n    if not tl.files.exists_or_mkdir(log_dir, verbose=False):\n        tl.logging.info(\"[TL] Log reportory was created at %s\" % log_dir)\n\n    if _platform == \"linux\" or _platform == \"linux2\":\n        raise NotImplementedError()\n    elif _platform == \"darwin\":\n        tl.logging.info('OS X: %s' % text)\n        subprocess.Popen(\n            sys.prefix + \" | python -m tensorflow.tensorboard --logdir=\" + log_dir + \" --port=\" + str(port), shell=True\n        )  # open tensorboard in localhost:6006/ or whatever port you chose\n    elif _platform == \"win32\":\n        raise NotImplementedError(\"this function is not supported on the Windows platform\")\n    else:\n        tl.logging.info(_platform + text2)",
    "doc": "Open Tensorboard.\n\n    Parameters\n    ----------\n    log_dir : str\n        Directory where your tensorboard logs are saved\n    port : int\n        TensorBoard port you want to open, 6006 is tensorboard default"
  },
  {
    "code": "def clear_all_placeholder_variables(printable=True):\n    \"\"\"Clears all the placeholder variables of keep prob,\n    including keeping probabilities of all dropout, denoising, dropconnect etc.\n\n    Parameters\n    ----------\n    printable : boolean\n        If True, print all deleted variables.\n\n    \"\"\"\n    tl.logging.info('clear all .....................................')\n    gl = globals().copy()\n    for var in gl:\n        if var[0] == '_': continue\n        if 'func' in str(globals()[var]): continue\n        if 'module' in str(globals()[var]): continue\n        if 'class' in str(globals()[var]): continue\n\n        if printable:\n            tl.logging.info(\" clear_all ------- %s\" % str(globals()[var]))\n\n        del globals()[var]",
    "doc": "Clears all the placeholder variables of keep prob,\n    including keeping probabilities of all dropout, denoising, dropconnect etc.\n\n    Parameters\n    ----------\n    printable : boolean\n        If True, print all deleted variables."
  },
  {
    "code": "def set_gpu_fraction(gpu_fraction=0.3):\n    \"\"\"Set the GPU memory fraction for the application.\n\n    Parameters\n    ----------\n    gpu_fraction : float\n        Fraction of GPU memory, (0 ~ 1]\n\n    References\n    ----------\n    - `TensorFlow using GPU <https://www.tensorflow.org/versions/r0.9/how_tos/using_gpu/index.html>`__\n\n    \"\"\"\n    tl.logging.info(\"[TL]: GPU MEM Fraction %f\" % gpu_fraction)\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n    return sess",
    "doc": "Set the GPU memory fraction for the application.\n\n    Parameters\n    ----------\n    gpu_fraction : float\n        Fraction of GPU memory, (0 ~ 1]\n\n    References\n    ----------\n    - `TensorFlow using GPU <https://www.tensorflow.org/versions/r0.9/how_tos/using_gpu/index.html>`__"
  },
  {
    "code": "def generate_skip_gram_batch(data, batch_size, num_skips, skip_window, data_index=0):\n    \"\"\"Generate a training batch for the Skip-Gram model.\n\n    See `Word2Vec example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_word2vec_basic.py>`__.\n\n    Parameters\n    ----------\n    data : list of data\n        To present context, usually a list of integers.\n    batch_size : int\n        Batch size to return.\n    num_skips : int\n        How many times to reuse an input to generate a label.\n    skip_window : int\n        How many words to consider left and right.\n    data_index : int\n        Index of the context location. This code use `data_index` to instead of yield like ``tl.iterate``.\n\n    Returns\n    -------\n    batch : list of data\n        Inputs.\n    labels : list of data\n        Labels\n    data_index : int\n        Index of the context location.\n\n    Examples\n    --------\n    Setting num_skips=2, skip_window=1, use the right and left words.\n    In the same way, num_skips=4, skip_window=2 means use the nearby 4 words.\n\n    >>> data = [1,2,3,4,5,6,7,8,9,10,11]\n    >>> batch, labels, data_index = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    >>> print(batch)\n    [2 2 3 3 4 4 5 5]\n    >>> print(labels)\n    [[3]\n    [1]\n    [4]\n    [2]\n    [5]\n    [3]\n    [4]\n    [6]]\n\n    \"\"\"\n    # global data_index   # you can put data_index outside the function, then\n    #       modify the global data_index in the function without return it.\n    # note: without using yield, this code use data_index to instead.\n\n    if batch_size % num_skips != 0:\n        raise Exception(\"batch_size should be able to be divided by num_skips.\")\n    if num_skips > 2 * skip_window:\n        raise Exception(\"num_skips <= 2 * skip_window\")\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    return batch, labels, data_index",
    "doc": "Generate a training batch for the Skip-Gram model.\n\n    See `Word2Vec example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_word2vec_basic.py>`__.\n\n    Parameters\n    ----------\n    data : list of data\n        To present context, usually a list of integers.\n    batch_size : int\n        Batch size to return.\n    num_skips : int\n        How many times to reuse an input to generate a label.\n    skip_window : int\n        How many words to consider left and right.\n    data_index : int\n        Index of the context location. This code use `data_index` to instead of yield like ``tl.iterate``.\n\n    Returns\n    -------\n    batch : list of data\n        Inputs.\n    labels : list of data\n        Labels\n    data_index : int\n        Index of the context location.\n\n    Examples\n    --------\n    Setting num_skips=2, skip_window=1, use the right and left words.\n    In the same way, num_skips=4, skip_window=2 means use the nearby 4 words.\n\n    >>> data = [1,2,3,4,5,6,7,8,9,10,11]\n    >>> batch, labels, data_index = tl.nlp.generate_skip_gram_batch(data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)\n    >>> print(batch)\n    [2 2 3 3 4 4 5 5]\n    >>> print(labels)\n    [[3]\n    [1]\n    [4]\n    [2]\n    [5]\n    [3]\n    [4]\n    [6]]"
  },
  {
    "code": "def sample(a=None, temperature=1.0):\n    \"\"\"Sample an index from a probability array.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    temperature : float or None\n        The higher the more uniform. When a = [0.1, 0.2, 0.7],\n            - temperature = 0.7, the distribution will be sharpen [0.05048273,  0.13588945,  0.81362782]\n            - temperature = 1.0, the distribution will be the same [0.1,    0.2,    0.7]\n            - temperature = 1.5, the distribution will be filtered [0.16008435,  0.25411807,  0.58579758]\n            - If None, it will be ``np.argmax(a)``\n\n    Notes\n    ------\n    - No matter what is the temperature and input list, the sum of all probabilities will be one. Even if input list = [1, 100, 200], the sum of all probabilities will still be one.\n    - For large vocabulary size, choice a higher temperature or ``tl.nlp.sample_top`` to avoid error.\n\n    \"\"\"\n    if a is None:\n        raise Exception(\"a : list of float\")\n    b = np.copy(a)\n    try:\n        if temperature == 1:\n            return np.argmax(np.random.multinomial(1, a, 1))\n        if temperature is None:\n            return np.argmax(a)\n        else:\n            a = np.log(a) / temperature\n            a = np.exp(a) / np.sum(np.exp(a))\n            return np.argmax(np.random.multinomial(1, a, 1))\n    except Exception:\n        # np.set_printoptions(threshold=np.nan)\n        # tl.logging.info(a)\n        # tl.logging.info(np.sum(a))\n        # tl.logging.info(np.max(a))\n        # tl.logging.info(np.min(a))\n        # exit()\n        message = \"For large vocabulary_size, choice a higher temperature\\\n         to avoid log error. Hint : use ``sample_top``. \"\n\n        warnings.warn(message, Warning)\n        # tl.logging.info(a)\n        # tl.logging.info(b)\n        return np.argmax(np.random.multinomial(1, b, 1))",
    "doc": "Sample an index from a probability array.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    temperature : float or None\n        The higher the more uniform. When a = [0.1, 0.2, 0.7],\n            - temperature = 0.7, the distribution will be sharpen [0.05048273,  0.13588945,  0.81362782]\n            - temperature = 1.0, the distribution will be the same [0.1,    0.2,    0.7]\n            - temperature = 1.5, the distribution will be filtered [0.16008435,  0.25411807,  0.58579758]\n            - If None, it will be ``np.argmax(a)``\n\n    Notes\n    ------\n    - No matter what is the temperature and input list, the sum of all probabilities will be one. Even if input list = [1, 100, 200], the sum of all probabilities will still be one.\n    - For large vocabulary size, choice a higher temperature or ``tl.nlp.sample_top`` to avoid error."
  },
  {
    "code": "def sample_top(a=None, top_k=10):\n    \"\"\"Sample from ``top_k`` probabilities.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    top_k : int\n        Number of candidates to be considered.\n\n    \"\"\"\n    if a is None:\n        a = []\n\n    idx = np.argpartition(a, -top_k)[-top_k:]\n    probs = a[idx]\n    # tl.logging.info(\"new %f\" % probs)\n    probs = probs / np.sum(probs)\n    choice = np.random.choice(idx, p=probs)\n    return choice",
    "doc": "Sample from ``top_k`` probabilities.\n\n    Parameters\n    ----------\n    a : list of float\n        List of probabilities.\n    top_k : int\n        Number of candidates to be considered."
  },
  {
    "code": "def process_sentence(sentence, start_word=\"<S>\", end_word=\"</S>\"):\n    \"\"\"Seperate a sentence string into a list of string words, add start_word and end_word,\n    see ``create_vocab()`` and ``tutorial_tfrecord3.py``.\n\n    Parameters\n    ----------\n    sentence : str\n        A sentence.\n    start_word : str or None\n        The start word. If None, no start word will be appended.\n    end_word : str or None\n        The end word. If None, no end word will be appended.\n\n    Returns\n    ---------\n    list of str\n        A list of strings that separated into words.\n\n    Examples\n    -----------\n    >>> c = \"how are you?\"\n    >>> c = tl.nlp.process_sentence(c)\n    >>> print(c)\n    ['<S>', 'how', 'are', 'you', '?', '</S>']\n\n    Notes\n    -------\n    - You have to install the following package.\n    - `Installing NLTK <http://www.nltk.org/install.html>`__\n    - `Installing NLTK data <http://www.nltk.org/data.html>`__\n\n    \"\"\"\n    if start_word is not None:\n        process_sentence = [start_word]\n    else:\n        process_sentence = []\n    process_sentence.extend(nltk.tokenize.word_tokenize(sentence.lower()))\n\n    if end_word is not None:\n        process_sentence.append(end_word)\n    return process_sentence",
    "doc": "Seperate a sentence string into a list of string words, add start_word and end_word,\n    see ``create_vocab()`` and ``tutorial_tfrecord3.py``.\n\n    Parameters\n    ----------\n    sentence : str\n        A sentence.\n    start_word : str or None\n        The start word. If None, no start word will be appended.\n    end_word : str or None\n        The end word. If None, no end word will be appended.\n\n    Returns\n    ---------\n    list of str\n        A list of strings that separated into words.\n\n    Examples\n    -----------\n    >>> c = \"how are you?\"\n    >>> c = tl.nlp.process_sentence(c)\n    >>> print(c)\n    ['<S>', 'how', 'are', 'you', '?', '</S>']\n\n    Notes\n    -------\n    - You have to install the following package.\n    - `Installing NLTK <http://www.nltk.org/install.html>`__\n    - `Installing NLTK data <http://www.nltk.org/data.html>`__"
  },
  {
    "code": "def create_vocab(sentences, word_counts_output_file, min_word_count=1):\n    \"\"\"Creates the vocabulary of word to word_id.\n\n    See ``tutorial_tfrecord3.py``.\n\n    The vocabulary is saved to disk in a text file of word counts. The id of each\n    word in the file is its corresponding 0-based line number.\n\n    Parameters\n    ------------\n    sentences : list of list of str\n        All sentences for creating the vocabulary.\n    word_counts_output_file : str\n        The file name.\n    min_word_count : int\n        Minimum number of occurrences for a word.\n\n    Returns\n    --------\n    :class:`SimpleVocabulary`\n        The simple vocabulary object, see :class:`Vocabulary` for more.\n\n    Examples\n    --------\n    Pre-process sentences\n\n    >>> captions = [\"one two , three\", \"four five five\"]\n    >>> processed_capts = []\n    >>> for c in captions:\n    >>>     c = tl.nlp.process_sentence(c, start_word=\"<S>\", end_word=\"</S>\")\n    >>>     processed_capts.append(c)\n    >>> print(processed_capts)\n    ...[['<S>', 'one', 'two', ',', 'three', '</S>'], ['<S>', 'four', 'five', 'five', '</S>']]\n\n    Create vocabulary\n\n    >>> tl.nlp.create_vocab(processed_capts, word_counts_output_file='vocab.txt', min_word_count=1)\n    Creating vocabulary.\n      Total words: 8\n      Words in vocabulary: 8\n      Wrote vocabulary file: vocab.txt\n\n    Get vocabulary object\n\n    >>> vocab = tl.nlp.Vocabulary('vocab.txt', start_word=\"<S>\", end_word=\"</S>\", unk_word=\"<UNK>\")\n    INFO:tensorflow:Initializing vocabulary from file: vocab.txt\n    [TL] Vocabulary from vocab.txt : <S> </S> <UNK>\n    vocabulary with 10 words (includes start_word, end_word, unk_word)\n        start_id: 2\n        end_id: 3\n        unk_id: 9\n        pad_id: 0\n\n    \"\"\"\n    tl.logging.info(\"Creating vocabulary.\")\n\n    counter = Counter()\n\n    for c in sentences:\n        counter.update(c)\n        # tl.logging.info('c',c)\n    tl.logging.info(\"    Total words: %d\" % len(counter))\n\n    # Filter uncommon words and sort by descending count.\n    word_counts = [x for x in counter.items() if x[1] >= min_word_count]\n    word_counts.sort(key=lambda x: x[1], reverse=True)\n    word_counts = [(\"<PAD>\", 0)] + word_counts  # 1st id should be reserved for padding\n    # tl.logging.info(word_counts)\n    tl.logging.info(\"    Words in vocabulary: %d\" % len(word_counts))\n\n    # Write out the word counts file.\n    with tf.gfile.FastGFile(word_counts_output_file, \"w\") as f:\n        f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word_counts]))\n    tl.logging.info(\"    Wrote vocabulary file: %s\" % word_counts_output_file)\n\n    # Create the vocabulary dictionary.\n    reverse_vocab = [x[0] for x in word_counts]\n    unk_id = len(reverse_vocab)\n    vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n    vocab = SimpleVocabulary(vocab_dict, unk_id)\n\n    return vocab",
    "doc": "Creates the vocabulary of word to word_id.\n\n    See ``tutorial_tfrecord3.py``.\n\n    The vocabulary is saved to disk in a text file of word counts. The id of each\n    word in the file is its corresponding 0-based line number.\n\n    Parameters\n    ------------\n    sentences : list of list of str\n        All sentences for creating the vocabulary.\n    word_counts_output_file : str\n        The file name.\n    min_word_count : int\n        Minimum number of occurrences for a word.\n\n    Returns\n    --------\n    :class:`SimpleVocabulary`\n        The simple vocabulary object, see :class:`Vocabulary` for more.\n\n    Examples\n    --------\n    Pre-process sentences\n\n    >>> captions = [\"one two , three\", \"four five five\"]\n    >>> processed_capts = []\n    >>> for c in captions:\n    >>>     c = tl.nlp.process_sentence(c, start_word=\"<S>\", end_word=\"</S>\")\n    >>>     processed_capts.append(c)\n    >>> print(processed_capts)\n    ...[['<S>', 'one', 'two', ',', 'three', '</S>'], ['<S>', 'four', 'five', 'five', '</S>']]\n\n    Create vocabulary\n\n    >>> tl.nlp.create_vocab(processed_capts, word_counts_output_file='vocab.txt', min_word_count=1)\n    Creating vocabulary.\n      Total words: 8\n      Words in vocabulary: 8\n      Wrote vocabulary file: vocab.txt\n\n    Get vocabulary object\n\n    >>> vocab = tl.nlp.Vocabulary('vocab.txt', start_word=\"<S>\", end_word=\"</S>\", unk_word=\"<UNK>\")\n    INFO:tensorflow:Initializing vocabulary from file: vocab.txt\n    [TL] Vocabulary from vocab.txt : <S> </S> <UNK>\n    vocabulary with 10 words (includes start_word, end_word, unk_word)\n        start_id: 2\n        end_id: 3\n        unk_id: 9\n        pad_id: 0"
  },
  {
    "code": "def read_words(filename=\"nietzsche.txt\", replace=None):\n    \"\"\"Read list format context from a file.\n\n    For customized read_words method, see ``tutorial_generate_text.py``.\n\n    Parameters\n    ----------\n    filename : str\n        a file path.\n    replace : list of str\n        replace original string by target string.\n\n    Returns\n    -------\n    list of str\n        The context in a list (split using space).\n    \"\"\"\n    if replace is None:\n        replace = ['\\n', '<eos>']\n\n    with tf.gfile.GFile(filename, \"r\") as f:\n        try:  # python 3.4 or older\n            context_list = f.read().replace(*replace).split()\n        except Exception:  # python 3.5\n            f.seek(0)\n            replace = [x.encode('utf-8') for x in replace]\n            context_list = f.read().replace(*replace).split()\n        return context_list",
    "doc": "Read list format context from a file.\n\n    For customized read_words method, see ``tutorial_generate_text.py``.\n\n    Parameters\n    ----------\n    filename : str\n        a file path.\n    replace : list of str\n        replace original string by target string.\n\n    Returns\n    -------\n    list of str\n        The context in a list (split using space)."
  },
  {
    "code": "def read_analogies_file(eval_file='questions-words.txt', word2id=None):\n    \"\"\"Reads through an analogy question file, return its id format.\n\n    Parameters\n    ----------\n    eval_file : str\n        The file name.\n    word2id : dictionary\n        a dictionary that maps word to ID.\n\n    Returns\n    --------\n    numpy.array\n        A ``[n_examples, 4]`` numpy array containing the analogy question's word IDs.\n\n    Examples\n    ---------\n    The file should be in this format\n\n    >>> : capital-common-countries\n    >>> Athens Greece Baghdad Iraq\n    >>> Athens Greece Bangkok Thailand\n    >>> Athens Greece Beijing China\n    >>> Athens Greece Berlin Germany\n    >>> Athens Greece Bern Switzerland\n    >>> Athens Greece Cairo Egypt\n    >>> Athens Greece Canberra Australia\n    >>> Athens Greece Hanoi Vietnam\n    >>> Athens Greece Havana Cuba\n\n    Get the tokenized analogy question data\n\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    >>> print(analogy_questions)\n    [[ 3068  1248  7161  1581]\n    [ 3068  1248 28683  5642]\n    [ 3068  1248  3878   486]\n    ...,\n    [ 1216  4309 19982 25506]\n    [ 1216  4309  3194  8650]\n    [ 1216  4309   140   312]]\n\n    \"\"\"\n    if word2id is None:\n        word2id = {}\n\n    questions = []\n    questions_skipped = 0\n    with open(eval_file, \"rb\") as analogy_f:\n        for line in analogy_f:\n            if line.startswith(b\":\"):  # Skip comments.\n                continue\n            words = line.strip().lower().split(b\" \")  # lowercase\n            ids = [word2id.get(w.strip()) for w in words]\n            if None in ids or len(ids) != 4:\n                questions_skipped += 1\n            else:\n                questions.append(np.array(ids))\n    tl.logging.info(\"Eval analogy file: %s\" % eval_file)\n    tl.logging.info(\"Questions: %d\", len(questions))\n    tl.logging.info(\"Skipped: %d\", questions_skipped)\n    analogy_questions = np.array(questions, dtype=np.int32)\n    return analogy_questions",
    "doc": "Reads through an analogy question file, return its id format.\n\n    Parameters\n    ----------\n    eval_file : str\n        The file name.\n    word2id : dictionary\n        a dictionary that maps word to ID.\n\n    Returns\n    --------\n    numpy.array\n        A ``[n_examples, 4]`` numpy array containing the analogy question's word IDs.\n\n    Examples\n    ---------\n    The file should be in this format\n\n    >>> : capital-common-countries\n    >>> Athens Greece Baghdad Iraq\n    >>> Athens Greece Bangkok Thailand\n    >>> Athens Greece Beijing China\n    >>> Athens Greece Berlin Germany\n    >>> Athens Greece Bern Switzerland\n    >>> Athens Greece Cairo Egypt\n    >>> Athens Greece Canberra Australia\n    >>> Athens Greece Hanoi Vietnam\n    >>> Athens Greece Havana Cuba\n\n    Get the tokenized analogy question data\n\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> analogy_questions = tl.nlp.read_analogies_file(eval_file='questions-words.txt', word2id=dictionary)\n    >>> print(analogy_questions)\n    [[ 3068  1248  7161  1581]\n    [ 3068  1248 28683  5642]\n    [ 3068  1248  3878   486]\n    ...,\n    [ 1216  4309 19982 25506]\n    [ 1216  4309  3194  8650]\n    [ 1216  4309   140   312]]"
  },
  {
    "code": "def build_reverse_dictionary(word_to_id):\n    \"\"\"Given a dictionary that maps word to integer id.\n    Returns a reverse dictionary that maps a id to word.\n\n    Parameters\n    ----------\n    word_to_id : dictionary\n        that maps word to ID.\n\n    Returns\n    --------\n    dictionary\n        A dictionary that maps IDs to words.\n\n    \"\"\"\n    reverse_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n    return reverse_dictionary",
    "doc": "Given a dictionary that maps word to integer id.\n    Returns a reverse dictionary that maps a id to word.\n\n    Parameters\n    ----------\n    word_to_id : dictionary\n        that maps word to ID.\n\n    Returns\n    --------\n    dictionary\n        A dictionary that maps IDs to words."
  },
  {
    "code": "def build_words_dataset(words=None, vocabulary_size=50000, printable=True, unk_key='UNK'):\n    \"\"\"Build the words dictionary and replace rare words with 'UNK' token.\n    The most common word has the smallest integer id.\n\n    Parameters\n    ----------\n    words : list of str or byte\n        The context in list format. You may need to do preprocessing on the words, such as lower case, remove marks etc.\n    vocabulary_size : int\n        The maximum vocabulary size, limiting the vocabulary size. Then the script replaces rare words with 'UNK' token.\n    printable : boolean\n        Whether to print the read vocabulary size of the given words.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    data : list of int\n        The context in a list of ID.\n    count : list of tuple and list\n        Pair words and IDs.\n            - count[0] is a list : the number of rare words\n            - count[1:] are tuples : the number of occurrence of each word\n            - e.g. [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\n    dictionary : dictionary\n        It is `word_to_id` that maps word to ID.\n    reverse_dictionary : a dictionary\n        It is `id_to_word` that maps ID to word.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size)\n\n    References\n    -----------------\n    - `tensorflow/examples/tutorials/word2vec/word2vec_basic.py <https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/word2vec/word2vec_basic.py>`__\n\n    \"\"\"\n    if words is None:\n        raise Exception(\"words : list of str or byte\")\n\n    count = [[unk_key, -1]]\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    if printable:\n        tl.logging.info('Real vocabulary size    %d' % len(collections.Counter(words).keys()))\n        tl.logging.info('Limited vocabulary size {}'.format(vocabulary_size))\n    if len(collections.Counter(words).keys()) < vocabulary_size:\n        raise Exception(\n            \"len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size\"\n        )\n    return data, count, dictionary, reverse_dictionary",
    "doc": "Build the words dictionary and replace rare words with 'UNK' token.\n    The most common word has the smallest integer id.\n\n    Parameters\n    ----------\n    words : list of str or byte\n        The context in list format. You may need to do preprocessing on the words, such as lower case, remove marks etc.\n    vocabulary_size : int\n        The maximum vocabulary size, limiting the vocabulary size. Then the script replaces rare words with 'UNK' token.\n    printable : boolean\n        Whether to print the read vocabulary size of the given words.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    data : list of int\n        The context in a list of ID.\n    count : list of tuple and list\n        Pair words and IDs.\n            - count[0] is a list : the number of rare words\n            - count[1:] are tuples : the number of occurrence of each word\n            - e.g. [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\n    dictionary : dictionary\n        It is `word_to_id` that maps word to ID.\n    reverse_dictionary : a dictionary\n        It is `id_to_word` that maps ID to word.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size)\n\n    References\n    -----------------\n    - `tensorflow/examples/tutorials/word2vec/word2vec_basic.py <https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/word2vec/word2vec_basic.py>`__"
  },
  {
    "code": "def words_to_word_ids(data=None, word_to_id=None, unk_key='UNK'):\n    \"\"\"Convert a list of string (words) to IDs.\n\n    Parameters\n    ----------\n    data : list of string or byte\n        The context in list format\n    word_to_id : a dictionary\n        that maps word to ID.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    list of int\n        A list of IDs to represent the context.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> context = [b'hello', b'how', b'are', b'you']\n    >>> ids = tl.nlp.words_to_word_ids(words, dictionary)\n    >>> context = tl.nlp.word_ids_to_words(ids, reverse_dictionary)\n    >>> print(ids)\n    [6434, 311, 26, 207]\n    >>> print(context)\n    [b'hello', b'how', b'are', b'you']\n\n    References\n    ---------------\n    - `tensorflow.models.rnn.ptb.reader <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb>`__\n\n    \"\"\"\n    if data is None:\n        raise Exception(\"data : list of string or byte\")\n    if word_to_id is None:\n        raise Exception(\"word_to_id : a dictionary\")\n    # if isinstance(data[0], six.string_types):\n    #     tl.logging.info(type(data[0]))\n    #     # exit()\n    #     tl.logging.info(data[0])\n    #     tl.logging.info(word_to_id)\n    #     return [word_to_id[str(word)] for word in data]\n    # else:\n\n    word_ids = []\n    for word in data:\n        if word_to_id.get(word) is not None:\n            word_ids.append(word_to_id[word])\n        else:\n            word_ids.append(word_to_id[unk_key])\n    return word_ids",
    "doc": "Convert a list of string (words) to IDs.\n\n    Parameters\n    ----------\n    data : list of string or byte\n        The context in list format\n    word_to_id : a dictionary\n        that maps word to ID.\n    unk_key : str\n        Represent the unknown words.\n\n    Returns\n    --------\n    list of int\n        A list of IDs to represent the context.\n\n    Examples\n    --------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> context = [b'hello', b'how', b'are', b'you']\n    >>> ids = tl.nlp.words_to_word_ids(words, dictionary)\n    >>> context = tl.nlp.word_ids_to_words(ids, reverse_dictionary)\n    >>> print(ids)\n    [6434, 311, 26, 207]\n    >>> print(context)\n    [b'hello', b'how', b'are', b'you']\n\n    References\n    ---------------\n    - `tensorflow.models.rnn.ptb.reader <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb>`__"
  },
  {
    "code": "def save_vocab(count=None, name='vocab.txt'):\n    \"\"\"Save the vocabulary to a file so the model can be reloaded.\n\n    Parameters\n    ----------\n    count : a list of tuple and list\n        count[0] is a list : the number of rare words,\n        count[1:] are tuples : the number of occurrence of each word,\n        e.g. [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\n\n    Examples\n    ---------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    >>> vocab_text8.txt\n    UNK 418391\n    the 1061396\n    of 593677\n    and 416629\n    one 411764\n    in 372201\n    a 325873\n    to 316376\n\n    \"\"\"\n    if count is None:\n        count = []\n\n    pwd = os.getcwd()\n    vocabulary_size = len(count)\n    with open(os.path.join(pwd, name), \"w\") as f:\n        for i in xrange(vocabulary_size):\n            f.write(\"%s %d\\n\" % (tf.compat.as_text(count[i][0]), count[i][1]))\n    tl.logging.info(\"%d vocab saved to %s in %s\" % (vocabulary_size, name, pwd))",
    "doc": "Save the vocabulary to a file so the model can be reloaded.\n\n    Parameters\n    ----------\n    count : a list of tuple and list\n        count[0] is a list : the number of rare words,\n        count[1:] are tuples : the number of occurrence of each word,\n        e.g. [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\n\n    Examples\n    ---------\n    >>> words = tl.files.load_matt_mahoney_text8_dataset()\n    >>> vocabulary_size = 50000\n    >>> data, count, dictionary, reverse_dictionary = tl.nlp.build_words_dataset(words, vocabulary_size, True)\n    >>> tl.nlp.save_vocab(count, name='vocab_text8.txt')\n    >>> vocab_text8.txt\n    UNK 418391\n    the 1061396\n    of 593677\n    and 416629\n    one 411764\n    in 372201\n    a 325873\n    to 316376"
  },
  {
    "code": "def basic_tokenizer(sentence, _WORD_SPLIT=re.compile(b\"([.,!?\\\"':;)(])\")):\n    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n    _WORD_SPLIT : regular expression for word spliting.\n\n\n    Examples\n    --------\n    >>> see create_vocabulary\n    >>> from tensorflow.python.platform import gfile\n    >>> train_path = \"wmt/giga-fren.release2\"\n    >>> with gfile.GFile(train_path + \".en\", mode=\"rb\") as f:\n    >>>    for line in f:\n    >>>       tokens = tl.nlp.basic_tokenizer(line)\n    >>>       tl.logging.info(tokens)\n    >>>       exit()\n    [b'Changing', b'Lives', b'|', b'Changing', b'Society', b'|', b'How',\n      b'It', b'Works', b'|', b'Technology', b'Drives', b'Change', b'Home',\n      b'|', b'Concepts', b'|', b'Teachers', b'|', b'Search', b'|', b'Overview',\n      b'|', b'Credits', b'|', b'HHCC', b'Web', b'|', b'Reference', b'|',\n      b'Feedback', b'Virtual', b'Museum', b'of', b'Canada', b'Home', b'Page']\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    \"\"\"\n    words = []\n    sentence = tf.compat.as_bytes(sentence)\n    for space_separated_fragment in sentence.strip().split():\n        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n    return [w for w in words if w]",
    "doc": "Very basic tokenizer: split the sentence into a list of tokens.\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n    _WORD_SPLIT : regular expression for word spliting.\n\n\n    Examples\n    --------\n    >>> see create_vocabulary\n    >>> from tensorflow.python.platform import gfile\n    >>> train_path = \"wmt/giga-fren.release2\"\n    >>> with gfile.GFile(train_path + \".en\", mode=\"rb\") as f:\n    >>>    for line in f:\n    >>>       tokens = tl.nlp.basic_tokenizer(line)\n    >>>       tl.logging.info(tokens)\n    >>>       exit()\n    [b'Changing', b'Lives', b'|', b'Changing', b'Society', b'|', b'How',\n      b'It', b'Works', b'|', b'Technology', b'Drives', b'Change', b'Home',\n      b'|', b'Concepts', b'|', b'Teachers', b'|', b'Search', b'|', b'Overview',\n      b'|', b'Credits', b'|', b'HHCC', b'Web', b'|', b'Reference', b'|',\n      b'Feedback', b'Virtual', b'Museum', b'of', b'Canada', b'Home', b'Page']\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``"
  },
  {
    "code": "def create_vocabulary(\n        vocabulary_path, data_path, max_vocabulary_size, tokenizer=None, normalize_digits=True,\n        _DIGIT_RE=re.compile(br\"\\d\"), _START_VOCAB=None\n):\n    r\"\"\"Create vocabulary file (if it does not exist yet) from data file.\n\n    Data file is assumed to contain one sentence per line. Each sentence is\n    tokenized and digits are normalized (if normalize_digits is set).\n    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n    We write it to vocabulary_path in a one-token-per-line format, so that later\n    token in the first line gets id=0, second line gets id=1, and so on.\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path where the vocabulary will be created.\n    data_path : str\n        Data file that will be used to create vocabulary.\n    max_vocabulary_size : int\n        Limit on the size of the created vocabulary.\n    tokenizer : function\n        A function to use to tokenize each data sentence. If None, basic_tokenizer will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by `0`.\n    _DIGIT_RE : regular expression function\n        Default is ``re.compile(br\"\\d\")``.\n    _START_VOCAB : list of str\n        The pad, go, eos and unk token, default is ``[b\"_PAD\", b\"_GO\", b\"_EOS\", b\"_UNK\"]``.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    \"\"\"\n    if _START_VOCAB is None:\n        _START_VOCAB = [b\"_PAD\", b\"_GO\", b\"_EOS\", b\"_UNK\"]\n    if not gfile.Exists(vocabulary_path):\n        tl.logging.info(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n        vocab = {}\n        with gfile.GFile(data_path, mode=\"rb\") as f:\n            counter = 0\n            for line in f:\n                counter += 1\n                if counter % 100000 == 0:\n                    tl.logging.info(\"  processing line %d\" % counter)\n                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n                for w in tokens:\n                    word = re.sub(_DIGIT_RE, b\"0\", w) if normalize_digits else w\n                    if word in vocab:\n                        vocab[word] += 1\n                    else:\n                        vocab[word] = 1\n            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n            if len(vocab_list) > max_vocabulary_size:\n                vocab_list = vocab_list[:max_vocabulary_size]\n            with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n                for w in vocab_list:\n                    vocab_file.write(w + b\"\\n\")\n    else:\n        tl.logging.info(\"Vocabulary %s from data %s exists\" % (vocabulary_path, data_path))",
    "doc": "r\"\"\"Create vocabulary file (if it does not exist yet) from data file.\n\n    Data file is assumed to contain one sentence per line. Each sentence is\n    tokenized and digits are normalized (if normalize_digits is set).\n    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n    We write it to vocabulary_path in a one-token-per-line format, so that later\n    token in the first line gets id=0, second line gets id=1, and so on.\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path where the vocabulary will be created.\n    data_path : str\n        Data file that will be used to create vocabulary.\n    max_vocabulary_size : int\n        Limit on the size of the created vocabulary.\n    tokenizer : function\n        A function to use to tokenize each data sentence. If None, basic_tokenizer will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by `0`.\n    _DIGIT_RE : regular expression function\n        Default is ``re.compile(br\"\\d\")``.\n    _START_VOCAB : list of str\n        The pad, go, eos and unk token, default is ``[b\"_PAD\", b\"_GO\", b\"_EOS\", b\"_UNK\"]``.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``"
  },
  {
    "code": "def initialize_vocabulary(vocabulary_path):\n    \"\"\"Initialize vocabulary from file, return the `word_to_id` (dictionary)\n    and `id_to_word` (list).\n\n    We assume the vocabulary is stored one-item-per-line, so a file will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will also return the reversed-vocabulary [\"dog\", \"cat\"].\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path to the file containing the vocabulary.\n\n    Returns\n    --------\n    vocab : dictionary\n        a dictionary that maps word to ID.\n    rev_vocab : list of int\n        a list that maps ID to word.\n\n    Examples\n    ---------\n    >>> Assume 'test' contains\n    dog\n    cat\n    bird\n    >>> vocab, rev_vocab = tl.nlp.initialize_vocabulary(\"test\")\n    >>> print(vocab)\n    >>> {b'cat': 1, b'dog': 0, b'bird': 2}\n    >>> print(rev_vocab)\n    >>> [b'dog', b'cat', b'bird']\n\n    Raises\n    -------\n    ValueError : if the provided vocabulary_path does not exist.\n\n    \"\"\"\n    if gfile.Exists(vocabulary_path):\n        rev_vocab = []\n        with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n            rev_vocab.extend(f.readlines())\n        rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n        return vocab, rev_vocab\n    else:\n        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)",
    "doc": "Initialize vocabulary from file, return the `word_to_id` (dictionary)\n    and `id_to_word` (list).\n\n    We assume the vocabulary is stored one-item-per-line, so a file will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will also return the reversed-vocabulary [\"dog\", \"cat\"].\n\n    Parameters\n    -----------\n    vocabulary_path : str\n        Path to the file containing the vocabulary.\n\n    Returns\n    --------\n    vocab : dictionary\n        a dictionary that maps word to ID.\n    rev_vocab : list of int\n        a list that maps ID to word.\n\n    Examples\n    ---------\n    >>> Assume 'test' contains\n    dog\n    cat\n    bird\n    >>> vocab, rev_vocab = tl.nlp.initialize_vocabulary(\"test\")\n    >>> print(vocab)\n    >>> {b'cat': 1, b'dog': 0, b'bird': 2}\n    >>> print(rev_vocab)\n    >>> [b'dog', b'cat', b'bird']\n\n    Raises\n    -------\n    ValueError : if the provided vocabulary_path does not exist."
  },
  {
    "code": "def sentence_to_token_ids(\n        sentence, vocabulary, tokenizer=None, normalize_digits=True, UNK_ID=3, _DIGIT_RE=re.compile(br\"\\d\")\n):\n    \"\"\"Convert a string to list of integers representing token-ids.\n\n    For example, a sentence \"I have a dog\" may become tokenized into\n    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n        The sentence in bytes format to convert to token-ids, see ``basic_tokenizer()`` and ``data_to_token_ids()``.\n    vocabulary : dictionary\n        Mmapping tokens to integers.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    Returns\n    --------\n    list of int\n        The token-ids for the sentence.\n\n    \"\"\"\n    if tokenizer:\n        words = tokenizer(sentence)\n    else:\n        words = basic_tokenizer(sentence)\n    if not normalize_digits:\n        return [vocabulary.get(w, UNK_ID) for w in words]\n    # Normalize digits by 0 before looking words up in the vocabulary.\n    return [vocabulary.get(re.sub(_DIGIT_RE, b\"0\", w), UNK_ID) for w in words]",
    "doc": "Convert a string to list of integers representing token-ids.\n\n    For example, a sentence \"I have a dog\" may become tokenized into\n    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n\n    Parameters\n    -----------\n    sentence : tensorflow.python.platform.gfile.GFile Object\n        The sentence in bytes format to convert to token-ids, see ``basic_tokenizer()`` and ``data_to_token_ids()``.\n    vocabulary : dictionary\n        Mmapping tokens to integers.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    Returns\n    --------\n    list of int\n        The token-ids for the sentence."
  },
  {
    "code": "def data_to_token_ids(\n        data_path, target_path, vocabulary_path, tokenizer=None, normalize_digits=True, UNK_ID=3,\n        _DIGIT_RE=re.compile(br\"\\d\")\n):\n    \"\"\"Tokenize data file and turn into token-ids using given vocabulary file.\n\n    This function loads data line-by-line from data_path, calls the above\n    sentence_to_token_ids, and saves the result to target_path. See comment\n    for sentence_to_token_ids on the details of token-ids format.\n\n    Parameters\n    -----------\n    data_path : str\n        Path to the data file in one-sentence-per-line format.\n    target_path : str\n        Path where the file with token-ids will be created.\n    vocabulary_path : str\n        Path to the vocabulary file.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``\n\n    \"\"\"\n    if not gfile.Exists(target_path):\n        tl.logging.info(\"Tokenizing data in %s\" % data_path)\n        vocab, _ = initialize_vocabulary(vocabulary_path)\n        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n                counter = 0\n                for line in data_file:\n                    counter += 1\n                    if counter % 100000 == 0:\n                        tl.logging.info(\"  tokenizing line %d\" % counter)\n                    token_ids = sentence_to_token_ids(\n                        line, vocab, tokenizer, normalize_digits, UNK_ID=UNK_ID, _DIGIT_RE=_DIGIT_RE\n                    )\n                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n    else:\n        tl.logging.info(\"Target path %s exists\" % target_path)",
    "doc": "Tokenize data file and turn into token-ids using given vocabulary file.\n\n    This function loads data line-by-line from data_path, calls the above\n    sentence_to_token_ids, and saves the result to target_path. See comment\n    for sentence_to_token_ids on the details of token-ids format.\n\n    Parameters\n    -----------\n    data_path : str\n        Path to the data file in one-sentence-per-line format.\n    target_path : str\n        Path where the file with token-ids will be created.\n    vocabulary_path : str\n        Path to the vocabulary file.\n    tokenizer : function\n        A function to use to tokenize each sentence. If None, ``basic_tokenizer`` will be used.\n    normalize_digits : boolean\n        If true, all digits are replaced by 0.\n\n    References\n    ----------\n    - Code from ``/tensorflow/models/rnn/translation/data_utils.py``"
  },
  {
    "code": "def moses_multi_bleu(hypotheses, references, lowercase=False):\n    \"\"\"Calculate the bleu score for hypotheses and references\n    using the MOSES ulti-bleu.perl script.\n\n    Parameters\n    ------------\n    hypotheses : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    references : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    lowercase : boolean\n        If True, pass the \"-lc\" flag to the multi-bleu script\n\n    Examples\n    ---------\n    >>> hypotheses = [\"a bird is flying on the sky\"]\n    >>> references = [\"two birds are flying on the sky\", \"a bird is on the top of the tree\", \"an airplane is on the sky\",]\n    >>> score = tl.nlp.moses_multi_bleu(hypotheses, references)\n\n    Returns\n    --------\n    float\n        The BLEU score\n\n    References\n    ----------\n    - `Google/seq2seq/metric/bleu <https://github.com/google/seq2seq>`__\n\n    \"\"\"\n    if np.size(hypotheses) == 0:\n        return np.float32(0.0)\n\n    # Get MOSES multi-bleu script\n    try:\n        multi_bleu_path, _ = urllib.request.urlretrieve(\n            \"https://raw.githubusercontent.com/moses-smt/mosesdecoder/\"\n            \"master/scripts/generic/multi-bleu.perl\"\n        )\n        os.chmod(multi_bleu_path, 0o755)\n    except Exception:  # pylint: disable=W0702\n        tl.logging.info(\"Unable to fetch multi-bleu.perl script, using local.\")\n        metrics_dir = os.path.dirname(os.path.realpath(__file__))\n        bin_dir = os.path.abspath(os.path.join(metrics_dir, \"..\", \"..\", \"bin\"))\n        multi_bleu_path = os.path.join(bin_dir, \"tools/multi-bleu.perl\")\n\n    # Dump hypotheses and references to tempfiles\n    hypothesis_file = tempfile.NamedTemporaryFile()\n    hypothesis_file.write(\"\\n\".join(hypotheses).encode(\"utf-8\"))\n    hypothesis_file.write(b\"\\n\")\n    hypothesis_file.flush()\n    reference_file = tempfile.NamedTemporaryFile()\n    reference_file.write(\"\\n\".join(references).encode(\"utf-8\"))\n    reference_file.write(b\"\\n\")\n    reference_file.flush()\n\n    # Calculate BLEU using multi-bleu script\n    with open(hypothesis_file.name, \"r\") as read_pred:\n        bleu_cmd = [multi_bleu_path]\n        if lowercase:\n            bleu_cmd += [\"-lc\"]\n        bleu_cmd += [reference_file.name]\n        try:\n            bleu_out = subprocess.check_output(bleu_cmd, stdin=read_pred, stderr=subprocess.STDOUT)\n            bleu_out = bleu_out.decode(\"utf-8\")\n            bleu_score = re.search(r\"BLEU = (.+?),\", bleu_out).group(1)\n            bleu_score = float(bleu_score)\n        except subprocess.CalledProcessError as error:\n            if error.output is not None:\n                tl.logging.warning(\"multi-bleu.perl script returned non-zero exit code\")\n                tl.logging.warning(error.output)\n            bleu_score = np.float32(0.0)\n\n    # Close temp files\n    hypothesis_file.close()\n    reference_file.close()\n\n    return np.float32(bleu_score)",
    "doc": "Calculate the bleu score for hypotheses and references\n    using the MOSES ulti-bleu.perl script.\n\n    Parameters\n    ------------\n    hypotheses : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    references : numpy.array.string\n        A numpy array of strings where each string is a single example.\n    lowercase : boolean\n        If True, pass the \"-lc\" flag to the multi-bleu script\n\n    Examples\n    ---------\n    >>> hypotheses = [\"a bird is flying on the sky\"]\n    >>> references = [\"two birds are flying on the sky\", \"a bird is on the top of the tree\", \"an airplane is on the sky\",]\n    >>> score = tl.nlp.moses_multi_bleu(hypotheses, references)\n\n    Returns\n    --------\n    float\n        The BLEU score\n\n    References\n    ----------\n    - `Google/seq2seq/metric/bleu <https://github.com/google/seq2seq>`__"
  },
  {
    "code": "def word_to_id(self, word):\n        \"\"\"Returns the integer id of a word string.\"\"\"\n        if word in self._vocab:\n            return self._vocab[word]\n        else:\n            return self._unk_id",
    "doc": "Returns the integer id of a word string."
  },
  {
    "code": "def word_to_id(self, word):\n        \"\"\"Returns the integer word id of a word string.\"\"\"\n        if word in self.vocab:\n            return self.vocab[word]\n        else:\n            return self.unk_id",
    "doc": "Returns the integer word id of a word string."
  },
  {
    "code": "def id_to_word(self, word_id):\n        \"\"\"Returns the word string of an integer word id.\"\"\"\n        if word_id >= len(self.reverse_vocab):\n            return self.reverse_vocab[self.unk_id]\n        else:\n            return self.reverse_vocab[word_id]",
    "doc": "Returns the word string of an integer word id."
  },
  {
    "code": "def basic_clean_str(string):\n    \"\"\"Tokenization/string cleaning for a datasets.\"\"\"\n    string = re.sub(r\"\\n\", \" \", string)  # '\\n'      --> ' '\n    string = re.sub(r\"\\'s\", \" \\'s\", string)  # it's      --> it 's\n    string = re.sub(r\"\\\u2019s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" have\", string)  # they've   --> they have\n    string = re.sub(r\"\\\u2019ve\", \" have\", string)\n    string = re.sub(r\"\\'t\", \" not\", string)  # can't     --> can not\n    string = re.sub(r\"\\\u2019t\", \" not\", string)\n    string = re.sub(r\"\\'re\", \" are\", string)  # they're   --> they are\n    string = re.sub(r\"\\\u2019re\", \" are\", string)\n    string = re.sub(r\"\\'d\", \"\", string)  # I'd (I had, I would) --> I\n    string = re.sub(r\"\\\u2019d\", \"\", string)\n    string = re.sub(r\"\\'ll\", \" will\", string)  # I'll      --> I will\n    string = re.sub(r\"\\\u2019ll\", \" will\", string)\n    string = re.sub(r\"\\\u201c\", \"  \", string)  # \u201ca\u201d       --> \u201c a \u201d\n    string = re.sub(r\"\\\u201d\", \"  \", string)\n    string = re.sub(r\"\\\"\", \"  \", string)  # \"a\"       --> \" a \"\n    string = re.sub(r\"\\'\", \"  \", string)  # they'     --> they '\n    string = re.sub(r\"\\\u2019\", \"  \", string)  # they\u2019     --> they \u2019\n    string = re.sub(r\"\\.\", \" . \", string)  # they.     --> they .\n    string = re.sub(r\"\\,\", \" , \", string)  # they,     --> they ,\n    string = re.sub(r\"\\!\", \" ! \", string)\n    string = re.sub(r\"\\-\", \"  \", string)  # \"low-cost\"--> lost cost\n    string = re.sub(r\"\\(\", \"  \", string)  # (they)    --> ( they)\n    string = re.sub(r\"\\)\", \"  \", string)  # ( they)   --> ( they )\n    string = re.sub(r\"\\]\", \"  \", string)  # they]     --> they ]\n    string = re.sub(r\"\\[\", \"  \", string)  # they[     --> they [\n    string = re.sub(r\"\\?\", \"  \", string)  # they?     --> they ?\n    string = re.sub(r\"\\>\", \"  \", string)  # they>     --> they >\n    string = re.sub(r\"\\<\", \"  \", string)  # they<     --> they <\n    string = re.sub(r\"\\=\", \"  \", string)  # easier=   --> easier =\n    string = re.sub(r\"\\;\", \"  \", string)  # easier;   --> easier ;\n    string = re.sub(r\"\\;\", \"  \", string)\n    string = re.sub(r\"\\:\", \"  \", string)  # easier:   --> easier :\n    string = re.sub(r\"\\\"\", \"  \", string)  # easier\"   --> easier \"\n    string = re.sub(r\"\\$\", \"  \", string)  # $380      --> $ 380\n    string = re.sub(r\"\\_\", \"  \", string)  # _100     --> _ 100\n    string = re.sub(r\"\\s{2,}\", \" \", string)  # Akara is    handsome --> Akara is handsome\n    return string.strip().lower()",
    "doc": "Tokenization/string cleaning for a datasets."
  },
  {
    "code": "def main_restore_embedding_layer():\n    \"\"\"How to use Embedding layer, and how to convert IDs to vector,\n    IDs to words, etc.\n    \"\"\"\n    # Step 1: Build the embedding matrix and load the existing embedding matrix.\n    vocabulary_size = 50000\n    embedding_size = 128\n    model_file_name = \"model_word2vec_50k_128\"\n    batch_size = None\n\n    print(\"Load existing embedding matrix and dictionaries\")\n    all_var = tl.files.load_npy_to_any(name=model_file_name + '.npy')\n    data = all_var['data']\n    count = all_var['count']\n    dictionary = all_var['dictionary']\n    reverse_dictionary = all_var['reverse_dictionary']\n\n    tl.nlp.save_vocab(count, name='vocab_' + model_file_name + '.txt')\n\n    del all_var, data, count\n\n    load_params = tl.files.load_npz(name=model_file_name + '.npz')\n\n    x = tf.placeholder(tf.int32, shape=[batch_size])\n\n    emb_net = tl.layers.EmbeddingInputlayer(x, vocabulary_size, embedding_size, name='emb')\n\n    # sess.run(tf.global_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    tl.files.assign_params(sess, [load_params[0]], emb_net)\n\n    emb_net.print_params()\n    emb_net.print_layers()\n\n    # Step 2: Input word(s), output the word vector(s).\n    word = b'hello'\n    word_id = dictionary[word]\n    print('word_id:', word_id)\n\n    words = [b'i', b'am', b'tensor', b'layer']\n    word_ids = tl.nlp.words_to_word_ids(words, dictionary, _UNK)\n    context = tl.nlp.word_ids_to_words(word_ids, reverse_dictionary)\n    print('word_ids:', word_ids)\n    print('context:', context)\n\n    vector = sess.run(emb_net.outputs, feed_dict={x: [word_id]})\n    print('vector:', vector.shape)\n\n    vectors = sess.run(emb_net.outputs, feed_dict={x: word_ids})\n    print('vectors:', vectors.shape)",
    "doc": "How to use Embedding layer, and how to convert IDs to vector,\n    IDs to words, etc."
  },
  {
    "code": "def main_lstm_generate_text():\n    \"\"\"Generate text by Synced sequence input and output.\"\"\"\n    # rnn model and update  (describtion: see tutorial_ptb_lstm.py)\n    init_scale = 0.1\n    learning_rate = 1.0\n    max_grad_norm = 5\n    sequence_length = 20\n    hidden_size = 200\n    max_epoch = 4\n    max_max_epoch = 100\n    lr_decay = 0.9\n    batch_size = 20\n\n    top_k_list = [1, 3, 5, 10]\n    print_length = 30\n\n    model_file_name = \"model_generate_text.npz\"\n\n    # ===== Prepare Data\n    words = customized_read_words(input_fpath=\"data/trump/trump_text.txt\")\n\n    vocab = tl.nlp.create_vocab([words], word_counts_output_file='vocab.txt', min_word_count=1)\n    vocab = tl.nlp.Vocabulary('vocab.txt', unk_word=\"<UNK>\")\n    vocab_size = vocab.unk_id + 1\n    train_data = [vocab.word_to_id(word) for word in words]\n\n    # Set the seed to generate sentence.\n    seed = \"it is a\"\n    # seed = basic_clean_str(seed).split()\n    seed = nltk.tokenize.word_tokenize(seed)\n    print('seed : %s' % seed)\n\n    sess = tf.InteractiveSession()\n\n    # ===== Define model\n    input_data = tf.placeholder(tf.int32, [batch_size, sequence_length])\n    targets = tf.placeholder(tf.int32, [batch_size, sequence_length])\n    # Testing (Evaluation), for generate text\n    input_data_test = tf.placeholder(tf.int32, [1, 1])\n\n    def inference(x, is_train, sequence_length, reuse=None):\n        \"\"\"If reuse is True, the inferences use the existing parameters,\n        then different inferences share the same parameters.\n        \"\"\"\n        print(\"\\nsequence_length: %d, is_train: %s, reuse: %s\" % (sequence_length, is_train, reuse))\n        rnn_init = tf.random_uniform_initializer(-init_scale, init_scale)\n        with tf.variable_scope(\"model\", reuse=reuse):\n            network = EmbeddingInputlayer(x, vocab_size, hidden_size, rnn_init, name='embedding')\n            network = RNNLayer(\n                network, cell_fn=tf.contrib.rnn.BasicLSTMCell, cell_init_args={\n                    'forget_bias': 0.0,\n                    'state_is_tuple': True\n                }, n_hidden=hidden_size, initializer=rnn_init, n_steps=sequence_length, return_last=False,\n                return_seq_2d=True, name='lstm1'\n            )\n            lstm1 = network\n            network = DenseLayer(network, vocab_size, W_init=rnn_init, b_init=rnn_init, act=None, name='output')\n        return network, lstm1\n\n    # Inference for Training\n    network, lstm1 = inference(input_data, is_train=True, sequence_length=sequence_length, reuse=None)\n    # Inference for generate text, sequence_length=1\n    network_test, lstm1_test = inference(input_data_test, is_train=False, sequence_length=1, reuse=True)\n    y_linear = network_test.outputs\n    y_soft = tf.nn.softmax(y_linear)\n\n    # y_id = tf.argmax(tf.nn.softmax(y), 1)\n\n    # ===== Define train ops\n    def loss_fn(outputs, targets, batch_size, sequence_length):\n        # Returns the cost function of Cross-entropy of two sequences, implement\n        # softmax internally.\n        # outputs : 2D tensor [n_examples, n_outputs]\n        # targets : 2D tensor [n_examples, n_outputs]\n        # n_examples = batch_size * sequence_length\n        # so\n        # cost is the averaged cost of each mini-batch (concurrent process).\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n            [outputs], [tf.reshape(targets, [-1])], [tf.ones([batch_size * sequence_length])]\n        )\n        cost = tf.reduce_sum(loss) / batch_size\n        return cost\n\n    # Cost for Training\n    cost = loss_fn(network.outputs, targets, batch_size, sequence_length)\n\n    # Truncated Backpropagation for training\n    with tf.variable_scope('learning_rate'):\n        lr = tf.Variable(0.0, trainable=False)\n    # You can get all trainable parameters as follow.\n    # tvars = tf.trainable_variables()\n    # Alternatively, you can specify the parameters for training as follw.\n    #  tvars = network.all_params      $ all parameters\n    #  tvars = network.all_params[1:]  $ parameters except embedding matrix\n    # Train the whole network.\n    tvars = network.all_params\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(lr)\n    train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    # ===== Training\n    sess.run(tf.global_variables_initializer())\n\n    print(\"\\nStart learning a model to generate text\")\n    for i in range(max_max_epoch):\n        # decrease the learning_rate after ``max_epoch``, by multipling lr_decay.\n        new_lr_decay = lr_decay**max(i - max_epoch, 0.0)\n        sess.run(tf.assign(lr, learning_rate * new_lr_decay))\n\n        print(\"Epoch: %d/%d Learning rate: %.8f\" % (i + 1, max_max_epoch, sess.run(lr)))\n        epoch_size = ((len(train_data) // batch_size) - 1) // sequence_length\n\n        start_time = time.time()\n        costs = 0.0\n        iters = 0\n        # reset all states at the begining of every epoch\n        state1 = tl.layers.initialize_rnn_state(lstm1.initial_state)\n        for step, (x, y) in enumerate(tl.iterate.ptb_iterator(train_data, batch_size, sequence_length)):\n            _cost, state1, _ = sess.run(\n                [cost, lstm1.final_state, train_op], feed_dict={\n                    input_data: x,\n                    targets: y,\n                    lstm1.initial_state: state1\n                }\n            )\n            costs += _cost\n            iters += sequence_length\n\n            if step % (epoch_size // 10) == 1:\n                print(\n                    \"%.3f perplexity: %.3f speed: %.0f wps\" %\n                    (step * 1.0 / epoch_size, np.exp(costs / iters), iters * batch_size / (time.time() - start_time))\n                )\n        train_perplexity = np.exp(costs / iters)\n        # print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n        print(\"Epoch: %d/%d Train Perplexity: %.3f\" % (i + 1, max_max_epoch, train_perplexity))\n\n        # for diversity in diversity_list:\n        # testing: sample from top k words\n        for top_k in top_k_list:\n            # Testing, generate some text from a given seed.\n            state1 = tl.layers.initialize_rnn_state(lstm1_test.initial_state)\n            # state2 = tl.layers.initialize_rnn_state(lstm2_test.initial_state)\n            outs_id = [vocab.word_to_id(w) for w in seed]\n            # feed the seed to initialize the state for generation.\n            for ids in outs_id[:-1]:\n                a_id = np.asarray(ids).reshape(1, 1)\n                state1 = sess.run(\n                    [lstm1_test.final_state], feed_dict={\n                        input_data_test: a_id,\n                        lstm1_test.initial_state: state1\n                    }\n                )\n            # feed the last word in seed, and start to generate sentence.\n            a_id = outs_id[-1]\n            for _ in range(print_length):\n                a_id = np.asarray(a_id).reshape(1, 1)\n                out, state1 = sess.run(\n                    [y_soft, lstm1_test.final_state], feed_dict={\n                        input_data_test: a_id,\n                        lstm1_test.initial_state: state1\n                    }\n                )\n                # Without sampling\n                # a_id = np.argmax(out[0])\n                # Sample from all words, if vocab_size is large,\n                # this may have numeric error.\n                # a_id = tl.nlp.sample(out[0], diversity)\n                # Sample from the top k words.\n                a_id = tl.nlp.sample_top(out[0], top_k=top_k)\n                outs_id.append(a_id)\n            sentence = [vocab.id_to_word(w) for w in outs_id]\n            sentence = \" \".join(sentence)\n            # print(diversity, ':', sentence)\n            print(top_k, ':', sentence)\n\n    print(\"Save model\")\n    tl.files.save_npz(network_test.all_params, name=model_file_name)",
    "doc": "Generate text by Synced sequence input and output."
  },
  {
    "code": "def createAndStartSwarm(client, clientInfo=\"\", clientKey=\"\", params=\"\",\n                        minimumWorkers=None, maximumWorkers=None,\n                        alreadyRunning=False):\n  \"\"\"Create and start a swarm job.\n\n  Args:\n    client - A string identifying the calling client. There is a small limit\n        for the length of the value. See ClientJobsDAO.CLIENT_MAX_LEN.\n    clientInfo - JSON encoded dict of client specific information.\n    clientKey - Foreign key. Limited in length, see ClientJobsDAO._initTables.\n    params - JSON encoded dict of the parameters for the job. This can be\n        fetched out of the database by the worker processes based on the jobID.\n    minimumWorkers - The minimum workers to allocate to the swarm. Set to None\n        to use the default.\n    maximumWorkers - The maximum workers to allocate to the swarm. Set to None\n        to use the swarm default. Set to 0 to use the maximum scheduler value.\n    alreadyRunning - Insert a job record for an already running process. Used\n        for testing.\n  \"\"\"\n  if minimumWorkers is None:\n    minimumWorkers = Configuration.getInt(\n        \"nupic.hypersearch.minWorkersPerSwarm\")\n  if maximumWorkers is None:\n    maximumWorkers = Configuration.getInt(\n        \"nupic.hypersearch.maxWorkersPerSwarm\")\n\n  return ClientJobsDAO.get().jobInsert(\n      client=client,\n      cmdLine=\"$HYPERSEARCH\",\n      clientInfo=clientInfo,\n      clientKey=clientKey,\n      alreadyRunning=alreadyRunning,\n      params=params,\n      minimumWorkers=minimumWorkers,\n      maximumWorkers=maximumWorkers,\n      jobType=ClientJobsDAO.JOB_TYPE_HS)",
    "doc": "Create and start a swarm job.\n\n  Args:\n    client - A string identifying the calling client. There is a small limit\n        for the length of the value. See ClientJobsDAO.CLIENT_MAX_LEN.\n    clientInfo - JSON encoded dict of client specific information.\n    clientKey - Foreign key. Limited in length, see ClientJobsDAO._initTables.\n    params - JSON encoded dict of the parameters for the job. This can be\n        fetched out of the database by the worker processes based on the jobID.\n    minimumWorkers - The minimum workers to allocate to the swarm. Set to None\n        to use the default.\n    maximumWorkers - The maximum workers to allocate to the swarm. Set to None\n        to use the swarm default. Set to 0 to use the maximum scheduler value.\n    alreadyRunning - Insert a job record for an already running process. Used\n        for testing."
  },
  {
    "code": "def getSwarmModelParams(modelID):\n  \"\"\"Retrieve the Engine-level model params from a Swarm model\n\n  Args:\n    modelID - Engine-level model ID of the Swarm model\n\n  Returns:\n    JSON-encoded string containing Model Params\n  \"\"\"\n\n  # TODO: the use of nupic.frameworks.opf.helpers.loadExperimentDescriptionScriptFromDir when\n  #  retrieving module params results in a leakage of pf_base_descriptionNN and\n  #  pf_descriptionNN module imports for every call to getSwarmModelParams, so\n  #  the leakage is unlimited when getSwarmModelParams is called by a\n  #  long-running process.  An alternate solution is to execute the guts of\n  #  this function's logic in a seprate process (via multiprocessing module).\n\n  cjDAO = ClientJobsDAO.get()\n\n  (jobID, description) = cjDAO.modelsGetFields(\n    modelID,\n    [\"jobId\", \"genDescription\"])\n\n  (baseDescription,) = cjDAO.jobGetFields(jobID, [\"genBaseDescription\"])\n\n  # Construct a directory with base.py and description.py for loading model\n  # params, and use nupic.frameworks.opf.helpers to extract model params from\n  # those files\n  descriptionDirectory = tempfile.mkdtemp()\n  try:\n    baseDescriptionFilePath = os.path.join(descriptionDirectory, \"base.py\")\n    with open(baseDescriptionFilePath, mode=\"wb\") as f:\n      f.write(baseDescription)\n\n    descriptionFilePath = os.path.join(descriptionDirectory, \"description.py\")\n    with open(descriptionFilePath, mode=\"wb\") as f:\n      f.write(description)\n\n    expIface = helpers.getExperimentDescriptionInterfaceFromModule(\n      helpers.loadExperimentDescriptionScriptFromDir(descriptionDirectory))\n\n    return json.dumps(\n      dict(\n        modelConfig=expIface.getModelDescription(),\n        inferenceArgs=expIface.getModelControl().get(\"inferenceArgs\", None)))\n  finally:\n    shutil.rmtree(descriptionDirectory, ignore_errors=True)",
    "doc": "Retrieve the Engine-level model params from a Swarm model\n\n  Args:\n    modelID - Engine-level model ID of the Swarm model\n\n  Returns:\n    JSON-encoded string containing Model Params"
  },
  {
    "code": "def enableConcurrencyChecks(maxConcurrency, raiseException=True):\n  \"\"\" Enable the diagnostic feature for debugging unexpected concurrency in\n  acquiring ConnectionWrapper instances.\n\n  NOTE: This MUST be done early in your application's execution, BEFORE any\n  accesses to ConnectionFactory or connection policies from your application\n  (including imports and sub-imports of your app).\n\n  Parameters:\n  ----------------------------------------------------------------\n  maxConcurrency:   A non-negative integer that represents the maximum expected\n                      number of outstanding connections.  When this value is\n                      exceeded, useful information will be logged and, depending\n                      on the value of the raiseException arg,\n                      ConcurrencyExceededError may be raised.\n  raiseException:   If true, ConcurrencyExceededError will be raised when\n                      maxConcurrency is exceeded.\n  \"\"\"\n  global g_max_concurrency, g_max_concurrency_raise_exception\n\n  assert maxConcurrency >= 0\n\n  g_max_concurrency = maxConcurrency\n  g_max_concurrency_raise_exception = raiseException\n  return",
    "doc": "Enable the diagnostic feature for debugging unexpected concurrency in\n  acquiring ConnectionWrapper instances.\n\n  NOTE: This MUST be done early in your application's execution, BEFORE any\n  accesses to ConnectionFactory or connection policies from your application\n  (including imports and sub-imports of your app).\n\n  Parameters:\n  ----------------------------------------------------------------\n  maxConcurrency:   A non-negative integer that represents the maximum expected\n                      number of outstanding connections.  When this value is\n                      exceeded, useful information will be logged and, depending\n                      on the value of the raiseException arg,\n                      ConcurrencyExceededError may be raised.\n  raiseException:   If true, ConcurrencyExceededError will be raised when\n                      maxConcurrency is exceeded."
  },
  {
    "code": "def _getCommonSteadyDBArgsDict():\n  \"\"\" Returns a dictionary of arguments for DBUtils.SteadyDB.SteadyDBConnection\n  constructor.\n  \"\"\"\n\n  return dict(\n      creator = pymysql,\n      host = Configuration.get('nupic.cluster.database.host'),\n      port = int(Configuration.get('nupic.cluster.database.port')),\n      user = Configuration.get('nupic.cluster.database.user'),\n      passwd = Configuration.get('nupic.cluster.database.passwd'),\n      charset = 'utf8',\n      use_unicode = True,\n      setsession = ['SET AUTOCOMMIT = 1'])",
    "doc": "Returns a dictionary of arguments for DBUtils.SteadyDB.SteadyDBConnection\n  constructor."
  },
  {
    "code": "def _getLogger(cls, logLevel=None):\n  \"\"\" Gets a logger for the given class in this module\n  \"\"\"\n  logger = logging.getLogger(\n    \".\".join(['com.numenta', _MODULE_NAME, cls.__name__]))\n\n  if logLevel is not None:\n    logger.setLevel(logLevel)\n\n  return logger",
    "doc": "Gets a logger for the given class in this module"
  },
  {
    "code": "def get(cls):\n    \"\"\" Acquire a ConnectionWrapper instance that represents a connection\n    to the SQL server per nupic.cluster.database.* configuration settings.\n\n    NOTE: caller is responsible for calling the ConnectionWrapper instance's\n    release() method after using the connection in order to release resources.\n    Better yet, use the returned ConnectionWrapper instance in a Context Manager\n    statement for automatic invocation of release():\n    Example:\n        # If using Jython 2.5.x, first import with_statement at the very top of\n        your script (don't need this import for Jython/Python 2.6.x and later):\n        from __future__ import with_statement\n        # Then:\n        from nupic.database.Connection import ConnectionFactory\n        # Then use it like this\n        with ConnectionFactory.get() as conn:\n          conn.cursor.execute(\"SELECT ...\")\n          conn.cursor.fetchall()\n          conn.cursor.execute(\"INSERT ...\")\n\n    WARNING: DO NOT close the underlying connection or cursor as it may be\n    shared by other modules in your process.  ConnectionWrapper's release()\n    method will do the right thing.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller is responsible\n                    for releasing resources as described above.\n    \"\"\"\n    if cls._connectionPolicy is None:\n      logger = _getLogger(cls)\n      logger.info(\"Creating db connection policy via provider %r\",\n                  cls._connectionPolicyInstanceProvider)\n      cls._connectionPolicy = cls._connectionPolicyInstanceProvider()\n\n      logger.debug(\"Created connection policy: %r\", cls._connectionPolicy)\n\n    return cls._connectionPolicy.acquireConnection()",
    "doc": "Acquire a ConnectionWrapper instance that represents a connection\n    to the SQL server per nupic.cluster.database.* configuration settings.\n\n    NOTE: caller is responsible for calling the ConnectionWrapper instance's\n    release() method after using the connection in order to release resources.\n    Better yet, use the returned ConnectionWrapper instance in a Context Manager\n    statement for automatic invocation of release():\n    Example:\n        # If using Jython 2.5.x, first import with_statement at the very top of\n        your script (don't need this import for Jython/Python 2.6.x and later):\n        from __future__ import with_statement\n        # Then:\n        from nupic.database.Connection import ConnectionFactory\n        # Then use it like this\n        with ConnectionFactory.get() as conn:\n          conn.cursor.execute(\"SELECT ...\")\n          conn.cursor.fetchall()\n          conn.cursor.execute(\"INSERT ...\")\n\n    WARNING: DO NOT close the underlying connection or cursor as it may be\n    shared by other modules in your process.  ConnectionWrapper's release()\n    method will do the right thing.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller is responsible\n                    for releasing resources as described above."
  },
  {
    "code": "def _createDefaultPolicy(cls):\n    \"\"\" [private] Create the default database connection policy instance\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:            The default database connection policy instance\n    \"\"\"\n    logger = _getLogger(cls)\n\n    logger.debug(\n      \"Creating database connection policy: platform=%r; pymysql.VERSION=%r\",\n      platform.system(), pymysql.VERSION)\n\n    if platform.system() == \"Java\":\n      # NOTE: PooledDB doesn't seem to work under Jython\n      # NOTE: not appropriate for multi-threaded applications.\n      # TODO: this was fixed in Webware DBUtils r8228, so once\n      #       we pick up a realease with this fix, we should use\n      #       PooledConnectionPolicy for both Jython and Python.\n      policy = SingleSharedConnectionPolicy()\n    else:\n      policy = PooledConnectionPolicy()\n\n    return policy",
    "doc": "[private] Create the default database connection policy instance\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:            The default database connection policy instance"
  },
  {
    "code": "def release(self):\n    \"\"\" Release the database connection and cursor\n\n    The receiver of the Connection instance MUST call this method in order\n    to reclaim resources\n    \"\"\"\n\n    self._logger.debug(\"Releasing: %r\", self)\n\n    # Discard self from set of outstanding instances\n    if self._addedToInstanceSet:\n      try:\n        self._clsOutstandingInstances.remove(self)\n      except:\n        self._logger.exception(\n          \"Failed to remove self from _clsOutstandingInstances: %r;\", self)\n        raise\n\n    self._releaser(dbConn=self.dbConn, cursor=self.cursor)\n\n    self.__class__._clsNumOutstanding -= 1\n    assert self._clsNumOutstanding >= 0,  \\\n           \"_clsNumOutstanding=%r\" % (self._clsNumOutstanding,)\n\n    self._releaser = None\n    self.cursor = None\n    self.dbConn = None\n    self._creationTracebackString = None\n    self._addedToInstanceSet = False\n    self._logger = None\n    return",
    "doc": "Release the database connection and cursor\n\n    The receiver of the Connection instance MUST call this method in order\n    to reclaim resources"
  },
  {
    "code": "def _trackInstanceAndCheckForConcurrencyViolation(self):\n    \"\"\" Check for concurrency violation and add self to\n    _clsOutstandingInstances.\n\n    ASSUMPTION: Called from constructor BEFORE _clsNumOutstanding is\n    incremented\n    \"\"\"\n    global g_max_concurrency, g_max_concurrency_raise_exception\n\n    assert g_max_concurrency is not None\n    assert self not in self._clsOutstandingInstances, repr(self)\n\n    # Populate diagnostic info\n    self._creationTracebackString = traceback.format_stack()\n\n    # Check for concurrency violation\n    if self._clsNumOutstanding >= g_max_concurrency:\n      # NOTE: It's possible for _clsNumOutstanding to be greater than\n      #  len(_clsOutstandingInstances) if concurrency check was enabled after\n      #  unrelease allocations.\n      errorMsg = (\"With numOutstanding=%r, exceeded concurrency limit=%r \"\n                  \"when requesting %r. OTHER TRACKED UNRELEASED \"\n                  \"INSTANCES (%s): %r\") % (\n        self._clsNumOutstanding, g_max_concurrency, self,\n        len(self._clsOutstandingInstances), self._clsOutstandingInstances,)\n\n      self._logger.error(errorMsg)\n\n      if g_max_concurrency_raise_exception:\n        raise ConcurrencyExceededError(errorMsg)\n\n\n    # Add self to tracked instance set\n    self._clsOutstandingInstances.add(self)\n    self._addedToInstanceSet = True\n\n    return",
    "doc": "Check for concurrency violation and add self to\n    _clsOutstandingInstances.\n\n    ASSUMPTION: Called from constructor BEFORE _clsNumOutstanding is\n    incremented"
  },
  {
    "code": "def close(self):\n    \"\"\" Close the policy instance and its shared database connection. \"\"\"\n    self._logger.info(\"Closing\")\n    if self._conn is not None:\n      self._conn.close()\n      self._conn = None\n    else:\n      self._logger.warning(\n        \"close() called, but connection policy was alredy closed\")\n    return",
    "doc": "Close the policy instance and its shared database connection."
  },
  {
    "code": "def acquireConnection(self):\n    \"\"\" Get a Connection instance.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources.\n    \"\"\"\n    self._logger.debug(\"Acquiring connection\")\n\n    # Check connection and attempt to re-establish it if it died (this is\n    #   what PooledDB does)\n    self._conn._ping_check()\n    connWrap = ConnectionWrapper(dbConn=self._conn,\n                                 cursor=self._conn.cursor(),\n                                 releaser=self._releaseConnection,\n                                 logger=self._logger)\n    return connWrap",
    "doc": "Get a Connection instance.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources."
  },
  {
    "code": "def close(self):\n    \"\"\" Close the policy instance and its database connection pool. \"\"\"\n    self._logger.info(\"Closing\")\n\n    if self._pool is not None:\n      self._pool.close()\n      self._pool = None\n    else:\n      self._logger.warning(\n        \"close() called, but connection policy was alredy closed\")\n    return",
    "doc": "Close the policy instance and its database connection pool."
  },
  {
    "code": "def acquireConnection(self):\n    \"\"\" Get a connection from the pool.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources.\n    \"\"\"\n    self._logger.debug(\"Acquiring connection\")\n\n    dbConn = self._pool.connection(shareable=False)\n    connWrap = ConnectionWrapper(dbConn=dbConn,\n                                 cursor=dbConn.cursor(),\n                                 releaser=self._releaseConnection,\n                                 logger=self._logger)\n    return connWrap",
    "doc": "Get a connection from the pool.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources."
  },
  {
    "code": "def close(self):\n    \"\"\" Close the policy instance. \"\"\"\n    self._logger.info(\"Closing\")\n\n    if self._opened:\n      self._opened = False\n    else:\n      self._logger.warning(\n        \"close() called, but connection policy was alredy closed\")\n\n    return",
    "doc": "Close the policy instance."
  },
  {
    "code": "def acquireConnection(self):\n    \"\"\" Create a Connection instance.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources.\n    \"\"\"\n    self._logger.debug(\"Acquiring connection\")\n\n    dbConn = SteadyDB.connect(** _getCommonSteadyDBArgsDict())\n    connWrap = ConnectionWrapper(dbConn=dbConn,\n                                 cursor=dbConn.cursor(),\n                                 releaser=self._releaseConnection,\n                                 logger=self._logger)\n    return connWrap",
    "doc": "Create a Connection instance.\n\n    Parameters:\n    ----------------------------------------------------------------\n    retval:       A ConnectionWrapper instance. NOTE: Caller\n                    is responsible for calling the  ConnectionWrapper\n                    instance's release() method or use it in a context manager\n                    expression (with ... as:) to release resources."
  },
  {
    "code": "def _releaseConnection(self, dbConn, cursor):\n    \"\"\" Release database connection and cursor; passed as a callback to\n    ConnectionWrapper\n    \"\"\"\n    self._logger.debug(\"Releasing connection\")\n\n    # Close the cursor\n    cursor.close()\n\n    # ... then close the database connection\n    dbConn.close()\n    return",
    "doc": "Release database connection and cursor; passed as a callback to\n    ConnectionWrapper"
  },
  {
    "code": "def getSpec(cls):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n    \"\"\"\n    ns = dict(\n        description=KNNAnomalyClassifierRegion.__doc__,\n        singleNodeOnly=True,\n        inputs=dict(\n          spBottomUpOut=dict(\n            description=\"\"\"The output signal generated from the bottom-up inputs\n                            from lower levels.\"\"\",\n            dataType='Real32',\n            count=0,\n            required=True,\n            regionLevel=False,\n            isDefaultInput=True,\n            requireSplitterMap=False),\n\n          tpTopDownOut=dict(\n            description=\"\"\"The top-down inputsignal, generated from\n                          feedback from upper levels\"\"\",\n            dataType='Real32',\n            count=0,\n            required=True,\n            regionLevel=False,\n            isDefaultInput=True,\n            requireSplitterMap=False),\n\n          tpLrnActiveStateT=dict(\n            description=\"\"\"Active cells in the learn state at time T from TM.\n                        This is used to classify on.\"\"\",\n            dataType='Real32',\n            count=0,\n            required=True,\n            regionLevel=False,\n            isDefaultInput=True,\n            requireSplitterMap=False),\n\n          sequenceIdIn=dict(\n            description=\"Sequence ID\",\n            dataType='UInt64',\n            count=1,\n            required=False,\n            regionLevel=True,\n            isDefaultInput=False,\n            requireSplitterMap=False),\n\n        ),\n\n        outputs=dict(\n        ),\n\n        parameters=dict(\n          trainRecords=dict(\n            description='Number of records to wait for training',\n            dataType='UInt32',\n            count=1,\n            constraints='',\n            defaultValue=0,\n            accessMode='Create'),\n\n          anomalyThreshold=dict(\n            description='Threshold used to classify anomalies.',\n            dataType='Real32',\n            count=1,\n            constraints='',\n            defaultValue=0,\n            accessMode='Create'),\n\n          cacheSize=dict(\n            description='Number of records to store in cache.',\n            dataType='UInt32',\n            count=1,\n            constraints='',\n            defaultValue=0,\n            accessMode='Create'),\n\n          classificationVectorType=dict(\n            description=\"\"\"Vector type to use when classifying.\n              1 - Vector Column with Difference (TM and SP)\n            \"\"\",\n            dataType='UInt32',\n            count=1,\n            constraints='',\n            defaultValue=1,\n            accessMode='ReadWrite'),\n\n          activeColumnCount=dict(\n            description=\"\"\"Number of active columns in a given step. Typically\n            equivalent to SP.numActiveColumnsPerInhArea\"\"\",\n            dataType='UInt32',\n            count=1,\n            constraints='',\n            defaultValue=40,\n            accessMode='ReadWrite'),\n\n          classificationMaxDist=dict(\n            description=\"\"\"Maximum distance a sample can be from an anomaly\n            in the classifier to be labeled as an anomaly.\n\n            Ex: With rawOverlap distance, a value of 0.65 means that the points\n            must be at most a distance 0.65 apart from each other. This\n            translates to they must be at least 35% similar.\"\"\",\n            dataType='Real32',\n            count=1,\n            constraints='',\n            defaultValue=0.65,\n            accessMode='Create'\n            )\n        ),\n        commands=dict(\n          getLabels=dict(description=\n            \"Returns a list of label dicts with properties ROWID and labels.\"\n            \"ROWID corresponds to the records id and labels is a list of \"\n            \"strings representing the records labels.  Takes additional \"\n            \"integer properties start and end representing the range that \"\n            \"will be returned.\"),\n\n          addLabel=dict(description=\n            \"Takes parameters start, end and labelName. Adds the label \"\n            \"labelName to the records from start to end. This will recalculate \"\n            \"labels from end to the most recent record.\"),\n\n          removeLabels=dict(description=\n            \"Takes additional parameters start, end, labelFilter.  Start and \"\n            \"end correspond to range to remove the label. Remove labels from \"\n            \"each record with record ROWID in range from start to end, \"\n            \"noninclusive of end. Removes all records if labelFilter is None, \"\n            \"otherwise only removes the labels eqaul to labelFilter.\")\n        )\n      )\n\n    ns['parameters'].update(KNNClassifierRegion.getSpec()['parameters'])\n\n    return ns",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getSpec`."
  },
  {
    "code": "def getParameter(self, name, index=-1):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getParameter`.\n    \"\"\"\n    if name == \"trainRecords\":\n      return self.trainRecords\n    elif name == \"anomalyThreshold\":\n      return self.anomalyThreshold\n    elif name == \"activeColumnCount\":\n      return self._activeColumnCount\n    elif name == \"classificationMaxDist\":\n      return self._classificationMaxDist\n    else:\n      # If any spec parameter name is the same as an attribute, this call\n      # will get it automatically, e.g. self.learningMode\n      return PyRegion.getParameter(self, name, index)",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getParameter`."
  },
  {
    "code": "def setParameter(self, name, index, value):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.setParameter`.\n    \"\"\"\n    if name == \"trainRecords\":\n      # Ensure that the trainRecords can only be set to minimum of the ROWID in\n      # the saved states\n      if not (isinstance(value, float) or isinstance(value, int)):\n        raise HTMPredictionModelInvalidArgument(\"Invalid argument type \\'%s\\'. threshold \"\n          \"must be a number.\" % (type(value)))\n\n      if len(self._recordsCache) > 0 and value < self._recordsCache[0].ROWID:\n        raise HTMPredictionModelInvalidArgument(\"Invalid value. autoDetectWaitRecord \"\n          \"value must be valid record within output stream. Current minimum \"\n          \" ROWID in output stream is %d.\" % (self._recordsCache[0].ROWID))\n\n      self.trainRecords = value\n      # Remove any labels before the first cached record (wont be used anymore)\n      self._deleteRangeFromKNN(0, self._recordsCache[0].ROWID)\n      # Reclassify all states\n      self._classifyStates()\n    elif name == \"anomalyThreshold\":\n      if not (isinstance(value, float) or isinstance(value, int)):\n        raise HTMPredictionModelInvalidArgument(\"Invalid argument type \\'%s\\'. threshold \"\n          \"must be a number.\" % (type(value)))\n      self.anomalyThreshold = value\n      self._classifyStates()\n    elif name == \"classificationMaxDist\":\n      if not (isinstance(value, float) or isinstance(value, int)):\n        raise HTMPredictionModelInvalidArgument(\"Invalid argument type \\'%s\\'. \"\n          \"classificationMaxDist must be a number.\" % (type(value)))\n      self._classificationMaxDist = value\n      self._classifyStates()\n    elif name == \"activeColumnCount\":\n      self._activeColumnCount = value\n    else:\n      return PyRegion.setParameter(self, name, index, value)",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.setParameter`."
  },
  {
    "code": "def compute(self, inputs, outputs):\n    \"\"\"\n    Process one input sample.\n    This method is called by the runtime engine.\n    \"\"\"\n    record = self._constructClassificationRecord(inputs)\n\n    #Classify this point after waiting the classification delay\n    if record.ROWID >= self.getParameter('trainRecords'):\n      self._classifyState(record)\n\n    #Save new classification record and keep history as moving window\n    self._recordsCache.append(record)\n    while len(self._recordsCache) > self.cacheSize:\n      self._recordsCache.pop(0)\n\n    self.labelResults = record.anomalyLabel\n\n    self._iteration += 1",
    "doc": "Process one input sample.\n    This method is called by the runtime engine."
  },
  {
    "code": "def _classifyState(self, state):\n    \"\"\"\n    Reclassifies given state.\n    \"\"\"\n    # Record is before wait period do not classifiy\n    if state.ROWID < self.getParameter('trainRecords'):\n      if not state.setByUser:\n        state.anomalyLabel = []\n        self._deleteRecordsFromKNN([state])\n      return\n\n    label = KNNAnomalyClassifierRegion.AUTO_THRESHOLD_CLASSIFIED_LABEL\n    autoLabel = label + KNNAnomalyClassifierRegion.AUTO_TAG\n\n    # Update the label based on classifications\n    newCategory = self._recomputeRecordFromKNN(state)\n    labelList = self._categoryToLabelList(newCategory)\n\n    if state.setByUser:\n      if label in state.anomalyLabel:\n        state.anomalyLabel.remove(label)\n      if autoLabel in state.anomalyLabel:\n        state.anomalyLabel.remove(autoLabel)\n      labelList.extend(state.anomalyLabel)\n\n    # Add threshold classification label if above threshold, else if\n    # classified to add the auto threshold classification.\n    if state.anomalyScore >= self.getParameter('anomalyThreshold'):\n      labelList.append(label)\n    elif label in labelList:\n      ind = labelList.index(label)\n      labelList[ind] = autoLabel\n\n    # Make all entries unique\n    labelList = list(set(labelList))\n\n    # If both above threshold and auto classified above - remove auto label\n    if label in labelList and autoLabel in labelList:\n      labelList.remove(autoLabel)\n\n    if state.anomalyLabel == labelList:\n      return\n\n    # Update state's labeling\n    state.anomalyLabel = labelList\n\n    # Update KNN Classifier with new labeling\n    if state.anomalyLabel == []:\n      self._deleteRecordsFromKNN([state])\n    else:\n      self._addRecordToKNN(state)",
    "doc": "Reclassifies given state."
  },
  {
    "code": "def _constructClassificationRecord(self, inputs):\n    \"\"\"\n    Construct a _HTMClassificationRecord based on the state of the model\n    passed in through the inputs.\n\n    Types for self.classificationVectorType:\n      1 - TM active cells in learn state\n      2 - SP columns concatenated with error from TM column predictions and SP\n    \"\"\"\n    # Count the number of unpredicted columns\n    allSPColumns = inputs[\"spBottomUpOut\"]\n    activeSPColumns = allSPColumns.nonzero()[0]\n\n    score = anomaly.computeRawAnomalyScore(activeSPColumns,\n                                           self._prevPredictedColumns)\n\n    spSize = len(allSPColumns)\n\n\n    allTPCells = inputs['tpTopDownOut']\n    tpSize = len(inputs['tpLrnActiveStateT'])\n\n    classificationVector = numpy.array([])\n\n    if self.classificationVectorType == 1:\n      # Classification Vector: [---TM Cells---]\n      classificationVector = numpy.zeros(tpSize)\n      activeCellMatrix = inputs[\"tpLrnActiveStateT\"].reshape(tpSize, 1)\n      activeCellIdx = numpy.where(activeCellMatrix > 0)[0]\n      if activeCellIdx.shape[0] > 0:\n        classificationVector[numpy.array(activeCellIdx, dtype=numpy.uint16)] = 1\n    elif self.classificationVectorType == 2:\n      # Classification Vecotr: [---SP---|---(TM-SP)----]\n      classificationVector = numpy.zeros(spSize+spSize)\n      if activeSPColumns.shape[0] > 0:\n        classificationVector[activeSPColumns] = 1.0\n\n      errorColumns = numpy.setdiff1d(self._prevPredictedColumns,\n          activeSPColumns)\n      if errorColumns.shape[0] > 0:\n        errorColumnIndexes = ( numpy.array(errorColumns, dtype=numpy.uint16) +\n          spSize )\n        classificationVector[errorColumnIndexes] = 1.0\n    else:\n      raise TypeError(\"Classification vector type must be either 'tpc' or\"\n        \" 'sp_tpe', current value is %s\" % (self.classificationVectorType))\n\n    # Store the state for next time step\n    numPredictedCols = len(self._prevPredictedColumns)\n    predictedColumns = allTPCells.nonzero()[0]\n    self._prevPredictedColumns = copy.deepcopy(predictedColumns)\n\n    if self._anomalyVectorLength is None:\n      self._anomalyVectorLength = len(classificationVector)\n\n    result = _CLAClassificationRecord(\n      ROWID=self._iteration, #__numRunCalls called\n        #at beginning of model.run\n      anomalyScore=score,\n      anomalyVector=classificationVector.nonzero()[0].tolist(),\n      anomalyLabel=[]\n    )\n    return result",
    "doc": "Construct a _HTMClassificationRecord based on the state of the model\n    passed in through the inputs.\n\n    Types for self.classificationVectorType:\n      1 - TM active cells in learn state\n      2 - SP columns concatenated with error from TM column predictions and SP"
  },
  {
    "code": "def _addRecordToKNN(self, record):\n    \"\"\"\n    Adds the record to the KNN classifier.\n    \"\"\"\n    knn = self._knnclassifier._knn\n\n    prototype_idx = self._knnclassifier.getParameter('categoryRecencyList')\n    category = self._labelListToCategoryNumber(record.anomalyLabel)\n\n    # If record is already in the classifier, overwrite its labeling\n    if record.ROWID in prototype_idx:\n      knn.prototypeSetCategory(record.ROWID, category)\n      return\n\n    # Learn this pattern in the knn\n    pattern = self._getStateAnomalyVector(record)\n    rowID = record.ROWID\n    knn.learn(pattern, category, rowID=rowID)",
    "doc": "Adds the record to the KNN classifier."
  },
  {
    "code": "def _deleteRecordsFromKNN(self, recordsToDelete):\n    \"\"\"\n    Removes the given records from the classifier.\n\n    parameters\n    ------------\n    recordsToDelete - list of records to delete from the classififier\n    \"\"\"\n    prototype_idx = self._knnclassifier.getParameter('categoryRecencyList')\n\n    idsToDelete = ([r.ROWID for r in recordsToDelete if\n      not r.setByUser and r.ROWID in prototype_idx])\n\n    nProtos = self._knnclassifier._knn._numPatterns\n    self._knnclassifier._knn.removeIds(idsToDelete)\n    assert self._knnclassifier._knn._numPatterns == nProtos - len(idsToDelete)",
    "doc": "Removes the given records from the classifier.\n\n    parameters\n    ------------\n    recordsToDelete - list of records to delete from the classififier"
  },
  {
    "code": "def _deleteRangeFromKNN(self, start=0, end=None):\n    \"\"\"\n    Removes any stored records within the range from start to\n    end. Noninclusive of end.\n\n    parameters\n    ------------\n    start - integer representing the ROWID of the start of the deletion range,\n    end - integer representing the ROWID of the end of the deletion range,\n      if None, it will default to end.\n    \"\"\"\n    prototype_idx = numpy.array(\n      self._knnclassifier.getParameter('categoryRecencyList'))\n\n    if end is None:\n      end = prototype_idx.max() + 1\n\n    idsIdxToDelete = numpy.logical_and(prototype_idx >= start,\n                                       prototype_idx < end)\n    idsToDelete = prototype_idx[idsIdxToDelete]\n\n    nProtos = self._knnclassifier._knn._numPatterns\n    self._knnclassifier._knn.removeIds(idsToDelete.tolist())\n    assert self._knnclassifier._knn._numPatterns == nProtos - len(idsToDelete)",
    "doc": "Removes any stored records within the range from start to\n    end. Noninclusive of end.\n\n    parameters\n    ------------\n    start - integer representing the ROWID of the start of the deletion range,\n    end - integer representing the ROWID of the end of the deletion range,\n      if None, it will default to end."
  },
  {
    "code": "def _recomputeRecordFromKNN(self, record):\n    \"\"\"\n    returns the classified labeling of record\n    \"\"\"\n    inputs = {\n      \"categoryIn\": [None],\n      \"bottomUpIn\": self._getStateAnomalyVector(record),\n    }\n\n    outputs = {\"categoriesOut\": numpy.zeros((1,)),\n               \"bestPrototypeIndices\":numpy.zeros((1,)),\n               \"categoryProbabilitiesOut\":numpy.zeros((1,))}\n\n    # Only use points before record to classify and after the wait period.\n    classifier_indexes = numpy.array(\n        self._knnclassifier.getParameter('categoryRecencyList'))\n    valid_idx = numpy.where(\n        (classifier_indexes >= self.getParameter('trainRecords')) &\n        (classifier_indexes < record.ROWID)\n      )[0].tolist()\n\n    if len(valid_idx) == 0:\n      return None\n\n    self._knnclassifier.setParameter('inferenceMode', None, True)\n    self._knnclassifier.setParameter('learningMode', None, False)\n    self._knnclassifier.compute(inputs, outputs)\n    self._knnclassifier.setParameter('learningMode', None, True)\n\n    classifier_distances = self._knnclassifier.getLatestDistances()\n    valid_distances = classifier_distances[valid_idx]\n    if valid_distances.min() <= self._classificationMaxDist:\n      classifier_indexes_prev = classifier_indexes[valid_idx]\n      rowID = classifier_indexes_prev[valid_distances.argmin()]\n      indexID = numpy.where(classifier_indexes == rowID)[0][0]\n      category = self._knnclassifier.getCategoryList()[indexID]\n      return category\n    return None",
    "doc": "returns the classified labeling of record"
  },
  {
    "code": "def _labelToCategoryNumber(self, label):\n    \"\"\"\n    Since the KNN Classifier stores categories as numbers, we must store each\n    label as a number. This method converts from a label to a unique number.\n    Each label is assigned a unique bit so multiple labels may be assigned to\n    a single record.\n    \"\"\"\n    if label not in self.saved_categories:\n      self.saved_categories.append(label)\n    return pow(2, self.saved_categories.index(label))",
    "doc": "Since the KNN Classifier stores categories as numbers, we must store each\n    label as a number. This method converts from a label to a unique number.\n    Each label is assigned a unique bit so multiple labels may be assigned to\n    a single record."
  },
  {
    "code": "def _labelListToCategoryNumber(self, labelList):\n    \"\"\"\n    This method takes a list of labels and returns a unique category number.\n    This enables this class to store a list of categories for each point since\n    the KNN classifier only stores a single number category for each record.\n    \"\"\"\n    categoryNumber = 0\n    for label in labelList:\n      categoryNumber += self._labelToCategoryNumber(label)\n    return categoryNumber",
    "doc": "This method takes a list of labels and returns a unique category number.\n    This enables this class to store a list of categories for each point since\n    the KNN classifier only stores a single number category for each record."
  },
  {
    "code": "def _categoryToLabelList(self, category):\n    \"\"\"\n    Converts a category number into a list of labels\n    \"\"\"\n    if category is None:\n      return []\n\n    labelList = []\n    labelNum = 0\n    while category > 0:\n      if category % 2 == 1:\n        labelList.append(self.saved_categories[labelNum])\n      labelNum += 1\n      category = category >> 1\n    return labelList",
    "doc": "Converts a category number into a list of labels"
  },
  {
    "code": "def _getStateAnomalyVector(self, state):\n    \"\"\"\n    Returns a state's anomaly vertor converting it from spare to dense\n    \"\"\"\n    vector = numpy.zeros(self._anomalyVectorLength)\n    vector[state.anomalyVector] = 1\n    return vector",
    "doc": "Returns a state's anomaly vertor converting it from spare to dense"
  },
  {
    "code": "def getLabels(self, start=None, end=None):\n    \"\"\"\n    Get the labels on classified points within range start to end. Not inclusive\n    of end.\n\n    :returns: (dict) with format:\n\n      ::\n\n        {\n          'isProcessing': boolean,\n          'recordLabels': list of results\n        }\n\n      ``isProcessing`` - currently always false as recalculation blocks; used if\n      reprocessing of records is still being performed;\n\n      Each item in ``recordLabels`` is of format:\n      \n      ::\n      \n        {\n          'ROWID': id of the row,\n          'labels': list of strings\n        }\n\n    \"\"\"\n    if len(self._recordsCache) == 0:\n      return {\n        'isProcessing': False,\n        'recordLabels': []\n      }\n    try:\n      start = int(start)\n    except Exception:\n      start = 0\n\n    try:\n      end = int(end)\n    except Exception:\n      end = self._recordsCache[-1].ROWID\n\n    if end <= start:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for 'getLabels'.\",\n                                                debugInfo={\n          'requestRange': {\n            'startRecordID': start,\n            'endRecordID': end\n          },\n          'numRecordsStored': len(self._recordsCache)\n        })\n\n    results = {\n      'isProcessing': False,\n      'recordLabels': []\n    }\n\n    ROWIDX = numpy.array(\n        self._knnclassifier.getParameter('categoryRecencyList'))\n    validIdx = numpy.where((ROWIDX >= start) & (ROWIDX < end))[0].tolist()\n    categories = self._knnclassifier.getCategoryList()\n    for idx in validIdx:\n      row = dict(\n        ROWID=int(ROWIDX[idx]),\n        labels=self._categoryToLabelList(categories[idx]))\n      results['recordLabels'].append(row)\n\n    return results",
    "doc": "Get the labels on classified points within range start to end. Not inclusive\n    of end.\n\n    :returns: (dict) with format:\n\n      ::\n\n        {\n          'isProcessing': boolean,\n          'recordLabels': list of results\n        }\n\n      ``isProcessing`` - currently always false as recalculation blocks; used if\n      reprocessing of records is still being performed;\n\n      Each item in ``recordLabels`` is of format:\n      \n      ::\n      \n        {\n          'ROWID': id of the row,\n          'labels': list of strings\n        }"
  },
  {
    "code": "def addLabel(self, start, end, labelName):\n    \"\"\"\n    Add the label labelName to each record with record ROWID in range from\n    ``start`` to ``end``, noninclusive of end.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n\n    :param start: (int) start index \n    :param end: (int) end index (noninclusive)\n    :param labelName: (string) label name\n    \"\"\"\n    if len(self._recordsCache) == 0:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for 'addLabel'. \"\n        \"Model has no saved records.\")\n\n    try:\n      start = int(start)\n    except Exception:\n      start = 0\n\n    try:\n      end = int(end)\n    except Exception:\n      end = int(self._recordsCache[-1].ROWID)\n\n    startID = self._recordsCache[0].ROWID\n\n    clippedStart = max(0, start - startID)\n    clippedEnd = max(0, min( len( self._recordsCache) , end - startID))\n\n    if clippedEnd <= clippedStart:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for 'addLabel'.\",\n                                                debugInfo={\n          'requestRange': {\n            'startRecordID': start,\n            'endRecordID': end\n          },\n          'clippedRequestRange': {\n            'startRecordID': clippedStart,\n            'endRecordID': clippedEnd\n          },\n          'validRange': {\n            'startRecordID': startID,\n            'endRecordID': self._recordsCache[len(self._recordsCache)-1].ROWID\n          },\n          'numRecordsStored': len(self._recordsCache)\n        })\n\n    # Add label to range [clippedStart, clippedEnd)\n    for state in self._recordsCache[clippedStart:clippedEnd]:\n      if labelName not in state.anomalyLabel:\n        state.anomalyLabel.append(labelName)\n        state.setByUser = True\n        self._addRecordToKNN(state)\n\n    assert len(self.saved_categories) > 0\n\n    # Recompute [end, ...)\n    for state in self._recordsCache[clippedEnd:]:\n      self._classifyState(state)",
    "doc": "Add the label labelName to each record with record ROWID in range from\n    ``start`` to ``end``, noninclusive of end.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n\n    :param start: (int) start index \n    :param end: (int) end index (noninclusive)\n    :param labelName: (string) label name"
  },
  {
    "code": "def removeLabels(self, start=None, end=None, labelFilter=None):\n    \"\"\"\n    Remove labels from each record with record ROWID in range from\n    ``start`` to ``end``, noninclusive of end. Removes all records if \n    ``labelFilter`` is None, otherwise only removes the labels equal to \n    ``labelFilter``.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n    \n    :param start: (int) start index \n    :param end: (int) end index (noninclusive)\n    :param labelFilter: (string) label filter\n    \"\"\"\n    if len(self._recordsCache) == 0:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for \"\n        \"'removeLabels'. Model has no saved records.\")\n\n    try:\n      start = int(start)\n    except Exception:\n      start = 0\n\n    try:\n      end = int(end)\n    except Exception:\n      end = self._recordsCache[-1].ROWID\n\n    startID = self._recordsCache[0].ROWID\n\n    clippedStart = 0 if start is None else max(0, start - startID)\n    clippedEnd = len(self._recordsCache) if end is None else \\\n      max(0, min( len( self._recordsCache) , end - startID))\n\n    if clippedEnd <= clippedStart:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for \"\n        \"'removeLabels'.\", debugInfo={\n          'requestRange': {\n            'startRecordID': start,\n            'endRecordID': end\n          },\n          'clippedRequestRange': {\n            'startRecordID': clippedStart,\n            'endRecordID': clippedEnd\n          },\n          'validRange': {\n            'startRecordID': startID,\n            'endRecordID': self._recordsCache[len(self._recordsCache)-1].ROWID\n          },\n          'numRecordsStored': len(self._recordsCache)\n        })\n\n    # Remove records within the cache\n    recordsToDelete = []\n    for state in self._recordsCache[clippedStart:clippedEnd]:\n      if labelFilter is not None:\n        if labelFilter in state.anomalyLabel:\n          state.anomalyLabel.remove(labelFilter)\n      else:\n        state.anomalyLabel = []\n      state.setByUser = False\n      recordsToDelete.append(state)\n    self._deleteRecordsFromKNN(recordsToDelete)\n\n    # Remove records not in cache\n    self._deleteRangeFromKNN(start, end)\n\n    # Recompute [clippedEnd, ...)\n    for state in self._recordsCache[clippedEnd:]:\n      self._classifyState(state)",
    "doc": "Remove labels from each record with record ROWID in range from\n    ``start`` to ``end``, noninclusive of end. Removes all records if \n    ``labelFilter`` is None, otherwise only removes the labels equal to \n    ``labelFilter``.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n    \n    :param start: (int) start index \n    :param end: (int) end index (noninclusive)\n    :param labelFilter: (string) label filter"
  },
  {
    "code": "def match(self, record):\n    '''\n    Returns True if the record matches any of the provided filters\n    '''\n\n    for field, meta in self.filterDict.iteritems():\n      index = meta['index']\n      categories = meta['categories']\n      for category in categories:\n        # Record might be blank, handle this\n        if not record:\n          continue\n        if record[index].find(category) != -1:\n          '''\n          This field contains the string we're searching for\n          so we'll keep the records\n          '''\n          return True\n\n    # None of the categories were found in this record\n    return False",
    "doc": "Returns True if the record matches any of the provided filters"
  },
  {
    "code": "def replace(self, columnIndex, bitmap):\n    \"\"\" Wraps replaceSparseRow()\"\"\"\n    return super(_SparseMatrixCorticalColumnAdapter, self).replaceSparseRow(\n      columnIndex, bitmap\n    )",
    "doc": "Wraps replaceSparseRow()"
  },
  {
    "code": "def update(self, columnIndex, vector):\n    \"\"\" Wraps setRowFromDense()\"\"\"\n    return super(_SparseMatrixCorticalColumnAdapter, self).setRowFromDense(\n      columnIndex, vector\n    )",
    "doc": "Wraps setRowFromDense()"
  },
  {
    "code": "def setLocalAreaDensity(self, localAreaDensity):\n    \"\"\"\n    Sets the local area density. Invalidates the 'numActiveColumnsPerInhArea'\n    parameter\n    \n    :param localAreaDensity: (float) value to set\n    \"\"\"\n    assert(localAreaDensity > 0 and localAreaDensity <= 1)\n    self._localAreaDensity = localAreaDensity\n    self._numActiveColumnsPerInhArea = 0",
    "doc": "Sets the local area density. Invalidates the 'numActiveColumnsPerInhArea'\n    parameter\n    \n    :param localAreaDensity: (float) value to set"
  },
  {
    "code": "def getPotential(self, columnIndex, potential):\n    \"\"\"\n    :param columnIndex: (int) column index to get potential for.\n    :param potential: (list) will be overwritten with column potentials. Must \n           match the number of inputs.\n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    potential[:] = self._potentialPools[columnIndex]",
    "doc": ":param columnIndex: (int) column index to get potential for.\n    :param potential: (list) will be overwritten with column potentials. Must \n           match the number of inputs."
  },
  {
    "code": "def setPotential(self, columnIndex, potential):\n    \"\"\"\n    Sets the potential mapping for a given column. ``potential`` size must match \n    the number of inputs, and must be greater than ``stimulusThreshold``.\n    \n    :param columnIndex: (int) column index to set potential for.\n    :param potential: (list) value to set.\n    \"\"\"\n    assert(columnIndex < self._numColumns)\n\n    potentialSparse = numpy.where(potential > 0)[0]\n    if len(potentialSparse) < self._stimulusThreshold:\n      raise Exception(\"This is likely due to a \" +\n      \"value of stimulusThreshold that is too large relative \" +\n      \"to the input size.\")\n\n    self._potentialPools.replace(columnIndex, potentialSparse)",
    "doc": "Sets the potential mapping for a given column. ``potential`` size must match \n    the number of inputs, and must be greater than ``stimulusThreshold``.\n    \n    :param columnIndex: (int) column index to set potential for.\n    :param potential: (list) value to set."
  },
  {
    "code": "def getPermanence(self, columnIndex, permanence):\n    \"\"\"\n    Returns the permanence values for a given column. ``permanence`` size\n    must match the number of inputs.\n    \n    :param columnIndex: (int) column index to get permanence for.\n    :param permanence: (list) will be overwritten with permanences. \n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    permanence[:] = self._permanences[columnIndex]",
    "doc": "Returns the permanence values for a given column. ``permanence`` size\n    must match the number of inputs.\n    \n    :param columnIndex: (int) column index to get permanence for.\n    :param permanence: (list) will be overwritten with permanences."
  },
  {
    "code": "def setPermanence(self, columnIndex, permanence):\n    \"\"\"\n    Sets the permanence values for a given column. ``permanence`` size must \n    match the number of inputs.\n    \n    :param columnIndex: (int) column index to set permanence for.\n    :param permanence: (list) value to set. \n    \"\"\"\n    assert(columnIndex < self._numColumns)\n    self._updatePermanencesForColumn(permanence, columnIndex, raisePerm=False)",
    "doc": "Sets the permanence values for a given column. ``permanence`` size must \n    match the number of inputs.\n    \n    :param columnIndex: (int) column index to set permanence for.\n    :param permanence: (list) value to set."
  },
  {
    "code": "def getConnectedSynapses(self, columnIndex, connectedSynapses):\n    \"\"\"\n    :param connectedSynapses: (list) will be overwritten\n    :returns: (iter) the connected synapses for a given column.\n              ``connectedSynapses`` size must match the number of inputs\"\"\"\n    assert(columnIndex < self._numColumns)\n    connectedSynapses[:] = self._connectedSynapses[columnIndex]",
    "doc": ":param connectedSynapses: (list) will be overwritten\n    :returns: (iter) the connected synapses for a given column.\n              ``connectedSynapses`` size must match the number of inputs"
  },
  {
    "code": "def stripUnlearnedColumns(self, activeArray):\n    \"\"\"\n    Removes the set of columns who have never been active from the set of\n    active columns selected in the inhibition round. Such columns cannot\n    represent learned pattern and are therefore meaningless if only inference\n    is required. This should not be done when using a random, unlearned SP\n    since you would end up with no active columns.\n\n    :param activeArray: An array whose size is equal to the number of columns.\n        Any columns marked as active with an activeDutyCycle of 0 have\n        never been activated before and therefore are not active due to\n        learning. Any of these (unlearned) columns will be disabled (set to 0).\n    \"\"\"\n    neverLearned = numpy.where(self._activeDutyCycles == 0)[0]\n    activeArray[neverLearned] = 0",
    "doc": "Removes the set of columns who have never been active from the set of\n    active columns selected in the inhibition round. Such columns cannot\n    represent learned pattern and are therefore meaningless if only inference\n    is required. This should not be done when using a random, unlearned SP\n    since you would end up with no active columns.\n\n    :param activeArray: An array whose size is equal to the number of columns.\n        Any columns marked as active with an activeDutyCycle of 0 have\n        never been activated before and therefore are not active due to\n        learning. Any of these (unlearned) columns will be disabled (set to 0)."
  },
  {
    "code": "def _updateMinDutyCycles(self):\n    \"\"\"\n    Updates the minimum duty cycles defining normal activity for a column. A\n    column with activity duty cycle below this minimum threshold is boosted.\n    \"\"\"\n    if self._globalInhibition or self._inhibitionRadius > self._numInputs:\n      self._updateMinDutyCyclesGlobal()\n    else:\n      self._updateMinDutyCyclesLocal()",
    "doc": "Updates the minimum duty cycles defining normal activity for a column. A\n    column with activity duty cycle below this minimum threshold is boosted."
  },
  {
    "code": "def _updateMinDutyCyclesGlobal(self):\n    \"\"\"\n    Updates the minimum duty cycles in a global fashion. Sets the minimum duty\n    cycles for the overlap all columns to be a percent of the maximum in the\n    region, specified by minPctOverlapDutyCycle. Functionality it is equivalent\n    to _updateMinDutyCyclesLocal, but this function exploits the globality of\n    the computation to perform it in a straightforward, and efficient manner.\n    \"\"\"\n    self._minOverlapDutyCycles.fill(\n        self._minPctOverlapDutyCycles * self._overlapDutyCycles.max()\n      )",
    "doc": "Updates the minimum duty cycles in a global fashion. Sets the minimum duty\n    cycles for the overlap all columns to be a percent of the maximum in the\n    region, specified by minPctOverlapDutyCycle. Functionality it is equivalent\n    to _updateMinDutyCyclesLocal, but this function exploits the globality of\n    the computation to perform it in a straightforward, and efficient manner."
  },
  {
    "code": "def _updateMinDutyCyclesLocal(self):\n    \"\"\"\n    Updates the minimum duty cycles. The minimum duty cycles are determined\n    locally. Each column's minimum duty cycles are set to be a percent of the\n    maximum duty cycles in the column's neighborhood. Unlike\n    _updateMinDutyCyclesGlobal, here the values can be quite different for\n    different columns.\n    \"\"\"\n    for column in xrange(self._numColumns):\n      neighborhood = self._getColumnNeighborhood(column)\n\n      maxActiveDuty = self._activeDutyCycles[neighborhood].max()\n      maxOverlapDuty = self._overlapDutyCycles[neighborhood].max()\n\n      self._minOverlapDutyCycles[column] = (maxOverlapDuty *\n                                            self._minPctOverlapDutyCycles)",
    "doc": "Updates the minimum duty cycles. The minimum duty cycles are determined\n    locally. Each column's minimum duty cycles are set to be a percent of the\n    maximum duty cycles in the column's neighborhood. Unlike\n    _updateMinDutyCyclesGlobal, here the values can be quite different for\n    different columns."
  },
  {
    "code": "def _updateDutyCycles(self, overlaps, activeColumns):\n    \"\"\"\n    Updates the duty cycles for each column. The OVERLAP duty cycle is a moving\n    average of the number of inputs which overlapped with the each column. The\n    ACTIVITY duty cycles is a moving average of the frequency of activation for\n    each column.\n\n    Parameters:\n    ----------------------------\n    :param overlaps:\n                    An array containing the overlap score for each column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param activeColumns:\n                    An array containing the indices of the active columns,\n                    the sparse set of columns which survived inhibition\n    \"\"\"\n    overlapArray = numpy.zeros(self._numColumns, dtype=realDType)\n    activeArray = numpy.zeros(self._numColumns, dtype=realDType)\n    overlapArray[overlaps > 0] = 1\n    activeArray[activeColumns] = 1\n\n    period = self._dutyCyclePeriod\n    if (period > self._iterationNum):\n      period = self._iterationNum\n\n    self._overlapDutyCycles = self._updateDutyCyclesHelper(\n                                self._overlapDutyCycles,\n                                overlapArray,\n                                period\n                              )\n\n    self._activeDutyCycles = self._updateDutyCyclesHelper(\n                                self._activeDutyCycles,\n                                activeArray,\n                                period\n                              )",
    "doc": "Updates the duty cycles for each column. The OVERLAP duty cycle is a moving\n    average of the number of inputs which overlapped with the each column. The\n    ACTIVITY duty cycles is a moving average of the frequency of activation for\n    each column.\n\n    Parameters:\n    ----------------------------\n    :param overlaps:\n                    An array containing the overlap score for each column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param activeColumns:\n                    An array containing the indices of the active columns,\n                    the sparse set of columns which survived inhibition"
  },
  {
    "code": "def _updateInhibitionRadius(self):\n    \"\"\"\n    Update the inhibition radius. The inhibition radius is a measure of the\n    square (or hypersquare) of columns that each a column is \"connected to\"\n    on average. Since columns are are not connected to each other directly, we\n    determine this quantity by first figuring out how many *inputs* a column is\n    connected to, and then multiplying it by the total number of columns that\n    exist for each input. For multiple dimension the aforementioned\n    calculations are averaged over all dimensions of inputs and columns. This\n    value is meaningless if global inhibition is enabled.\n    \"\"\"\n    if self._globalInhibition:\n      self._inhibitionRadius = int(self._columnDimensions.max())\n      return\n\n    avgConnectedSpan = numpy.average(\n                          [self._avgConnectedSpanForColumnND(i)\n                          for i in xrange(self._numColumns)]\n                        )\n    columnsPerInput = self._avgColumnsPerInput()\n    diameter = avgConnectedSpan * columnsPerInput\n    radius = (diameter - 1) / 2.0\n    radius = max(1.0, radius)\n    self._inhibitionRadius = int(radius + 0.5)",
    "doc": "Update the inhibition radius. The inhibition radius is a measure of the\n    square (or hypersquare) of columns that each a column is \"connected to\"\n    on average. Since columns are are not connected to each other directly, we\n    determine this quantity by first figuring out how many *inputs* a column is\n    connected to, and then multiplying it by the total number of columns that\n    exist for each input. For multiple dimension the aforementioned\n    calculations are averaged over all dimensions of inputs and columns. This\n    value is meaningless if global inhibition is enabled."
  },
  {
    "code": "def _avgColumnsPerInput(self):\n    \"\"\"\n    The average number of columns per input, taking into account the topology\n    of the inputs and columns. This value is used to calculate the inhibition\n    radius. This function supports an arbitrary number of dimensions. If the\n    number of column dimensions does not match the number of input dimensions,\n    we treat the missing, or phantom dimensions as 'ones'.\n    \"\"\"\n    #TODO: extend to support different number of dimensions for inputs and\n    # columns\n    numDim = max(self._columnDimensions.size, self._inputDimensions.size)\n    colDim = numpy.ones(numDim)\n    colDim[:self._columnDimensions.size] = self._columnDimensions\n\n    inputDim = numpy.ones(numDim)\n    inputDim[:self._inputDimensions.size] = self._inputDimensions\n\n    columnsPerInput = colDim.astype(realDType) / inputDim\n    return numpy.average(columnsPerInput)",
    "doc": "The average number of columns per input, taking into account the topology\n    of the inputs and columns. This value is used to calculate the inhibition\n    radius. This function supports an arbitrary number of dimensions. If the\n    number of column dimensions does not match the number of input dimensions,\n    we treat the missing, or phantom dimensions as 'ones'."
  },
  {
    "code": "def _avgConnectedSpanForColumn1D(self, columnIndex):\n    \"\"\"\n    The range of connected synapses for column. This is used to\n    calculate the inhibition radius. This variation of the function only\n    supports a 1 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices\n    \"\"\"\n    assert(self._inputDimensions.size == 1)\n    connected = self._connectedSynapses[columnIndex].nonzero()[0]\n    if connected.size == 0:\n      return 0\n    else:\n      return max(connected) - min(connected) + 1",
    "doc": "The range of connected synapses for column. This is used to\n    calculate the inhibition radius. This variation of the function only\n    supports a 1 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices"
  },
  {
    "code": "def _avgConnectedSpanForColumn2D(self, columnIndex):\n    \"\"\"\n    The range of connectedSynapses per column, averaged for each dimension.\n    This value is used to calculate the inhibition radius. This variation of\n    the  function only supports a 2 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices\n    \"\"\"\n    assert(self._inputDimensions.size == 2)\n    connected = self._connectedSynapses[columnIndex]\n    (rows, cols) = connected.reshape(self._inputDimensions).nonzero()\n    if  rows.size == 0 and cols.size == 0:\n      return 0\n    rowSpan = rows.max() - rows.min() + 1\n    colSpan = cols.max() - cols.min() + 1\n    return numpy.average([rowSpan, colSpan])",
    "doc": "The range of connectedSynapses per column, averaged for each dimension.\n    This value is used to calculate the inhibition radius. This variation of\n    the  function only supports a 2 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    :param columnIndex:   The index identifying a column in the permanence,\n                          potential and connectivity matrices"
  },
  {
    "code": "def _bumpUpWeakColumns(self):\n    \"\"\"\n    This method increases the permanence values of synapses of columns whose\n    activity level has been too low. Such columns are identified by having an\n    overlap duty cycle that drops too much below those of their peers. The\n    permanence values for such columns are increased.\n    \"\"\"\n    weakColumns = numpy.where(self._overlapDutyCycles\n                                < self._minOverlapDutyCycles)[0]\n    for columnIndex in weakColumns:\n      perm = self._permanences[columnIndex].astype(realDType)\n      maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n      perm[maskPotential] += self._synPermBelowStimulusInc\n      self._updatePermanencesForColumn(perm, columnIndex, raisePerm=False)",
    "doc": "This method increases the permanence values of synapses of columns whose\n    activity level has been too low. Such columns are identified by having an\n    overlap duty cycle that drops too much below those of their peers. The\n    permanence values for such columns are increased."
  },
  {
    "code": "def _raisePermanenceToThreshold(self, perm, mask):\n    \"\"\"\n    This method ensures that each column has enough connections to input bits\n    to allow it to become active. Since a column must have at least\n    'self._stimulusThreshold' overlaps in order to be considered during the\n    inhibition phase, columns without such minimal number of connections, even\n    if all the input bits they are connected to turn on, have no chance of\n    obtaining the minimum threshold. For such columns, the permanence values\n    are increased until the minimum number of connections are formed.\n\n\n    Parameters:\n    ----------------------------\n    :param perm:    An array of permanence values for a column. The array is\n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    :param mask:    the indices of the columns whose permanences need to be\n                    raised.\n    \"\"\"\n    if len(mask) < self._stimulusThreshold:\n      raise Exception(\"This is likely due to a \" +\n      \"value of stimulusThreshold that is too large relative \" +\n      \"to the input size. [len(mask) < self._stimulusThreshold]\")\n\n    numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n    while True:\n      numConnected = numpy.nonzero(\n        perm > self._synPermConnected - PERMANENCE_EPSILON)[0].size\n\n      if numConnected >= self._stimulusThreshold:\n        return\n      perm[mask] += self._synPermBelowStimulusInc",
    "doc": "This method ensures that each column has enough connections to input bits\n    to allow it to become active. Since a column must have at least\n    'self._stimulusThreshold' overlaps in order to be considered during the\n    inhibition phase, columns without such minimal number of connections, even\n    if all the input bits they are connected to turn on, have no chance of\n    obtaining the minimum threshold. For such columns, the permanence values\n    are increased until the minimum number of connections are formed.\n\n\n    Parameters:\n    ----------------------------\n    :param perm:    An array of permanence values for a column. The array is\n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    :param mask:    the indices of the columns whose permanences need to be\n                    raised."
  },
  {
    "code": "def _updatePermanencesForColumn(self, perm, columnIndex, raisePerm=True):\n    \"\"\"\n    This method updates the permanence matrix with a column's new permanence\n    values. The column is identified by its index, which reflects the row in\n    the matrix, and the permanence is given in 'dense' form, i.e. a full\n    array containing all the zeros as well as the non-zero values. It is in\n    charge of implementing 'clipping' - ensuring that the permanence values are\n    always between 0 and 1 - and 'trimming' - enforcing sparsity by zeroing out\n    all permanence values below '_synPermTrimThreshold'. It also maintains\n    the consistency between 'self._permanences' (the matrix storing the\n    permanence values), 'self._connectedSynapses', (the matrix storing the bits\n    each column is connected to), and 'self._connectedCounts' (an array storing\n    the number of input bits each column is connected to). Every method wishing\n    to modify the permanence matrix should do so through this method.\n\n    Parameters:\n    ----------------------------\n    :param perm:    An array of permanence values for a column. The array is\n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices\n    :param raisePerm: A boolean value indicating whether the permanence values\n                    should be raised until a minimum number are synapses are in\n                    a connected state. Should be set to 'false' when a direct\n                    assignment is required.\n    \"\"\"\n    maskPotential = numpy.where(self._potentialPools[columnIndex] > 0)[0]\n    if raisePerm:\n      self._raisePermanenceToThreshold(perm, maskPotential)\n    perm[perm < self._synPermTrimThreshold] = 0\n    numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n    newConnected = numpy.where(perm >=\n                               self._synPermConnected - PERMANENCE_EPSILON)[0]\n    self._permanences.update(columnIndex, perm)\n    self._connectedSynapses.replace(columnIndex, newConnected)\n    self._connectedCounts[columnIndex] = newConnected.size",
    "doc": "This method updates the permanence matrix with a column's new permanence\n    values. The column is identified by its index, which reflects the row in\n    the matrix, and the permanence is given in 'dense' form, i.e. a full\n    array containing all the zeros as well as the non-zero values. It is in\n    charge of implementing 'clipping' - ensuring that the permanence values are\n    always between 0 and 1 - and 'trimming' - enforcing sparsity by zeroing out\n    all permanence values below '_synPermTrimThreshold'. It also maintains\n    the consistency between 'self._permanences' (the matrix storing the\n    permanence values), 'self._connectedSynapses', (the matrix storing the bits\n    each column is connected to), and 'self._connectedCounts' (an array storing\n    the number of input bits each column is connected to). Every method wishing\n    to modify the permanence matrix should do so through this method.\n\n    Parameters:\n    ----------------------------\n    :param perm:    An array of permanence values for a column. The array is\n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices\n    :param raisePerm: A boolean value indicating whether the permanence values\n                    should be raised until a minimum number are synapses are in\n                    a connected state. Should be set to 'false' when a direct\n                    assignment is required."
  },
  {
    "code": "def _initPermConnected(self):\n    \"\"\"\n    Returns a randomly generated permanence value for a synapses that is\n    initialized in a connected state. The basic idea here is to initialize\n    permanence values very close to synPermConnected so that a small number of\n    learning steps could make it disconnected or connected.\n\n    Note: experimentation was done a long time ago on the best way to initialize\n    permanence values, but the history for this particular scheme has been lost.\n    \"\"\"\n    p = self._synPermConnected + (\n        self._synPermMax - self._synPermConnected)*self._random.getReal64()\n\n    # Ensure we don't have too much unnecessary precision. A full 64 bits of\n    # precision causes numerical stability issues across platforms and across\n    # implementations\n    p = int(p*100000) / 100000.0\n    return p",
    "doc": "Returns a randomly generated permanence value for a synapses that is\n    initialized in a connected state. The basic idea here is to initialize\n    permanence values very close to synPermConnected so that a small number of\n    learning steps could make it disconnected or connected.\n\n    Note: experimentation was done a long time ago on the best way to initialize\n    permanence values, but the history for this particular scheme has been lost."
  },
  {
    "code": "def _initPermNonConnected(self):\n    \"\"\"\n    Returns a randomly generated permanence value for a synapses that is to be\n    initialized in a non-connected state.\n    \"\"\"\n    p = self._synPermConnected * self._random.getReal64()\n\n    # Ensure we don't have too much unnecessary precision. A full 64 bits of\n    # precision causes numerical stability issues across platforms and across\n    # implementations\n    p = int(p*100000) / 100000.0\n    return p",
    "doc": "Returns a randomly generated permanence value for a synapses that is to be\n    initialized in a non-connected state."
  },
  {
    "code": "def _initPermanence(self, potential, connectedPct):\n    \"\"\"\n    Initializes the permanences of a column. The method\n    returns a 1-D array the size of the input, where each entry in the\n    array represents the initial permanence value between the input bit\n    at the particular index in the array, and the column represented by\n    the 'index' parameter.\n\n    Parameters:\n    ----------------------------\n    :param potential: A numpy array specifying the potential pool of the column.\n                    Permanence values will only be generated for input bits\n                    corresponding to indices for which the mask value is 1.\n    :param connectedPct: A value between 0 or 1 governing the chance, for each\n                         permanence, that the initial permanence value will\n                         be a value that is considered connected.\n    \"\"\"\n    # Determine which inputs bits will start out as connected\n    # to the inputs. Initially a subset of the input bits in a\n    # column's potential pool will be connected. This number is\n    # given by the parameter \"connectedPct\"\n    perm = numpy.zeros(self._numInputs, dtype=realDType)\n    for i in xrange(self._numInputs):\n      if (potential[i] < 1):\n        continue\n\n      if (self._random.getReal64() <= connectedPct):\n        perm[i] = self._initPermConnected()\n      else:\n        perm[i] = self._initPermNonConnected()\n\n    # Clip off low values. Since we use a sparse representation\n    # to store the permanence values this helps reduce memory\n    # requirements.\n    perm[perm < self._synPermTrimThreshold] = 0\n\n    return perm",
    "doc": "Initializes the permanences of a column. The method\n    returns a 1-D array the size of the input, where each entry in the\n    array represents the initial permanence value between the input bit\n    at the particular index in the array, and the column represented by\n    the 'index' parameter.\n\n    Parameters:\n    ----------------------------\n    :param potential: A numpy array specifying the potential pool of the column.\n                    Permanence values will only be generated for input bits\n                    corresponding to indices for which the mask value is 1.\n    :param connectedPct: A value between 0 or 1 governing the chance, for each\n                         permanence, that the initial permanence value will\n                         be a value that is considered connected."
  },
  {
    "code": "def _mapColumn(self, index):\n    \"\"\"\n    Maps a column to its respective input index, keeping to the topology of\n    the region. It takes the index of the column as an argument and determines\n    what is the index of the flattened input vector that is to be the center of\n    the column's potential pool. It distributes the columns over the inputs\n    uniformly. The return value is an integer representing the index of the\n    input bit. Examples of the expected output of this method:\n    * If the topology is one dimensional, and the column index is 0, this\n      method will return the input index 0. If the column index is 1, and there\n      are 3 columns over 7 inputs, this method will return the input index 3.\n    * If the topology is two dimensional, with column dimensions [3, 5] and\n      input dimensions [7, 11], and the column index is 3, the method\n      returns input index 8.\n\n    Parameters:\n    ----------------------------\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices.\n    :param wrapAround: A boolean value indicating that boundaries should be\n                    ignored.\n    \"\"\"\n    columnCoords = numpy.unravel_index(index, self._columnDimensions)\n    columnCoords = numpy.array(columnCoords, dtype=realDType)\n    ratios = columnCoords / self._columnDimensions\n    inputCoords = self._inputDimensions * ratios\n    inputCoords += 0.5 * self._inputDimensions / self._columnDimensions\n    inputCoords = inputCoords.astype(int)\n    inputIndex = numpy.ravel_multi_index(inputCoords, self._inputDimensions)\n    return inputIndex",
    "doc": "Maps a column to its respective input index, keeping to the topology of\n    the region. It takes the index of the column as an argument and determines\n    what is the index of the flattened input vector that is to be the center of\n    the column's potential pool. It distributes the columns over the inputs\n    uniformly. The return value is an integer representing the index of the\n    input bit. Examples of the expected output of this method:\n    * If the topology is one dimensional, and the column index is 0, this\n      method will return the input index 0. If the column index is 1, and there\n      are 3 columns over 7 inputs, this method will return the input index 3.\n    * If the topology is two dimensional, with column dimensions [3, 5] and\n      input dimensions [7, 11], and the column index is 3, the method\n      returns input index 8.\n\n    Parameters:\n    ----------------------------\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices.\n    :param wrapAround: A boolean value indicating that boundaries should be\n                    ignored."
  },
  {
    "code": "def _mapPotential(self, index):\n    \"\"\"\n    Maps a column to its input bits. This method encapsulates the topology of\n    the region. It takes the index of the column as an argument and determines\n    what are the indices of the input vector that are located within the\n    column's potential pool. The return value is a list containing the indices\n    of the input bits. The current implementation of the base class only\n    supports a 1 dimensional topology of columns with a 1 dimensional topology\n    of inputs. To extend this class to support 2-D topology you will need to\n    override this method. Examples of the expected output of this method:\n    * If the potentialRadius is greater than or equal to the largest input\n      dimension then each column connects to all of the inputs.\n    * If the topology is one dimensional, the input space is divided up evenly\n      among the columns and each column is centered over its share of the\n      inputs.  If the potentialRadius is 5, then each column connects to the\n      input it is centered above as well as the 5 inputs to the left of that\n      input and the five inputs to the right of that input, wrapping around if\n      wrapAround=True.\n    * If the topology is two dimensional, the input space is again divided up\n      evenly among the columns and each column is centered above its share of\n      the inputs.  If the potentialRadius is 5, the column connects to a square\n      that has 11 inputs on a side and is centered on the input that the column\n      is centered above.\n\n    Parameters:\n    ----------------------------\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices.\n    \"\"\"\n\n    centerInput = self._mapColumn(index)\n    columnInputs = self._getInputNeighborhood(centerInput).astype(uintType)\n\n    # Select a subset of the receptive field to serve as the\n    # the potential pool\n    numPotential = int(columnInputs.size * self._potentialPct + 0.5)\n    selectedInputs = numpy.empty(numPotential, dtype=uintType)\n    self._random.sample(columnInputs, selectedInputs)\n\n    potential = numpy.zeros(self._numInputs, dtype=uintType)\n    potential[selectedInputs] = 1\n\n    return potential",
    "doc": "Maps a column to its input bits. This method encapsulates the topology of\n    the region. It takes the index of the column as an argument and determines\n    what are the indices of the input vector that are located within the\n    column's potential pool. The return value is a list containing the indices\n    of the input bits. The current implementation of the base class only\n    supports a 1 dimensional topology of columns with a 1 dimensional topology\n    of inputs. To extend this class to support 2-D topology you will need to\n    override this method. Examples of the expected output of this method:\n    * If the potentialRadius is greater than or equal to the largest input\n      dimension then each column connects to all of the inputs.\n    * If the topology is one dimensional, the input space is divided up evenly\n      among the columns and each column is centered over its share of the\n      inputs.  If the potentialRadius is 5, then each column connects to the\n      input it is centered above as well as the 5 inputs to the left of that\n      input and the five inputs to the right of that input, wrapping around if\n      wrapAround=True.\n    * If the topology is two dimensional, the input space is again divided up\n      evenly among the columns and each column is centered above its share of\n      the inputs.  If the potentialRadius is 5, the column connects to a square\n      that has 11 inputs on a side and is centered on the input that the column\n      is centered above.\n\n    Parameters:\n    ----------------------------\n    :param index:   The index identifying a column in the permanence, potential\n                    and connectivity matrices."
  },
  {
    "code": "def _updateBoostFactorsGlobal(self):\n    \"\"\"\n    Update boost factors when global inhibition is used\n    \"\"\"\n    # When global inhibition is enabled, the target activation level is\n    # the sparsity of the spatial pooler\n    if (self._localAreaDensity > 0):\n      targetDensity = self._localAreaDensity\n    else:\n      inhibitionArea = ((2 * self._inhibitionRadius + 1)\n                        ** self._columnDimensions.size)\n      inhibitionArea = min(self._numColumns, inhibitionArea)\n      targetDensity = float(self._numActiveColumnsPerInhArea) / inhibitionArea\n      targetDensity = min(targetDensity, 0.5)\n\n    self._boostFactors = numpy.exp(\n      (targetDensity - self._activeDutyCycles) * self._boostStrength)",
    "doc": "Update boost factors when global inhibition is used"
  },
  {
    "code": "def _updateBoostFactorsLocal(self):\n    \"\"\"\n    Update boost factors when local inhibition is used\n    \"\"\"\n    # Determine the target activation level for each column\n    # The targetDensity is the average activeDutyCycles of the neighboring\n    # columns of each column.\n    targetDensity = numpy.zeros(self._numColumns, dtype=realDType)\n    for i in xrange(self._numColumns):\n      maskNeighbors = self._getColumnNeighborhood(i)\n      targetDensity[i] = numpy.mean(self._activeDutyCycles[maskNeighbors])\n\n    self._boostFactors = numpy.exp(\n      (targetDensity - self._activeDutyCycles) * self._boostStrength)",
    "doc": "Update boost factors when local inhibition is used"
  },
  {
    "code": "def _calculateOverlap(self, inputVector):\n    \"\"\"\n    This function determines each column's overlap with the current input\n    vector. The overlap of a column is the number of synapses for that column\n    that are connected (permanence value is greater than '_synPermConnected')\n    to input bits which are turned on. The implementation takes advantage of\n    the SparseBinaryMatrix class to perform this calculation efficiently.\n\n    Parameters:\n    ----------------------------\n    :param inputVector: a numpy array of 0's and 1's that comprises the input to\n                    the spatial pooler.\n    \"\"\"\n    overlaps = numpy.zeros(self._numColumns, dtype=realDType)\n    self._connectedSynapses.rightVecSumAtNZ_fast(inputVector.astype(realDType),\n                                                 overlaps)\n    return overlaps",
    "doc": "This function determines each column's overlap with the current input\n    vector. The overlap of a column is the number of synapses for that column\n    that are connected (permanence value is greater than '_synPermConnected')\n    to input bits which are turned on. The implementation takes advantage of\n    the SparseBinaryMatrix class to perform this calculation efficiently.\n\n    Parameters:\n    ----------------------------\n    :param inputVector: a numpy array of 0's and 1's that comprises the input to\n                    the spatial pooler."
  },
  {
    "code": "def _inhibitColumns(self, overlaps):\n    \"\"\"\n    Performs inhibition. This method calculates the necessary values needed to\n    actually perform inhibition and then delegates the task of picking the\n    active columns to helper functions.\n\n    Parameters:\n    ----------------------------\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    \"\"\"\n    # determine how many columns should be selected in the inhibition phase.\n    # This can be specified by either setting the 'numActiveColumnsPerInhArea'\n    # parameter or the 'localAreaDensity' parameter when initializing the class\n    if (self._localAreaDensity > 0):\n      density = self._localAreaDensity\n    else:\n      inhibitionArea = ((2*self._inhibitionRadius + 1)\n                                    ** self._columnDimensions.size)\n      inhibitionArea = min(self._numColumns, inhibitionArea)\n      density = float(self._numActiveColumnsPerInhArea) / inhibitionArea\n      density = min(density, 0.5)\n\n    if self._globalInhibition or \\\n      self._inhibitionRadius > max(self._columnDimensions):\n      return self._inhibitColumnsGlobal(overlaps, density)\n    else:\n      return self._inhibitColumnsLocal(overlaps, density)",
    "doc": "Performs inhibition. This method calculates the necessary values needed to\n    actually perform inhibition and then delegates the task of picking the\n    active columns to helper functions.\n\n    Parameters:\n    ----------------------------\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on."
  },
  {
    "code": "def _inhibitColumnsGlobal(self, overlaps, density):\n    \"\"\"\n    Perform global inhibition. Performing global inhibition entails picking the\n    top 'numActive' columns with the highest overlap score in the entire\n    region. At most half of the columns in a local neighborhood are allowed to\n    be active. Columns with an overlap score below the 'stimulusThreshold' are\n    always inhibited.\n\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param density: The fraction of columns to survive inhibition.\n    @return list with indices of the winning columns\n    \"\"\"\n    #calculate num active per inhibition area\n    numActive = int(density * self._numColumns)\n\n    # Calculate winners using stable sort algorithm (mergesort)\n    # for compatibility with C++\n    sortedWinnerIndices = numpy.argsort(overlaps, kind='mergesort')\n\n    # Enforce the stimulus threshold\n    start = len(sortedWinnerIndices) - numActive\n    while start < len(sortedWinnerIndices):\n      i = sortedWinnerIndices[start]\n      if overlaps[i] >= self._stimulusThreshold:\n        break\n      else:\n        start += 1\n\n    return sortedWinnerIndices[start:][::-1]",
    "doc": "Perform global inhibition. Performing global inhibition entails picking the\n    top 'numActive' columns with the highest overlap score in the entire\n    region. At most half of the columns in a local neighborhood are allowed to\n    be active. Columns with an overlap score below the 'stimulusThreshold' are\n    always inhibited.\n\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param density: The fraction of columns to survive inhibition.\n    @return list with indices of the winning columns"
  },
  {
    "code": "def _inhibitColumnsLocal(self, overlaps, density):\n    \"\"\"\n    Performs local inhibition. Local inhibition is performed on a column by\n    column basis. Each column observes the overlaps of its neighbors and is\n    selected if its overlap score is within the top 'numActive' in its local\n    neighborhood. At most half of the columns in a local neighborhood are\n    allowed to be active. Columns with an overlap score below the\n    'stimulusThreshold' are always inhibited.\n\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param density: The fraction of columns to survive inhibition. This\n                    value is only an intended target. Since the surviving\n                    columns are picked in a local fashion, the exact fraction\n                    of surviving columns is likely to vary.\n    @return list with indices of the winning columns\n    \"\"\"\n\n    activeArray = numpy.zeros(self._numColumns, dtype=\"bool\")\n\n    for column, overlap in enumerate(overlaps):\n      if overlap >= self._stimulusThreshold:\n        neighborhood = self._getColumnNeighborhood(column)\n        neighborhoodOverlaps = overlaps[neighborhood]\n\n        numBigger = numpy.count_nonzero(neighborhoodOverlaps > overlap)\n\n        # When there is a tie, favor neighbors that are already selected as\n        # active.\n        ties = numpy.where(neighborhoodOverlaps == overlap)\n        tiedNeighbors = neighborhood[ties]\n        numTiesLost = numpy.count_nonzero(activeArray[tiedNeighbors])\n\n        numActive = int(0.5 + density * len(neighborhood))\n        if numBigger + numTiesLost < numActive:\n          activeArray[column] = True\n\n    return activeArray.nonzero()[0]",
    "doc": "Performs local inhibition. Local inhibition is performed on a column by\n    column basis. Each column observes the overlaps of its neighbors and is\n    selected if its overlap score is within the top 'numActive' in its local\n    neighborhood. At most half of the columns in a local neighborhood are\n    allowed to be active. Columns with an overlap score below the\n    'stimulusThreshold' are always inhibited.\n\n    :param overlaps: an array containing the overlap score for each  column.\n                    The overlap score for a column is defined as the number\n                    of synapses in a \"connected state\" (connected synapses)\n                    that are connected to input bits which are turned on.\n    :param density: The fraction of columns to survive inhibition. This\n                    value is only an intended target. Since the surviving\n                    columns are picked in a local fashion, the exact fraction\n                    of surviving columns is likely to vary.\n    @return list with indices of the winning columns"
  },
  {
    "code": "def _getColumnNeighborhood(self, centerColumn):\n    \"\"\"\n    Gets a neighborhood of columns.\n\n    Simply calls topology.neighborhood or topology.wrappingNeighborhood\n\n    A subclass can insert different topology behavior by overriding this method.\n\n    :param centerColumn (int)\n    The center of the neighborhood.\n\n    @returns (1D numpy array of integers)\n    The columns in the neighborhood.\n    \"\"\"\n    if self._wrapAround:\n      return topology.wrappingNeighborhood(centerColumn,\n                                           self._inhibitionRadius,\n                                           self._columnDimensions)\n\n    else:\n      return topology.neighborhood(centerColumn,\n                                   self._inhibitionRadius,\n                                   self._columnDimensions)",
    "doc": "Gets a neighborhood of columns.\n\n    Simply calls topology.neighborhood or topology.wrappingNeighborhood\n\n    A subclass can insert different topology behavior by overriding this method.\n\n    :param centerColumn (int)\n    The center of the neighborhood.\n\n    @returns (1D numpy array of integers)\n    The columns in the neighborhood."
  },
  {
    "code": "def _getInputNeighborhood(self, centerInput):\n    \"\"\"\n    Gets a neighborhood of inputs.\n\n    Simply calls topology.wrappingNeighborhood or topology.neighborhood.\n\n    A subclass can insert different topology behavior by overriding this method.\n\n    :param centerInput (int)\n    The center of the neighborhood.\n\n    @returns (1D numpy array of integers)\n    The inputs in the neighborhood.\n    \"\"\"\n    if self._wrapAround:\n      return topology.wrappingNeighborhood(centerInput,\n                                           self._potentialRadius,\n                                           self._inputDimensions)\n    else:\n      return topology.neighborhood(centerInput,\n                                   self._potentialRadius,\n                                   self._inputDimensions)",
    "doc": "Gets a neighborhood of inputs.\n\n    Simply calls topology.wrappingNeighborhood or topology.neighborhood.\n\n    A subclass can insert different topology behavior by overriding this method.\n\n    :param centerInput (int)\n    The center of the neighborhood.\n\n    @returns (1D numpy array of integers)\n    The inputs in the neighborhood."
  },
  {
    "code": "def _seed(self, seed=-1):\n    \"\"\"\n    Initialize the random seed\n    \"\"\"\n    if seed != -1:\n      self._random = NupicRandom(seed)\n    else:\n      self._random = NupicRandom()",
    "doc": "Initialize the random seed"
  },
  {
    "code": "def handleGetValue(self, topContainer):\n    \"\"\" This method overrides ValueGetterBase's \"pure virtual\" method.  It\n    returns the referenced value.  The derived class is NOT responsible for\n    fully resolving the reference'd value in the event the value resolves to\n    another ValueGetterBase-based instance -- this is handled automatically\n    within ValueGetterBase implementation.\n\n    topContainer: The top-level container (dict, tuple, or list [sub-]instance)\n                  within whose context the value-getter is applied.  If\n                  self.__referenceDict is None, then topContainer will be used\n                  as the reference dictionary for resolving our dictionary key\n                  chain.\n\n    Returns:      The value referenced by this instance (which may be another\n                  value-getter instance)\n    \"\"\"\n    value = self.__referenceDict if self.__referenceDict is not None else topContainer\n    for key in self.__dictKeyChain:\n      value = value[key]\n\n    return value",
    "doc": "This method overrides ValueGetterBase's \"pure virtual\" method.  It\n    returns the referenced value.  The derived class is NOT responsible for\n    fully resolving the reference'd value in the event the value resolves to\n    another ValueGetterBase-based instance -- this is handled automatically\n    within ValueGetterBase implementation.\n\n    topContainer: The top-level container (dict, tuple, or list [sub-]instance)\n                  within whose context the value-getter is applied.  If\n                  self.__referenceDict is None, then topContainer will be used\n                  as the reference dictionary for resolving our dictionary key\n                  chain.\n\n    Returns:      The value referenced by this instance (which may be another\n                  value-getter instance)"
  },
  {
    "code": "def Array(dtype, size=None, ref=False):\n  \"\"\"Factory function that creates typed Array or ArrayRef objects\n\n  dtype - the data type of the array (as string).\n    Supported types are: Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64, Real32, Real64\n\n  size - the size of the array. Must be positive integer.\n  \"\"\"\n\n  def getArrayType(self):\n    \"\"\"A little function to replace the getType() method of arrays\n\n    It returns a string representation of the array element type instead of the\n    integer value (NTA_BasicType enum) returned by the origianl array\n    \"\"\"\n    return self._dtype\n\n\n  # ArrayRef can't be allocated\n  if ref:\n    assert size is None\n\n  index = basicTypes.index(dtype)\n  if index == -1:\n    raise Exception('Invalid data type: ' + dtype)\n  if size and size <= 0:\n    raise Exception('Array size must be positive')\n  suffix = 'ArrayRef' if ref else 'Array'\n  arrayFactory = getattr(engine_internal, dtype + suffix)\n  arrayFactory.getType = getArrayType\n\n  if size:\n    a = arrayFactory(size)\n  else:\n    a = arrayFactory()\n\n  a._dtype = basicTypes[index]\n  return a",
    "doc": "Factory function that creates typed Array or ArrayRef objects\n\n  dtype - the data type of the array (as string).\n    Supported types are: Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64, Real32, Real64\n\n  size - the size of the array. Must be positive integer."
  },
  {
    "code": "def getInputNames(self):\n    \"\"\"\n    Returns list of input names in spec.\n    \"\"\"\n    inputs = self.getSpec().inputs\n    return [inputs.getByIndex(i)[0] for i in xrange(inputs.getCount())]",
    "doc": "Returns list of input names in spec."
  },
  {
    "code": "def getOutputNames(self):\n    \"\"\"\n    Returns list of output names in spec.\n    \"\"\"\n    outputs = self.getSpec().outputs\n    return [outputs.getByIndex(i)[0] for i in xrange(outputs.getCount())]",
    "doc": "Returns list of output names in spec."
  },
  {
    "code": "def _getParameterMethods(self, paramName):\n    \"\"\"Returns functions to set/get the parameter. These are\n    the strongly typed functions get/setParameterUInt32, etc.\n    The return value is a pair:\n        setfunc, getfunc\n    If the parameter is not available on this region, setfunc/getfunc\n    are None. \"\"\"\n    if paramName in self._paramTypeCache:\n      return self._paramTypeCache[paramName]\n    try:\n      # Catch the error here. We will re-throw in getParameter or\n      # setParameter with a better error message than we could generate here\n      paramSpec = self.getSpec().parameters.getByName(paramName)\n    except:\n      return (None, None)\n    dataType = paramSpec.dataType\n    dataTypeName = basicTypes[dataType]\n    count = paramSpec.count\n    if count == 1:\n      # Dynamically generate the proper typed get/setParameter<dataType>\n      x = 'etParameter' + dataTypeName\n      try:\n        g = getattr(self, 'g' + x) # get the typed getParameter method\n        s = getattr(self, 's' + x) # get the typed setParameter method\n      except AttributeError:\n        raise Exception(\"Internal error: unknown parameter type %s\" %\n                        dataTypeName)\n      info = (s, g)\n    else:\n      if dataTypeName == \"Byte\":\n        info = (self.setParameterString, self.getParameterString)\n      else:\n        helper = _ArrayParameterHelper(self, dataType)\n        info = (self.setParameterArray, helper.getParameterArray)\n\n    self._paramTypeCache[paramName] = info\n    return info",
    "doc": "Returns functions to set/get the parameter. These are\n    the strongly typed functions get/setParameterUInt32, etc.\n    The return value is a pair:\n        setfunc, getfunc\n    If the parameter is not available on this region, setfunc/getfunc\n    are None."
  },
  {
    "code": "def getParameter(self, paramName):\n    \"\"\"Get parameter value\"\"\"\n    (setter, getter) = self._getParameterMethods(paramName)\n    if getter is None:\n      import exceptions\n      raise exceptions.Exception(\n          \"getParameter -- parameter name '%s' does not exist in region %s of type %s\"\n          % (paramName, self.name, self.type))\n    return getter(paramName)",
    "doc": "Get parameter value"
  },
  {
    "code": "def setParameter(self, paramName, value):\n    \"\"\"Set parameter value\"\"\"\n    (setter, getter) = self._getParameterMethods(paramName)\n    if setter is None:\n      import exceptions\n      raise exceptions.Exception(\n          \"setParameter -- parameter name '%s' does not exist in region %s of type %s\"\n          % (paramName, self.name, self.type))\n    setter(paramName, value)",
    "doc": "Set parameter value"
  },
  {
    "code": "def _getRegions(self):\n    \"\"\"Get the collection of regions in a network\n\n    This is a tricky one. The collection of regions returned from\n    from the internal network is a collection of internal regions.\n    The desired collection is a collelcion of net.Region objects\n    that also points to this network (net.network) and not to\n    the internal network. To achieve that a CollectionWrapper\n    class is used with a custom makeRegion() function (see bellow)\n    as a value wrapper. The CollectionWrapper class wraps each value in the\n    original collection with the result of the valueWrapper.\n    \"\"\"\n\n    def makeRegion(name, r):\n      \"\"\"Wrap a engine region with a nupic.engine_internal.Region\n\n      Also passes the containing nupic.engine_internal.Network network in _network. This\n      function is passed a value wrapper to the CollectionWrapper\n      \"\"\"\n      r = Region(r, self)\n      #r._network = self\n      return r\n\n    regions = CollectionWrapper(engine_internal.Network.getRegions(self), makeRegion)\n    return regions",
    "doc": "Get the collection of regions in a network\n\n    This is a tricky one. The collection of regions returned from\n    from the internal network is a collection of internal regions.\n    The desired collection is a collelcion of net.Region objects\n    that also points to this network (net.network) and not to\n    the internal network. To achieve that a CollectionWrapper\n    class is used with a custom makeRegion() function (see bellow)\n    as a value wrapper. The CollectionWrapper class wraps each value in the\n    original collection with the result of the valueWrapper."
  },
  {
    "code": "def getRegionsByType(self, regionClass):\n    \"\"\"\n    Gets all region instances of a given class\n    (for example, nupic.regions.sp_region.SPRegion).\n    \"\"\"\n    regions = []\n\n    for region in self.regions.values():\n      if type(region.getSelf()) is regionClass:\n        regions.append(region)\n\n    return regions",
    "doc": "Gets all region instances of a given class\n    (for example, nupic.regions.sp_region.SPRegion)."
  },
  {
    "code": "def getSpec(cls):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n    \"\"\"\n    ns = dict(\n      description=SDRClassifierRegion.__doc__,\n      singleNodeOnly=True,\n\n      inputs=dict(\n        actValueIn=dict(\n          description=\"Actual value of the field to predict. Only taken \"\n                      \"into account if the input has no category field.\",\n          dataType=\"Real32\",\n          count=0,\n          required=False,\n          regionLevel=False,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        bucketIdxIn=dict(\n          description=\"Active index of the encoder bucket for the \"\n                      \"actual value of the field to predict. Only taken \"\n                      \"into account if the input has no category field.\",\n          dataType=\"UInt64\",\n          count=0,\n          required=False,\n          regionLevel=False,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        categoryIn=dict(\n          description='Vector of categories of the input sample',\n          dataType='Real32',\n          count=0,\n          required=True,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        bottomUpIn=dict(\n          description='Belief values over children\\'s groups',\n          dataType='Real32',\n          count=0,\n          required=True,\n          regionLevel=False,\n          isDefaultInput=True,\n          requireSplitterMap=False),\n\n        predictedActiveCells=dict(\n          description=\"The cells that are active and predicted\",\n          dataType='Real32',\n          count=0,\n          required=True,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        sequenceIdIn=dict(\n          description=\"Sequence ID\",\n          dataType='UInt64',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n      ),\n\n      outputs=dict(\n        categoriesOut=dict(\n          description='Classification results',\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False,\n          requireSplitterMap=False),\n\n        actualValues=dict(\n          description='Classification results',\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False,\n          requireSplitterMap=False),\n\n        probabilities=dict(\n          description='Classification results',\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False,\n          requireSplitterMap=False),\n      ),\n\n      parameters=dict(\n        learningMode=dict(\n          description='Boolean (0/1) indicating whether or not a region '\n                      'is in learning mode.',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=1,\n          accessMode='ReadWrite'),\n\n        inferenceMode=dict(\n          description='Boolean (0/1) indicating whether or not a region '\n                      'is in inference mode.',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n\n        maxCategoryCount=dict(\n          description='The maximal number of categories the '\n                      'classifier will distinguish between.',\n          dataType='UInt32',\n          required=True,\n          count=1,\n          constraints='',\n          # arbitrarily large value\n          defaultValue=2000,\n          accessMode='Create'),\n\n        steps=dict(\n          description='Comma separated list of the desired steps of '\n                      'prediction that the classifier should learn',\n          dataType=\"Byte\",\n          count=0,\n          constraints='',\n          defaultValue='0',\n          accessMode='Create'),\n\n        alpha=dict(\n          description='The alpha is the learning rate of the classifier.'\n                      'lower alpha results in longer term memory and slower '\n                      'learning',\n          dataType=\"Real32\",\n          count=1,\n          constraints='',\n          defaultValue=0.001,\n          accessMode='Create'),\n\n        implementation=dict(\n          description='The classifier implementation to use.',\n          accessMode='ReadWrite',\n          dataType='Byte',\n          count=0,\n          constraints='enum: py, cpp'),\n\n        verbosity=dict(\n          description='An integer that controls the verbosity level, '\n                      '0 means no verbose output, increasing integers '\n                      'provide more verbosity.',\n          dataType='UInt32',\n          count=1,\n          constraints='',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n      ),\n      commands=dict()\n    )\n\n    return ns",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getSpec`."
  },
  {
    "code": "def initialize(self):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.initialize`.\n\n    Is called once by NuPIC before the first call to compute().\n    Initializes self._sdrClassifier if it is not already initialized.\n    \"\"\"\n    if self._sdrClassifier is None:\n      self._sdrClassifier = SDRClassifierFactory.create(\n        steps=self.stepsList,\n        alpha=self.alpha,\n        verbosity=self.verbosity,\n        implementation=self.implementation,\n      )",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.initialize`.\n\n    Is called once by NuPIC before the first call to compute().\n    Initializes self._sdrClassifier if it is not already initialized."
  },
  {
    "code": "def setParameter(self, name, index, value):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.setParameter`.\n    \"\"\"\n    if name == \"learningMode\":\n      self.learningMode = bool(int(value))\n    elif name == \"inferenceMode\":\n      self.inferenceMode = bool(int(value))\n    else:\n      return PyRegion.setParameter(self, name, index, value)",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.setParameter`."
  },
  {
    "code": "def writeToProto(self, proto):\n    \"\"\"\n    Write state to proto object.\n\n    :param proto: SDRClassifierRegionProto capnproto object\n    \"\"\"\n    proto.implementation = self.implementation\n    proto.steps = self.steps\n    proto.alpha = self.alpha\n    proto.verbosity = self.verbosity\n    proto.maxCategoryCount = self.maxCategoryCount\n    proto.learningMode = self.learningMode\n    proto.inferenceMode = self.inferenceMode\n    proto.recordNum = self.recordNum\n\n    self._sdrClassifier.write(proto.sdrClassifier)",
    "doc": "Write state to proto object.\n\n    :param proto: SDRClassifierRegionProto capnproto object"
  },
  {
    "code": "def readFromProto(cls, proto):\n    \"\"\"\n    Read state from proto object.\n\n    :param proto: SDRClassifierRegionProto capnproto object\n    \"\"\"\n    instance = cls()\n\n    instance.implementation = proto.implementation\n    instance.steps = proto.steps\n    instance.stepsList = [int(i) for i in proto.steps.split(\",\")]\n    instance.alpha = proto.alpha\n    instance.verbosity = proto.verbosity\n    instance.maxCategoryCount = proto.maxCategoryCount\n\n    instance._sdrClassifier = SDRClassifierFactory.read(proto)\n\n    instance.learningMode = proto.learningMode\n    instance.inferenceMode = proto.inferenceMode\n    instance.recordNum = proto.recordNum\n\n    return instance",
    "doc": "Read state from proto object.\n\n    :param proto: SDRClassifierRegionProto capnproto object"
  },
  {
    "code": "def compute(self, inputs, outputs):\n    \"\"\"\n    Process one input sample.\n    This method is called by the runtime engine.\n\n    :param inputs: (dict) mapping region input names to numpy.array values\n    :param outputs: (dict) mapping region output names to numpy.arrays that \n           should be populated with output values by this method\n    \"\"\"\n\n    # This flag helps to prevent double-computation, in case the deprecated\n    # customCompute() method is being called in addition to compute() called\n    # when network.run() is called\n    self._computeFlag = True\n\n    patternNZ = inputs[\"bottomUpIn\"].nonzero()[0]\n\n    if self.learningMode:\n      # An input can potentially belong to multiple categories.\n      # If a category value is < 0, it means that the input does not belong to\n      # that category.\n      categories = [category for category in inputs[\"categoryIn\"]\n                    if category >= 0]\n\n      if len(categories) > 0:\n        # Allow to train on multiple input categories.\n        bucketIdxList = []\n        actValueList = []\n        for category in categories:\n          bucketIdxList.append(int(category))\n          if \"actValueIn\" not in inputs:\n            actValueList.append(int(category))\n          else:\n            actValueList.append(float(inputs[\"actValueIn\"]))\n\n        classificationIn = {\"bucketIdx\": bucketIdxList,\n                            \"actValue\": actValueList}\n      else:\n        # If the input does not belong to a category, i.e. len(categories) == 0,\n        # then look for bucketIdx and actValueIn.\n        if \"bucketIdxIn\" not in inputs:\n          raise KeyError(\"Network link missing: bucketIdxOut -> bucketIdxIn\")\n        if \"actValueIn\" not in inputs:\n          raise KeyError(\"Network link missing: actValueOut -> actValueIn\")\n\n        classificationIn = {\"bucketIdx\": int(inputs[\"bucketIdxIn\"]),\n                            \"actValue\": float(inputs[\"actValueIn\"])}\n    else:\n      # Use Dummy classification input, because this param is required even for\n      # inference mode. Because learning is off, the classifier is not learning\n      # this dummy input. Inference only here.\n      classificationIn = {\"actValue\": 0, \"bucketIdx\": 0}\n\n    # Perform inference if self.inferenceMode is True\n    # Train classifier if self.learningMode is True\n    clResults = self._sdrClassifier.compute(recordNum=self.recordNum,\n                                            patternNZ=patternNZ,\n                                            classification=classificationIn,\n                                            learn=self.learningMode,\n                                            infer=self.inferenceMode)\n\n    # fill outputs with clResults\n    if clResults is not None and len(clResults) > 0:\n      outputs['actualValues'][:len(clResults[\"actualValues\"])] = \\\n        clResults[\"actualValues\"]\n\n      for step in self.stepsList:\n        stepIndex = self.stepsList.index(step)\n        categoryOut = clResults[\"actualValues\"][clResults[step].argmax()]\n        outputs['categoriesOut'][stepIndex] = categoryOut\n\n        # Flatten the rest of the output. For example:\n        #   Original dict  {1 : [0.1, 0.3, 0.2, 0.7]\n        #                   4 : [0.2, 0.4, 0.3, 0.5]}\n        #   becomes: [0.1, 0.3, 0.2, 0.7, 0.2, 0.4, 0.3, 0.5]\n        stepProbabilities = clResults[step]\n        for categoryIndex in xrange(self.maxCategoryCount):\n          flatIndex = categoryIndex + stepIndex * self.maxCategoryCount\n          if categoryIndex < len(stepProbabilities):\n            outputs['probabilities'][flatIndex] = \\\n              stepProbabilities[categoryIndex]\n          else:\n            outputs['probabilities'][flatIndex] = 0.0\n\n    self.recordNum += 1",
    "doc": "Process one input sample.\n    This method is called by the runtime engine.\n\n    :param inputs: (dict) mapping region input names to numpy.array values\n    :param outputs: (dict) mapping region output names to numpy.arrays that \n           should be populated with output values by this method"
  },
  {
    "code": "def customCompute(self, recordNum, patternNZ, classification):\n    \"\"\"\n    Just return the inference value from one input sample. The actual\n    learning happens in compute() -- if, and only if learning is enabled --\n    which is called when you run the network.\n\n    .. warning:: This method is deprecated and exists only to maintain backward \n       compatibility. This method is deprecated, and will be removed. Use \n       :meth:`nupic.engine.Network.run` instead, which will call \n       :meth:`~nupic.regions.sdr_classifier_region.compute`.\n\n    :param recordNum: (int) Record number of the input sample.\n    :param patternNZ: (list) of the active indices from the output below\n    :param classification: (dict) of the classification information:\n    \n           * ``bucketIdx``: index of the encoder bucket\n           * ``actValue``:  actual value going into the encoder\n\n    :returns: (dict) containing inference results, one entry for each step in\n              ``self.steps``. The key is the number of steps, the value is an\n              array containing the relative likelihood for each ``bucketIdx``\n              starting from 0.\n\n              For example:\n              \n              :: \n              \n                {'actualValues': [0.0, 1.0, 2.0, 3.0]\n                  1 : [0.1, 0.3, 0.2, 0.7]\n                  4 : [0.2, 0.4, 0.3, 0.5]}\n    \"\"\"\n\n    # If the compute flag has not been initialized (for example if we\n    # restored a model from an old checkpoint) initialize it to False.\n    if not hasattr(self, \"_computeFlag\"):\n      self._computeFlag = False\n\n    if self._computeFlag:\n      # Will raise an exception if the deprecated method customCompute() is\n      # being used at the same time as the compute function.\n      warnings.simplefilter('error', DeprecationWarning)\n      warnings.warn(\"The customCompute() method should not be \"\n                    \"called at the same time as the compute() \"\n                    \"method. The compute() method is called \"\n                    \"whenever network.run() is called.\",\n                    DeprecationWarning)\n\n    return self._sdrClassifier.compute(recordNum,\n                                       patternNZ,\n                                       classification,\n                                       self.learningMode,\n                                       self.inferenceMode)",
    "doc": "Just return the inference value from one input sample. The actual\n    learning happens in compute() -- if, and only if learning is enabled --\n    which is called when you run the network.\n\n    .. warning:: This method is deprecated and exists only to maintain backward \n       compatibility. This method is deprecated, and will be removed. Use \n       :meth:`nupic.engine.Network.run` instead, which will call \n       :meth:`~nupic.regions.sdr_classifier_region.compute`.\n\n    :param recordNum: (int) Record number of the input sample.\n    :param patternNZ: (list) of the active indices from the output below\n    :param classification: (dict) of the classification information:\n    \n           * ``bucketIdx``: index of the encoder bucket\n           * ``actValue``:  actual value going into the encoder\n\n    :returns: (dict) containing inference results, one entry for each step in\n              ``self.steps``. The key is the number of steps, the value is an\n              array containing the relative likelihood for each ``bucketIdx``\n              starting from 0.\n\n              For example:\n              \n              :: \n              \n                {'actualValues': [0.0, 1.0, 2.0, 3.0]\n                  1 : [0.1, 0.3, 0.2, 0.7]\n                  4 : [0.2, 0.4, 0.3, 0.5]}"
  },
  {
    "code": "def getOutputElementCount(self, outputName):\n    \"\"\"\n    Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getOutputElementCount`.\n    \"\"\"\n    if outputName == \"categoriesOut\":\n      return len(self.stepsList)\n    elif outputName == \"probabilities\":\n      return len(self.stepsList) * self.maxCategoryCount\n    elif outputName == \"actualValues\":\n      return self.maxCategoryCount\n    else:\n      raise ValueError(\"Unknown output {}.\".format(outputName))",
    "doc": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.getOutputElementCount`."
  },
  {
    "code": "def run(self):\n    \"\"\" Runs the OPF Model\n\n    Parameters:\n    -------------------------------------------------------------------------\n    retval:  (completionReason, completionMsg)\n              where completionReason is one of the ClientJobsDAO.CMPL_REASON_XXX\n                equates.\n    \"\"\"\n    # -----------------------------------------------------------------------\n    # Load the experiment's description.py module\n    descriptionPyModule = helpers.loadExperimentDescriptionScriptFromDir(\n      self._experimentDir)\n    expIface = helpers.getExperimentDescriptionInterfaceFromModule(\n      descriptionPyModule)\n    expIface.normalizeStreamSources()\n\n    modelDescription = expIface.getModelDescription()\n    self._modelControl = expIface.getModelControl()\n\n    # -----------------------------------------------------------------------\n    # Create the input data stream for this task\n    streamDef = self._modelControl['dataset']\n\n    from nupic.data.stream_reader import StreamReader\n    readTimeout = 0\n\n    self._inputSource = StreamReader(streamDef, isBlocking=False,\n                                     maxTimeout=readTimeout)\n\n\n    # -----------------------------------------------------------------------\n    #Get field statistics from the input source\n    fieldStats = self._getFieldStats()\n    # -----------------------------------------------------------------------\n    # Construct the model instance\n    self._model = ModelFactory.create(modelDescription)\n    self._model.setFieldStatistics(fieldStats)\n    self._model.enableLearning()\n    self._model.enableInference(self._modelControl.get(\"inferenceArgs\", None))\n\n    # -----------------------------------------------------------------------\n    # Instantiate the metrics\n    self.__metricMgr = MetricsManager(self._modelControl.get('metrics',None),\n                                      self._model.getFieldInfo(),\n                                      self._model.getInferenceType())\n\n    self.__loggedMetricPatterns = self._modelControl.get(\"loggedMetrics\", [])\n\n    self._optimizedMetricLabel = self.__getOptimizedMetricLabel()\n    self._reportMetricLabels = matchPatterns(self._reportKeyPatterns,\n                                              self._getMetricLabels())\n\n\n    # -----------------------------------------------------------------------\n    # Initialize periodic activities (e.g., for model result updates)\n    self._periodic = self._initPeriodicActivities()\n\n    # -----------------------------------------------------------------------\n    # Create our top-level loop-control iterator\n    numIters = self._modelControl.get('iterationCount', -1)\n\n    # Are we asked to turn off learning for a certain # of iterations near the\n    #  end?\n    learningOffAt = None\n    iterationCountInferOnly = self._modelControl.get('iterationCountInferOnly', 0)\n    if iterationCountInferOnly == -1:\n      self._model.disableLearning()\n    elif iterationCountInferOnly > 0:\n      assert numIters > iterationCountInferOnly, \"when iterationCountInferOnly \" \\\n        \"is specified, iterationCount must be greater than \" \\\n        \"iterationCountInferOnly.\"\n      learningOffAt = numIters - iterationCountInferOnly\n\n    self.__runTaskMainLoop(numIters, learningOffAt=learningOffAt)\n\n    # -----------------------------------------------------------------------\n    # Perform final operations for model\n    self._finalize()\n\n    return (self._cmpReason, None)",
    "doc": "Runs the OPF Model\n\n    Parameters:\n    -------------------------------------------------------------------------\n    retval:  (completionReason, completionMsg)\n              where completionReason is one of the ClientJobsDAO.CMPL_REASON_XXX\n                equates."
  },
  {
    "code": "def __runTaskMainLoop(self, numIters, learningOffAt=None):\n    \"\"\" Main loop of the OPF Model Runner.\n\n    Parameters:\n    -----------------------------------------------------------------------\n\n    recordIterator:    Iterator for counting number of records (see _runTask)\n    learningOffAt:     If not None, learning is turned off when we reach this\n                        iteration number\n\n    \"\"\"\n\n    ## Reset sequence states in the model, so it starts looking for a new\n    ## sequence\n    self._model.resetSequenceStates()\n\n    self._currentRecordIndex = -1\n    while True:\n\n      # If killed by a terminator, stop running\n      if self._isKilled:\n        break\n\n      # If job stops or hypersearch ends, stop running\n      if self._isCanceled:\n        break\n\n      # If the process is about to be killed, set as orphaned\n      if self._isInterrupted.isSet():\n        self.__setAsOrphaned()\n        break\n\n      # If model is mature, stop running ONLY IF  we are not the best model\n      # for the job. Otherwise, keep running so we can keep returning\n      # predictions to the user\n      if self._isMature:\n        if not self._isBestModel:\n          self._cmpReason = self._jobsDAO.CMPL_REASON_STOPPED\n          break\n        else:\n          self._cmpReason = self._jobsDAO.CMPL_REASON_EOF\n\n      # Turn off learning?\n        if learningOffAt is not None \\\n                  and self._currentRecordIndex == learningOffAt:\n          self._model.disableLearning()\n\n      # Read input record. Note that any failure here is a critical JOB failure\n      #  and results in the job being immediately canceled and marked as\n      #  failed. The runModelXXX code in hypesearch.utils, if it sees an\n      #  exception of type utils.JobFailException, will cancel the job and\n      #  copy the error message into the job record.\n      try:\n        inputRecord = self._inputSource.getNextRecordDict()\n        if self._currentRecordIndex < 0:\n          self._inputSource.setTimeout(10)\n      except Exception, e:\n        raise utils.JobFailException(ErrorCodes.streamReading, str(e.args),\n                                     traceback.format_exc())\n\n      if inputRecord is None:\n        # EOF\n        self._cmpReason = self._jobsDAO.CMPL_REASON_EOF\n        break\n\n      if inputRecord:\n        # Process input record\n        self._currentRecordIndex += 1\n\n        result = self._model.run(inputRecord=inputRecord)\n\n        # Compute metrics.\n        result.metrics = self.__metricMgr.update(result)\n        # If there are None, use defaults. see MetricsManager.getMetrics()\n        # TODO remove this when JAVA API server is gone\n        if not result.metrics:\n          result.metrics = self.__metricMgr.getMetrics()\n\n\n        # Write the result to the output cache. Don't write encodings, if they\n        # were computed\n        if InferenceElement.encodings in result.inferences:\n          result.inferences.pop(InferenceElement.encodings)\n        result.sensorInput.dataEncodings = None\n        self._writePrediction(result)\n\n        # Run periodic activities\n        self._periodic.tick()\n\n        if numIters >= 0 and self._currentRecordIndex >= numIters-1:\n          break\n\n      else:\n        # Input source returned an empty record.\n        #\n        # NOTE: This is okay with Stream-based Source (when it times out\n        # waiting for next record), but not okay with FileSource, which should\n        # always return either with a valid record or None for EOF.\n        raise ValueError(\"Got an empty record from FileSource: %r\" %\n                         inputRecord)",
    "doc": "Main loop of the OPF Model Runner.\n\n    Parameters:\n    -----------------------------------------------------------------------\n\n    recordIterator:    Iterator for counting number of records (see _runTask)\n    learningOffAt:     If not None, learning is turned off when we reach this\n                        iteration number"
  },
  {
    "code": "def _finalize(self):\n    \"\"\"Run final activities after a model has run. These include recording and\n    logging the final score\"\"\"\n\n    self._logger.info(\n      \"Finished: modelID=%r; %r records processed. Performing final activities\",\n      self._modelID, self._currentRecordIndex + 1)\n\n    # =========================================================================\n    # Dump the experiment metrics at the end of the task\n    # =========================================================================\n    self._updateModelDBResults()\n\n    # =========================================================================\n    # Check if the current model is the best. Create a milestone if necessary\n    # If the model has been killed, it is not a candidate for \"best model\",\n    # and its output cache should be destroyed\n    # =========================================================================\n    if not self._isKilled:\n      self.__updateJobResults()\n    else:\n      self.__deleteOutputCache(self._modelID)\n\n    # =========================================================================\n    # Close output stream, if necessary\n    # =========================================================================\n    if self._predictionLogger:\n      self._predictionLogger.close()\n\n    # =========================================================================\n    # Close input stream, if necessary\n    # =========================================================================\n    if self._inputSource: \n      self._inputSource.close()",
    "doc": "Run final activities after a model has run. These include recording and\n    logging the final score"
  },
  {
    "code": "def __createModelCheckpoint(self):\n    \"\"\" Create a checkpoint from the current model, and store it in a dir named\n    after checkpoint GUID, and finally store the GUID in the Models DB \"\"\"\n\n    if self._model is None or self._modelCheckpointGUID is None:\n      return\n\n    # Create an output store, if one doesn't exist already\n    if self._predictionLogger is None:\n      self._createPredictionLogger()\n\n    predictions = StringIO.StringIO()\n    self._predictionLogger.checkpoint(\n      checkpointSink=predictions,\n      maxRows=int(Configuration.get('nupic.model.checkpoint.maxPredictionRows')))\n\n    self._model.save(os.path.join(self._experimentDir, str(self._modelCheckpointGUID)))\n    self._jobsDAO.modelSetFields(modelID,\n                                 {'modelCheckpointId':str(self._modelCheckpointGUID)},\n                                 ignoreUnchanged=True)\n\n    self._logger.info(\"Checkpointed Hypersearch Model: modelID: %r, \"\n                      \"checkpointID: %r\", self._modelID, checkpointID)\n    return",
    "doc": "Create a checkpoint from the current model, and store it in a dir named\n    after checkpoint GUID, and finally store the GUID in the Models DB"
  },
  {
    "code": "def __deleteModelCheckpoint(self, modelID):\n    \"\"\"\n    Delete the stored checkpoint for the specified modelID. This function is\n    called if the current model is now the best model, making the old model's\n    checkpoint obsolete\n\n    Parameters:\n    -----------------------------------------------------------------------\n    modelID:      The modelID for the checkpoint to delete. This is NOT the\n                  unique checkpointID\n    \"\"\"\n\n    checkpointID = \\\n        self._jobsDAO.modelsGetFields(modelID, ['modelCheckpointId'])[0]\n\n    if checkpointID is None:\n      return\n\n    try:\n      shutil.rmtree(os.path.join(self._experimentDir, str(self._modelCheckpointGUID)))\n    except:\n      self._logger.warn(\"Failed to delete model checkpoint %s. \"\\\n                        \"Assuming that another worker has already deleted it\",\n                        checkpointID)\n      return\n\n    self._jobsDAO.modelSetFields(modelID,\n                                 {'modelCheckpointId':None},\n                                 ignoreUnchanged=True)\n    return",
    "doc": "Delete the stored checkpoint for the specified modelID. This function is\n    called if the current model is now the best model, making the old model's\n    checkpoint obsolete\n\n    Parameters:\n    -----------------------------------------------------------------------\n    modelID:      The modelID for the checkpoint to delete. This is NOT the\n                  unique checkpointID"
  },
  {
    "code": "def _createPredictionLogger(self):\n    \"\"\"\n    Creates the model's PredictionLogger object, which is an interface to write\n    model results to a permanent storage location\n    \"\"\"\n    # Write results to a file\n    self._predictionLogger = BasicPredictionLogger(\n      fields=self._model.getFieldInfo(),\n      experimentDir=self._experimentDir,\n      label = \"hypersearch-worker\",\n      inferenceType=self._model.getInferenceType())\n\n    if self.__loggedMetricPatterns:\n      metricLabels = self.__metricMgr.getMetricLabels()\n      loggedMetrics = matchPatterns(self.__loggedMetricPatterns, metricLabels)\n      self._predictionLogger.setLoggedMetrics(loggedMetrics)",
    "doc": "Creates the model's PredictionLogger object, which is an interface to write\n    model results to a permanent storage location"
  },
  {
    "code": "def __getOptimizedMetricLabel(self):\n    \"\"\" Get the label for the metric being optimized. This function also caches\n    the label in the instance variable self._optimizedMetricLabel\n\n    Parameters:\n    -----------------------------------------------------------------------\n    metricLabels:   A sequence of all the labels being computed for this model\n\n    Returns:        The label for the metric being optmized over\n    \"\"\"\n    matchingKeys = matchPatterns([self._optimizeKeyPattern],\n                                  self._getMetricLabels())\n\n    if len(matchingKeys) == 0:\n      raise Exception(\"None of the generated metrics match the specified \"\n                      \"optimization pattern: %s. Available metrics are %s\" % \\\n                       (self._optimizeKeyPattern, self._getMetricLabels()))\n    elif len(matchingKeys) > 1:\n      raise Exception(\"The specified optimization pattern '%s' matches more \"\n              \"than one metric: %s\" % (self._optimizeKeyPattern, matchingKeys))\n\n    return matchingKeys[0]",
    "doc": "Get the label for the metric being optimized. This function also caches\n    the label in the instance variable self._optimizedMetricLabel\n\n    Parameters:\n    -----------------------------------------------------------------------\n    metricLabels:   A sequence of all the labels being computed for this model\n\n    Returns:        The label for the metric being optmized over"
  },
  {
    "code": "def _getFieldStats(self):\n    \"\"\"\n    Method which returns a dictionary of field statistics received from the\n    input source.\n\n    Returns:\n\n      fieldStats: dict of dicts where the first level is the field name and\n        the second level is the statistic. ie. fieldStats['pounds']['min']\n\n    \"\"\"\n\n    fieldStats = dict()\n    fieldNames = self._inputSource.getFieldNames()\n    for field in fieldNames:\n      curStats = dict()\n      curStats['min'] = self._inputSource.getFieldMin(field)\n      curStats['max'] = self._inputSource.getFieldMax(field)\n      fieldStats[field] = curStats\n    return fieldStats",
    "doc": "Method which returns a dictionary of field statistics received from the\n    input source.\n\n    Returns:\n\n      fieldStats: dict of dicts where the first level is the field name and\n        the second level is the statistic. ie. fieldStats['pounds']['min']"
  },
  {
    "code": "def _updateModelDBResults(self):\n    \"\"\" Retrieves the current results and updates the model's record in\n    the Model database.\n    \"\"\"\n\n    # -----------------------------------------------------------------------\n    # Get metrics\n    metrics = self._getMetrics()\n\n    # -----------------------------------------------------------------------\n    # Extract report metrics that match the requested report REs\n    reportDict = dict([(k,metrics[k]) for k in self._reportMetricLabels])\n\n    # -----------------------------------------------------------------------\n    # Extract the report item that matches the optimize key RE\n    # TODO cache optimizedMetricLabel sooner\n    metrics = self._getMetrics()\n    optimizeDict = dict()\n    if self._optimizeKeyPattern is not None:\n      optimizeDict[self._optimizedMetricLabel] = \\\n                                      metrics[self._optimizedMetricLabel]\n\n    # -----------------------------------------------------------------------\n    # Update model results\n    results = json.dumps((metrics , optimizeDict))\n    self._jobsDAO.modelUpdateResults(self._modelID,  results=results,\n                              metricValue=optimizeDict.values()[0],\n                              numRecords=(self._currentRecordIndex + 1))\n\n    self._logger.debug(\n      \"Model Results: modelID=%s; numRecords=%s; results=%s\" % \\\n        (self._modelID, self._currentRecordIndex + 1, results))\n\n    return",
    "doc": "Retrieves the current results and updates the model's record in\n    the Model database."
  },
  {
    "code": "def __updateJobResultsPeriodic(self):\n    \"\"\"\n    Periodic check to see if this is the best model. This should only have an\n    effect if this is the *first* model to report its progress\n    \"\"\"\n    if self._isBestModelStored and not self._isBestModel:\n      return\n\n    while True:\n      jobResultsStr = self._jobsDAO.jobGetFields(self._jobID, ['results'])[0]\n      if jobResultsStr is None:\n          jobResults = {}\n      else:\n        self._isBestModelStored = True\n        if not self._isBestModel:\n          return\n\n        jobResults = json.loads(jobResultsStr)\n\n      bestModel = jobResults.get('bestModel', None)\n      bestMetric = jobResults.get('bestValue', None)\n      isSaved = jobResults.get('saved', False)\n\n      # If there is a best model, and it is not the same as the current model\n      # we should wait till we have processed all of our records to see if\n      # we are the the best\n      if (bestModel is not None) and (self._modelID != bestModel):\n        self._isBestModel = False\n        return\n\n      # Make sure prediction output stream is ready before we present our model\n      # as \"bestModel\"; sometimes this takes a long time, so update the model's\n      # timestamp to help avoid getting orphaned\n      self.__flushPredictionCache()\n      self._jobsDAO.modelUpdateTimestamp(self._modelID)\n\n      metrics = self._getMetrics()\n\n      jobResults['bestModel'] = self._modelID\n      jobResults['bestValue'] = metrics[self._optimizedMetricLabel]\n      jobResults['metrics'] = metrics\n      jobResults['saved'] = False\n\n      newResults = json.dumps(jobResults)\n\n      isUpdated = self._jobsDAO.jobSetFieldIfEqual(self._jobID,\n                                                    fieldName='results',\n                                                    curValue=jobResultsStr,\n                                                    newValue=newResults)\n      if isUpdated or (not isUpdated and newResults==jobResultsStr):\n        self._isBestModel = True\n        break",
    "doc": "Periodic check to see if this is the best model. This should only have an\n    effect if this is the *first* model to report its progress"
  },
  {
    "code": "def __checkIfBestCompletedModel(self):\n    \"\"\"\n    Reads the current \"best model\" for the job and returns whether or not the\n    current model is better than the \"best model\" stored for the job\n\n    Returns: (isBetter, storedBest, origResultsStr)\n\n    isBetter:\n      True if the current model is better than the stored \"best model\"\n    storedResults:\n      A dict of the currently stored results in the jobs table record\n    origResultsStr:\n      The json-encoded string that currently resides in the \"results\" field\n      of the jobs record (used to create atomicity)\n    \"\"\"\n\n    jobResultsStr = self._jobsDAO.jobGetFields(self._jobID, ['results'])[0]\n\n    if jobResultsStr is None:\n        jobResults = {}\n    else:\n      jobResults = json.loads(jobResultsStr)\n\n    isSaved = jobResults.get('saved', False)\n    bestMetric = jobResults.get('bestValue', None)\n\n    currentMetric = self._getMetrics()[self._optimizedMetricLabel]\n    self._isBestModel = (not isSaved) \\\n                        or (currentMetric < bestMetric)\n\n\n\n    return self._isBestModel, jobResults, jobResultsStr",
    "doc": "Reads the current \"best model\" for the job and returns whether or not the\n    current model is better than the \"best model\" stored for the job\n\n    Returns: (isBetter, storedBest, origResultsStr)\n\n    isBetter:\n      True if the current model is better than the stored \"best model\"\n    storedResults:\n      A dict of the currently stored results in the jobs table record\n    origResultsStr:\n      The json-encoded string that currently resides in the \"results\" field\n      of the jobs record (used to create atomicity)"
  },
  {
    "code": "def __updateJobResults(self):\n    \"\"\"\"\n    Check if this is the best model\n    If so:\n      1) Write it's checkpoint\n      2) Record this model as the best\n      3) Delete the previous best's output cache\n    Otherwise:\n      1) Delete our output cache\n     \"\"\"\n    isSaved = False\n    while True:\n      self._isBestModel, jobResults, jobResultsStr = \\\n                                              self.__checkIfBestCompletedModel()\n\n      # -----------------------------------------------------------------------\n      # If the current model is the best:\n      #   1) Save the model's predictions\n      #   2) Checkpoint the model state\n      #   3) Update the results for the job\n      if self._isBestModel:\n\n        # Save the current model and its results\n        if not isSaved:\n          self.__flushPredictionCache()\n          self._jobsDAO.modelUpdateTimestamp(self._modelID)\n          self.__createModelCheckpoint()\n          self._jobsDAO.modelUpdateTimestamp(self._modelID)\n          isSaved = True\n\n        # Now record the model as the best for the job\n        prevBest = jobResults.get('bestModel', None)\n        prevWasSaved = jobResults.get('saved', False)\n\n        # If the current model is the best, it shouldn't already be checkpointed\n        if prevBest == self._modelID:\n          assert not prevWasSaved\n\n        metrics = self._getMetrics()\n\n        jobResults['bestModel'] = self._modelID\n        jobResults['bestValue'] = metrics[self._optimizedMetricLabel]\n        jobResults['metrics'] = metrics\n        jobResults['saved'] = True\n\n        isUpdated = self._jobsDAO.jobSetFieldIfEqual(self._jobID,\n                                                    fieldName='results',\n                                                    curValue=jobResultsStr,\n                                                    newValue=json.dumps(jobResults))\n        if isUpdated:\n          if prevWasSaved:\n            self.__deleteOutputCache(prevBest)\n            self._jobsDAO.modelUpdateTimestamp(self._modelID)\n            self.__deleteModelCheckpoint(prevBest)\n            self._jobsDAO.modelUpdateTimestamp(self._modelID)\n\n          self._logger.info(\"Model %d chosen as best model\", self._modelID)\n          break\n\n      # -----------------------------------------------------------------------\n      # If the current model is not the best, delete its outputs\n      else:\n        # NOTE: we update model timestamp around these occasionally-lengthy\n        #  operations to help prevent the model from becoming orphaned\n        self.__deleteOutputCache(self._modelID)\n        self._jobsDAO.modelUpdateTimestamp(self._modelID)\n        self.__deleteModelCheckpoint(self._modelID)\n        self._jobsDAO.modelUpdateTimestamp(self._modelID)\n        break",
    "doc": "Check if this is the best model\n    If so:\n      1) Write it's checkpoint\n      2) Record this model as the best\n      3) Delete the previous best's output cache\n    Otherwise:\n      1) Delete our output cache"
  },
  {
    "code": "def _writePrediction(self, result):\n    \"\"\"\n    Writes the results of one iteration of a model. The results are written to\n    this ModelRunner's in-memory cache unless this model is the \"best model\" for\n    the job. If this model is the \"best model\", the predictions are written out\n    to a permanent store via a prediction output stream instance\n\n\n    Parameters:\n    -----------------------------------------------------------------------\n    result:      A opf_utils.ModelResult object, which contains the input and\n                  output for this iteration\n    \"\"\"\n    self.__predictionCache.append(result)\n\n    if self._isBestModel:\n     self.__flushPredictionCache()",
    "doc": "Writes the results of one iteration of a model. The results are written to\n    this ModelRunner's in-memory cache unless this model is the \"best model\" for\n    the job. If this model is the \"best model\", the predictions are written out\n    to a permanent store via a prediction output stream instance\n\n\n    Parameters:\n    -----------------------------------------------------------------------\n    result:      A opf_utils.ModelResult object, which contains the input and\n                  output for this iteration"
  },
  {
    "code": "def __flushPredictionCache(self):\n    \"\"\"\n    Writes the contents of this model's in-memory prediction cache to a permanent\n    store via the prediction output stream instance\n    \"\"\"\n\n    if not self.__predictionCache:\n      return\n\n    # Create an output store, if one doesn't exist already\n    if self._predictionLogger is None:\n      self._createPredictionLogger()\n\n    startTime = time.time()\n    self._predictionLogger.writeRecords(self.__predictionCache,\n                                        progressCB=self.__writeRecordsCallback)\n    self._logger.info(\"Flushed prediction cache; numrows=%s; elapsed=%s sec.\",\n                      len(self.__predictionCache), time.time() - startTime)\n    self.__predictionCache.clear()",
    "doc": "Writes the contents of this model's in-memory prediction cache to a permanent\n    store via the prediction output stream instance"
  },
  {
    "code": "def __deleteOutputCache(self, modelID):\n    \"\"\"\n    Delete's the output cache associated with the given modelID. This actually\n    clears up the resources associated with the cache, rather than deleting al\n    the records in the cache\n\n    Parameters:\n    -----------------------------------------------------------------------\n    modelID:      The id of the model whose output cache is being deleted\n\n    \"\"\"\n\n    # If this is our output, we should close the connection\n    if modelID == self._modelID and self._predictionLogger is not None:\n      self._predictionLogger.close()\n      del self.__predictionCache\n      self._predictionLogger = None\n      self.__predictionCache = None",
    "doc": "Delete's the output cache associated with the given modelID. This actually\n    clears up the resources associated with the cache, rather than deleting al\n    the records in the cache\n\n    Parameters:\n    -----------------------------------------------------------------------\n    modelID:      The id of the model whose output cache is being deleted"
  },
  {
    "code": "def _initPeriodicActivities(self):\n    \"\"\" Creates and returns a PeriodicActivityMgr instance initialized with\n    our periodic activities\n\n    Parameters:\n    -------------------------------------------------------------------------\n    retval:             a PeriodicActivityMgr instance\n    \"\"\"\n\n    # Activity to update the metrics for this model\n    # in the models table\n    updateModelDBResults = PeriodicActivityRequest(repeating=True,\n                                                 period=100,\n                                                 cb=self._updateModelDBResults)\n\n    updateJobResults = PeriodicActivityRequest(repeating=True,\n                                               period=100,\n                                               cb=self.__updateJobResultsPeriodic)\n\n    checkCancelation = PeriodicActivityRequest(repeating=True,\n                                               period=50,\n                                               cb=self.__checkCancelation)\n\n    checkMaturity = PeriodicActivityRequest(repeating=True,\n                                            period=10,\n                                            cb=self.__checkMaturity)\n\n\n    # Do an initial update of the job record after 2 iterations to make\n    # sure that it is populated with something without having to wait too long\n    updateJobResultsFirst = PeriodicActivityRequest(repeating=False,\n                                               period=2,\n                                               cb=self.__updateJobResultsPeriodic)\n\n\n    periodicActivities = [updateModelDBResults,\n                          updateJobResultsFirst,\n                          updateJobResults,\n                          checkCancelation]\n\n    if self._isMaturityEnabled:\n      periodicActivities.append(checkMaturity)\n\n    return PeriodicActivityMgr(requestedActivities=periodicActivities)",
    "doc": "Creates and returns a PeriodicActivityMgr instance initialized with\n    our periodic activities\n\n    Parameters:\n    -------------------------------------------------------------------------\n    retval:             a PeriodicActivityMgr instance"
  },
  {
    "code": "def __checkCancelation(self):\n    \"\"\" Check if the cancelation flag has been set for this model\n    in the Model DB\"\"\"\n\n    # Update a hadoop job counter at least once every 600 seconds so it doesn't\n    #  think our map task is dead\n    print >>sys.stderr, \"reporter:counter:HypersearchWorker,numRecords,50\"\n\n    # See if the job got cancelled\n    jobCancel = self._jobsDAO.jobGetFields(self._jobID, ['cancel'])[0]\n    if jobCancel:\n      self._cmpReason = ClientJobsDAO.CMPL_REASON_KILLED\n      self._isCanceled = True\n      self._logger.info(\"Model %s canceled because Job %s was stopped.\",\n                        self._modelID, self._jobID)\n    else:\n      stopReason = self._jobsDAO.modelsGetFields(self._modelID, ['engStop'])[0]\n\n      if stopReason is None:\n        pass\n\n      elif stopReason == ClientJobsDAO.STOP_REASON_KILLED:\n        self._cmpReason = ClientJobsDAO.CMPL_REASON_KILLED\n        self._isKilled = True\n        self._logger.info(\"Model %s canceled because it was killed by hypersearch\",\n                          self._modelID)\n\n      elif stopReason == ClientJobsDAO.STOP_REASON_STOPPED:\n        self._cmpReason = ClientJobsDAO.CMPL_REASON_STOPPED\n        self._isCanceled = True\n        self._logger.info(\"Model %s stopped because hypersearch ended\", self._modelID)\n      else:\n        raise RuntimeError (\"Unexpected stop reason encountered: %s\" % (stopReason))",
    "doc": "Check if the cancelation flag has been set for this model\n    in the Model DB"
  },
  {
    "code": "def __checkMaturity(self):\n    \"\"\" Save the current metric value and see if the model's performance has\n    'leveled off.' We do this by looking at some number of previous number of\n    recordings \"\"\"\n\n    if self._currentRecordIndex+1 < self._MIN_RECORDS_TO_BE_BEST:\n      return\n\n    # If we are already mature, don't need to check anything\n    if self._isMature:\n      return\n\n    metric = self._getMetrics()[self._optimizedMetricLabel]\n    self._metricRegression.addPoint(x=self._currentRecordIndex, y=metric)\n\n   # Perform a linear regression to see if the error is leveled off\n    #pctChange = self._metricRegression.getPctChange()\n    #if pctChange  is not None and abs(pctChange ) <= self._MATURITY_MAX_CHANGE:\n    pctChange, absPctChange = self._metricRegression.getPctChanges()\n    if pctChange  is not None and absPctChange <= self._MATURITY_MAX_CHANGE:\n      self._jobsDAO.modelSetFields(self._modelID,\n                                   {'engMatured':True})\n\n      # TODO: Don't stop if we are currently the best model. Also, if we\n      # are still running after maturity, we have to periodically check to\n      # see if we are still the best model. As soon we lose to some other\n      # model, then we should stop at that point.\n      self._cmpReason = ClientJobsDAO.CMPL_REASON_STOPPED\n      self._isMature = True\n\n      self._logger.info(\"Model %d has matured (pctChange=%s, n=%d). \\n\"\\\n                        \"Scores = %s\\n\"\\\n                         \"Stopping execution\",self._modelID, pctChange,\n                                              self._MATURITY_NUM_POINTS,\n                                              self._metricRegression._window)",
    "doc": "Save the current metric value and see if the model's performance has\n    'leveled off.' We do this by looking at some number of previous number of\n    recordings"
  },
  {
    "code": "def __setAsOrphaned(self):\n    \"\"\"\n    Sets the current model as orphaned. This is called when the scheduler is\n    about to kill the process to reallocate the worker to a different process.\n    \"\"\"\n    cmplReason = ClientJobsDAO.CMPL_REASON_ORPHAN\n    cmplMessage = \"Killed by Scheduler\"\n    self._jobsDAO.modelSetCompleted(self._modelID, cmplReason, cmplMessage)",
    "doc": "Sets the current model as orphaned. This is called when the scheduler is\n    about to kill the process to reallocate the worker to a different process."
  },
  {
    "code": "def readStateFromDB(self):\n    \"\"\"Set our state to that obtained from the engWorkerState field of the\n    job record.\n\n\n    Parameters:\n    ---------------------------------------------------------------------\n    stateJSON:    JSON encoded state from job record\n\n    \"\"\"\n    self._priorStateJSON = self._hsObj._cjDAO.jobGetFields(self._hsObj._jobID,\n                                                    ['engWorkerState'])[0]\n\n    # Init if no prior state yet\n    if self._priorStateJSON is None:\n      swarms = dict()\n\n      # Fast Swarm, first and only sprint has one swarm for each field\n      # in fixedFields\n      if self._hsObj._fixedFields is not None:\n        print self._hsObj._fixedFields\n        encoderSet = []\n        for field in self._hsObj._fixedFields:\n            if field =='_classifierInput':\n              continue\n            encoderName = self.getEncoderKeyFromName(field)\n            assert encoderName in self._hsObj._encoderNames, \"The field '%s' \" \\\n              \" specified in the fixedFields list is not present in this \" \\\n              \" model.\" % (field)\n            encoderSet.append(encoderName)\n        encoderSet.sort()\n        swarms['.'.join(encoderSet)] = {\n                                'status': 'active',\n                                'bestModelId': None,\n                                'bestErrScore': None,\n                                'sprintIdx': 0,\n                                }\n      # Temporal prediction search, first sprint has N swarms of 1 field each,\n      #  the predicted field may or may not be that one field.\n      elif self._hsObj._searchType == HsSearchType.temporal:\n        for encoderName in self._hsObj._encoderNames:\n          swarms[encoderName] = {\n                                  'status': 'active',\n                                  'bestModelId': None,\n                                  'bestErrScore': None,\n                                  'sprintIdx': 0,\n                                  }\n\n\n      # Classification prediction search, first sprint has N swarms of 1 field\n      #  each where this field can NOT be the predicted field.\n      elif self._hsObj._searchType == HsSearchType.classification:\n        for encoderName in self._hsObj._encoderNames:\n          if encoderName == self._hsObj._predictedFieldEncoder:\n            continue\n          swarms[encoderName] = {\n                                  'status': 'active',\n                                  'bestModelId': None,\n                                  'bestErrScore': None,\n                                  'sprintIdx': 0,\n                                  }\n\n      # Legacy temporal. This is either a model that uses reconstruction or\n      #  an older multi-step model that doesn't have a separate\n      #  'classifierOnly' encoder for the predicted field. Here, the predicted\n      #  field must ALWAYS be present and the first sprint tries the predicted\n      #  field only\n      elif self._hsObj._searchType == HsSearchType.legacyTemporal:\n        swarms[self._hsObj._predictedFieldEncoder] = {\n                       'status': 'active',\n                       'bestModelId': None,\n                       'bestErrScore': None,\n                       'sprintIdx': 0,\n                       }\n\n      else:\n        raise RuntimeError(\"Unsupported search type: %s\" % \\\n                            (self._hsObj._searchType))\n\n      # Initialize the state.\n      self._state = dict(\n        # The last time the state was updated by a worker.\n        lastUpdateTime = time.time(),\n\n        # Set from within setSwarmState() if we detect that the sprint we just\n        #  completed did worse than a prior sprint. This stores the index of\n        #  the last good sprint.\n        lastGoodSprint = None,\n\n        # Set from within setSwarmState() if lastGoodSprint is True and all\n        #  sprints have completed.\n        searchOver = False,\n\n        # This is a summary of the active swarms - this information can also\n        #  be obtained from the swarms entry that follows, but is summarized here\n        #  for easier reference when viewing the state as presented by\n        #  log messages and prints of the hsState data structure (by\n        #  permutations_runner).\n        activeSwarms = swarms.keys(),\n\n        # All the swarms that have been created so far.\n        swarms = swarms,\n\n        # All the sprints that have completed or are in progress.\n        sprints = [{'status': 'active',\n                    'bestModelId': None,\n                    'bestErrScore': None}],\n\n        # The list of encoders we have \"blacklisted\" because they\n        #  performed so poorly.\n        blackListedEncoders = [],\n        )\n\n      # This will do nothing if the value of engWorkerState is not still None.\n      self._hsObj._cjDAO.jobSetFieldIfEqual(\n          self._hsObj._jobID, 'engWorkerState', json.dumps(self._state), None)\n\n      self._priorStateJSON = self._hsObj._cjDAO.jobGetFields(\n          self._hsObj._jobID, ['engWorkerState'])[0]\n      assert (self._priorStateJSON is not None)\n\n    # Read state from the database\n    self._state = json.loads(self._priorStateJSON)\n    self._dirty = False",
    "doc": "Set our state to that obtained from the engWorkerState field of the\n    job record.\n\n\n    Parameters:\n    ---------------------------------------------------------------------\n    stateJSON:    JSON encoded state from job record"
  },
  {
    "code": "def writeStateToDB(self):\n    \"\"\"Update the state in the job record with our local changes (if any).\n    If we don't have the latest state in our priorStateJSON, then re-load\n    in the latest state and return False. If we were successful writing out\n    our changes, return True\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:    True if we were successful writing out our changes\n               False if our priorState is not the latest that was in the DB.\n               In this case, we will re-load our state from the DB\n    \"\"\"\n    # If no changes, do nothing\n    if not self._dirty:\n      return True\n\n    # Set the update time\n    self._state['lastUpdateTime'] = time.time()\n    newStateJSON = json.dumps(self._state)\n    success = self._hsObj._cjDAO.jobSetFieldIfEqual(self._hsObj._jobID,\n                'engWorkerState', str(newStateJSON), str(self._priorStateJSON))\n\n    if success:\n      self.logger.debug(\"Success changing hsState to: \\n%s \" % \\\n                       (pprint.pformat(self._state, indent=4)))\n      self._priorStateJSON = newStateJSON\n\n    # If no success, read in the current state from the DB\n    else:\n      self.logger.debug(\"Failed to change hsState to: \\n%s \" % \\\n                       (pprint.pformat(self._state, indent=4)))\n\n      self._priorStateJSON = self._hsObj._cjDAO.jobGetFields(self._hsObj._jobID,\n                                                      ['engWorkerState'])[0]\n      self._state =  json.loads(self._priorStateJSON)\n\n      self.logger.info(\"New hsState has been set by some other worker to: \"\n                       \" \\n%s\" % (pprint.pformat(self._state, indent=4)))\n\n    return success",
    "doc": "Update the state in the job record with our local changes (if any).\n    If we don't have the latest state in our priorStateJSON, then re-load\n    in the latest state and return False. If we were successful writing out\n    our changes, return True\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:    True if we were successful writing out our changes\n               False if our priorState is not the latest that was in the DB.\n               In this case, we will re-load our state from the DB"
  },
  {
    "code": "def getFieldContributions(self):\n    \"\"\"Return the field contributions statistics.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   Dictionary where the keys are the field names and the values\n                are how much each field contributed to the best score.\n    \"\"\"\n\n    #in the fast swarm, there is only 1 sprint and field contributions are\n    #not defined\n    if self._hsObj._fixedFields is not None:\n      return dict(), dict()\n    # Get the predicted field encoder name\n    predictedEncoderName = self._hsObj._predictedFieldEncoder\n\n    # -----------------------------------------------------------------------\n    # Collect all the single field scores\n    fieldScores = []\n    for swarmId, info in self._state['swarms'].iteritems():\n      encodersUsed = swarmId.split('.')\n      if len(encodersUsed) != 1:\n        continue\n      field = self.getEncoderNameFromKey(encodersUsed[0])\n      bestScore = info['bestErrScore']\n\n      # If the bestScore is None, this swarm hasn't completed yet (this could\n      #  happen if we're exiting because of maxModels), so look up the best\n      #  score so far\n      if bestScore is None:\n        (_modelId, bestScore) = \\\n            self._hsObj._resultsDB.bestModelIdAndErrScore(swarmId)\n\n      fieldScores.append((bestScore, field))\n\n\n    # -----------------------------------------------------------------------\n    # If we only have 1 field that was tried in the first sprint, then use that\n    #  as the base and get the contributions from the fields in the next sprint.\n    if self._hsObj._searchType == HsSearchType.legacyTemporal:\n      assert(len(fieldScores)==1)\n      (baseErrScore, baseField) = fieldScores[0]\n\n      for swarmId, info in self._state['swarms'].iteritems():\n        encodersUsed = swarmId.split('.')\n        if len(encodersUsed) != 2:\n          continue\n\n        fields = [self.getEncoderNameFromKey(name) for name in encodersUsed]\n        fields.remove(baseField)\n\n        fieldScores.append((info['bestErrScore'], fields[0]))\n\n    # The first sprint tried a bunch of fields, pick the worst performing one\n    #  (within the top self._hsObj._maxBranching ones) as the base\n    else:\n      fieldScores.sort(reverse=True)\n\n      # If maxBranching was specified, pick the worst performing field within\n      #  the top maxBranching+1 fields as our base, which will give that field\n      #  a contribution of 0.\n      if self._hsObj._maxBranching > 0 \\\n              and len(fieldScores) > self._hsObj._maxBranching:\n        baseErrScore = fieldScores[-self._hsObj._maxBranching-1][0]\n      else:\n        baseErrScore = fieldScores[0][0]\n\n\n    # -----------------------------------------------------------------------\n    # Prepare and return the fieldContributions dict\n    pctFieldContributionsDict = dict()\n    absFieldContributionsDict = dict()\n\n    # If we have no base score, can't compute field contributions. This can\n    #  happen when we exit early due to maxModels or being cancelled\n    if baseErrScore is not None:\n\n      # If the base error score is 0, we can't compute a percent difference\n      #  off of it, so move it to a very small float\n      if abs(baseErrScore) < 0.00001:\n        baseErrScore = 0.00001\n      for (errScore, field) in fieldScores:\n        if errScore is not None:\n          pctBetter = (baseErrScore - errScore) * 100.0 / baseErrScore\n        else:\n          pctBetter = 0.0\n          errScore = baseErrScore   # for absFieldContribution\n\n        pctFieldContributionsDict[field] = pctBetter\n        absFieldContributionsDict[field] = baseErrScore - errScore\n\n    self.logger.debug(\"FieldContributions: %s\" % (pctFieldContributionsDict))\n    return pctFieldContributionsDict, absFieldContributionsDict",
    "doc": "Return the field contributions statistics.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   Dictionary where the keys are the field names and the values\n                are how much each field contributed to the best score."
  },
  {
    "code": "def getAllSwarms(self, sprintIdx):\n    \"\"\"Return the list of all swarms in the given sprint.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids in the given sprint\n    \"\"\"\n    swarmIds = []\n    for swarmId, info in self._state['swarms'].iteritems():\n      if info['sprintIdx'] == sprintIdx:\n        swarmIds.append(swarmId)\n\n    return swarmIds",
    "doc": "Return the list of all swarms in the given sprint.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids in the given sprint"
  },
  {
    "code": "def getCompletedSwarms(self):\n    \"\"\"Return the list of all completed swarms.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids\n    \"\"\"\n    swarmIds = []\n    for swarmId, info in self._state['swarms'].iteritems():\n      if info['status'] == 'completed':\n        swarmIds.append(swarmId)\n\n    return swarmIds",
    "doc": "Return the list of all completed swarms.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids"
  },
  {
    "code": "def getCompletingSwarms(self):\n    \"\"\"Return the list of all completing swarms.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids\n    \"\"\"\n    swarmIds = []\n    for swarmId, info in self._state['swarms'].iteritems():\n      if info['status'] == 'completing':\n        swarmIds.append(swarmId)\n\n    return swarmIds",
    "doc": "Return the list of all completing swarms.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   list of active swarm Ids"
  },
  {
    "code": "def bestModelInSprint(self, sprintIdx):\n    \"\"\"Return the best model ID and it's errScore from the given sprint,\n    which may still be in progress. This returns the best score from all models\n    in the sprint which have matured so far.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   (modelId, errScore)\n    \"\"\"\n    # Get all the swarms in this sprint\n    swarms = self.getAllSwarms(sprintIdx)\n\n    # Get the best model and score from each swarm\n    bestModelId = None\n    bestErrScore = numpy.inf\n    for swarmId in swarms:\n      (modelId, errScore) = self._hsObj._resultsDB.bestModelIdAndErrScore(swarmId)\n      if errScore < bestErrScore:\n        bestModelId = modelId\n        bestErrScore = errScore\n\n    return (bestModelId, bestErrScore)",
    "doc": "Return the best model ID and it's errScore from the given sprint,\n    which may still be in progress. This returns the best score from all models\n    in the sprint which have matured so far.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   (modelId, errScore)"
  },
  {
    "code": "def setSwarmState(self, swarmId, newStatus):\n    \"\"\"Change the given swarm's state to 'newState'. If 'newState' is\n    'completed', then bestModelId and bestErrScore must be provided.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    swarmId:      swarm Id\n    newStatus:    new status, either 'active', 'completing', 'completed', or\n                    'killed'\n    \"\"\"\n    assert (newStatus in ['active', 'completing', 'completed', 'killed'])\n\n    # Set the swarm status\n    swarmInfo = self._state['swarms'][swarmId]\n    if swarmInfo['status'] == newStatus:\n      return\n\n    # If some other worker noticed it as completed, setting it to completing\n    #  is obviously old information....\n    if swarmInfo['status'] == 'completed' and newStatus == 'completing':\n      return\n\n    self._dirty = True\n    swarmInfo['status'] = newStatus\n    if newStatus == 'completed':\n      (modelId, errScore) = self._hsObj._resultsDB.bestModelIdAndErrScore(swarmId)\n      swarmInfo['bestModelId'] = modelId\n      swarmInfo['bestErrScore'] = errScore\n\n    # If no longer active, remove it from the activeSwarms entry\n    if newStatus != 'active' and swarmId in self._state['activeSwarms']:\n      self._state['activeSwarms'].remove(swarmId)\n\n    # If new status is 'killed', kill off any running particles in that swarm\n    if newStatus=='killed':\n      self._hsObj.killSwarmParticles(swarmId)\n\n    # In case speculative particles are enabled, make sure we generate a new\n    #  swarm at this time if all of the swarms in the current sprint have\n    #  completed. This will insure that we don't mark the sprint as completed\n    #  before we've created all the possible swarms.\n    sprintIdx = swarmInfo['sprintIdx']\n    self.isSprintActive(sprintIdx)\n\n    # Update the sprint status. Check all the swarms that belong to this sprint.\n    #  If they are all completed, the sprint is completed.\n    sprintInfo = self._state['sprints'][sprintIdx]\n\n    statusCounts = dict(active=0, completing=0, completed=0, killed=0)\n    bestModelIds = []\n    bestErrScores = []\n    for info in self._state['swarms'].itervalues():\n      if info['sprintIdx'] != sprintIdx:\n        continue\n      statusCounts[info['status']] += 1\n      if info['status'] == 'completed':\n        bestModelIds.append(info['bestModelId'])\n        bestErrScores.append(info['bestErrScore'])\n\n    if statusCounts['active'] > 0:\n      sprintStatus = 'active'\n    elif statusCounts['completing'] > 0:\n      sprintStatus = 'completing'\n    else:\n      sprintStatus = 'completed'\n    sprintInfo['status'] = sprintStatus\n\n    # If the sprint is complete, get the best model from all of its swarms and\n    #  store that as the sprint best\n    if sprintStatus == 'completed':\n      if len(bestErrScores) > 0:\n        whichIdx = numpy.array(bestErrScores).argmin()\n        sprintInfo['bestModelId'] = bestModelIds[whichIdx]\n        sprintInfo['bestErrScore'] = bestErrScores[whichIdx]\n      else:\n        # This sprint was empty, most likely because all particles were\n        #  killed. Give it a huge error score\n        sprintInfo['bestModelId'] = 0\n        sprintInfo['bestErrScore'] = numpy.inf\n\n\n      # See if our best err score got NO BETTER as compared to a previous\n      #  sprint. If so, stop exploring subsequent sprints (lastGoodSprint\n      #  is no longer None).\n      bestPrior = numpy.inf\n      for idx in range(sprintIdx):\n        if self._state['sprints'][idx]['status'] == 'completed':\n          (_, errScore) = self.bestModelInCompletedSprint(idx)\n          if errScore is None:\n            errScore = numpy.inf\n        else:\n          errScore = numpy.inf\n        if errScore < bestPrior:\n          bestPrior = errScore\n\n      if sprintInfo['bestErrScore'] >= bestPrior:\n        self._state['lastGoodSprint'] = sprintIdx-1\n\n      # If ALL sprints up to the last good one are done, the search is now over\n      if self._state['lastGoodSprint'] is not None \\\n            and not self.anyGoodSprintsActive():\n        self._state['searchOver'] = True",
    "doc": "Change the given swarm's state to 'newState'. If 'newState' is\n    'completed', then bestModelId and bestErrScore must be provided.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    swarmId:      swarm Id\n    newStatus:    new status, either 'active', 'completing', 'completed', or\n                    'killed'"
  },
  {
    "code": "def anyGoodSprintsActive(self):\n    \"\"\"Return True if there are any more good sprints still being explored.\n    A 'good' sprint is one that is earlier than where we detected an increase\n    in error from sprint to subsequent sprint.\n    \"\"\"\n    if self._state['lastGoodSprint'] is not None:\n      goodSprints = self._state['sprints'][0:self._state['lastGoodSprint']+1]\n    else:\n      goodSprints = self._state['sprints']\n\n    for sprint in goodSprints:\n      if sprint['status'] == 'active':\n        anyActiveSprints = True\n        break\n    else:\n      anyActiveSprints = False\n\n    return anyActiveSprints",
    "doc": "Return True if there are any more good sprints still being explored.\n    A 'good' sprint is one that is earlier than where we detected an increase\n    in error from sprint to subsequent sprint."
  },
  {
    "code": "def isSprintCompleted(self, sprintIdx):\n    \"\"\"Return True if the given sprint has completed.\"\"\"\n    numExistingSprints = len(self._state['sprints'])\n    if sprintIdx >= numExistingSprints:\n      return False\n\n    return (self._state['sprints'][sprintIdx]['status'] == 'completed')",
    "doc": "Return True if the given sprint has completed."
  },
  {
    "code": "def killUselessSwarms(self):\n    \"\"\"See if we can kill off some speculative swarms. If an earlier sprint\n    has finally completed, we can now tell which fields should *really* be present\n    in the sprints we've already started due to speculation, and kill off the\n    swarms that should not have been included.\n    \"\"\"\n    # Get number of existing sprints\n    numExistingSprints = len(self._state['sprints'])\n\n    # Should we bother killing useless swarms?\n    if self._hsObj._searchType == HsSearchType.legacyTemporal:\n      if numExistingSprints <= 2:\n        return\n    else:\n      if numExistingSprints <= 1:\n        return\n\n    # Form completedSwarms as a list of tuples, each tuple contains:\n    #  (swarmName, swarmState, swarmBestErrScore)\n    # ex. completedSwarms:\n    #    [('a', {...}, 1.4),\n    #     ('b', {...}, 2.0),\n    #     ('c', {...}, 3.0)]\n    completedSwarms = self.getCompletedSwarms()\n    completedSwarms = [(swarm, self._state[\"swarms\"][swarm],\n                        self._state[\"swarms\"][swarm][\"bestErrScore\"]) \\\n                                                for swarm in completedSwarms]\n\n    # Form the completedMatrix. Each row corresponds to a sprint. Each row\n    #  contains the list of swarm tuples that belong to that sprint, sorted\n    #  by best score. Each swarm tuple contains (swarmName, swarmState,\n    #  swarmBestErrScore).\n    # ex. completedMatrix:\n    #    [(('a', {...}, 1.4), ('b', {...}, 2.0), ('c', {...}, 3.0)),\n    #     (('a.b', {...}, 3.0), ('b.c', {...}, 4.0))]\n    completedMatrix = [[] for i in range(numExistingSprints)]\n    for swarm in completedSwarms:\n      completedMatrix[swarm[1][\"sprintIdx\"]].append(swarm)\n    for sprint in completedMatrix:\n      sprint.sort(key=itemgetter(2))\n\n    # Form activeSwarms as a list of tuples, each tuple contains:\n    #  (swarmName, swarmState, swarmBestErrScore)\n    # Include all activeSwarms and completingSwarms\n    # ex. activeSwarms:\n    #    [('d', {...}, 1.4),\n    #     ('e', {...}, 2.0),\n    #     ('f', {...}, 3.0)]\n    activeSwarms = self.getActiveSwarms()\n    # Append the completing swarms\n    activeSwarms.extend(self.getCompletingSwarms())\n    activeSwarms = [(swarm, self._state[\"swarms\"][swarm],\n                     self._state[\"swarms\"][swarm][\"bestErrScore\"]) \\\n                                                for swarm in activeSwarms]\n\n    # Form the activeMatrix. Each row corresponds to a sprint. Each row\n    #  contains the list of swarm tuples that belong to that sprint, sorted\n    #  by best score. Each swarm tuple contains (swarmName, swarmState,\n    #  swarmBestErrScore)\n    # ex. activeMatrix:\n    #    [(('d', {...}, 1.4), ('e', {...}, 2.0), ('f', {...}, 3.0)),\n    #     (('d.e', {...}, 3.0), ('e.f', {...}, 4.0))]\n    activeMatrix = [[] for i in range(numExistingSprints)]\n    for swarm in activeSwarms:\n      activeMatrix[swarm[1][\"sprintIdx\"]].append(swarm)\n    for sprint in activeMatrix:\n      sprint.sort(key=itemgetter(2))\n\n\n    # Figure out which active swarms to kill\n    toKill = []\n    for i in range(1, numExistingSprints):\n      for swarm in activeMatrix[i]:\n        curSwarmEncoders = swarm[0].split(\".\")\n\n        # If previous sprint is complete, get the best swarm and kill all active\n        #  sprints that are not supersets\n        if(len(activeMatrix[i-1])==0):\n          # If we are trying all possible 3 field combinations, don't kill any\n          #  off in sprint 2\n          if i==2 and (self._hsObj._tryAll3FieldCombinations or \\\n                self._hsObj._tryAll3FieldCombinationsWTimestamps):\n            pass\n          else:\n            bestInPrevious = completedMatrix[i-1][0]\n            bestEncoders = bestInPrevious[0].split('.')\n            for encoder in bestEncoders:\n              if not encoder in curSwarmEncoders:\n                toKill.append(swarm)\n\n        # if there are more than two completed encoders sets that are complete and\n        # are worse than at least one active swarm in the previous sprint. Remove\n        # any combinations that have any pair of them since they cannot have the best encoder.\n        #elif(len(completedMatrix[i-1])>1):\n        #  for completedSwarm in completedMatrix[i-1]:\n        #    activeMatrix[i-1][0][2]<completed\n\n    # Mark the bad swarms as killed\n    if len(toKill) > 0:\n      print \"ParseMe: Killing encoders:\" + str(toKill)\n\n    for swarm in toKill:\n      self.setSwarmState(swarm[0], \"killed\")\n\n    return",
    "doc": "See if we can kill off some speculative swarms. If an earlier sprint\n    has finally completed, we can now tell which fields should *really* be present\n    in the sprints we've already started due to speculation, and kill off the\n    swarms that should not have been included."
  },
  {
    "code": "def isSprintActive(self, sprintIdx):\n    \"\"\"If the given sprint exists and is active, return active=True.\n\n    If the sprint does not exist yet, this call will create it (and return\n    active=True). If it already exists, but is completing or complete, return\n    active=False.\n\n    If sprintIdx is past the end of the possible sprints, return\n      active=False, noMoreSprints=True\n\n    IMPORTANT: When speculative particles are enabled, this call has some\n    special processing to handle speculative sprints:\n\n      * When creating a new speculative sprint (creating sprint N before\n      sprint N-1 has completed), it initially only puts in only ONE swarm into\n      the sprint.\n\n      * Every time it is asked if sprint N is active, it also checks to see if\n      it is time to add another swarm to the sprint, and adds a new swarm if\n      appropriate before returning active=True\n\n      * We decide it is time to add a new swarm to a speculative sprint when ALL\n      of the currently active swarms in the sprint have all the workers they\n      need (number of running (not mature) particles is _minParticlesPerSwarm).\n      This means that we have capacity to run additional particles in a new\n      swarm.\n\n    It is expected that the sprints will be checked IN ORDER from 0 on up. (It\n    is an error not to) The caller should always try to allocate from the first\n    active sprint it finds. If it can't, then it can call this again to\n    find/create the next active sprint.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   (active, noMoreSprints)\n                active: True if the given sprint is active\n                noMoreSprints: True if there are no more sprints possible\n    \"\"\"\n\n    while True:\n      numExistingSprints = len(self._state['sprints'])\n\n      # If this sprint already exists, see if it is active\n      if sprintIdx <= numExistingSprints-1:\n\n        # With speculation off, it's simple, just return whether or not the\n        #  asked for sprint has active status\n        if not self._hsObj._speculativeParticles:\n          active = (self._state['sprints'][sprintIdx]['status'] == 'active')\n          return (active, False)\n\n        # With speculation on, if the sprint is still marked active, we also\n        #  need to see if it's time to add a new swarm to it.\n        else:\n          active = (self._state['sprints'][sprintIdx]['status'] == 'active')\n          if not active:\n            return (active, False)\n\n          # See if all of the existing swarms are at capacity (have all the\n          # workers they need):\n          activeSwarmIds = self.getActiveSwarms(sprintIdx)\n          swarmSizes = [self._hsObj._resultsDB.getParticleInfos(swarmId,\n                              matured=False)[0] for swarmId in activeSwarmIds]\n          notFullSwarms = [len(swarm) for swarm in swarmSizes \\\n                           if len(swarm) < self._hsObj._minParticlesPerSwarm]\n\n          # If some swarms have room return that the swarm is active.\n          if len(notFullSwarms) > 0:\n            return (True, False)\n\n          # If the existing swarms are at capacity, we will fall through to the\n          #  logic below which tries to add a new swarm to the sprint.\n\n      # Stop creating new sprints?\n      if self._state['lastGoodSprint'] is not None:\n        return (False, True)\n\n      # if fixedFields is set, we are running a fast swarm and only run sprint0\n      if self._hsObj._fixedFields is not None:\n        return (False, True)\n\n      # ----------------------------------------------------------------------\n      # Get the best model (if there is one) from the prior sprint. That gives\n      # us the base encoder set for the next sprint. For sprint zero make sure\n      # it does not take the last sprintidx because of wrapping.\n      if sprintIdx > 0  \\\n            and self._state['sprints'][sprintIdx-1]['status'] == 'completed':\n        (bestModelId, _) = self.bestModelInCompletedSprint(sprintIdx-1)\n        (particleState, _, _, _, _) = self._hsObj._resultsDB.getParticleInfo(\n                                                                  bestModelId)\n        bestSwarmId = particleState['swarmId']\n        baseEncoderSets = [bestSwarmId.split('.')]\n\n      # If there is no best model yet, then use all encoder sets from the prior\n      #  sprint that were not killed\n      else:\n        bestSwarmId = None\n        particleState = None\n        # Build up more combinations, using ALL of the sets in the current\n        #  sprint.\n        baseEncoderSets = []\n        for swarmId in self.getNonKilledSwarms(sprintIdx-1):\n          baseEncoderSets.append(swarmId.split('.'))\n\n      # ----------------------------------------------------------------------\n      # Which encoders should we add to the current base set?\n      encoderAddSet = []\n\n      # If we have constraints on how many fields we carry forward into\n      # subsequent sprints (either nupic.hypersearch.max.field.branching or\n      # nupic.hypersearch.min.field.contribution was set), then be more\n      # picky about which fields we add in.\n      limitFields = False\n      if self._hsObj._maxBranching > 0 \\\n            or self._hsObj._minFieldContribution >= 0:\n        if self._hsObj._searchType == HsSearchType.temporal or \\\n            self._hsObj._searchType == HsSearchType.classification:\n          if sprintIdx >= 1:\n            limitFields = True\n            baseSprintIdx = 0\n        elif self._hsObj._searchType == HsSearchType.legacyTemporal:\n          if sprintIdx >= 2:\n            limitFields = True\n            baseSprintIdx = 1\n        else:\n          raise RuntimeError(\"Unimplemented search type %s\" % \\\n                                  (self._hsObj._searchType))\n\n\n      # Only add top _maxBranching encoders to the swarms?\n      if limitFields:\n\n        # Get field contributions to filter added fields\n        pctFieldContributions, absFieldContributions = \\\n                                                self.getFieldContributions()\n        toRemove = []\n        self.logger.debug(\"FieldContributions min: %s\" % \\\n                          (self._hsObj._minFieldContribution))\n        for fieldname in pctFieldContributions:\n          if pctFieldContributions[fieldname] < self._hsObj._minFieldContribution:\n            self.logger.debug(\"FieldContributions removing: %s\" % (fieldname))\n            toRemove.append(self.getEncoderKeyFromName(fieldname))\n          else:\n            self.logger.debug(\"FieldContributions keeping: %s\" % (fieldname))\n\n\n        # Grab the top maxBranching base sprint swarms.\n        swarms = self._state[\"swarms\"]\n        sprintSwarms = [(swarm, swarms[swarm][\"bestErrScore\"]) \\\n            for swarm in swarms if swarms[swarm][\"sprintIdx\"] == baseSprintIdx]\n        sprintSwarms = sorted(sprintSwarms, key=itemgetter(1))\n        if self._hsObj._maxBranching > 0:\n          sprintSwarms = sprintSwarms[0:self._hsObj._maxBranching]\n\n        # Create encoder set to generate further swarms.\n        for swarm in sprintSwarms:\n          swarmEncoders = swarm[0].split(\".\")\n          for encoder in swarmEncoders:\n            if not encoder in encoderAddSet:\n              encoderAddSet.append(encoder)\n        encoderAddSet = [encoder for encoder in encoderAddSet \\\n                         if not str(encoder) in toRemove]\n\n      # If no limit on the branching or min contribution, simply use all of the\n      # encoders.\n      else:\n        encoderAddSet = self._hsObj._encoderNames\n\n\n      # -----------------------------------------------------------------------\n      # Build up the new encoder combinations for the next sprint.\n      newSwarmIds = set()\n\n      # See if the caller wants to try more extensive field combinations with\n      #  3 fields.\n      if (self._hsObj._searchType == HsSearchType.temporal \\\n           or self._hsObj._searchType == HsSearchType.legacyTemporal) \\\n          and sprintIdx == 2 \\\n          and (self._hsObj._tryAll3FieldCombinations or \\\n               self._hsObj._tryAll3FieldCombinationsWTimestamps):\n\n        if self._hsObj._tryAll3FieldCombinations:\n          newEncoders = set(self._hsObj._encoderNames)\n          if self._hsObj._predictedFieldEncoder in newEncoders:\n            newEncoders.remove(self._hsObj._predictedFieldEncoder)\n        else:\n          # Just make sure the timestamp encoders are part of the mix\n          newEncoders = set(encoderAddSet)\n          if self._hsObj._predictedFieldEncoder in newEncoders:\n            newEncoders.remove(self._hsObj._predictedFieldEncoder)\n          for encoder in self._hsObj._encoderNames:\n            if encoder.endswith('_timeOfDay') or encoder.endswith('_weekend') \\\n                or encoder.endswith('_dayOfWeek'):\n              newEncoders.add(encoder)\n\n        allCombos = list(itertools.combinations(newEncoders, 2))\n        for combo in allCombos:\n          newSet = list(combo)\n          newSet.append(self._hsObj._predictedFieldEncoder)\n          newSet.sort()\n          newSwarmId = '.'.join(newSet)\n          if newSwarmId not in self._state['swarms']:\n            newSwarmIds.add(newSwarmId)\n\n            # If a speculative sprint, only add the first encoder, if not add\n            #   all of them.\n            if (len(self.getActiveSwarms(sprintIdx-1)) > 0):\n              break\n\n      # Else, we only build up by adding 1 new encoder to the best combination(s)\n      #  we've seen from the prior sprint\n      else:\n        for baseEncoderSet in baseEncoderSets:\n          for encoder in encoderAddSet:\n            if encoder not in self._state['blackListedEncoders'] \\\n                and encoder not in baseEncoderSet:\n              newSet = list(baseEncoderSet)\n              newSet.append(encoder)\n              newSet.sort()\n              newSwarmId = '.'.join(newSet)\n              if newSwarmId not in self._state['swarms']:\n                newSwarmIds.add(newSwarmId)\n\n                # If a speculative sprint, only add the first encoder, if not add\n                #   all of them.\n                if (len(self.getActiveSwarms(sprintIdx-1)) > 0):\n                  break\n\n\n      # ----------------------------------------------------------------------\n      # Sort the new swarm Ids\n      newSwarmIds = sorted(newSwarmIds)\n\n      # If no more swarms can be found for this sprint...\n      if len(newSwarmIds) == 0:\n        # if sprint is not an empty sprint return that it is active but do not\n        #  add anything to it.\n        if len(self.getAllSwarms(sprintIdx)) > 0:\n          return (True, False)\n\n        # If this is an empty sprint and we couldn't find any new swarms to\n        #   add (only bad fields are remaining), the search is over\n        else:\n          return (False, True)\n\n      # Add this sprint and the swarms that are in it to our state\n      self._dirty = True\n\n      # Add in the new sprint if necessary\n      if len(self._state[\"sprints\"]) == sprintIdx:\n        self._state['sprints'].append({'status': 'active',\n                                       'bestModelId': None,\n                                       'bestErrScore': None})\n\n      # Add in the new swarm(s) to the sprint\n      for swarmId in newSwarmIds:\n        self._state['swarms'][swarmId] = {'status': 'active',\n                                            'bestModelId': None,\n                                            'bestErrScore': None,\n                                            'sprintIdx': sprintIdx}\n\n      # Update the list of active swarms\n      self._state['activeSwarms'] = self.getActiveSwarms()\n\n      # Try to set new state\n      success = self.writeStateToDB()\n\n      # Return result if successful\n      if success:\n        return (True, False)",
    "doc": "If the given sprint exists and is active, return active=True.\n\n    If the sprint does not exist yet, this call will create it (and return\n    active=True). If it already exists, but is completing or complete, return\n    active=False.\n\n    If sprintIdx is past the end of the possible sprints, return\n      active=False, noMoreSprints=True\n\n    IMPORTANT: When speculative particles are enabled, this call has some\n    special processing to handle speculative sprints:\n\n      * When creating a new speculative sprint (creating sprint N before\n      sprint N-1 has completed), it initially only puts in only ONE swarm into\n      the sprint.\n\n      * Every time it is asked if sprint N is active, it also checks to see if\n      it is time to add another swarm to the sprint, and adds a new swarm if\n      appropriate before returning active=True\n\n      * We decide it is time to add a new swarm to a speculative sprint when ALL\n      of the currently active swarms in the sprint have all the workers they\n      need (number of running (not mature) particles is _minParticlesPerSwarm).\n      This means that we have capacity to run additional particles in a new\n      swarm.\n\n    It is expected that the sprints will be checked IN ORDER from 0 on up. (It\n    is an error not to) The caller should always try to allocate from the first\n    active sprint it finds. If it can't, then it can call this again to\n    find/create the next active sprint.\n\n    Parameters:\n    ---------------------------------------------------------------------\n    retval:   (active, noMoreSprints)\n                active: True if the given sprint is active\n                noMoreSprints: True if there are no more sprints possible"
  },
  {
    "code": "def addEncoder(self, name, encoder):\n    \"\"\"\n    Adds one encoder.\n\n    :param name: (string) name of encoder, should be unique\n    :param encoder: (:class:`.Encoder`) the encoder to add\n    \"\"\"\n    self.encoders.append((name, encoder, self.width))\n    for d in encoder.getDescription():\n      self.description.append((d[0], d[1] + self.width))\n    self.width += encoder.getWidth()",
    "doc": "Adds one encoder.\n\n    :param name: (string) name of encoder, should be unique\n    :param encoder: (:class:`.Encoder`) the encoder to add"
  },
  {
    "code": "def invariant(self):\n    \"\"\"Verify the validity of the node spec object\n\n    The type of each sub-object is verified and then\n    the validity of each node spec item is verified by calling\n    it invariant() method. It also makes sure that there is at most\n    one default input and one default output.\n    \"\"\"\n    # Verify the description and singleNodeOnly attributes\n    assert isinstance(self.description, str)\n    assert isinstance(self.singleNodeOnly, bool)\n\n    # Make sure that all items dicts are really dicts\n    assert isinstance(self.inputs, dict)\n    assert isinstance(self.outputs, dict)\n    assert isinstance(self.parameters, dict)\n    assert isinstance(self.commands, dict)\n\n    # Verify all item dicts\n    hasDefaultInput = False\n    for k, v in self.inputs.items():\n      assert isinstance(k, str)\n      assert isinstance(v, InputSpec)\n      v.invariant()\n      if v.isDefaultInput:\n        assert not hasDefaultInput\n        hasDefaultInput = True\n\n\n    hasDefaultOutput = False\n    for k, v in self.outputs.items():\n      assert isinstance(k, str)\n      assert isinstance(v, OutputSpec)\n      v.invariant()\n      if v.isDefaultOutput:\n        assert not hasDefaultOutput\n        hasDefaultOutput = True\n\n    for k, v in self.parameters.items():\n      assert isinstance(k, str)\n      assert isinstance(v, ParameterSpec)\n      v.invariant()\n\n    for k, v in self.commands.items():\n      assert isinstance(k, str)\n      assert isinstance(v, CommandSpec)\n      v.invariant()",
    "doc": "Verify the validity of the node spec object\n\n    The type of each sub-object is verified and then\n    the validity of each node spec item is verified by calling\n    it invariant() method. It also makes sure that there is at most\n    one default input and one default output."
  },
  {
    "code": "def toDict(self):\n    \"\"\"Convert the information of the node spec to a plain dict of basic types\n\n    The description and singleNodeOnly attributes are placed directly in\n    the result dicts. The inputs, outputs, parameters and commands dicts\n    contain Spec item objects (InputSpec, OutputSpec, etc). Each such object\n    is converted also to a plain dict using the internal items2dict() function\n    (see bellow).\n    \"\"\"\n\n    def items2dict(items):\n      \"\"\"Convert a dict of node spec items to a plain dict\n\n      Each node spec item object will be converted to a dict of its\n      attributes. The entire items dict will become a dict of dicts (same keys).\n      \"\"\"\n      d = {}\n      for k, v in items.items():\n        d[k] = v.__dict__\n\n      return d\n\n    self.invariant()\n    return dict(description=self.description,\n                singleNodeOnly=self.singleNodeOnly,\n                inputs=items2dict(self.inputs),\n                outputs=items2dict(self.outputs),\n                parameters=items2dict(self.parameters),\n                commands=items2dict(self.commands))",
    "doc": "Convert the information of the node spec to a plain dict of basic types\n\n    The description and singleNodeOnly attributes are placed directly in\n    the result dicts. The inputs, outputs, parameters and commands dicts\n    contain Spec item objects (InputSpec, OutputSpec, etc). Each such object\n    is converted also to a plain dict using the internal items2dict() function\n    (see bellow)."
  },
  {
    "code": "def updateResultsForJob(self, forceUpdate=True):\n    \"\"\" Chooses the best model for a given job.\n\n    Parameters\n    -----------------------------------------------------------------------\n    forceUpdate:  (True/False). If True, the update will ignore all the\n                  restrictions on the minimum time to update and the minimum\n                  number of records to update. This should typically only be\n                  set to true if the model has completed running\n    \"\"\"\n    updateInterval = time.time() - self._lastUpdateAttemptTime\n    if updateInterval < self._MIN_UPDATE_INTERVAL and not forceUpdate:\n      return\n\n    self.logger.info(\"Attempting model selection for jobID=%d: time=%f\"\\\n                     \"  lastUpdate=%f\"%(self._jobID,\n                                        time.time(),\n                                        self._lastUpdateAttemptTime))\n\n    timestampUpdated = self._cjDB.jobUpdateSelectionSweep(self._jobID,\n                                                          self._MIN_UPDATE_INTERVAL)\n    if not timestampUpdated:\n      self.logger.info(\"Unable to update selection sweep timestamp: jobID=%d\" \\\n                       \" updateTime=%f\"%(self._jobID, self._lastUpdateAttemptTime))\n      if not forceUpdate:\n        return\n\n    self._lastUpdateAttemptTime = time.time()\n    self.logger.info(\"Succesfully updated selection sweep timestamp jobid=%d updateTime=%f\"\\\n                     %(self._jobID, self._lastUpdateAttemptTime))\n\n    minUpdateRecords = self._MIN_UPDATE_THRESHOLD\n\n    jobResults = self._getJobResults()\n    if forceUpdate or jobResults is None:\n      minUpdateRecords = 0\n\n    candidateIDs, bestMetric = self._cjDB.modelsGetCandidates(self._jobID, minUpdateRecords)\n\n    self.logger.info(\"Candidate models=%s, metric=%s, jobID=%s\"\\\n                     %(candidateIDs, bestMetric, self._jobID))\n\n    if len(candidateIDs) == 0:\n      return\n\n    self._jobUpdateCandidate(candidateIDs[0], bestMetric, results=jobResults)",
    "doc": "Chooses the best model for a given job.\n\n    Parameters\n    -----------------------------------------------------------------------\n    forceUpdate:  (True/False). If True, the update will ignore all the\n                  restrictions on the minimum time to update and the minimum\n                  number of records to update. This should typically only be\n                  set to true if the model has completed running"
  },
  {
    "code": "def createEncoder():\n  \"\"\"Create the encoder instance for our test and return it.\"\"\"\n  consumption_encoder = ScalarEncoder(21, 0.0, 100.0, n=50, name=\"consumption\",\n      clipInput=True)\n  time_encoder = DateEncoder(timeOfDay=(21, 9.5), name=\"timestamp_timeOfDay\")\n\n  encoder = MultiEncoder()\n  encoder.addEncoder(\"consumption\", consumption_encoder)\n  encoder.addEncoder(\"timestamp\", time_encoder)\n\n  return encoder",
    "doc": "Create the encoder instance for our test and return it."
  },
  {
    "code": "def createNetwork(dataSource):\n  \"\"\"Create the Network instance.\n\n  The network has a sensor region reading data from `dataSource` and passing\n  the encoded representation to an SPRegion. The SPRegion output is passed to\n  a TMRegion.\n\n  :param dataSource: a RecordStream instance to get data from\n  :returns: a Network instance ready to run\n  \"\"\"\n  network = Network()\n\n  # Our input is sensor data from the gym file. The RecordSensor region\n  # allows us to specify a file record stream as the input source via the\n  # dataSource attribute.\n  network.addRegion(\"sensor\", \"py.RecordSensor\",\n                    json.dumps({\"verbosity\": _VERBOSITY}))\n  sensor = network.regions[\"sensor\"].getSelf()\n  # The RecordSensor needs to know how to encode the input values\n  sensor.encoder = createEncoder()\n  # Specify the dataSource as a file record stream instance\n  sensor.dataSource = dataSource\n\n  # Create the spatial pooler region\n  SP_PARAMS[\"inputWidth\"] = sensor.encoder.getWidth()\n  network.addRegion(\"spatialPoolerRegion\", \"py.SPRegion\", json.dumps(SP_PARAMS))\n\n  # Link the SP region to the sensor input\n  network.link(\"sensor\", \"spatialPoolerRegion\", \"UniformLink\", \"\")\n  network.link(\"sensor\", \"spatialPoolerRegion\", \"UniformLink\", \"\",\n               srcOutput=\"resetOut\", destInput=\"resetIn\")\n  network.link(\"spatialPoolerRegion\", \"sensor\", \"UniformLink\", \"\",\n               srcOutput=\"spatialTopDownOut\", destInput=\"spatialTopDownIn\")\n  network.link(\"spatialPoolerRegion\", \"sensor\", \"UniformLink\", \"\",\n               srcOutput=\"temporalTopDownOut\", destInput=\"temporalTopDownIn\")\n\n  # Add the TMRegion on top of the SPRegion\n  network.addRegion(\"temporalPoolerRegion\", \"py.TMRegion\",\n                    json.dumps(TM_PARAMS))\n\n  network.link(\"spatialPoolerRegion\", \"temporalPoolerRegion\", \"UniformLink\", \"\")\n  network.link(\"temporalPoolerRegion\", \"spatialPoolerRegion\", \"UniformLink\", \"\",\n               srcOutput=\"topDownOut\", destInput=\"topDownIn\")\n\n  # Add the AnomalyLikelihoodRegion on top of the TMRegion\n  network.addRegion(\"anomalyLikelihoodRegion\", \"py.AnomalyLikelihoodRegion\",\n    json.dumps({}))\n  \n  network.link(\"temporalPoolerRegion\", \"anomalyLikelihoodRegion\", \"UniformLink\",\n               \"\", srcOutput=\"anomalyScore\", destInput=\"rawAnomalyScore\")\n  network.link(\"sensor\", \"anomalyLikelihoodRegion\", \"UniformLink\", \"\",\n               srcOutput=\"sourceOut\", destInput=\"metricValue\")\n  \n\n  spatialPoolerRegion = network.regions[\"spatialPoolerRegion\"]\n\n  # Make sure learning is enabled\n  spatialPoolerRegion.setParameter(\"learningMode\", True)\n  # We want temporal anomalies so disable anomalyMode in the SP. This mode is\n  # used for computing anomalies in a non-temporal model.\n  spatialPoolerRegion.setParameter(\"anomalyMode\", False)\n\n  temporalPoolerRegion = network.regions[\"temporalPoolerRegion\"]\n\n  # Enable topDownMode to get the predicted columns output\n  temporalPoolerRegion.setParameter(\"topDownMode\", True)\n  # Make sure learning is enabled (this is the default)\n  temporalPoolerRegion.setParameter(\"learningMode\", True)\n  # Enable inference mode so we get predictions\n  temporalPoolerRegion.setParameter(\"inferenceMode\", True)\n  # Enable anomalyMode to compute the anomaly score to be passed to the anomaly\n  # likelihood region. \n  temporalPoolerRegion.setParameter(\"anomalyMode\", True)\n\n  return network",
    "doc": "Create the Network instance.\n\n  The network has a sensor region reading data from `dataSource` and passing\n  the encoded representation to an SPRegion. The SPRegion output is passed to\n  a TMRegion.\n\n  :param dataSource: a RecordStream instance to get data from\n  :returns: a Network instance ready to run"
  },
  {
    "code": "def runNetwork(network, writer):\n  \"\"\"Run the network and write output to writer.\n\n  :param network: a Network instance to run\n  :param writer: a csv.writer instance to write output to\n  \"\"\"\n  sensorRegion = network.regions[\"sensor\"]\n  spatialPoolerRegion = network.regions[\"spatialPoolerRegion\"]\n  temporalPoolerRegion = network.regions[\"temporalPoolerRegion\"]\n  anomalyLikelihoodRegion = network.regions[\"anomalyLikelihoodRegion\"]\n\n  prevPredictedColumns = []\n\n  for i in xrange(_NUM_RECORDS):\n    # Run the network for a single iteration\n    network.run(1)\n\n    # Write out the anomaly likelihood along with the record number and consumption\n    # value.\n    consumption = sensorRegion.getOutputData(\"sourceOut\")[0]\n    anomalyScore = temporalPoolerRegion.getOutputData(\"anomalyScore\")[0]\n    anomalyLikelihood = anomalyLikelihoodRegion.getOutputData(\"anomalyLikelihood\")[0]\n    writer.writerow((i, consumption, anomalyScore, anomalyLikelihood))",
    "doc": "Run the network and write output to writer.\n\n  :param network: a Network instance to run\n  :param writer: a csv.writer instance to write output to"
  },
  {
    "code": "def __validateExperimentControl(self, control):\n    \"\"\" Validates control dictionary for the experiment context\"\"\"\n    # Validate task list\n    taskList = control.get('tasks', None)\n    if taskList is not None:\n      taskLabelsList = []\n\n      for task in taskList:\n        validateOpfJsonValue(task, \"opfTaskSchema.json\")\n        validateOpfJsonValue(task['taskControl'], \"opfTaskControlSchema.json\")\n\n        taskLabel = task['taskLabel']\n\n        assert isinstance(taskLabel, types.StringTypes), \\\n               \"taskLabel type: %r\" % type(taskLabel)\n        assert len(taskLabel) > 0, \"empty string taskLabel not is allowed\"\n\n        taskLabelsList.append(taskLabel.lower())\n\n      taskLabelDuplicates = filter(lambda x: taskLabelsList.count(x) > 1,\n                                   taskLabelsList)\n      assert len(taskLabelDuplicates) == 0, \\\n             \"Duplcate task labels are not allowed: %s\" % taskLabelDuplicates\n\n    return",
    "doc": "Validates control dictionary for the experiment context"
  },
  {
    "code": "def normalizeStreamSource(self, stream):\n    \"\"\"\n    TODO: document\n    :param stream:\n    \"\"\"\n    # The stream source in the task might be in several formats, so we need\n    # to make sure it gets converted into an absolute path:\n    source = stream['source'][len(FILE_SCHEME):]\n    # If the source is already an absolute path, we won't use pkg_resources,\n    # we'll just trust that the path exists. If not, it's a user problem.\n    if os.path.isabs(source):\n      sourcePath = source\n    else:\n      # First we'll check to see if this path exists within the nupic.datafiles\n      # package resource.\n      sourcePath = resource_filename(\"nupic.datafiles\", source)\n      if not os.path.exists(sourcePath):\n        # If this path doesn't exist as a package resource, the last option will\n        # be to treat it as a relative path to the current working directory.\n        sourcePath = os.path.join(os.getcwd(), source)\n    stream['source'] = FILE_SCHEME + sourcePath",
    "doc": "TODO: document\n    :param stream:"
  },
  {
    "code": "def normalizeStreamSources(self):\n    \"\"\"\n    TODO: document\n    \"\"\"\n    task = dict(self.__control)\n    if 'dataset' in task:\n      for stream in task['dataset']['streams']:\n        self.normalizeStreamSource(stream)\n    else:\n      for subtask in task['tasks']:\n        for stream in subtask['dataset']['streams']:\n          self.normalizeStreamSource(stream)",
    "doc": "TODO: document"
  },
  {
    "code": "def convertNupicEnvToOPF(self):\n    \"\"\"\n    TODO: document\n    \"\"\"\n\n    # We need to create a task structure, most of which is taken verbatim\n    # from the Nupic control dict\n    task = dict(self.__control)\n\n    task.pop('environment')\n    inferenceArgs = task.pop('inferenceArgs')\n    task['taskLabel'] = 'DefaultTask'\n\n    # Create the iterationCycle element that will be placed inside the\n    #  taskControl.\n    iterationCount = task.get('iterationCount', -1)\n    iterationCountInferOnly = task.pop('iterationCountInferOnly', 0)\n    if iterationCountInferOnly == -1:\n      iterationCycle = [IterationPhaseSpecInferOnly(1000, inferenceArgs=inferenceArgs)]\n    elif iterationCountInferOnly > 0:\n      assert iterationCount > 0, \"When iterationCountInferOnly is specified, \"\\\n        \"iterationCount must also be specified and not be -1\"\n      iterationCycle = [IterationPhaseSpecLearnAndInfer(iterationCount\n                                                    -iterationCountInferOnly, inferenceArgs=inferenceArgs),\n                        IterationPhaseSpecInferOnly(iterationCountInferOnly, inferenceArgs=inferenceArgs)]\n    else:\n      iterationCycle = [IterationPhaseSpecLearnAndInfer(1000, inferenceArgs=inferenceArgs)]\n\n\n    taskControl = dict(metrics = task.pop('metrics'),\n                       loggedMetrics = task.pop('loggedMetrics'),\n                       iterationCycle = iterationCycle)\n    task['taskControl'] = taskControl\n\n    # Create the new control\n    self.__control = dict(environment = OpfEnvironment.Nupic,\n                          tasks = [task])",
    "doc": "TODO: document"
  },
  {
    "code": "def createNetwork(dataSource):\n  \"\"\"Create the Network instance.\n\n  The network has a sensor region reading data from `dataSource` and passing\n  the encoded representation to an Identity Region.\n\n  :param dataSource: a RecordStream instance to get data from\n  :returns: a Network instance ready to run\n  \"\"\"\n  network = Network()\n\n  # Our input is sensor data from the gym file. The RecordSensor region\n  # allows us to specify a file record stream as the input source via the\n  # dataSource attribute.\n  network.addRegion(\"sensor\", \"py.RecordSensor\",\n                    json.dumps({\"verbosity\": _VERBOSITY}))\n  sensor = network.regions[\"sensor\"].getSelf()\n  # The RecordSensor needs to know how to encode the input values\n  sensor.encoder = createEncoder()\n  # Specify the dataSource as a file record stream instance\n  sensor.dataSource = dataSource\n\n  # CUSTOM REGION\n  # Add path to custom region to PYTHONPATH\n  # NOTE: Before using a custom region, please modify your PYTHONPATH\n  # export PYTHONPATH=\"<path to custom region module>:$PYTHONPATH\"\n  # In this demo, we have modified it using sys.path.append since we need it to\n  # have an effect on this program.\n  sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n  \n  from custom_region.identity_region import IdentityRegion\n\n  # Add custom region class to the network\n  Network.registerRegion(IdentityRegion)\n\n  # Create a custom region\n  network.addRegion(\"identityRegion\", \"py.IdentityRegion\",\n                    json.dumps({\n                      \"dataWidth\": sensor.encoder.getWidth(),\n                    }))\n\n  # Link the Identity region to the sensor output\n  network.link(\"sensor\", \"identityRegion\", \"UniformLink\", \"\")\n\n  network.initialize()\n\n  return network",
    "doc": "Create the Network instance.\n\n  The network has a sensor region reading data from `dataSource` and passing\n  the encoded representation to an Identity Region.\n\n  :param dataSource: a RecordStream instance to get data from\n  :returns: a Network instance ready to run"
  },
  {
    "code": "def runNetwork(network, writer):\n  \"\"\"Run the network and write output to writer.\n\n  :param network: a Network instance to run\n  :param writer: a csv.writer instance to write output to\n  \"\"\"\n  identityRegion = network.regions[\"identityRegion\"]\n\n  for i in xrange(_NUM_RECORDS):\n    # Run the network for a single iteration\n    network.run(1)\n\n    # Write out the record number and encoding\n    encoding = identityRegion.getOutputData(\"out\")\n    writer.writerow((i, encoding))",
    "doc": "Run the network and write output to writer.\n\n  :param network: a Network instance to run\n  :param writer: a csv.writer instance to write output to"
  },
  {
    "code": "def _appendReportKeys(keys, prefix, results):\n  \"\"\"\n  Generate a set of possible report keys for an experiment's results.\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n  This routine is called recursively to build keys that are multiple levels\n  deep from the results dict.\n\n  Parameters:\n  -----------------------------------------------------------\n  keys:         Set of report keys accumulated so far\n  prefix:       prefix formed so far, this is the colon separated list of key\n                  names that led up to the dict passed in results\n  results:      dictionary of results at this level.\n  \"\"\"\n\n  allKeys = results.keys()\n  allKeys.sort()\n  for key in allKeys:\n    if hasattr(results[key], 'keys'):\n      _appendReportKeys(keys, \"%s%s:\" % (prefix, key), results[key])\n    else:\n      keys.add(\"%s%s\" % (prefix, key))",
    "doc": "Generate a set of possible report keys for an experiment's results.\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n  This routine is called recursively to build keys that are multiple levels\n  deep from the results dict.\n\n  Parameters:\n  -----------------------------------------------------------\n  keys:         Set of report keys accumulated so far\n  prefix:       prefix formed so far, this is the colon separated list of key\n                  names that led up to the dict passed in results\n  results:      dictionary of results at this level."
  },
  {
    "code": "def _matchReportKeys(reportKeyREs=[], allReportKeys=[]):\n  \"\"\"\n  Extract all items from the 'allKeys' list whose key matches one of the regular\n  expressions passed in 'reportKeys'.\n\n  Parameters:\n  ----------------------------------------------------------------------------\n  reportKeyREs:     List of regular expressions\n  allReportKeys:    List of all keys\n\n  retval:         list of keys from allReportKeys that match the regular expressions\n                    in 'reportKeyREs'\n                  If an invalid regular expression was included in 'reportKeys',\n                    then BadKeyError() is raised\n  \"\"\"\n\n  matchingReportKeys = []\n\n  # Extract the report items of interest\n  for keyRE in reportKeyREs:\n    # Find all keys that match this regular expression\n    matchObj = re.compile(keyRE)\n    found = False\n    for keyName in allReportKeys:\n      match = matchObj.match(keyName)\n      if match and match.end() == len(keyName):\n        matchingReportKeys.append(keyName)\n        found = True\n    if not found:\n      raise _BadKeyError(keyRE)\n\n  return matchingReportKeys",
    "doc": "Extract all items from the 'allKeys' list whose key matches one of the regular\n  expressions passed in 'reportKeys'.\n\n  Parameters:\n  ----------------------------------------------------------------------------\n  reportKeyREs:     List of regular expressions\n  allReportKeys:    List of all keys\n\n  retval:         list of keys from allReportKeys that match the regular expressions\n                    in 'reportKeyREs'\n                  If an invalid regular expression was included in 'reportKeys',\n                    then BadKeyError() is raised"
  },
  {
    "code": "def _getReportItem(itemName, results):\n  \"\"\"\n  Get a specific item by name out of the results dict.\n\n  The format of itemName is a string of dictionary keys separated by colons,\n  each key being one level deeper into the results dict. For example,\n  'key1:key2' would fetch results['key1']['key2'].\n\n  If itemName is not found in results, then None is returned\n\n  \"\"\"\n\n  subKeys = itemName.split(':')\n  subResults = results\n  for subKey in subKeys:\n    subResults = subResults[subKey]\n\n  return subResults",
    "doc": "Get a specific item by name out of the results dict.\n\n  The format of itemName is a string of dictionary keys separated by colons,\n  each key being one level deeper into the results dict. For example,\n  'key1:key2' would fetch results['key1']['key2'].\n\n  If itemName is not found in results, then None is returned"
  },
  {
    "code": "def filterResults(allResults, reportKeys, optimizeKey=None):\n  \"\"\" Given the complete set of results generated by an experiment (passed in\n  'results'), filter out and return only the ones the caller wants, as\n  specified through 'reportKeys' and 'optimizeKey'.\n\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n\n  Parameters:\n  -------------------------------------------------------------------------\n  results:             dict of all results generated by an experiment\n  reportKeys:          list of items from the results dict to include in\n                       the report. These can be regular expressions.\n  optimizeKey:         Which report item, if any, we will be optimizing for. This can\n                       also be a regular expression, but is an error if it matches\n                       more than one key from the experiment's results.\n  retval:  (reportDict, optimizeDict)\n              reportDict: a dictionary of the metrics named by desiredReportKeys\n              optimizeDict: A dictionary containing 1 item: the full name and\n                    value of the metric identified by the optimizeKey\n\n  \"\"\"\n\n  # Init return values\n  optimizeDict = dict()\n\n  # Get all available report key names for this experiment\n  allReportKeys = set()\n  _appendReportKeys(keys=allReportKeys, prefix='', results=allResults)\n\n  #----------------------------------------------------------------------------\n  # Extract the report items that match the regular expressions passed in reportKeys\n  matchingKeys = _matchReportKeys(reportKeys, allReportKeys)\n\n  # Extract the values of the desired items\n  reportDict = dict()\n  for keyName in matchingKeys:\n    value = _getReportItem(keyName, allResults)\n    reportDict[keyName] = value\n\n\n  # -------------------------------------------------------------------------\n  # Extract the report item that matches the regular expression passed in\n  #   optimizeKey\n  if optimizeKey is not None:\n    matchingKeys = _matchReportKeys([optimizeKey], allReportKeys)\n    if len(matchingKeys) == 0:\n      raise _BadKeyError(optimizeKey)\n    elif len(matchingKeys) > 1:\n      raise _BadOptimizeKeyError(optimizeKey, matchingKeys)\n    optimizeKeyFullName = matchingKeys[0]\n\n    # Get the value of the optimize metric\n    value = _getReportItem(optimizeKeyFullName, allResults)\n    optimizeDict[optimizeKeyFullName] = value\n    reportDict[optimizeKeyFullName] = value\n\n  # Return info\n  return(reportDict, optimizeDict)",
    "doc": "Given the complete set of results generated by an experiment (passed in\n  'results'), filter out and return only the ones the caller wants, as\n  specified through 'reportKeys' and 'optimizeKey'.\n\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n\n  Parameters:\n  -------------------------------------------------------------------------\n  results:             dict of all results generated by an experiment\n  reportKeys:          list of items from the results dict to include in\n                       the report. These can be regular expressions.\n  optimizeKey:         Which report item, if any, we will be optimizing for. This can\n                       also be a regular expression, but is an error if it matches\n                       more than one key from the experiment's results.\n  retval:  (reportDict, optimizeDict)\n              reportDict: a dictionary of the metrics named by desiredReportKeys\n              optimizeDict: A dictionary containing 1 item: the full name and\n                    value of the metric identified by the optimizeKey"
  },
  {
    "code": "def _handleModelRunnerException(jobID, modelID, jobsDAO, experimentDir, logger,\n                                e):\n  \"\"\" Perform standard handling of an exception that occurs while running\n  a model.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  jobID:                ID for this hypersearch job in the jobs table\n  modelID:              model ID\n  jobsDAO:              ClientJobsDAO instance\n  experimentDir:        directory containing the experiment\n  logger:               the logger to use\n  e:                    the exception that occurred\n  retval:               (completionReason, completionMsg)\n  \"\"\"\n\n  msg = StringIO.StringIO()\n  print >>msg, \"Exception occurred while running model %s: %r (%s)\" % (\n    modelID, e, type(e))\n  traceback.print_exc(None, msg)\n\n  completionReason = jobsDAO.CMPL_REASON_ERROR\n  completionMsg = msg.getvalue()\n  logger.error(completionMsg)\n\n  # Write results to the model database for the error case. Ignore\n  # InvalidConnectionException, as this is usually caused by orphaned models\n  #\n  # TODO: do we really want to set numRecords to 0? Last updated value might\n  #       be useful for debugging\n  if type(e) is not InvalidConnectionException:\n    jobsDAO.modelUpdateResults(modelID,  results=None, numRecords=0)\n\n  # TODO: Make sure this wasn't the best model in job. If so, set the best\n  # appropriately\n\n  # If this was an exception that should mark the job as failed, do that\n  # now.\n  if type(e) == JobFailException:\n    workerCmpReason = jobsDAO.jobGetFields(jobID,\n        ['workerCompletionReason'])[0]\n    if workerCmpReason == ClientJobsDAO.CMPL_REASON_SUCCESS:\n      jobsDAO.jobSetFields(jobID, fields=dict(\n          cancel=True,\n          workerCompletionReason = ClientJobsDAO.CMPL_REASON_ERROR,\n          workerCompletionMsg = \": \".join(str(i) for i in e.args)),\n          useConnectionID=False,\n          ignoreUnchanged=True)\n\n  return (completionReason, completionMsg)",
    "doc": "Perform standard handling of an exception that occurs while running\n  a model.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  jobID:                ID for this hypersearch job in the jobs table\n  modelID:              model ID\n  jobsDAO:              ClientJobsDAO instance\n  experimentDir:        directory containing the experiment\n  logger:               the logger to use\n  e:                    the exception that occurred\n  retval:               (completionReason, completionMsg)"
  },
  {
    "code": "def runModelGivenBaseAndParams(modelID, jobID, baseDescription, params,\n            predictedField, reportKeys, optimizeKey, jobsDAO,\n            modelCheckpointGUID, logLevel=None, predictionCacheMaxRecords=None):\n  \"\"\" This creates an experiment directory with a base.py description file\n  created from 'baseDescription' and a description.py generated from the\n  given params dict and then runs the experiment.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  modelID:              ID for this model in the models table\n  jobID:                ID for this hypersearch job in the jobs table\n  baseDescription:      Contents of a description.py with the base experiment\n                                          description\n  params:               Dictionary of specific parameters to override within\n                                  the baseDescriptionFile.\n  predictedField:       Name of the input field for which this model is being\n                                    optimized\n  reportKeys:           Which metrics of the experiment to store into the\n                                    results dict of the model's database entry\n  optimizeKey:          Which metric we are optimizing for\n  jobsDAO               Jobs data access object - the interface to the\n                                  jobs database which has the model's table.\n  modelCheckpointGUID:  A persistent, globally-unique identifier for\n                                  constructing the model checkpoint key\n  logLevel:             override logging level to this value, if not None\n\n  retval:               (completionReason, completionMsg)\n  \"\"\"\n  from nupic.swarming.ModelRunner import OPFModelRunner\n\n  # The logger for this method\n  logger = logging.getLogger('com.numenta.nupic.hypersearch.utils')\n\n\n  # --------------------------------------------------------------------------\n  # Create a temp directory for the experiment and the description files\n  experimentDir = tempfile.mkdtemp()\n  try:\n    logger.info(\"Using experiment directory: %s\" % (experimentDir))\n\n    # Create the decription.py from the overrides in params\n    paramsFilePath = os.path.join(experimentDir, 'description.py')\n    paramsFile = open(paramsFilePath, 'wb')\n    paramsFile.write(_paramsFileHead())\n\n    items = params.items()\n    items.sort()\n    for (key,value) in items:\n      quotedKey = _quoteAndEscape(key)\n      if isinstance(value, basestring):\n\n        paramsFile.write(\"  %s : '%s',\\n\" % (quotedKey , value))\n      else:\n        paramsFile.write(\"  %s : %s,\\n\" % (quotedKey , value))\n\n    paramsFile.write(_paramsFileTail())\n    paramsFile.close()\n\n\n    # Write out the base description\n    baseParamsFile = open(os.path.join(experimentDir, 'base.py'), 'wb')\n    baseParamsFile.write(baseDescription)\n    baseParamsFile.close()\n\n\n    # Store the experiment's sub-description file into the model table\n    #  for reference\n    fd = open(paramsFilePath)\n    expDescription = fd.read()\n    fd.close()\n    jobsDAO.modelSetFields(modelID, {'genDescription': expDescription})\n\n\n    # Run the experiment now\n    try:\n      runner = OPFModelRunner(\n        modelID=modelID,\n        jobID=jobID,\n        predictedField=predictedField,\n        experimentDir=experimentDir,\n        reportKeyPatterns=reportKeys,\n        optimizeKeyPattern=optimizeKey,\n        jobsDAO=jobsDAO,\n        modelCheckpointGUID=modelCheckpointGUID,\n        logLevel=logLevel,\n        predictionCacheMaxRecords=predictionCacheMaxRecords)\n\n      signal.signal(signal.SIGINT, runner.handleWarningSignal)\n\n      (completionReason, completionMsg) = runner.run()\n\n    except InvalidConnectionException:\n      raise\n    except Exception, e:\n\n      (completionReason, completionMsg) = _handleModelRunnerException(jobID,\n                                     modelID, jobsDAO, experimentDir, logger, e)\n\n  finally:\n    # delete our temporary directory tree\n    shutil.rmtree(experimentDir)\n    signal.signal(signal.SIGINT, signal.default_int_handler)\n\n  # Return completion reason and msg\n  return (completionReason, completionMsg)",
    "doc": "This creates an experiment directory with a base.py description file\n  created from 'baseDescription' and a description.py generated from the\n  given params dict and then runs the experiment.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  modelID:              ID for this model in the models table\n  jobID:                ID for this hypersearch job in the jobs table\n  baseDescription:      Contents of a description.py with the base experiment\n                                          description\n  params:               Dictionary of specific parameters to override within\n                                  the baseDescriptionFile.\n  predictedField:       Name of the input field for which this model is being\n                                    optimized\n  reportKeys:           Which metrics of the experiment to store into the\n                                    results dict of the model's database entry\n  optimizeKey:          Which metric we are optimizing for\n  jobsDAO               Jobs data access object - the interface to the\n                                  jobs database which has the model's table.\n  modelCheckpointGUID:  A persistent, globally-unique identifier for\n                                  constructing the model checkpoint key\n  logLevel:             override logging level to this value, if not None\n\n  retval:               (completionReason, completionMsg)"
  },
  {
    "code": "def rCopy(d, f=identityConversion, discardNoneKeys=True, deepCopy=True):\n  \"\"\"Recursively copies a dict and returns the result.\n\n  Args:\n    d: The dict to copy.\n    f: A function to apply to values when copying that takes the value and the\n        list of keys from the root of the dict to the value and returns a value\n        for the new dict.\n    discardNoneKeys: If True, discard key-value pairs when f returns None for\n        the value.\n    deepCopy: If True, all values in returned dict are true copies (not the\n        same object).\n  Returns:\n    A new dict with keys and values from d replaced with the result of f.\n  \"\"\"\n  # Optionally deep copy the dict.\n  if deepCopy:\n    d = copy.deepcopy(d)\n\n  newDict = {}\n  toCopy = [(k, v, newDict, ()) for k, v in d.iteritems()]\n  while len(toCopy) > 0:\n    k, v, d, prevKeys = toCopy.pop()\n    prevKeys = prevKeys + (k,)\n    if isinstance(v, dict):\n      d[k] = dict()\n      toCopy[0:0] = [(innerK, innerV, d[k], prevKeys)\n                     for innerK, innerV in v.iteritems()]\n    else:\n      #print k, v, prevKeys\n      newV = f(v, prevKeys)\n      if not discardNoneKeys or newV is not None:\n        d[k] = newV\n  return newDict",
    "doc": "Recursively copies a dict and returns the result.\n\n  Args:\n    d: The dict to copy.\n    f: A function to apply to values when copying that takes the value and the\n        list of keys from the root of the dict to the value and returns a value\n        for the new dict.\n    discardNoneKeys: If True, discard key-value pairs when f returns None for\n        the value.\n    deepCopy: If True, all values in returned dict are true copies (not the\n        same object).\n  Returns:\n    A new dict with keys and values from d replaced with the result of f."
  },
  {
    "code": "def rApply(d, f):\n  \"\"\"Recursively applies f to the values in dict d.\n\n  Args:\n    d: The dict to recurse over.\n    f: A function to apply to values in d that takes the value and a list of\n        keys from the root of the dict to the value.\n  \"\"\"\n  remainingDicts = [(d, ())]\n  while len(remainingDicts) > 0:\n    current, prevKeys = remainingDicts.pop()\n    for k, v in current.iteritems():\n      keys = prevKeys + (k,)\n      if isinstance(v, dict):\n        remainingDicts.insert(0, (v, keys))\n      else:\n        f(v, keys)",
    "doc": "Recursively applies f to the values in dict d.\n\n  Args:\n    d: The dict to recurse over.\n    f: A function to apply to values in d that takes the value and a list of\n        keys from the root of the dict to the value."
  },
  {
    "code": "def clippedObj(obj, maxElementSize=64):\n  \"\"\"\n  Return a clipped version of obj suitable for printing, This\n  is useful when generating log messages by printing data structures, but\n  don't want the message to be too long.\n\n  If passed in a dict, list, or namedtuple, each element of the structure's\n  string representation will be limited to 'maxElementSize' characters. This\n  will return a new object where the string representation of each element\n  has been truncated to fit within maxElementSize.\n  \"\"\"\n\n  # Is it a named tuple?\n  if hasattr(obj, '_asdict'):\n    obj = obj._asdict()\n\n\n  # Printing a dict?\n  if isinstance(obj, dict):\n    objOut = dict()\n    for key,val in obj.iteritems():\n      objOut[key] = clippedObj(val)\n\n  # Printing a list?\n  elif hasattr(obj, '__iter__'):\n    objOut = []\n    for val in obj:\n      objOut.append(clippedObj(val))\n\n  # Some other object\n  else:\n    objOut = str(obj)\n    if len(objOut) > maxElementSize:\n      objOut = objOut[0:maxElementSize] + '...'\n\n  return objOut",
    "doc": "Return a clipped version of obj suitable for printing, This\n  is useful when generating log messages by printing data structures, but\n  don't want the message to be too long.\n\n  If passed in a dict, list, or namedtuple, each element of the structure's\n  string representation will be limited to 'maxElementSize' characters. This\n  will return a new object where the string representation of each element\n  has been truncated to fit within maxElementSize."
  },
  {
    "code": "def validate(value, **kwds):\n  \"\"\" Validate a python value against json schema:\n  validate(value, schemaPath)\n  validate(value, schemaDict)\n\n  value:          python object to validate against the schema\n\n  The json schema may be specified either as a path of the file containing\n  the json schema or as a python dictionary using one of the\n  following keywords as arguments:\n    schemaPath:     Path of file containing the json schema object.\n    schemaDict:     Python dictionary containing the json schema object\n\n  Returns: nothing\n\n  Raises:\n          ValidationError when value fails json validation\n  \"\"\"\n\n  assert len(kwds.keys()) >= 1\n  assert 'schemaPath' in kwds or 'schemaDict' in kwds\n\n  schemaDict = None\n  if 'schemaPath' in kwds:\n    schemaPath = kwds.pop('schemaPath')\n    schemaDict = loadJsonValueFromFile(schemaPath)\n  elif 'schemaDict' in kwds:\n    schemaDict = kwds.pop('schemaDict')\n\n  try:\n    validictory.validate(value, schemaDict, **kwds)\n  except validictory.ValidationError as e:\n    raise ValidationError(e)",
    "doc": "Validate a python value against json schema:\n  validate(value, schemaPath)\n  validate(value, schemaDict)\n\n  value:          python object to validate against the schema\n\n  The json schema may be specified either as a path of the file containing\n  the json schema or as a python dictionary using one of the\n  following keywords as arguments:\n    schemaPath:     Path of file containing the json schema object.\n    schemaDict:     Python dictionary containing the json schema object\n\n  Returns: nothing\n\n  Raises:\n          ValidationError when value fails json validation"
  },
  {
    "code": "def loadJsonValueFromFile(inputFilePath):\n  \"\"\" Loads a json value from a file and converts it to the corresponding python\n  object.\n\n  inputFilePath:\n                  Path of the json file;\n\n  Returns:\n                  python value that represents the loaded json value\n\n  \"\"\"\n  with open(inputFilePath) as fileObj:\n    value = json.load(fileObj)\n\n  return value",
    "doc": "Loads a json value from a file and converts it to the corresponding python\n  object.\n\n  inputFilePath:\n                  Path of the json file;\n\n  Returns:\n                  python value that represents the loaded json value"
  },
  {
    "code": "def sortedJSONDumpS(obj):\n  \"\"\"\n  Return a JSON representation of obj with sorted keys on any embedded dicts.\n  This insures that the same object will always be represented by the same\n  string even if it contains dicts (where the sort order of the keys is\n  normally undefined).\n  \"\"\"\n\n  itemStrs = []\n\n  if isinstance(obj, dict):\n    items = obj.items()\n    items.sort()\n    for key, value in items:\n      itemStrs.append('%s: %s' % (json.dumps(key), sortedJSONDumpS(value)))\n    return '{%s}' % (', '.join(itemStrs))\n\n  elif hasattr(obj, '__iter__'):\n    for val in obj:\n      itemStrs.append(sortedJSONDumpS(val))\n    return '[%s]' % (', '.join(itemStrs))\n\n  else:\n    return json.dumps(obj)",
    "doc": "Return a JSON representation of obj with sorted keys on any embedded dicts.\n  This insures that the same object will always be represented by the same\n  string even if it contains dicts (where the sort order of the keys is\n  normally undefined)."
  },
  {
    "code": "def tick(self):\n    \"\"\" Activity tick handler; services all activities\n\n    Returns:      True if controlling iterator says it's okay to keep going;\n                  False to stop\n    \"\"\"\n\n    # Run activities whose time has come\n    for act in self.__activities:\n      if not act.iteratorHolder[0]:\n        continue\n\n      try:\n        next(act.iteratorHolder[0])\n      except StopIteration:\n        act.cb()\n        if act.repeating:\n          act.iteratorHolder[0] = iter(xrange(act.period))\n        else:\n          act.iteratorHolder[0] = None\n\n    return True",
    "doc": "Activity tick handler; services all activities\n\n    Returns:      True if controlling iterator says it's okay to keep going;\n                  False to stop"
  },
  {
    "code": "def rUpdate(original, updates):\n  \"\"\"Recursively updates the values in original with the values from updates.\"\"\"\n  # Keep a list of the sub-dictionaries that need to be updated to avoid having\n  # to use recursion (which could fail for dictionaries with a lot of nesting.\n  dictPairs = [(original, updates)]\n  while len(dictPairs) > 0:\n    original, updates = dictPairs.pop()\n    for k, v in updates.iteritems():\n      if k in original and isinstance(original[k], dict) and isinstance(v, dict):\n        dictPairs.append((original[k], v))\n      else:\n        original[k] = v",
    "doc": "Recursively updates the values in original with the values from updates."
  },
  {
    "code": "def dictDiffAndReport(da, db):\n  \"\"\" Compares two python dictionaries at the top level and report differences,\n  if any, to stdout\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        The same value as returned by dictDiff() for the given args\n  \"\"\"\n  differences = dictDiff(da, db)\n\n  if not differences:\n    return differences\n\n  if differences['inAButNotInB']:\n    print \">>> inAButNotInB: %s\" % differences['inAButNotInB']\n\n  if differences['inBButNotInA']:\n    print \">>> inBButNotInA: %s\" % differences['inBButNotInA']\n\n  for key in differences['differentValues']:\n    print \">>> da[%s] != db[%s]\" % (key, key)\n    print \"da[%s] = %r\" % (key, da[key])\n    print \"db[%s] = %r\" % (key, db[key])\n\n  return differences",
    "doc": "Compares two python dictionaries at the top level and report differences,\n  if any, to stdout\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        The same value as returned by dictDiff() for the given args"
  },
  {
    "code": "def dictDiff(da, db):\n  \"\"\" Compares two python dictionaries at the top level and return differences\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        None if dictionaries test equal; otherwise returns a\n                  dictionary as follows:\n                  {\n                    'inAButNotInB':\n                        <sequence of keys that are in da but not in db>\n                    'inBButNotInA':\n                        <sequence of keys that are in db but not in da>\n                    'differentValues':\n                        <sequence of keys whose corresponding values differ\n                         between da and db>\n                  }\n  \"\"\"\n  different = False\n\n  resultDict = dict()\n\n  resultDict['inAButNotInB'] = set(da) - set(db)\n  if resultDict['inAButNotInB']:\n    different = True\n\n  resultDict['inBButNotInA'] = set(db) - set(da)\n  if resultDict['inBButNotInA']:\n    different = True\n\n  resultDict['differentValues'] = []\n  for key in (set(da) - resultDict['inAButNotInB']):\n    comparisonResult = da[key] == db[key]\n    if isinstance(comparisonResult, bool):\n      isEqual = comparisonResult\n    else:\n      # This handles numpy arrays (but only at the top level)\n      isEqual = comparisonResult.all()\n    if not isEqual:\n      resultDict['differentValues'].append(key)\n      different = True\n\n  assert (((resultDict['inAButNotInB'] or resultDict['inBButNotInA'] or\n          resultDict['differentValues']) and different) or not different)\n\n  return resultDict if different else None",
    "doc": "Compares two python dictionaries at the top level and return differences\n\n  da:             first dictionary\n  db:             second dictionary\n\n  Returns:        None if dictionaries test equal; otherwise returns a\n                  dictionary as follows:\n                  {\n                    'inAButNotInB':\n                        <sequence of keys that are in da but not in db>\n                    'inBButNotInA':\n                        <sequence of keys that are in db but not in da>\n                    'differentValues':\n                        <sequence of keys whose corresponding values differ\n                         between da and db>\n                  }"
  },
  {
    "code": "def _seed(self, seed=-1):\n    \"\"\"\n    Initialize the random seed\n    \"\"\"\n    if seed != -1:\n      self.random = NupicRandom(seed)\n    else:\n      self.random = NupicRandom()",
    "doc": "Initialize the random seed"
  },
  {
    "code": "def _newRep(self):\n    \"\"\"Generate a new and unique representation. Returns a numpy array\n    of shape (n,). \"\"\"\n    maxAttempts = 1000\n\n    for _ in xrange(maxAttempts):\n      foundUnique = True\n      population = numpy.arange(self.n, dtype=numpy.uint32)\n      choices = numpy.arange(self.w, dtype=numpy.uint32)\n      oneBits = sorted(self.random.sample(population, choices))\n      sdr =  numpy.zeros(self.n, dtype='uint8')\n      sdr[oneBits] = 1\n      for i in xrange(self.ncategories):\n        if (sdr == self.sdrs[i]).all():\n          foundUnique = False\n          break\n      if foundUnique:\n        break;\n    if not foundUnique:\n      raise RuntimeError(\"Error, could not find unique pattern %d after \"\n                         \"%d attempts\" % (self.ncategories, maxAttempts))\n    return sdr",
    "doc": "Generate a new and unique representation. Returns a numpy array\n    of shape (n,)."
  },
  {
    "code": "def getScalars(self, input):\n    \"\"\" See method description in base.py \"\"\"\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n        return numpy.array([0])\n\n    index = self.categoryToIndex.get(input, None)\n    if index is None:\n      if self._learningEnabled:\n        self._addCategory(input)\n        index = self.ncategories - 1\n      else:\n        # if not found, we encode category 0\n        index = 0\n\n    return numpy.array([index])",
    "doc": "See method description in base.py"
  },
  {
    "code": "def decode(self, encoded, parentFieldName=''):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    assert (encoded[0:self.n] <= 1.0).all()\n\n    resultString =  \"\"\n    resultRanges = []\n\n    overlaps =  (self.sdrs * encoded[0:self.n]).sum(axis=1)\n\n    if self.verbosity >= 2:\n      print \"Overlaps for decoding:\"\n      for i in xrange(0, self.ncategories):\n        print \"%d %s\" % (overlaps[i], self.categories[i])\n\n    matchingCategories =  (overlaps > self.thresholdOverlap).nonzero()[0]\n\n    for index in matchingCategories:\n      if resultString != \"\":\n        resultString += \" \"\n      resultString +=  str(self.categories[index])\n      resultRanges.append([int(index),int(index)])\n\n    if parentFieldName != '':\n      fieldName = \"%s.%s\" % (parentFieldName, self.name)\n    else:\n      fieldName = self.name\n    return ({fieldName: (resultRanges, resultString)}, [fieldName])",
    "doc": "See the function description in base.py"
  },
  {
    "code": "def _getTopDownMapping(self):\n    \"\"\" Return the interal _topDownMappingM matrix used for handling the\n    bucketInfo() and topDownCompute() methods. This is a matrix, one row per\n    category (bucket) where each row contains the encoded output for that\n    category.\n    \"\"\"\n\n    # -------------------------------------------------------------------------\n    # Do we need to build up our reverse mapping table?\n    if self._topDownMappingM is None:\n\n      # Each row represents an encoded output pattern\n      self._topDownMappingM = SM32(self.ncategories, self.n)\n\n      outputSpace = numpy.zeros(self.n, dtype=GetNTAReal())\n      for i in xrange(self.ncategories):\n        self.encodeIntoArray(self.categories[i], outputSpace)\n        self._topDownMappingM.setRowFromDense(i, outputSpace)\n\n    return self._topDownMappingM",
    "doc": "Return the interal _topDownMappingM matrix used for handling the\n    bucketInfo() and topDownCompute() methods. This is a matrix, one row per\n    category (bucket) where each row contains the encoded output for that\n    category."
  },
  {
    "code": "def getBucketInfo(self, buckets):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    if self.ncategories==0:\n      return 0\n\n    topDownMappingM = self._getTopDownMapping()\n\n    categoryIndex = buckets[0]\n    category = self.categories[categoryIndex]\n    encoding = topDownMappingM.getRow(categoryIndex)\n\n    return [EncoderResult(value=category, scalar=categoryIndex,\n                          encoding=encoding)]",
    "doc": "See the function description in base.py"
  },
  {
    "code": "def topDownCompute(self, encoded):\n    \"\"\" See the function description in base.py\n    \"\"\"\n\n    if self.ncategories==0:\n      return 0\n\n    topDownMappingM = self._getTopDownMapping()\n\n    categoryIndex = topDownMappingM.rightVecProd(encoded).argmax()\n    category = self.categories[categoryIndex]\n    encoding = topDownMappingM.getRow(categoryIndex)\n\n    return EncoderResult(value=category, scalar=categoryIndex, encoding=encoding)",
    "doc": "See the function description in base.py"
  },
  {
    "code": "def getScalarNames(self, parentFieldName=''):\n    \"\"\" See method description in base.py \"\"\"\n\n    names = []\n\n    # This forms a name which is the concatenation of the parentFieldName\n    #   passed in and the encoder's own name.\n    def _formFieldName(encoder):\n      if parentFieldName == '':\n        return encoder.name\n      else:\n        return '%s.%s' % (parentFieldName, encoder.name)\n\n    # -------------------------------------------------------------------------\n    # Get the scalar values for each sub-field\n    if self.seasonEncoder is not None:\n      names.append(_formFieldName(self.seasonEncoder))\n\n    if self.dayOfWeekEncoder is not None:\n      names.append(_formFieldName(self.dayOfWeekEncoder))\n\n    if self.customDaysEncoder is not None:\n      names.append(_formFieldName(self.customDaysEncoder))\n\n    if self.weekendEncoder is not None:\n      names.append(_formFieldName(self.weekendEncoder))\n\n    if self.holidayEncoder is not None:\n      names.append(_formFieldName(self.holidayEncoder))\n\n    if self.timeOfDayEncoder is not None:\n      names.append(_formFieldName(self.timeOfDayEncoder))\n\n    return names",
    "doc": "See method description in base.py"
  },
  {
    "code": "def getEncodedValues(self, input):\n    \"\"\" See method description in base.py \"\"\"\n\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      return numpy.array([None])\n\n    assert isinstance(input, datetime.datetime)\n    values = []\n\n    # -------------------------------------------------------------------------\n    # Get the scalar values for each sub-field\n    timetuple = input.timetuple()\n    timeOfDay = timetuple.tm_hour + float(timetuple.tm_min)/60.0\n\n    if self.seasonEncoder is not None:\n      dayOfYear = timetuple.tm_yday\n      # input.timetuple() computes the day of year 1 based, so convert to 0 based\n      values.append(dayOfYear-1)\n\n    if self.dayOfWeekEncoder is not None:\n      dayOfWeek = timetuple.tm_wday + timeOfDay / 24.0\n      values.append(dayOfWeek)\n\n    if self.weekendEncoder is not None:\n      # saturday, sunday or friday evening\n      if timetuple.tm_wday == 6 or timetuple.tm_wday == 5 \\\n          or (timetuple.tm_wday == 4 and timeOfDay > 18):\n        weekend = 1\n      else:\n        weekend = 0\n      values.append(weekend)\n\n    if self.customDaysEncoder is not None:\n      if timetuple.tm_wday in self.customDays:\n        customDay = 1\n      else:\n        customDay = 0\n      values.append(customDay)\n    if self.holidayEncoder is not None:\n      # A \"continuous\" binary value. = 1 on the holiday itself and smooth ramp\n      #  0->1 on the day before the holiday and 1->0 on the day after the holiday.\n      # Currently the only holiday we know about is December 25\n      # holidays is a list of holidays that occur on a fixed date every year\n      if len(self.holidays) == 0:\n        holidays = [(12, 25)]\n      else:\n        holidays = self.holidays\n      val = 0\n      for h in holidays:\n        # hdate is midnight on the holiday\n        if len(h) == 3:\n          hdate = datetime.datetime(h[0], h[1], h[2], 0, 0, 0)\n        else:\n          hdate = datetime.datetime(timetuple.tm_year, h[0], h[1], 0, 0, 0)\n        if input > hdate:\n          diff = input - hdate\n          if diff.days == 0:\n            # return 1 on the holiday itself\n            val = 1\n            break\n          elif diff.days == 1:\n            # ramp smoothly from 1 -> 0 on the next day\n            val = 1.0 - (float(diff.seconds) / 86400)\n            break\n        else:\n          diff = hdate - input\n          if diff.days == 0:\n            # ramp smoothly from 0 -> 1 on the previous day\n            val = 1.0 - (float(diff.seconds) / 86400)\n\n      values.append(val)\n\n    if self.timeOfDayEncoder is not None:\n      values.append(timeOfDay)\n\n    return values",
    "doc": "See method description in base.py"
  },
  {
    "code": "def getBucketIndices(self, input):\n    \"\"\" See method description in base.py \"\"\"\n\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      # Encoder each sub-field\n      return [None] * len(self.encoders)\n\n    else:\n      assert isinstance(input, datetime.datetime)\n\n      # Get the scalar values for each sub-field\n      scalars = self.getScalars(input)\n\n      # Encoder each sub-field\n      result = []\n      for i in xrange(len(self.encoders)):\n        (name, encoder, offset) = self.encoders[i]\n        result.extend(encoder.getBucketIndices(scalars[i]))\n      return result",
    "doc": "See method description in base.py"
  },
  {
    "code": "def encodeIntoArray(self, input, output):\n    \"\"\" See method description in base.py \"\"\"\n\n    if input == SENTINEL_VALUE_FOR_MISSING_DATA:\n      output[0:] = 0\n    else:\n      if not isinstance(input, datetime.datetime):\n        raise ValueError(\"Input is type %s, expected datetime. Value: %s\" % (\n            type(input), str(input)))\n\n      # Get the scalar values for each sub-field\n      scalars = self.getScalars(input)\n      # Encoder each sub-field\n      for i in xrange(len(self.encoders)):\n        (name, encoder, offset) = self.encoders[i]\n        encoder.encodeIntoArray(scalars[i], output[offset:])",
    "doc": "See method description in base.py"
  },
  {
    "code": "def getSpec(cls):\n    \"\"\"Return the Spec for IdentityRegion.\n    \"\"\"\n    spec = {\n        \"description\":IdentityRegion.__doc__,\n        \"singleNodeOnly\":True,\n        \"inputs\":{\n          \"in\":{\n            \"description\":\"The input vector.\",\n            \"dataType\":\"Real32\",\n            \"count\":0,\n            \"required\":True,\n            \"regionLevel\":False,\n            \"isDefaultInput\":True,\n            \"requireSplitterMap\":False},\n        },\n        \"outputs\":{\n          \"out\":{\n            \"description\":\"A copy of the input vector.\",\n            \"dataType\":\"Real32\",\n            \"count\":0,\n            \"regionLevel\":True,\n            \"isDefaultOutput\":True},\n        },\n\n        \"parameters\":{\n          \"dataWidth\":{\n            \"description\":\"Size of inputs\",\n            \"accessMode\":\"Read\",\n            \"dataType\":\"UInt32\",\n            \"count\":1,\n            \"constraints\":\"\"},\n        },\n    }\n\n    return spec",
    "doc": "Return the Spec for IdentityRegion."
  },
  {
    "code": "def _setRandomEncoderResolution(minResolution=0.001):\n  \"\"\"\n  Given model params, figure out the correct resolution for the\n  RandomDistributed encoder. Modifies params in place.\n  \"\"\"\n  encoder = (\n    model_params.MODEL_PARAMS[\"modelParams\"][\"sensorParams\"][\"encoders\"][\"value\"]\n  )\n\n  if encoder[\"type\"] == \"RandomDistributedScalarEncoder\":\n    rangePadding = abs(_INPUT_MAX - _INPUT_MIN) * 0.2\n    minValue = _INPUT_MIN - rangePadding\n    maxValue = _INPUT_MAX + rangePadding\n    resolution = max(minResolution,\n                     (maxValue - minValue) / encoder.pop(\"numBuckets\")\n                    )\n    encoder[\"resolution\"] = resolution",
    "doc": "Given model params, figure out the correct resolution for the\n  RandomDistributed encoder. Modifies params in place."
  },
  {
    "code": "def addLabel(self, start, end, labelName):\n    \"\"\"\n    Add the label labelName to each record with record ROWID in range from\n    start to end, noninclusive of end.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n    \"\"\"\n    if len(self.saved_states) == 0:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for 'addLabel'. \"\n        \"Model has no saved records.\")\n\n    startID = self.saved_states[0].ROWID\n\n    clippedStart = max(0, start - startID)\n    clippedEnd = max(0, min( len( self.saved_states) , end - startID))\n\n    if clippedEnd <= clippedStart:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for 'addLabel'.\",\n                                                debugInfo={\n          'requestRange': {\n            'startRecordID': start,\n            'endRecordID': end\n          },\n          'clippedRequestRange': {\n            'startRecordID': clippedStart,\n            'endRecordID': clippedEnd\n          },\n          'validRange': {\n            'startRecordID': startID,\n            'endRecordID': self.saved_states[len(self.saved_states)-1].ROWID\n          },\n          'numRecordsStored': len(self.saved_states)\n        })\n\n    # Add label to range [clippedStart, clippedEnd)\n    for state in self.saved_states[clippedStart:clippedEnd]:\n      if labelName not in state.anomalyLabel:\n        state.anomalyLabel.append(labelName)\n        state.setByUser = True\n        self._addRecordToKNN(state)\n\n    assert len(self.saved_categories) > 0\n\n    # Recompute [end, ...)\n    for state in self.saved_states[clippedEnd:]:\n      self._updateState(state)",
    "doc": "Add the label labelName to each record with record ROWID in range from\n    start to end, noninclusive of end.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier."
  },
  {
    "code": "def removeLabels(self, start=None, end=None, labelFilter=None):\n    \"\"\"\n    Remove labels from each record with record ROWID in range from\n    start to end, noninclusive of end. Removes all records if labelFilter is\n    None, otherwise only removes the labels eqaul to labelFilter.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier.\n    \"\"\"\n\n    if len(self.saved_states) == 0:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for \"\n        \"'removeLabels'. Model has no saved records.\")\n\n    startID = self.saved_states[0].ROWID\n\n    clippedStart = 0 if start is None else max(0, start - startID)\n    clippedEnd = len(self.saved_states) if end is None else \\\n      max(0, min( len( self.saved_states) , end - startID))\n\n    if clippedEnd <= clippedStart:\n      raise HTMPredictionModelInvalidRangeError(\"Invalid supplied range for \"\n        \"'removeLabels'.\", debugInfo={\n          'requestRange': {\n            'startRecordID': start,\n            'endRecordID': end\n          },\n          'clippedRequestRange': {\n            'startRecordID': clippedStart,\n            'endRecordID': clippedEnd\n          },\n          'validRange': {\n            'startRecordID': startID,\n            'endRecordID': self.saved_states[len(self.saved_states)-1].ROWID\n          },\n          'numRecordsStored': len(self.saved_states)\n        })\n\n    # Remove records within the cache\n    recordsToDelete = []\n    for state in self.saved_states[clippedStart:clippedEnd]:\n      if labelFilter is not None:\n        if labelFilter in state.anomalyLabel:\n          state.anomalyLabel.remove(labelFilter)\n      else:\n        state.anomalyLabel = []\n      state.setByUser = False\n      recordsToDelete.append(state)\n    self._deleteRecordsFromKNN(recordsToDelete)\n\n    # Remove records not in cache\n    self._deleteRangeFromKNN(start, end)\n\n    # Recompute [clippedEnd, ...)\n    for state in self.saved_states[clippedEnd:]:\n      self._updateState(state)\n\n    return {'status': 'success'}",
    "doc": "Remove labels from each record with record ROWID in range from\n    start to end, noninclusive of end. Removes all records if labelFilter is\n    None, otherwise only removes the labels eqaul to labelFilter.\n\n    This will recalculate all points from end to the last record stored in the\n    internal cache of this classifier."
  },
  {
    "code": "def _addRecordToKNN(self, record):\n    \"\"\"\n    This method will add the record to the KNN classifier.\n    \"\"\"\n    classifier = self.htm_prediction_model._getAnomalyClassifier()\n    knn = classifier.getSelf()._knn\n\n    prototype_idx = classifier.getSelf().getParameter('categoryRecencyList')\n    category = self._labelListToCategoryNumber(record.anomalyLabel)\n\n    # If record is already in the classifier, overwrite its labeling\n    if record.ROWID in prototype_idx:\n      knn.prototypeSetCategory(record.ROWID, category)\n      return\n\n    # Learn this pattern in the knn\n    pattern = self._getStateAnomalyVector(record)\n    rowID = record.ROWID\n    knn.learn(pattern, category, rowID=rowID)",
    "doc": "This method will add the record to the KNN classifier."
  },
  {
    "code": "def _deleteRecordsFromKNN(self, recordsToDelete):\n    \"\"\"\n    This method will remove the given records from the classifier.\n\n    parameters\n    ------------\n    recordsToDelete - list of records to delete from the classififier\n    \"\"\"\n    classifier = self.htm_prediction_model._getAnomalyClassifier()\n    knn = classifier.getSelf()._knn\n\n    prototype_idx = classifier.getSelf().getParameter('categoryRecencyList')\n\n    idsToDelete = [r.ROWID for r in recordsToDelete if \\\n      not r.setByUser and r.ROWID in prototype_idx]\n\n    nProtos = knn._numPatterns\n    knn.removeIds(idsToDelete)\n    assert knn._numPatterns == nProtos - len(idsToDelete)",
    "doc": "This method will remove the given records from the classifier.\n\n    parameters\n    ------------\n    recordsToDelete - list of records to delete from the classififier"
  },
  {
    "code": "def _deleteRangeFromKNN(self, start=0, end=None):\n    \"\"\"\n    This method will remove any stored records within the range from start to\n    end. Noninclusive of end.\n\n    parameters\n    ------------\n    start - integer representing the ROWID of the start of the deletion range,\n    end - integer representing the ROWID of the end of the deletion range,\n      if None, it will default to end.\n    \"\"\"\n    classifier = self.htm_prediction_model._getAnomalyClassifier()\n    knn = classifier.getSelf()._knn\n\n    prototype_idx = numpy.array(\n      classifier.getSelf().getParameter('categoryRecencyList'))\n\n    if end is None:\n      end = prototype_idx.max() + 1\n\n    idsIdxToDelete = numpy.logical_and(prototype_idx >= start,\n                                       prototype_idx < end)\n    idsToDelete = prototype_idx[idsIdxToDelete]\n\n    nProtos = knn._numPatterns\n    knn.removeIds(idsToDelete.tolist())\n    assert knn._numPatterns == nProtos - len(idsToDelete)",
    "doc": "This method will remove any stored records within the range from start to\n    end. Noninclusive of end.\n\n    parameters\n    ------------\n    start - integer representing the ROWID of the start of the deletion range,\n    end - integer representing the ROWID of the end of the deletion range,\n      if None, it will default to end."
  },
  {
    "code": "def _recomputeRecordFromKNN(self, record):\n    \"\"\"\n    return the classified labeling of record\n    \"\"\"\n    inputs = {\n      \"categoryIn\": [None],\n      \"bottomUpIn\": self._getStateAnomalyVector(record),\n    }\n\n    outputs = {\"categoriesOut\": numpy.zeros((1,)),\n               \"bestPrototypeIndices\":numpy.zeros((1,)),\n               \"categoryProbabilitiesOut\":numpy.zeros((1,))}\n\n    # Run inference only to capture state before learning\n    classifier = self.htm_prediction_model._getAnomalyClassifier()\n    knn = classifier.getSelf()._knn\n\n    # Only use points before record to classify and after the wait period.\n    classifier_indexes = \\\n      numpy.array(classifier.getSelf().getParameter('categoryRecencyList'))\n    valid_idx = numpy.where(\n        (classifier_indexes >= self._autoDetectWaitRecords) &\n        (classifier_indexes < record.ROWID)\n      )[0].tolist()\n\n    if len(valid_idx) == 0:\n      return None\n\n    classifier.setParameter('inferenceMode', True)\n    classifier.setParameter('learningMode', False)\n    classifier.getSelf().compute(inputs, outputs)\n    classifier.setParameter('learningMode', True)\n\n    classifier_distances = classifier.getSelf().getLatestDistances()\n    valid_distances = classifier_distances[valid_idx]\n    if valid_distances.min() <= self._classificationMaxDist:\n      classifier_indexes_prev = classifier_indexes[valid_idx]\n      rowID = classifier_indexes_prev[valid_distances.argmin()]\n      indexID = numpy.where(classifier_indexes == rowID)[0][0]\n      category = classifier.getSelf().getCategoryList()[indexID]\n      return category\n    return None",
    "doc": "return the classified labeling of record"
  },
  {
    "code": "def _constructClassificationRecord(self):\n    \"\"\"\n    Construct a _HTMClassificationRecord based on the current state of the\n    htm_prediction_model of this classifier.\n\n    ***This will look into the internals of the model and may depend on the\n    SP, TM, and KNNClassifier***\n    \"\"\"\n    model = self.htm_prediction_model\n    sp = model._getSPRegion()\n    tm = model._getTPRegion()\n    tpImp = tm.getSelf()._tfdr\n\n    # Count the number of unpredicted columns\n    activeColumns = sp.getOutputData(\"bottomUpOut\").nonzero()[0]\n    score = numpy.in1d(activeColumns, self._prevPredictedColumns).sum()\n    score = (self._activeColumnCount - score)/float(self._activeColumnCount)\n\n    spSize = sp.getParameter('activeOutputCount')\n    tpSize = tm.getParameter('cellsPerColumn') * tm.getParameter('columnCount')\n\n    classificationVector = numpy.array([])\n\n    if self._vectorType == 'tpc':\n      # Classification Vector: [---TM Cells---]\n      classificationVector = numpy.zeros(tpSize)\n      activeCellMatrix = tpImp.getLearnActiveStateT().reshape(tpSize, 1)\n      activeCellIdx = numpy.where(activeCellMatrix > 0)[0]\n      if activeCellIdx.shape[0] > 0:\n        classificationVector[numpy.array(activeCellIdx, dtype=numpy.uint16)] = 1\n    elif self._vectorType == 'sp_tpe':\n      # Classification Vecotr: [---SP---|---(TM-SP)----]\n      classificationVector = numpy.zeros(spSize+spSize)\n      if activeColumns.shape[0] > 0:\n        classificationVector[activeColumns] = 1.0\n\n      errorColumns = numpy.setdiff1d(self._prevPredictedColumns, activeColumns)\n      if errorColumns.shape[0] > 0:\n        errorColumnIndexes = ( numpy.array(errorColumns, dtype=numpy.uint16) +\n          spSize )\n        classificationVector[errorColumnIndexes] = 1.0\n    else:\n      raise TypeError(\"Classification vector type must be either 'tpc' or\"\n        \" 'sp_tpe', current value is %s\" % (self._vectorType))\n\n    # Store the state for next time step\n    numPredictedCols = len(self._prevPredictedColumns)\n    predictedColumns = tm.getOutputData(\"topDownOut\").nonzero()[0]\n    self._prevPredictedColumns = copy.deepcopy(predictedColumns)\n\n    if self._anomalyVectorLength is None:\n      self._anomalyVectorLength = len(classificationVector)\n\n    result = _CLAClassificationRecord(\n      ROWID=int(model.getParameter('__numRunCalls') - 1), #__numRunCalls called\n        #at beginning of model.run\n      anomalyScore=score,\n      anomalyVector=classificationVector.nonzero()[0].tolist(),\n      anomalyLabel=[]\n    )\n    return result",
    "doc": "Construct a _HTMClassificationRecord based on the current state of the\n    htm_prediction_model of this classifier.\n\n    ***This will look into the internals of the model and may depend on the\n    SP, TM, and KNNClassifier***"
  },
  {
    "code": "def compute(self):\n    \"\"\"\n    Run an iteration of this anomaly classifier\n    \"\"\"\n    result = self._constructClassificationRecord()\n\n    # Classify this point after waiting the classification delay\n    if result.ROWID >= self._autoDetectWaitRecords:\n      self._updateState(result)\n\n    # Save new classification record and keep history as moving window\n    self.saved_states.append(result)\n    if len(self.saved_states) > self._history_length:\n      self.saved_states.pop(0)\n\n    return result",
    "doc": "Run an iteration of this anomaly classifier"
  },
  {
    "code": "def setAutoDetectWaitRecords(self, waitRecords):\n    \"\"\"\n    Sets the autoDetectWaitRecords.\n    \"\"\"\n    if not isinstance(waitRecords, int):\n      raise HTMPredictionModelInvalidArgument(\"Invalid argument type \\'%s\\'. WaitRecord \"\n        \"must be a number.\" % (type(waitRecords)))\n\n    if len(self.saved_states) > 0 and waitRecords < self.saved_states[0].ROWID:\n      raise HTMPredictionModelInvalidArgument(\"Invalid value. autoDetectWaitRecord value \"\n        \"must be valid record within output stream. Current minimum ROWID in \"\n        \"output stream is %d.\" % (self.saved_states[0].ROWID))\n\n    self._autoDetectWaitRecords = waitRecords\n\n    # Update all the states in the classifier's cache\n    for state in self.saved_states:\n      self._updateState(state)",
    "doc": "Sets the autoDetectWaitRecords."
  },
  {
    "code": "def setAutoDetectThreshold(self, threshold):\n    \"\"\"\n    Sets the autoDetectThreshold.\n    TODO: Ensure previously classified points outside of classifier are valid.\n    \"\"\"\n    if not (isinstance(threshold, float) or isinstance(threshold, int)):\n      raise HTMPredictionModelInvalidArgument(\"Invalid argument type \\'%s\\'. threshold \"\n        \"must be a number.\" % (type(threshold)))\n\n    self._autoDetectThreshold = threshold\n\n    # Update all the states in the classifier's cache\n    for state in self.saved_states:\n      self._updateState(state)",
    "doc": "Sets the autoDetectThreshold.\n    TODO: Ensure previously classified points outside of classifier are valid."
  },
  {
    "code": "def _getAdditionalSpecs(spatialImp, kwargs={}):\n  \"\"\"Build the additional specs in three groups (for the inspector)\n\n  Use the type of the default argument to set the Spec type, defaulting\n  to 'Byte' for None and complex types\n\n  Determines the spatial parameters based on the selected implementation.\n  It defaults to SpatialPooler.\n  \"\"\"\n  typeNames = {int: 'UInt32', float: 'Real32', str: 'Byte', bool: 'bool', tuple: 'tuple'}\n\n  def getArgType(arg):\n    t = typeNames.get(type(arg), 'Byte')\n    count = 0 if t == 'Byte' else 1\n    if t == 'tuple':\n      t = typeNames.get(type(arg[0]), 'Byte')\n      count = len(arg)\n    if t == 'bool':\n      t = 'UInt32'\n    return (t, count)\n\n  def getConstraints(arg):\n    t = typeNames.get(type(arg), 'Byte')\n    if t == 'Byte':\n      return 'multiple'\n    elif t == 'bool':\n      return 'bool'\n    else:\n      return ''\n\n  # Get arguments from spatial pooler constructors, figure out types of\n  # variables and populate spatialSpec.\n  SpatialClass = getSPClass(spatialImp)\n  sArgTuples = _buildArgs(SpatialClass.__init__)\n  spatialSpec = {}\n  for argTuple in sArgTuples:\n    d = dict(\n      description=argTuple[1],\n      accessMode='ReadWrite',\n      dataType=getArgType(argTuple[2])[0],\n      count=getArgType(argTuple[2])[1],\n      constraints=getConstraints(argTuple[2]))\n    spatialSpec[argTuple[0]] = d\n\n  # Add special parameters that weren't handled automatically\n  # Spatial parameters only!\n  spatialSpec.update(dict(\n\n    columnCount=dict(\n      description='Total number of columns (coincidences).',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    inputWidth=dict(\n      description='Size of inputs to the SP.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    spInputNonZeros=dict(\n      description='The indices of the non-zero inputs to the spatial pooler',\n      accessMode='Read',\n      dataType='UInt32',\n      count=0,\n      constraints=''),\n\n    spOutputNonZeros=dict(\n      description='The indices of the non-zero outputs from the spatial pooler',\n      accessMode='Read',\n      dataType='UInt32',\n      count=0,\n      constraints=''),\n\n    spOverlapDistribution=dict(\n      description=\"\"\"The overlaps between the active output coincidences\n      and the input. The overlap amounts for each coincidence are sorted\n      from highest to lowest. \"\"\",\n      accessMode='Read',\n      dataType='Real32',\n      count=0,\n      constraints=''),\n\n    sparseCoincidenceMatrix=dict(\n      description='The coincidences, as a SparseMatrix',\n      accessMode='Read',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n    denseOutput=dict(\n      description='Score for each coincidence.',\n      accessMode='Read',\n      dataType='Real32',\n      count=0,\n      constraints=''),\n\n    spLearningStatsStr=dict(\n      description=\"\"\"String representation of dictionary containing a number\n                     of statistics related to learning.\"\"\",\n      accessMode='Read',\n      dataType='Byte',\n      count=0,\n      constraints='handle'),\n\n    spatialImp=dict(\n        description=\"\"\"Which spatial pooler implementation to use. Set to either\n                      'py', or 'cpp'. The 'cpp' implementation is optimized for\n                      speed in C++.\"\"\",\n        accessMode='ReadWrite',\n        dataType='Byte',\n        count=0,\n        constraints='enum: py, cpp'),\n  ))\n\n\n  # The last group is for parameters that aren't specific to spatial pooler\n  otherSpec = dict(\n    learningMode=dict(\n      description='1 if the node is learning (default 1).',\n      accessMode='ReadWrite',\n      dataType='UInt32',\n      count=1,\n      constraints='bool'),\n\n    inferenceMode=dict(\n      description='1 if the node is inferring (default 0).',\n      accessMode='ReadWrite',\n      dataType='UInt32',\n      count=1,\n      constraints='bool'),\n\n    anomalyMode=dict(\n      description='1 if an anomaly score is being computed',\n      accessMode='ReadWrite',\n      dataType='UInt32',\n      count=1,\n      constraints='bool'),\n\n    topDownMode=dict(\n      description='1 if the node should do top down compute on the next call '\n                  'to compute into topDownOut (default 0).',\n      accessMode='ReadWrite',\n      dataType='UInt32',\n      count=1,\n      constraints='bool'),\n\n    activeOutputCount=dict(\n      description='Number of active elements in bottomUpOut output.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    logPathInput=dict(\n      description='Optional name of input log file. If set, every input vector'\n                  ' will be logged to this file.',\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n    logPathOutput=dict(\n      description='Optional name of output log file. If set, every output vector'\n                  ' will be logged to this file.',\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n    logPathOutputDense=dict(\n      description='Optional name of output log file. If set, every output vector'\n                  ' will be logged to this file as a dense vector.',\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n  )\n\n  return spatialSpec, otherSpec",
    "doc": "Build the additional specs in three groups (for the inspector)\n\n  Use the type of the default argument to set the Spec type, defaulting\n  to 'Byte' for None and complex types\n\n  Determines the spatial parameters based on the selected implementation.\n  It defaults to SpatialPooler."
  },
  {
    "code": "def _initializeEphemeralMembers(self):\n    \"\"\"\n    Initialize all ephemeral data members, and give the derived class the\n    opportunity to do the same by invoking the virtual member _initEphemerals(),\n    which is intended to be overridden.\n\n    NOTE: this is used by both __init__ and __setstate__ code paths.\n    \"\"\"\n\n    for attrName in self._getEphemeralMembersBase():\n      if attrName != \"_loaded\":\n        if hasattr(self, attrName):\n          if self._loaded:\n            # print self.__class__.__name__, \"contains base class member '%s' \" \\\n            #     \"after loading.\" % attrName\n            # TODO: Re-enable warning or turn into error in a future release.\n            pass\n          else:\n            print self.__class__.__name__, \"contains base class member '%s'\" % \\\n                attrName\n    if not self._loaded:\n      for attrName in self._getEphemeralMembersBase():\n        if attrName != \"_loaded\":\n          # if hasattr(self, attrName):\n          #   import pdb; pdb.set_trace()\n          assert not hasattr(self, attrName)\n        else:\n          assert hasattr(self, attrName)\n\n    # Profiling information\n    self._profileObj = None\n    self._iterations = 0\n\n    # Let derived class initialize ephemerals\n    self._initEphemerals()\n    self._checkEphemeralMembers()",
    "doc": "Initialize all ephemeral data members, and give the derived class the\n    opportunity to do the same by invoking the virtual member _initEphemerals(),\n    which is intended to be overridden.\n\n    NOTE: this is used by both __init__ and __setstate__ code paths."
  },
  {
    "code": "def initialize(self):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.initialize`.\n    \"\"\"\n    # Zero out the spatial output in case it is requested\n    self._spatialPoolerOutput = numpy.zeros(self.columnCount,\n                                            dtype=GetNTAReal())\n\n    # Zero out the rfInput in case it is requested\n    self._spatialPoolerInput = numpy.zeros((1, self.inputWidth),\n                                           dtype=GetNTAReal())\n\n    # Allocate the spatial pooler\n    self._allocateSpatialFDR(None)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.initialize`."
  },
  {
    "code": "def _allocateSpatialFDR(self, rfInput):\n    \"\"\"Allocate the spatial pooler instance.\"\"\"\n    if self._sfdr:\n      return\n\n    # Retrieve the necessary extra arguments that were handled automatically\n    autoArgs = dict((name, getattr(self, name))\n                     for name in self._spatialArgNames)\n\n    # Instantiate the spatial pooler class.\n    if ( (self.SpatialClass == CPPSpatialPooler) or\n         (self.SpatialClass == PYSpatialPooler) ):\n\n      autoArgs['columnDimensions'] = [self.columnCount]\n      autoArgs['inputDimensions'] = [self.inputWidth]\n      autoArgs['potentialRadius'] = self.inputWidth\n\n      self._sfdr = self.SpatialClass(\n        **autoArgs\n      )",
    "doc": "Allocate the spatial pooler instance."
  },
  {
    "code": "def compute(self, inputs, outputs):\n    \"\"\"\n    Run one iteration, profiling it if requested.\n\n    :param inputs: (dict) mapping region input names to numpy.array values\n    :param outputs: (dict) mapping region output names to numpy.arrays that \n           should be populated with output values by this method\n    \"\"\"\n\n    # Uncomment this to find out who is generating divide by 0, or other numpy warnings\n    # numpy.seterr(divide='raise', invalid='raise', over='raise')\n\n    # Modify this line to turn on profiling for a given node. The results file\n    #  ('hotshot.stats') will be sensed and printed out by the vision framework's\n    #  RunInference.py script at the end of inference.\n    # Also uncomment the hotshot import at the top of this file.\n    if False and self.learningMode \\\n        and self._iterations > 0 and self._iterations <= 10:\n\n      import hotshot\n      if self._iterations == 10:\n        print \"\\n  Collecting and sorting internal node profiling stats generated by hotshot...\"\n        stats = hotshot.stats.load(\"hotshot.stats\")\n        stats.strip_dirs()\n        stats.sort_stats('time', 'calls')\n        stats.print_stats()\n\n      # The guts of the compute are contained in the _compute() call so that we\n      # can profile it if requested.\n      if self._profileObj is None:\n        print \"\\n  Preparing to capture profile using hotshot...\"\n        if os.path.exists('hotshot.stats'):\n          # There is an old hotshot stats profile left over, remove it.\n          os.remove('hotshot.stats')\n        self._profileObj = hotshot.Profile(\"hotshot.stats\", 1, 1)\n                                          # filename, lineevents, linetimings\n      self._profileObj.runcall(self._compute, *[inputs, outputs])\n    else:\n      self._compute(inputs, outputs)",
    "doc": "Run one iteration, profiling it if requested.\n\n    :param inputs: (dict) mapping region input names to numpy.array values\n    :param outputs: (dict) mapping region output names to numpy.arrays that \n           should be populated with output values by this method"
  },
  {
    "code": "def _compute(self, inputs, outputs):\n    \"\"\"\n    Run one iteration of SPRegion's compute\n    \"\"\"\n\n    #if self.topDownMode and (not 'topDownIn' in inputs):\n    #  raise RuntimeError(\"The input topDownIn must be linked in if \"\n    #                     \"topDownMode is True\")\n\n    if self._sfdr is None:\n      raise RuntimeError(\"Spatial pooler has not been initialized\")\n\n\n    if not self.topDownMode:\n      #\n      # BOTTOM-UP compute\n      #\n\n      self._iterations += 1\n\n      # Get our inputs into numpy arrays\n      buInputVector = inputs['bottomUpIn']\n\n      resetSignal = False\n      if 'resetIn' in inputs:\n        assert len(inputs['resetIn']) == 1\n        resetSignal = inputs['resetIn'][0] != 0\n\n      # Perform inference and/or learning\n      rfOutput = self._doBottomUpCompute(\n        rfInput = buInputVector.reshape((1,buInputVector.size)),\n        resetSignal = resetSignal\n        )\n\n      outputs['bottomUpOut'][:] = rfOutput.flat\n\n    else:\n      #\n      # TOP-DOWN inference\n      #\n\n      topDownIn = inputs.get('topDownIn',None)\n      spatialTopDownOut, temporalTopDownOut = self._doTopDownInfer(topDownIn)\n      outputs['spatialTopDownOut'][:] = spatialTopDownOut\n      if temporalTopDownOut is not None:\n        outputs['temporalTopDownOut'][:] = temporalTopDownOut\n\n\n    # OBSOLETE\n    outputs['anomalyScore'][:] = 0",
    "doc": "Run one iteration of SPRegion's compute"
  },
  {
    "code": "def _doBottomUpCompute(self, rfInput, resetSignal):\n    \"\"\"\n    Do one iteration of inference and/or learning and return the result\n\n    Parameters:\n    --------------------------------------------\n    rfInput:      Input vector. Shape is: (1, inputVectorLen).\n    resetSignal:  True if reset is asserted\n\n    \"\"\"\n\n    # Conditional compute break\n    self._conditionalBreak()\n\n    # Save the rfInput for the spInputNonZeros parameter\n    self._spatialPoolerInput = rfInput.reshape(-1)\n    assert(rfInput.shape[0] == 1)\n\n    # Run inference using the spatial pooler. We learn on the coincidences only\n    # if we are in learning mode and trainingStep is set appropriately.\n\n    # Run SFDR bottom-up compute and cache output in self._spatialPoolerOutput\n\n    inputVector = numpy.array(rfInput[0]).astype('uint32')\n    outputVector = numpy.zeros(self._sfdr.getNumColumns()).astype('uint32')\n\n    self._sfdr.compute(inputVector, self.learningMode, outputVector)\n\n    self._spatialPoolerOutput[:] = outputVector[:]\n\n    # Direct logging of SP outputs if requested\n    if self._fpLogSP:\n      output = self._spatialPoolerOutput.reshape(-1)\n      outputNZ = output.nonzero()[0]\n      outStr = \" \".join([\"%d\" % int(token) for token in outputNZ])\n      print >>self._fpLogSP, output.size, outStr\n\n    # Direct logging of SP inputs\n    if self._fpLogSPInput:\n      output = rfInput.reshape(-1)\n      outputNZ = output.nonzero()[0]\n      outStr = \" \".join([\"%d\" % int(token) for token in outputNZ])\n      print >>self._fpLogSPInput, output.size, outStr\n\n    return self._spatialPoolerOutput",
    "doc": "Do one iteration of inference and/or learning and return the result\n\n    Parameters:\n    --------------------------------------------\n    rfInput:      Input vector. Shape is: (1, inputVectorLen).\n    resetSignal:  True if reset is asserted"
  },
  {
    "code": "def getBaseSpec(cls):\n    \"\"\"\n    Doesn't include the spatial, temporal and other parameters\n\n    :returns: (dict) The base Spec for SPRegion.\n    \"\"\"\n    spec = dict(\n      description=SPRegion.__doc__,\n      singleNodeOnly=True,\n      inputs=dict(\n        bottomUpIn=dict(\n          description=\"\"\"The input vector.\"\"\",\n          dataType='Real32',\n          count=0,\n          required=True,\n          regionLevel=False,\n          isDefaultInput=True,\n          requireSplitterMap=False),\n\n        resetIn=dict(\n          description=\"\"\"A boolean flag that indicates whether\n                         or not the input vector received in this compute cycle\n                         represents the start of a new temporal sequence.\"\"\",\n          dataType='Real32',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        topDownIn=dict(\n          description=\"\"\"The top-down input signal, generated from\n                        feedback from upper levels\"\"\",\n          dataType='Real32',\n          count=0,\n          required = False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        sequenceIdIn=dict(\n          description=\"Sequence ID\",\n          dataType='UInt64',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n      ),\n      outputs=dict(\n        bottomUpOut=dict(\n          description=\"\"\"The output signal generated from the bottom-up inputs\n                          from lower levels.\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=True),\n\n        topDownOut=dict(\n          description=\"\"\"The top-down output signal, generated from\n                        feedback from upper levels\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        spatialTopDownOut = dict(\n          description=\"\"\"The top-down output, generated only from the current\n                         SP output. This can be used to evaluate how well the\n                         SP is representing the inputs independent of the TM.\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        temporalTopDownOut = dict(\n          description=\"\"\"The top-down output, generated only from the current\n                         TM output feedback down through the SP.\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        anomalyScore = dict(\n          description=\"\"\"The score for how 'anomalous' (i.e. rare) this spatial\n                        input pattern is. Higher values are increasingly rare\"\"\",\n          dataType='Real32',\n          count=1,\n          regionLevel=True,\n          isDefaultOutput=False),\n      ),\n\n      parameters=dict(\n        breakPdb=dict(\n          description='Set to 1 to stop in the pdb debugger on the next compute',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n\n        breakKomodo=dict(\n          description='Set to 1 to stop in the Komodo debugger on the next compute',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n\n      ),\n    )\n\n    return spec",
    "doc": "Doesn't include the spatial, temporal and other parameters\n\n    :returns: (dict) The base Spec for SPRegion."
  },
  {
    "code": "def getSpec(cls):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n\n    The parameters collection is constructed based on the parameters specified\n    by the various components (spatialSpec, temporalSpec and otherSpec)\n    \"\"\"\n    spec = cls.getBaseSpec()\n    s, o = _getAdditionalSpecs(spatialImp=getDefaultSPImp())\n    spec['parameters'].update(s)\n    spec['parameters'].update(o)\n\n    return spec",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n\n    The parameters collection is constructed based on the parameters specified\n    by the various components (spatialSpec, temporalSpec and otherSpec)"
  },
  {
    "code": "def getParameter(self, parameterName, index=-1):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameter`.\n      \n    Most parameters are handled automatically by PyRegion's parameter get \n    mechanism. The ones that need special treatment are explicitly handled here.\n    \"\"\"\n\n    if parameterName == 'activeOutputCount':\n      return self.columnCount\n    elif parameterName == 'spatialPoolerInput':\n      return list(self._spatialPoolerInput.reshape(-1))\n    elif parameterName == 'spatialPoolerOutput':\n      return list(self._spatialPoolerOutput)\n    elif parameterName == 'spNumActiveOutputs':\n      return len(self._spatialPoolerOutput.nonzero()[0])\n    elif parameterName == 'spOutputNonZeros':\n      return [len(self._spatialPoolerOutput)] + \\\n              list(self._spatialPoolerOutput.nonzero()[0])\n    elif parameterName == 'spInputNonZeros':\n      import pdb; pdb.set_trace()\n      return [len(self._spatialPoolerInput)] + \\\n              list(self._spatialPoolerInput.nonzero()[0])\n    elif parameterName == 'spLearningStatsStr':\n      try:\n        return str(self._sfdr.getLearningStats())\n      except:\n        return str(dict())\n    else:\n      return PyRegion.getParameter(self, parameterName, index)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameter`.\n      \n    Most parameters are handled automatically by PyRegion's parameter get \n    mechanism. The ones that need special treatment are explicitly handled here."
  },
  {
    "code": "def setParameter(self, parameterName, index, parameterValue):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.setParameter`.\n\n    Set the value of a Spec parameter. Most parameters are handled\n    automatically by PyRegion's parameter set mechanism. The ones that need\n    special treatment are explicitly handled here.\n    \"\"\"\n    if parameterName in self._spatialArgNames:\n      setattr(self._sfdr, parameterName, parameterValue)\n\n    elif parameterName == \"logPathInput\":\n      self.logPathInput = parameterValue\n      # Close any existing log file\n      if self._fpLogSPInput:\n        self._fpLogSPInput.close()\n        self._fpLogSPInput = None\n      # Open a new log file\n      if parameterValue:\n        self._fpLogSPInput = open(self.logPathInput, 'w')\n\n    elif parameterName == \"logPathOutput\":\n      self.logPathOutput = parameterValue\n      # Close any existing log file\n      if self._fpLogSP:\n        self._fpLogSP.close()\n        self._fpLogSP = None\n      # Open a new log file\n      if parameterValue:\n        self._fpLogSP = open(self.logPathOutput, 'w')\n\n    elif parameterName == \"logPathOutputDense\":\n      self.logPathOutputDense = parameterValue\n      # Close any existing log file\n      if self._fpLogSPDense:\n        self._fpLogSPDense.close()\n        self._fpLogSPDense = None\n      # Open a new log file\n      if parameterValue:\n        self._fpLogSPDense = open(self.logPathOutputDense, 'w')\n\n    elif hasattr(self, parameterName):\n      setattr(self, parameterName, parameterValue)\n\n    else:\n      raise Exception('Unknown parameter: ' + parameterName)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.setParameter`.\n\n    Set the value of a Spec parameter. Most parameters are handled\n    automatically by PyRegion's parameter set mechanism. The ones that need\n    special treatment are explicitly handled here."
  },
  {
    "code": "def writeToProto(self, proto):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`.\n\n    Write state to proto object.\n\n    :param proto: SPRegionProto capnproto object\n    \"\"\"\n    proto.spatialImp = self.spatialImp\n    proto.columnCount = self.columnCount\n    proto.inputWidth = self.inputWidth\n    proto.learningMode = 1 if self.learningMode else 0\n    proto.inferenceMode = 1 if self.inferenceMode else 0\n    proto.anomalyMode = 1 if self.anomalyMode else 0\n    proto.topDownMode = 1 if self.topDownMode else 0\n\n    self._sfdr.write(proto.spatialPooler)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`.\n\n    Write state to proto object.\n\n    :param proto: SPRegionProto capnproto object"
  },
  {
    "code": "def readFromProto(cls, proto):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.\n\n    Read state from proto object.\n    \n    :param proto: SPRegionProto capnproto object\n    \"\"\"\n    instance = cls(proto.columnCount, proto.inputWidth)\n\n    instance.spatialImp = proto.spatialImp\n    instance.learningMode = proto.learningMode\n    instance.inferenceMode = proto.inferenceMode\n    instance.anomalyMode = proto.anomalyMode\n    instance.topDownMode = proto.topDownMode\n\n    spatialImp = proto.spatialImp\n    instance._sfdr = getSPClass(spatialImp).read(proto.spatialPooler)\n\n    return instance",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.\n\n    Read state from proto object.\n    \n    :param proto: SPRegionProto capnproto object"
  },
  {
    "code": "def _initEphemerals(self):\n    \"\"\"\n    Initialize all ephemerals used by derived classes.\n    \"\"\"\n\n    if hasattr(self, '_sfdr') and self._sfdr:\n      self._spatialPoolerOutput = numpy.zeros(self.columnCount,\n                                               dtype=GetNTAReal())\n    else:\n      self._spatialPoolerOutput = None  # Will be filled in initInNetwork\n\n    # Direct logging support (faster than node watch)\n    self._fpLogSPInput = None\n    self._fpLogSP = None\n    self._fpLogSPDense = None\n    self.logPathInput = \"\"\n    self.logPathOutput = \"\"\n    self.logPathOutputDense = \"\"",
    "doc": "Initialize all ephemerals used by derived classes."
  },
  {
    "code": "def getParameterArrayCount(self, name, index):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArrayCount`.\n\n    TODO: as a temporary hack, getParameterArrayCount checks to see if there's a\n    variable, private or not, with that name. If so, it returns the value of the\n    variable.\n    \"\"\"\n    p = self.getParameter(name)\n    if (not hasattr(p, '__len__')):\n      raise Exception(\"Attempt to access parameter '%s' as an array but it is not an array\" % name)\n    return len(p)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArrayCount`.\n\n    TODO: as a temporary hack, getParameterArrayCount checks to see if there's a\n    variable, private or not, with that name. If so, it returns the value of the\n    variable."
  },
  {
    "code": "def getParameterArray(self, name, index, a):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArray`.\n\n    TODO: as a temporary hack, getParameterArray checks to see if there's a\n    variable, private or not, with that name. If so, it returns the value of the\n    variable.\n    \"\"\"\n    p = self.getParameter(name)\n    if (not hasattr(p, '__len__')):\n      raise Exception(\"Attempt to access parameter '%s' as an array but it is not an array\" % name)\n\n    if len(p) >  0:\n      a[:] = p[:]",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArray`.\n\n    TODO: as a temporary hack, getParameterArray checks to see if there's a\n    variable, private or not, with that name. If so, it returns the value of the\n    variable."
  },
  {
    "code": "def _cacheSequenceInfoType(self):\n    \"\"\"Figure out whether reset, sequenceId,\n    both or neither are present in the data.\n    Compute once instead of every time.\n\n    Taken from filesource.py\"\"\"\n\n    hasReset = self.resetFieldName is not None\n    hasSequenceId = self.sequenceIdFieldName is not None\n\n    if hasReset and not hasSequenceId:\n      self._sequenceInfoType = self.SEQUENCEINFO_RESET_ONLY\n      self._prevSequenceId = 0\n    elif not hasReset and hasSequenceId:\n      self._sequenceInfoType = self.SEQUENCEINFO_SEQUENCEID_ONLY\n      self._prevSequenceId = None\n    elif hasReset and hasSequenceId:\n      self._sequenceInfoType = self.SEQUENCEINFO_BOTH\n    else:\n      self._sequenceInfoType = self.SEQUENCEINFO_NONE",
    "doc": "Figure out whether reset, sequenceId,\n    both or neither are present in the data.\n    Compute once instead of every time.\n\n    Taken from filesource.py"
  },
  {
    "code": "def _getTPClass(temporalImp):\n  \"\"\" Return the class corresponding to the given temporalImp string\n  \"\"\"\n\n  if temporalImp == 'py':\n    return backtracking_tm.BacktrackingTM\n  elif temporalImp == 'cpp':\n    return backtracking_tm_cpp.BacktrackingTMCPP\n  elif temporalImp == 'tm_py':\n    return backtracking_tm_shim.TMShim\n  elif temporalImp == 'tm_cpp':\n    return backtracking_tm_shim.TMCPPShim\n  elif temporalImp == 'monitored_tm_py':\n    return backtracking_tm_shim.MonitoredTMShim\n  else:\n    raise RuntimeError(\"Invalid temporalImp '%s'. Legal values are: 'py', \"\n              \"'cpp', 'tm_py', 'monitored_tm_py'\" % (temporalImp))",
    "doc": "Return the class corresponding to the given temporalImp string"
  },
  {
    "code": "def _buildArgs(f, self=None, kwargs={}):\n  \"\"\"\n  Get the default arguments from the function and assign as instance vars.\n\n  Return a list of 3-tuples with (name, description, defaultValue) for each\n    argument to the function.\n\n  Assigns all arguments to the function as instance variables of TMRegion.\n  If the argument was not provided, uses the default value.\n\n  Pops any values from kwargs that go to the function.\n  \"\"\"\n  # Get the name, description, and default value for each argument\n  argTuples = getArgumentDescriptions(f)\n  argTuples = argTuples[1:]  # Remove 'self'\n\n  # Get the names of the parameters to our own constructor and remove them\n  # Check for _originial_init first, because if LockAttributesMixin is used,\n  #  __init__'s signature will be just (self, *args, **kw), but\n  #  _original_init is created with the original signature\n  #init = getattr(self, '_original_init', self.__init__)\n  init = TMRegion.__init__\n  ourArgNames = [t[0] for t in getArgumentDescriptions(init)]\n  # Also remove a few other names that aren't in our constructor but are\n  #  computed automatically (e.g. numberOfCols for the TM)\n  ourArgNames += [\n    'numberOfCols',    # TM\n  ]\n  for argTuple in argTuples[:]:\n    if argTuple[0] in ourArgNames:\n      argTuples.remove(argTuple)\n\n  # Build the dictionary of arguments\n  if self:\n    for argTuple in argTuples:\n      argName = argTuple[0]\n      if argName in kwargs:\n        # Argument was provided\n        argValue = kwargs.pop(argName)\n      else:\n        # Argument was not provided; use the default value if there is one, and\n        #  raise an exception otherwise\n        if len(argTuple) == 2:\n          # No default value\n          raise TypeError(\"Must provide '%s'\" % argName)\n        argValue = argTuple[2]\n      # Set as an instance variable if 'self' was passed in\n      setattr(self, argName, argValue)\n\n  return argTuples",
    "doc": "Get the default arguments from the function and assign as instance vars.\n\n  Return a list of 3-tuples with (name, description, defaultValue) for each\n    argument to the function.\n\n  Assigns all arguments to the function as instance variables of TMRegion.\n  If the argument was not provided, uses the default value.\n\n  Pops any values from kwargs that go to the function."
  },
  {
    "code": "def _getAdditionalSpecs(temporalImp, kwargs={}):\n  \"\"\"Build the additional specs in three groups (for the inspector)\n\n  Use the type of the default argument to set the Spec type, defaulting\n  to 'Byte' for None and complex types\n\n  Determines the spatial parameters based on the selected implementation.\n  It defaults to TemporalMemory.\n  Determines the temporal parameters based on the temporalImp\n  \"\"\"\n  typeNames = {int: 'UInt32', float: 'Real32', str: 'Byte', bool: 'bool', tuple: 'tuple'}\n\n  def getArgType(arg):\n    t = typeNames.get(type(arg), 'Byte')\n    count = 0 if t == 'Byte' else 1\n    if t == 'tuple':\n      t = typeNames.get(type(arg[0]), 'Byte')\n      count = len(arg)\n    if t == 'bool':\n      t = 'UInt32'\n    return (t, count)\n\n  def getConstraints(arg):\n    t = typeNames.get(type(arg), 'Byte')\n    if t == 'Byte':\n      return 'multiple'\n    elif t == 'bool':\n      return 'bool'\n    else:\n      return ''\n\n  # Build up parameters from temporal memory's constructor\n  TemporalClass = _getTPClass(temporalImp)\n  tArgTuples = _buildArgs(TemporalClass.__init__)\n  temporalSpec = {}\n  for argTuple in tArgTuples:\n    d = dict(\n      description=argTuple[1],\n      accessMode='ReadWrite',\n      dataType=getArgType(argTuple[2])[0],\n      count=getArgType(argTuple[2])[1],\n      constraints=getConstraints(argTuple[2]))\n    temporalSpec[argTuple[0]] = d\n\n  # Add temporal parameters that weren't handled automatically\n  temporalSpec.update(dict(\n    columnCount=dict(\n      description='Total number of columns.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    cellsPerColumn=dict(\n      description='Number of cells per column.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    inputWidth=dict(\n      description='Number of inputs to the TM.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    predictedSegmentDecrement=dict(\n      description='Predicted segment decrement',\n      accessMode='Read',\n      dataType='Real',\n      count=1,\n      constraints=''),\n\n    orColumnOutputs=dict(\n      description=\"\"\"OR together the cell outputs from each column to produce\n      the temporal memory output. When this mode is enabled, the number of\n      cells per column must also be specified and the output size of the region\n      should be set the same as columnCount\"\"\",\n      accessMode='Read',\n      dataType='Bool',\n      count=1,\n      constraints='bool'),\n\n    cellsSavePath=dict(\n      description=\"\"\"Optional path to file in which large temporal memory cells\n                     data structure is to be saved.\"\"\",\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n    temporalImp=dict(\n      description=\"\"\"Which temporal memory implementation to use. Set to either\n       'py' or 'cpp'. The 'cpp' implementation is optimized for speed in C++.\"\"\",\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints='enum: py, cpp'),\n\n  ))\n\n  # The last group is for parameters that aren't strictly spatial or temporal\n  otherSpec = dict(\n    learningMode=dict(\n      description='True if the node is learning (default True).',\n      accessMode='ReadWrite',\n      dataType='Bool',\n      count=1,\n      defaultValue=True,\n      constraints='bool'),\n\n    inferenceMode=dict(\n      description='True if the node is inferring (default False).',\n      accessMode='ReadWrite',\n      dataType='Bool',\n      count=1,\n      defaultValue=False,\n      constraints='bool'),\n\n    computePredictedActiveCellIndices=dict(\n      description='True if active and predicted active indices should be computed',\n      accessMode='Create',\n      dataType='Bool',\n      count=1,\n      defaultValue=False,\n      constraints='bool'),\n\n    anomalyMode=dict(\n      description='True if an anomaly score is being computed',\n      accessMode='Create',\n      dataType='Bool',\n      count=1,\n      defaultValue=False,\n      constraints='bool'),\n\n    topDownMode=dict(\n      description='True if the node should do top down compute on the next call '\n                  'to compute into topDownOut (default False).',\n      accessMode='ReadWrite',\n      dataType='Bool',\n      count=1,\n      defaultValue=False,\n      constraints='bool'),\n\n    activeOutputCount=dict(\n      description='Number of active elements in bottomUpOut output.',\n      accessMode='Read',\n      dataType='UInt32',\n      count=1,\n      constraints=''),\n\n    storeDenseOutput=dict(\n      description=\"\"\"Whether to keep the dense column output (needed for\n                     denseOutput parameter).\"\"\",\n      accessMode='ReadWrite',\n      dataType='UInt32',\n      count=1,\n      constraints='bool'),\n\n    logPathOutput=dict(\n      description='Optional name of output log file. If set, every output vector'\n                  ' will be logged to this file as a sparse vector.',\n      accessMode='ReadWrite',\n      dataType='Byte',\n      count=0,\n      constraints=''),\n\n  )\n\n  return temporalSpec, otherSpec",
    "doc": "Build the additional specs in three groups (for the inspector)\n\n  Use the type of the default argument to set the Spec type, defaulting\n  to 'Byte' for None and complex types\n\n  Determines the spatial parameters based on the selected implementation.\n  It defaults to TemporalMemory.\n  Determines the temporal parameters based on the temporalImp"
  },
  {
    "code": "def initialize(self):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.initialize`.\n    \"\"\"\n    # Allocate appropriate temporal memory object\n    # Retrieve the necessary extra arguments that were handled automatically\n    autoArgs = dict((name, getattr(self, name))\n                    for name in self._temporalArgNames)\n\n    if self._tfdr is None:\n      tpClass = _getTPClass(self.temporalImp)\n\n      if self.temporalImp in ['py', 'cpp', 'r',\n                              'tm_py', 'tm_cpp',\n                              'monitored_tm_py',]:\n        self._tfdr = tpClass(\n             numberOfCols=self.columnCount,\n             cellsPerColumn=self.cellsPerColumn,\n             **autoArgs)\n      else:\n        raise RuntimeError(\"Invalid temporalImp\")",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.initialize`."
  },
  {
    "code": "def _compute(self, inputs, outputs):\n    \"\"\"\n    Run one iteration of TMRegion's compute\n    \"\"\"\n\n    #if self.topDownMode and (not 'topDownIn' in inputs):\n    # raise RuntimeError(\"The input topDownIn must be linked in if \"\n    #                    \"topDownMode is True\")\n\n    if self._tfdr is None:\n      raise RuntimeError(\"TM has not been initialized\")\n\n    # Conditional compute break\n    self._conditionalBreak()\n\n    self._iterations += 1\n\n    # Get our inputs as numpy array\n    buInputVector = inputs['bottomUpIn']\n\n    # Handle reset signal\n    resetSignal = False\n    if 'resetIn' in inputs:\n      assert len(inputs['resetIn']) == 1\n      if inputs['resetIn'][0] != 0:\n        self._tfdr.reset()\n        self._sequencePos = 0  # Position within the current sequence\n\n    if self.computePredictedActiveCellIndices:\n      prevPredictedState = self._tfdr.getPredictedState().reshape(-1).astype('float32')\n\n    if self.anomalyMode:\n      prevPredictedColumns = self._tfdr.topDownCompute().copy().nonzero()[0]\n\n    # Perform inference and/or learning\n    tpOutput = self._tfdr.compute(buInputVector, self.learningMode, self.inferenceMode)\n    self._sequencePos += 1\n\n    # OR'ing together the cells in each column?\n    if self.orColumnOutputs:\n      tpOutput= tpOutput.reshape(self.columnCount,\n                                     self.cellsPerColumn).max(axis=1)\n\n    # Direct logging of non-zero TM outputs\n    if self._fpLogTPOutput:\n      output = tpOutput.reshape(-1)\n      outputNZ = tpOutput.nonzero()[0]\n      outStr = \" \".join([\"%d\" % int(token) for token in outputNZ])\n      print >>self._fpLogTPOutput, output.size, outStr\n\n    # Write the bottom up out to our node outputs\n    outputs['bottomUpOut'][:] = tpOutput.flat\n\n    if self.topDownMode:\n      # Top-down compute\n      outputs['topDownOut'][:] = self._tfdr.topDownCompute().copy()\n\n    # Set output for use with anomaly classification region if in anomalyMode\n    if self.anomalyMode:\n      activeLearnCells = self._tfdr.getLearnActiveStateT()\n      size = activeLearnCells.shape[0] * activeLearnCells.shape[1]\n      outputs['lrnActiveStateT'][:] = activeLearnCells.reshape(size)\n\n      activeColumns = buInputVector.nonzero()[0]\n      outputs['anomalyScore'][:] = anomaly.computeRawAnomalyScore(\n        activeColumns, prevPredictedColumns)\n\n    if self.computePredictedActiveCellIndices:\n      # Reshape so we are dealing with 1D arrays\n      activeState = self._tfdr._getActiveState().reshape(-1).astype('float32')\n      activeIndices = numpy.where(activeState != 0)[0]\n      predictedIndices= numpy.where(prevPredictedState != 0)[0]\n      predictedActiveIndices = numpy.intersect1d(activeIndices, predictedIndices)\n      outputs[\"activeCells\"].fill(0)\n      outputs[\"activeCells\"][activeIndices] = 1\n      outputs[\"predictedActiveCells\"].fill(0)\n      outputs[\"predictedActiveCells\"][predictedActiveIndices] = 1",
    "doc": "Run one iteration of TMRegion's compute"
  },
  {
    "code": "def getBaseSpec(cls):\n    \"\"\"\n    Doesn't include the spatial, temporal and other parameters\n\n    :returns: (dict) the base Spec for TMRegion.\n    \"\"\"\n    spec = dict(\n      description=TMRegion.__doc__,\n      singleNodeOnly=True,\n      inputs=dict(\n        bottomUpIn=dict(\n          description=\"\"\"The input signal, conceptually organized as an\n                         image pyramid data structure, but internally\n                         organized as a flattened vector.\"\"\",\n          dataType='Real32',\n          count=0,\n          required=True,\n          regionLevel=False,\n          isDefaultInput=True,\n          requireSplitterMap=False),\n\n        resetIn=dict(\n          description=\"\"\"Effectively a boolean flag that indicates whether\n                         or not the input vector received in this compute cycle\n                         represents the first training presentation in a\n                         new temporal sequence.\"\"\",\n          dataType='Real32',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n        sequenceIdIn=dict(\n          description=\"Sequence ID\",\n          dataType='UInt64',\n          count=1,\n          required=False,\n          regionLevel=True,\n          isDefaultInput=False,\n          requireSplitterMap=False),\n\n      ),\n\n      outputs=dict(\n        bottomUpOut=dict(\n          description=\"\"\"The output signal generated from the bottom-up inputs\n                          from lower levels.\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=True),\n\n        topDownOut=dict(\n          description=\"\"\"The top-down inputsignal, generated from\n                        feedback from upper levels\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        activeCells=dict(\n          description=\"The cells that are active\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        predictedActiveCells=dict(\n          description=\"The cells that are active and predicted\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        anomalyScore = dict(\n          description=\"\"\"The score for how 'anomalous' (i.e. rare) the current\n                        sequence is. Higher values are increasingly rare\"\"\",\n          dataType='Real32',\n          count=1,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n        lrnActiveStateT = dict(\n          description=\"\"\"Active cells during learn phase at time t.  This is\n                        used for anomaly classification.\"\"\",\n          dataType='Real32',\n          count=0,\n          regionLevel=True,\n          isDefaultOutput=False),\n\n      ),\n\n      parameters=dict(\n        breakPdb=dict(\n          description='Set to 1 to stop in the pdb debugger on the next compute',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n\n        breakKomodo=dict(\n          description='Set to 1 to stop in the Komodo debugger on the next compute',\n          dataType='UInt32',\n          count=1,\n          constraints='bool',\n          defaultValue=0,\n          accessMode='ReadWrite'),\n\n      ),\n      commands = {}\n    )\n\n    return spec",
    "doc": "Doesn't include the spatial, temporal and other parameters\n\n    :returns: (dict) the base Spec for TMRegion."
  },
  {
    "code": "def getSpec(cls):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n\n    The parameters collection is constructed based on the parameters specified\n    by the various components (spatialSpec, temporalSpec and otherSpec)\n    \"\"\"\n    spec = cls.getBaseSpec()\n    t, o = _getAdditionalSpecs(temporalImp=gDefaultTemporalImp)\n    spec['parameters'].update(t)\n    spec['parameters'].update(o)\n\n    return spec",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n\n    The parameters collection is constructed based on the parameters specified\n    by the various components (spatialSpec, temporalSpec and otherSpec)"
  },
  {
    "code": "def getParameter(self, parameterName, index=-1):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameter`.\n\n    Get the value of a parameter. Most parameters are handled automatically by\n    :class:`~nupic.bindings.regions.PyRegion.PyRegion`'s parameter get mechanism. The \n    ones that need special treatment are explicitly handled here.\n    \"\"\"\n    if parameterName in self._temporalArgNames:\n      return getattr(self._tfdr, parameterName)\n    else:\n      return PyRegion.getParameter(self, parameterName, index)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameter`.\n\n    Get the value of a parameter. Most parameters are handled automatically by\n    :class:`~nupic.bindings.regions.PyRegion.PyRegion`'s parameter get mechanism. The \n    ones that need special treatment are explicitly handled here."
  },
  {
    "code": "def setParameter(self, parameterName, index, parameterValue):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.setParameter`.\n    \"\"\"\n    if parameterName in self._temporalArgNames:\n      setattr(self._tfdr, parameterName, parameterValue)\n\n    elif parameterName == \"logPathOutput\":\n      self.logPathOutput = parameterValue\n      # Close any existing log file\n      if self._fpLogTPOutput is not None:\n        self._fpLogTPOutput.close()\n        self._fpLogTPOutput = None\n\n      # Open a new log file if requested\n      if parameterValue:\n        self._fpLogTPOutput = open(self.logPathOutput, 'w')\n\n    elif hasattr(self, parameterName):\n      setattr(self, parameterName, parameterValue)\n\n    else:\n      raise Exception('Unknown parameter: ' + parameterName)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.setParameter`."
  },
  {
    "code": "def finishLearning(self):\n    \"\"\"\n    Perform an internal optimization step that speeds up inference if we know\n    learning will not be performed anymore. This call may, for example, remove\n    all potential inputs to each column.\n    \"\"\"\n    if self._tfdr is None:\n      raise RuntimeError(\"Temporal memory has not been initialized\")\n\n    if hasattr(self._tfdr, 'finishLearning'):\n      self.resetSequenceStates()\n      self._tfdr.finishLearning()",
    "doc": "Perform an internal optimization step that speeds up inference if we know\n    learning will not be performed anymore. This call may, for example, remove\n    all potential inputs to each column."
  },
  {
    "code": "def writeToProto(self, proto):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`.\n\n    Write state to proto object.\n\n    :param proto: TMRegionProto capnproto object\n    \"\"\"\n    proto.temporalImp = self.temporalImp\n    proto.columnCount = self.columnCount\n    proto.inputWidth = self.inputWidth\n    proto.cellsPerColumn = self.cellsPerColumn\n    proto.learningMode = self.learningMode\n    proto.inferenceMode = self.inferenceMode\n    proto.anomalyMode = self.anomalyMode\n    proto.topDownMode = self.topDownMode\n    proto.computePredictedActiveCellIndices = (\n      self.computePredictedActiveCellIndices)\n    proto.orColumnOutputs = self.orColumnOutputs\n\n    if self.temporalImp == \"py\":\n      tmProto = proto.init(\"backtrackingTM\")\n    elif self.temporalImp == \"cpp\":\n      tmProto = proto.init(\"backtrackingTMCpp\")\n    elif self.temporalImp == \"tm_py\":\n      tmProto = proto.init(\"temporalMemory\")\n    elif self.temporalImp == \"tm_cpp\":\n      tmProto = proto.init(\"temporalMemory\")\n    else:\n      raise TypeError(\n          \"Unsupported temporalImp for capnp serialization: {}\".format(\n              self.temporalImp))\n\n    self._tfdr.write(tmProto)",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`.\n\n    Write state to proto object.\n\n    :param proto: TMRegionProto capnproto object"
  },
  {
    "code": "def readFromProto(cls, proto):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.\n\n    Read state from proto object.\n\n    :param proto: TMRegionProto capnproto object\n    \"\"\"\n    instance = cls(proto.columnCount, proto.inputWidth, proto.cellsPerColumn)\n\n    instance.temporalImp = proto.temporalImp\n    instance.learningMode = proto.learningMode\n    instance.inferenceMode = proto.inferenceMode\n    instance.anomalyMode = proto.anomalyMode\n    instance.topDownMode = proto.topDownMode\n    instance.computePredictedActiveCellIndices = (\n      proto.computePredictedActiveCellIndices)\n    instance.orColumnOutputs = proto.orColumnOutputs\n\n    if instance.temporalImp == \"py\":\n      tmProto = proto.backtrackingTM\n    elif instance.temporalImp == \"cpp\":\n      tmProto = proto.backtrackingTMCpp\n    elif instance.temporalImp == \"tm_py\":\n      tmProto = proto.temporalMemory\n    elif instance.temporalImp == \"tm_cpp\":\n      tmProto = proto.temporalMemory\n    else:\n      raise TypeError(\n          \"Unsupported temporalImp for capnp serialization: {}\".format(\n              instance.temporalImp))\n\n    instance._tfdr = _getTPClass(proto.temporalImp).read(tmProto)\n\n    return instance",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.\n\n    Read state from proto object.\n\n    :param proto: TMRegionProto capnproto object"
  },
  {
    "code": "def getOutputElementCount(self, name):\n    \"\"\"\n    Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getOutputElementCount`.\n    \"\"\"\n    if name == 'bottomUpOut':\n      return self.outputWidth\n    elif name == 'topDownOut':\n      return self.columnCount\n    elif name == 'lrnActiveStateT':\n      return self.outputWidth\n    elif name == \"activeCells\":\n      return self.outputWidth\n    elif name == \"predictedActiveCells\":\n      return self.outputWidth\n    else:\n      raise Exception(\"Invalid output name specified\")",
    "doc": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getOutputElementCount`."
  },
  {
    "code": "def computeRawAnomalyScore(activeColumns, prevPredictedColumns):\n  \"\"\"Computes the raw anomaly score.\n\n  The raw anomaly score is the fraction of active columns not predicted.\n\n  :param activeColumns: array of active column indices\n  :param prevPredictedColumns: array of columns indices predicted in prev step\n  :returns: anomaly score 0..1 (float)\n  \"\"\"\n  nActiveColumns = len(activeColumns)\n  if nActiveColumns > 0:\n    # Test whether each element of a 1-D array is also present in a second\n    # array. Sum to get the total # of columns that are active and were\n    # predicted.\n    score = numpy.in1d(activeColumns, prevPredictedColumns).sum()\n    # Get the percent of active columns that were NOT predicted, that is\n    # our anomaly score.\n    score = (nActiveColumns - score) / float(nActiveColumns)\n  else:\n    # There are no active columns.\n    score = 0.0\n\n  return score",
    "doc": "Computes the raw anomaly score.\n\n  The raw anomaly score is the fraction of active columns not predicted.\n\n  :param activeColumns: array of active column indices\n  :param prevPredictedColumns: array of columns indices predicted in prev step\n  :returns: anomaly score 0..1 (float)"
  },
  {
    "code": "def compute(self, activeColumns, predictedColumns,\n              inputValue=None, timestamp=None):\n    \"\"\"Compute the anomaly score as the percent of active columns not predicted.\n\n    :param activeColumns: array of active column indices\n    :param predictedColumns: array of columns indices predicted in this step\n                             (used for anomaly in step T+1)\n    :param inputValue: (optional) value of current input to encoders\n                                  (eg \"cat\" for category encoder)\n                                  (used in anomaly-likelihood)\n    :param timestamp: (optional) date timestamp when the sample occured\n                                 (used in anomaly-likelihood)\n    :returns: the computed anomaly score; float 0..1\n    \"\"\"\n    # Start by computing the raw anomaly score.\n    anomalyScore = computeRawAnomalyScore(activeColumns, predictedColumns)\n\n    # Compute final anomaly based on selected mode.\n    if self._mode == Anomaly.MODE_PURE:\n      score = anomalyScore\n    elif self._mode == Anomaly.MODE_LIKELIHOOD:\n      if inputValue is None:\n        raise ValueError(\"Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' \"\n                 \"requires 'inputValue' as parameter to compute() method. \")\n\n      probability = self._likelihood.anomalyProbability(\n          inputValue, anomalyScore, timestamp)\n      # low likelihood -> hi anomaly\n      score = 1 - probability\n    elif self._mode == Anomaly.MODE_WEIGHTED:\n      probability = self._likelihood.anomalyProbability(\n          inputValue, anomalyScore, timestamp)\n      score = anomalyScore * (1 - probability)\n\n    # Last, do moving-average if windowSize was specified.\n    if self._movingAverage is not None:\n      score = self._movingAverage.next(score)\n\n    # apply binary discretization if required\n    if self._binaryThreshold is not None:\n      if score >= self._binaryThreshold:\n        score = 1.0\n      else:\n        score = 0.0\n\n    return score",
    "doc": "Compute the anomaly score as the percent of active columns not predicted.\n\n    :param activeColumns: array of active column indices\n    :param predictedColumns: array of columns indices predicted in this step\n                             (used for anomaly in step T+1)\n    :param inputValue: (optional) value of current input to encoders\n                                  (eg \"cat\" for category encoder)\n                                  (used in anomaly-likelihood)\n    :param timestamp: (optional) date timestamp when the sample occured\n                                 (used in anomaly-likelihood)\n    :returns: the computed anomaly score; float 0..1"
  },
  {
    "code": "def addGraph(self, data, position=111, xlabel=None, ylabel=None):\n    \"\"\" Adds a graph to the plot's figure.\n\n    @param data See matplotlib.Axes.plot documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    \"\"\"\n    ax = self._addBase(position, xlabel=xlabel, ylabel=ylabel)\n    ax.plot(data)\n    plt.draw()",
    "doc": "Adds a graph to the plot's figure.\n\n    @param data See matplotlib.Axes.plot documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis"
  },
  {
    "code": "def addHistogram(self, data, position=111, xlabel=None, ylabel=None,\n                   bins=None):\n    \"\"\" Adds a histogram to the plot's figure.\n\n    @param data See matplotlib.Axes.hist documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    \"\"\"\n    ax = self._addBase(position, xlabel=xlabel, ylabel=ylabel)\n    ax.hist(data, bins=bins, color=\"green\", alpha=0.8)\n    plt.draw()",
    "doc": "Adds a histogram to the plot's figure.\n\n    @param data See matplotlib.Axes.hist documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis"
  },
  {
    "code": "def add2DArray(self, data, position=111, xlabel=None, ylabel=None, cmap=None,\n                 aspect=\"auto\", interpolation=\"nearest\", name=None):\n    \"\"\" Adds an image to the plot's figure.\n\n    @param data a 2D array. See matplotlib.Axes.imshow documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    @param cmap color map used in the rendering\n    @param aspect how aspect ratio is handled during resize\n    @param interpolation interpolation method\n    \"\"\"\n    if cmap is None:\n      # The default colormodel is an ugly blue-red model.\n      cmap = cm.Greys\n\n    ax = self._addBase(position, xlabel=xlabel, ylabel=ylabel)\n    ax.imshow(data, cmap=cmap, aspect=aspect, interpolation=interpolation)\n\n    if self._show:\n      plt.draw()\n\n    if name is not None:\n      if not os.path.exists(\"log\"):\n        os.mkdir(\"log\")\n      plt.savefig(\"log/{name}.png\".format(name=name), bbox_inches=\"tight\",\n                  figsize=(8, 6), dpi=400)",
    "doc": "Adds an image to the plot's figure.\n\n    @param data a 2D array. See matplotlib.Axes.imshow documentation.\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    @param cmap color map used in the rendering\n    @param aspect how aspect ratio is handled during resize\n    @param interpolation interpolation method"
  },
  {
    "code": "def _addBase(self, position, xlabel=None, ylabel=None):\n    \"\"\" Adds a subplot to the plot's figure at specified position.\n\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    @returns (matplotlib.Axes) Axes instance\n    \"\"\"\n    ax = self._fig.add_subplot(position)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return ax",
    "doc": "Adds a subplot to the plot's figure at specified position.\n\n    @param position A 3-digit number. The first two digits define a 2D grid\n            where subplots may be added. The final digit specifies the nth grid\n            location for the added subplot\n    @param xlabel text to be displayed on the x-axis\n    @param ylabel text to be displayed on the y-axis\n    @returns (matplotlib.Axes) Axes instance"
  },
  {
    "code": "def _generateOverlapping(filename=\"overlap.csv\", numSequences=2, elementsPerSeq=3, \n                    numRepeats=10, hub=[0,1], hubOffset=1, resets=False):\n  \n  \"\"\" Generate a temporal dataset containing sequences that overlap one or more\n  elements with other sequences. \n  \n  Parameters:\n  ----------------------------------------------------\n  filename:       name of the file to produce, including extension. It will\n                  be created in a 'datasets' sub-directory within the \n                  directory containing this script. \n  numSequences:   how many sequences to generate\n  elementsPerSeq: length of each sequence\n  numRepeats:     how many times to repeat each sequence in the output \n  hub:            sub-sequence to place within each other sequence \n  hubOffset:      where, within each sequence, to place the hub\n  resets:         if True, turn on reset at start of each sequence\n  \"\"\"\n  \n  # Check for conflicts in arguments\n  assert (hubOffset + len(hub) <= elementsPerSeq)\n  \n  # Create the output file\n  scriptDir = os.path.dirname(__file__)\n  pathname = os.path.join(scriptDir, 'datasets', filename)\n  print \"Creating %s...\" % (pathname)\n  fields = [('reset', 'int', 'R'), \n            ('field1', 'string', ''),  \n            ('field2', 'float', '')]  \n  outFile = FileRecordStream(pathname, write=True, fields=fields)\n  \n\n  # Create the sequences with the hub in the middle\n  sequences = []\n  nextElemIdx = max(hub)+1\n  \n  for _ in range(numSequences):\n    seq = []\n    for j in range(hubOffset):\n      seq.append(nextElemIdx)\n      nextElemIdx += 1\n    for j in hub:\n      seq.append(j)\n    j = hubOffset + len(hub)\n    while j < elementsPerSeq:\n      seq.append(nextElemIdx)\n      nextElemIdx += 1\n      j += 1\n    sequences.append(seq)\n  \n  # Write out the sequences in random order\n  seqIdxs = []\n  for _ in range(numRepeats):\n    seqIdxs += range(numSequences)\n  random.shuffle(seqIdxs)\n  \n  for seqIdx in seqIdxs:\n    reset = int(resets)\n    seq = sequences[seqIdx]\n    for (x) in seq:\n      outFile.appendRecord([reset, str(x), x])\n      reset = 0\n\n  outFile.close()",
    "doc": "Generate a temporal dataset containing sequences that overlap one or more\n  elements with other sequences. \n  \n  Parameters:\n  ----------------------------------------------------\n  filename:       name of the file to produce, including extension. It will\n                  be created in a 'datasets' sub-directory within the \n                  directory containing this script. \n  numSequences:   how many sequences to generate\n  elementsPerSeq: length of each sequence\n  numRepeats:     how many times to repeat each sequence in the output \n  hub:            sub-sequence to place within each other sequence \n  hubOffset:      where, within each sequence, to place the hub\n  resets:         if True, turn on reset at start of each sequence"
  },
  {
    "code": "def _generateFirstOrder0():\n  \"\"\" Generate the initial, first order, and second order transition\n  probabilities for 'probability0'. For this model, we generate the following\n  set of sequences:\n  \n    .1   .75\n  0----1-----2\n   \\    \\   \n    \\    \\  .25\n     \\    \\-----3\n      \\\n       \\ .9     .5 \n        \\--- 4--------- 2\n              \\\n               \\   .5\n                \\---------3   \n          \n  \n  \n  \n  Parameters:\n  ----------------------------------------------------------------------\n  retval: (initProb, firstOrder, secondOrder, seqLen)\n            initProb:     Initial probability for each category. This is a vector\n                            of length len(categoryList).\n            firstOrder:   A dictionary of the 1st order probabilities. The key\n                            is the 1st element of the sequence, the value is\n                            the probability of each 2nd element given the first. \n            secondOrder:  A dictionary of the 2nd order probabilities. The key\n                            is the first 2 elements of the sequence, the value is\n                            the probability of each possible 3rd element given the \n                            first two. \n            seqLen:       Desired length of each sequence. The 1st element will\n                          be generated using the initProb, the 2nd element by the\n                          firstOrder table, and the 3rd and all successive \n                          elements by the secondOrder table. \n            categoryList:  list of category names to use\n\n\n  Here is an example of some return values when there are 3 categories\n  initProb:         [0.7, 0.2, 0.1]\n  \n  firstOrder:       {'[0]': [0.3, 0.3, 0.4],\n                     '[1]': [0.3, 0.3, 0.4],\n                     '[2]': [0.3, 0.3, 0.4]}\n                     \n  secondOrder:      {'[0,0]': [0.3, 0.3, 0.4],\n                     '[0,1]': [0.3, 0.3, 0.4],\n                     '[0,2]': [0.3, 0.3, 0.4],\n                     '[1,0]': [0.3, 0.3, 0.4],\n                     '[1,1]': [0.3, 0.3, 0.4],\n                     '[1,2]': [0.3, 0.3, 0.4],\n                     '[2,0]': [0.3, 0.3, 0.4],\n                     '[2,1]': [0.3, 0.3, 0.4],\n                     '[2,2]': [0.3, 0.3, 0.4]}\n  \"\"\"\n\n\n  # --------------------------------------------------------------------\n  # Initial probabilities, 'a' and 'e' equally likely\n  numCategories = 5\n  initProb = numpy.zeros(numCategories)\n  initProb[0] = 1.0\n  \n\n  # --------------------------------------------------------------------\n  # 1st order transitions\n  firstOrder = dict()\n  firstOrder['0'] = numpy.array([0, 0.1, 0, 0, 0.9])\n  firstOrder['1'] = numpy.array([0, 0, 0.75, 0.25, 0])\n  firstOrder['2'] = numpy.array([1.0, 0, 0, 0, 0])\n  firstOrder['3'] = numpy.array([1.0, 0, 0, 0, 0])\n  firstOrder['4'] = numpy.array([0, 0, 0.5, 0.5, 0])\n   \n  # --------------------------------------------------------------------\n  # 2nd order transitions don't apply\n  secondOrder = None\n  \n  # Generate the category list\n  categoryList = ['%d' % x for x in range(5)]\n  return (initProb, firstOrder, secondOrder, 3, categoryList)",
    "doc": "Generate the initial, first order, and second order transition\n  probabilities for 'probability0'. For this model, we generate the following\n  set of sequences:\n  \n    .1   .75\n  0----1-----2\n   \\    \\   \n    \\    \\  .25\n     \\    \\-----3\n      \\\n       \\ .9     .5 \n        \\--- 4--------- 2\n              \\\n               \\   .5\n                \\---------3   \n          \n  \n  \n  \n  Parameters:\n  ----------------------------------------------------------------------\n  retval: (initProb, firstOrder, secondOrder, seqLen)\n            initProb:     Initial probability for each category. This is a vector\n                            of length len(categoryList).\n            firstOrder:   A dictionary of the 1st order probabilities. The key\n                            is the 1st element of the sequence, the value is\n                            the probability of each 2nd element given the first. \n            secondOrder:  A dictionary of the 2nd order probabilities. The key\n                            is the first 2 elements of the sequence, the value is\n                            the probability of each possible 3rd element given the \n                            first two. \n            seqLen:       Desired length of each sequence. The 1st element will\n                          be generated using the initProb, the 2nd element by the\n                          firstOrder table, and the 3rd and all successive \n                          elements by the secondOrder table. \n            categoryList:  list of category names to use\n\n\n  Here is an example of some return values when there are 3 categories\n  initProb:         [0.7, 0.2, 0.1]\n  \n  firstOrder:       {'[0]': [0.3, 0.3, 0.4],\n                     '[1]': [0.3, 0.3, 0.4],\n                     '[2]': [0.3, 0.3, 0.4]}\n                     \n  secondOrder:      {'[0,0]': [0.3, 0.3, 0.4],\n                     '[0,1]': [0.3, 0.3, 0.4],\n                     '[0,2]': [0.3, 0.3, 0.4],\n                     '[1,0]': [0.3, 0.3, 0.4],\n                     '[1,1]': [0.3, 0.3, 0.4],\n                     '[1,2]': [0.3, 0.3, 0.4],\n                     '[2,0]': [0.3, 0.3, 0.4],\n                     '[2,1]': [0.3, 0.3, 0.4],\n                     '[2,2]': [0.3, 0.3, 0.4]}"
  },
  {
    "code": "def _generateFileFromProb(filename, numRecords, categoryList, initProb, \n      firstOrderProb, secondOrderProb, seqLen, numNoise=0, resetsEvery=None):\n  \"\"\" Generate a set of records reflecting a set of probabilities.\n  \n  Parameters:\n  ----------------------------------------------------------------\n  filename:         name of .csv file to generate\n  numRecords:       number of records to generate\n  categoryList:     list of category names\n  initProb:         Initial probability for each category. This is a vector\n                      of length len(categoryList).\n  firstOrderProb:   A dictionary of the 1st order probabilities. The key\n                      is the 1st element of the sequence, the value is\n                      the probability of each 2nd element given the first. \n  secondOrderProb:  A dictionary of the 2nd order probabilities. The key\n                      is the first 2 elements of the sequence, the value is\n                      the probability of each possible 3rd element given the \n                      first two. If this is None, then the sequences will be\n                      first order only. \n  seqLen:           Desired length of each sequence. The 1st element will\n                      be generated using the initProb, the 2nd element by the\n                      firstOrder table, and the 3rd and all successive \n                      elements by the secondOrder table. None means infinite\n                      length. \n  numNoise:         Number of noise elements to place between each \n                      sequence. The noise elements are evenly distributed from \n                      all categories. \n  resetsEvery:      If not None, generate a reset every N records\n                      \n                      \n  Here is an example of some parameters:\n  \n  categoryList:     ['cat1', 'cat2', 'cat3']\n  \n  initProb:         [0.7, 0.2, 0.1]\n  \n  firstOrderProb:   {'[0]': [0.3, 0.3, 0.4],\n                     '[1]': [0.3, 0.3, 0.4],\n                     '[2]': [0.3, 0.3, 0.4]}\n                     \n  secondOrderProb:  {'[0,0]': [0.3, 0.3, 0.4],\n                     '[0,1]': [0.3, 0.3, 0.4],\n                     '[0,2]': [0.3, 0.3, 0.4],\n                     '[1,0]': [0.3, 0.3, 0.4],\n                     '[1,1]': [0.3, 0.3, 0.4],\n                     '[1,2]': [0.3, 0.3, 0.4],\n                     '[2,0]': [0.3, 0.3, 0.4],\n                     '[2,1]': [0.3, 0.3, 0.4],\n                     '[2,2]': [0.3, 0.3, 0.4]}\n                   \n  \"\"\"\n  \n  # Create the file\n  print \"Creating %s...\" % (filename)\n  fields = [('reset', 'int', 'R'), \n            ('field1', 'string', ''),\n            ('field2', 'float', '')]\n\n  scriptDir = os.path.dirname(__file__)\n  pathname = os.path.join(scriptDir, 'datasets', filename)\n  outFile = FileRecordStream(pathname, write=True, fields=fields)\n  \n  # --------------------------------------------------------------------\n  # Convert the probabilitie tables into cumulative probabilities\n  initCumProb = initProb.cumsum()\n  \n  firstOrderCumProb = dict()\n  for (key,value) in firstOrderProb.iteritems():\n    firstOrderCumProb[key] = value.cumsum()\n    \n  if secondOrderProb is not None:\n    secondOrderCumProb = dict()\n    for (key,value) in secondOrderProb.iteritems():\n      secondOrderCumProb[key] = value.cumsum()\n  else:\n    secondOrderCumProb = None\n    \n\n  # --------------------------------------------------------------------\n  # Write out the sequences\n  elementsInSeq = []\n  numElementsSinceReset = 0\n  maxCatIdx = len(categoryList) - 1\n  for _ in xrange(numRecords):\n\n    # Generate a reset?\n    if numElementsSinceReset == 0:\n      reset = 1\n    else:\n      reset = 0\n      \n    # Pick the next element, based on how are we are into the 2nd order\n    #   sequence. \n    rand = numpy.random.rand()\n    \n    # Generate 1st order sequences\n    if secondOrderCumProb is None:\n      if len(elementsInSeq) == 0:\n        catIdx = numpy.searchsorted(initCumProb, rand)\n      elif len(elementsInSeq) >= 1 and \\\n                    (seqLen is None or len(elementsInSeq) < seqLen-numNoise):\n        catIdx = numpy.searchsorted(firstOrderCumProb[str(elementsInSeq[-1])], \n                                    rand)\n      else:   # random \"noise\"\n        catIdx = numpy.random.randint(len(categoryList))\n\n    # Generate 2nd order sequences\n    else:\n      if len(elementsInSeq) == 0:\n        catIdx = numpy.searchsorted(initCumProb, rand)\n      elif len(elementsInSeq) == 1:\n        catIdx = numpy.searchsorted(firstOrderCumProb[str(elementsInSeq)], rand)\n      elif (len(elementsInSeq) >=2) and \\\n                    (seqLen is None or len(elementsInSeq) < seqLen-numNoise):\n        catIdx = numpy.searchsorted(secondOrderCumProb[str(elementsInSeq[-2:])], rand)\n      else:   # random \"noise\"\n        catIdx = numpy.random.randint(len(categoryList))\n      \n    # -------------------------------------------------------------------\n    # Write out the record\n    catIdx = min(maxCatIdx, catIdx)\n    outFile.appendRecord([reset, categoryList[catIdx], catIdx])    \n    #print categoryList[catIdx]\n    \n    # ------------------------------------------------------------\n    # Increment counters\n    elementsInSeq.append(catIdx)\n    numElementsSinceReset += 1\n    \n    # Generate another reset?\n    if resetsEvery is not None and numElementsSinceReset == resetsEvery:\n      numElementsSinceReset = 0\n      elementsInSeq = []\n    \n    # Start another 2nd order sequence?\n    if seqLen is not None and (len(elementsInSeq) == seqLen+numNoise):\n      elementsInSeq = []\n      \n  \n  outFile.close()",
    "doc": "Generate a set of records reflecting a set of probabilities.\n  \n  Parameters:\n  ----------------------------------------------------------------\n  filename:         name of .csv file to generate\n  numRecords:       number of records to generate\n  categoryList:     list of category names\n  initProb:         Initial probability for each category. This is a vector\n                      of length len(categoryList).\n  firstOrderProb:   A dictionary of the 1st order probabilities. The key\n                      is the 1st element of the sequence, the value is\n                      the probability of each 2nd element given the first. \n  secondOrderProb:  A dictionary of the 2nd order probabilities. The key\n                      is the first 2 elements of the sequence, the value is\n                      the probability of each possible 3rd element given the \n                      first two. If this is None, then the sequences will be\n                      first order only. \n  seqLen:           Desired length of each sequence. The 1st element will\n                      be generated using the initProb, the 2nd element by the\n                      firstOrder table, and the 3rd and all successive \n                      elements by the secondOrder table. None means infinite\n                      length. \n  numNoise:         Number of noise elements to place between each \n                      sequence. The noise elements are evenly distributed from \n                      all categories. \n  resetsEvery:      If not None, generate a reset every N records\n                      \n                      \n  Here is an example of some parameters:\n  \n  categoryList:     ['cat1', 'cat2', 'cat3']\n  \n  initProb:         [0.7, 0.2, 0.1]\n  \n  firstOrderProb:   {'[0]': [0.3, 0.3, 0.4],\n                     '[1]': [0.3, 0.3, 0.4],\n                     '[2]': [0.3, 0.3, 0.4]}\n                     \n  secondOrderProb:  {'[0,0]': [0.3, 0.3, 0.4],\n                     '[0,1]': [0.3, 0.3, 0.4],\n                     '[0,2]': [0.3, 0.3, 0.4],\n                     '[1,0]': [0.3, 0.3, 0.4],\n                     '[1,1]': [0.3, 0.3, 0.4],\n                     '[1,2]': [0.3, 0.3, 0.4],\n                     '[2,0]': [0.3, 0.3, 0.4],\n                     '[2,1]': [0.3, 0.3, 0.4],\n                     '[2,2]': [0.3, 0.3, 0.4]}"
  },
  {
    "code": "def getVersion():\n  \"\"\"\n  Get version from local file.\n  \"\"\"\n  with open(os.path.join(REPO_DIR, \"VERSION\"), \"r\") as versionFile:\n    return versionFile.read().strip()",
    "doc": "Get version from local file."
  },
  {
    "code": "def nupicBindingsPrereleaseInstalled():\n  \"\"\"\n  Make an attempt to determine if a pre-release version of nupic.bindings is\n  installed already.\n\n  @return: boolean\n  \"\"\"\n  try:\n    nupicDistribution = pkg_resources.get_distribution(\"nupic.bindings\")\n    if pkg_resources.parse_version(nupicDistribution.version).is_prerelease:\n      # A pre-release dev version of nupic.bindings is installed.\n      return True\n  except pkg_resources.DistributionNotFound:\n    pass  # Silently ignore.  The absence of nupic.bindings will be handled by\n    # setuptools by default\n\n  return False",
    "doc": "Make an attempt to determine if a pre-release version of nupic.bindings is\n  installed already.\n\n  @return: boolean"
  },
  {
    "code": "def findRequirements():\n  \"\"\"\n  Read the requirements.txt file and parse into requirements for setup's\n  install_requirements option.\n  \"\"\"\n  requirementsPath = os.path.join(REPO_DIR, \"requirements.txt\")\n  requirements = parse_file(requirementsPath)\n\n  if nupicBindingsPrereleaseInstalled():\n    # User has a pre-release version of nupic.bindings installed, which is only\n    # possible if the user installed and built nupic.bindings from source and\n    # it is up to the user to decide when to update nupic.bindings.  We'll\n    # quietly remove the entry in requirements.txt so as to not conflate the\n    # two.\n    requirements = [req for req in requirements if \"nupic.bindings\" not in req]\n\n  return requirements",
    "doc": "Read the requirements.txt file and parse into requirements for setup's\n  install_requirements option."
  },
  {
    "code": "def _handleDescriptionOption(cmdArgStr, outDir, usageStr, hsVersion,\n                             claDescriptionTemplateFile):\n  \"\"\"\n  Parses and validates the --description option args and executes the\n  request\n\n  Parameters:\n  -----------------------------------------------------------------------\n  cmdArgStr:  JSON string compatible with _gExperimentDescriptionSchema\n  outDir:     where to place generated experiment files\n  usageStr:   program usage string\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n  retval:     nothing\n\n\n  \"\"\"\n  # convert --description arg from JSON string to dict\n  try:\n    args = json.loads(cmdArgStr)\n  except Exception, e:\n    raise _InvalidCommandArgException(\n      _makeUsageErrorStr(\n        (\"JSON arg parsing failed for --description: %s\\n\" + \\\n         \"ARG=<%s>\") % (str(e), cmdArgStr), usageStr))\n\n  #print \"PARSED JSON ARGS=\\n%s\" % (json.dumps(args, indent=4))\n\n  filesDescription = _generateExperiment(args, outDir, hsVersion=hsVersion,\n                    claDescriptionTemplateFile = claDescriptionTemplateFile)\n\n  pprint.pprint(filesDescription)\n\n  return",
    "doc": "Parses and validates the --description option args and executes the\n  request\n\n  Parameters:\n  -----------------------------------------------------------------------\n  cmdArgStr:  JSON string compatible with _gExperimentDescriptionSchema\n  outDir:     where to place generated experiment files\n  usageStr:   program usage string\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n  retval:     nothing"
  },
  {
    "code": "def _handleDescriptionFromFileOption(filename, outDir, usageStr, hsVersion,\n                             claDescriptionTemplateFile):\n  \"\"\"\n  Parses and validates the --descriptionFromFile option and executes the\n  request\n\n  Parameters:\n  -----------------------------------------------------------------------\n  filename:   File from which we'll extract description JSON\n  outDir:     where to place generated experiment files\n  usageStr:   program usage string\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n  retval:     nothing\n  \"\"\"\n\n  try:\n    fileHandle = open(filename, 'r')\n    JSONStringFromFile = fileHandle.read().splitlines()\n    JSONStringFromFile = ''.join(JSONStringFromFile)\n\n  except Exception, e:\n    raise _InvalidCommandArgException(\n      _makeUsageErrorStr(\n        (\"File open failed for --descriptionFromFile: %s\\n\" + \\\n         \"ARG=<%s>\") % (str(e), filename), usageStr))\n\n  _handleDescriptionOption(JSONStringFromFile, outDir, usageStr,\n        hsVersion=hsVersion,\n        claDescriptionTemplateFile = claDescriptionTemplateFile)\n  return",
    "doc": "Parses and validates the --descriptionFromFile option and executes the\n  request\n\n  Parameters:\n  -----------------------------------------------------------------------\n  filename:   File from which we'll extract description JSON\n  outDir:     where to place generated experiment files\n  usageStr:   program usage string\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n  retval:     nothing"
  },
  {
    "code": "def _isInt(x, precision = 0.0001):\n  \"\"\"\n  Return (isInt, intValue) for a given floating point number.\n\n  Parameters:\n  ----------------------------------------------------------------------\n  x:  floating point number to evaluate\n  precision: desired precision\n  retval:   (isInt, intValue)\n            isInt: True if x is close enough to an integer value\n            intValue: x as an integer\n  \"\"\"\n\n  xInt = int(round(x))\n  return (abs(x - xInt) < precision * x, xInt)",
    "doc": "Return (isInt, intValue) for a given floating point number.\n\n  Parameters:\n  ----------------------------------------------------------------------\n  x:  floating point number to evaluate\n  precision: desired precision\n  retval:   (isInt, intValue)\n            isInt: True if x is close enough to an integer value\n            intValue: x as an integer"
  },
  {
    "code": "def _indentLines(str, indentLevels = 1, indentFirstLine=True):\n  \"\"\" Indent all lines in the given string\n\n  str:          input string\n  indentLevels: number of levels of indentation to apply\n  indentFirstLine: if False, the 1st line will not be indented\n\n  Returns:      The result string with all lines indented\n  \"\"\"\n\n  indent = _ONE_INDENT * indentLevels\n\n  lines = str.splitlines(True)\n  result = ''\n\n  if len(lines) > 0 and not indentFirstLine:\n    first = 1\n    result += lines[0]\n  else:\n    first = 0\n\n  for line in lines[first:]:\n    result += indent + line\n\n  return result",
    "doc": "Indent all lines in the given string\n\n  str:          input string\n  indentLevels: number of levels of indentation to apply\n  indentFirstLine: if False, the 1st line will not be indented\n\n  Returns:      The result string with all lines indented"
  },
  {
    "code": "def _generateMetricSpecString(inferenceElement, metric,\n                              params=None, field=None,\n                              returnLabel=False):\n  \"\"\" Generates the string representation of a MetricSpec object, and returns\n  the metric key associated with the metric.\n\n\n  Parameters:\n  -----------------------------------------------------------------------\n  inferenceElement:\n    An InferenceElement value that indicates which part of the inference this\n    metric is computed on\n\n  metric:\n    The type of the metric being computed (e.g. aae, avg_error)\n\n  params:\n    A dictionary of parameters for the metric. The keys are the parameter names\n    and the values should be the parameter values (e.g. window=200)\n\n  field:\n    The name of the field for which this metric is being computed\n\n  returnLabel:\n    If True, returns the label of the MetricSpec that was generated\n  \"\"\"\n\n  metricSpecArgs = dict(metric=metric,\n                        field=field,\n                        params=params,\n                        inferenceElement=inferenceElement)\n\n  metricSpecAsString = \"MetricSpec(%s)\" % \\\n    ', '.join(['%s=%r' % (item[0],item[1])\n              for item in metricSpecArgs.iteritems()])\n\n  if not returnLabel:\n    return metricSpecAsString\n\n  spec = MetricSpec(**metricSpecArgs)\n  metricLabel = spec.getLabel()\n  return metricSpecAsString, metricLabel",
    "doc": "Generates the string representation of a MetricSpec object, and returns\n  the metric key associated with the metric.\n\n\n  Parameters:\n  -----------------------------------------------------------------------\n  inferenceElement:\n    An InferenceElement value that indicates which part of the inference this\n    metric is computed on\n\n  metric:\n    The type of the metric being computed (e.g. aae, avg_error)\n\n  params:\n    A dictionary of parameters for the metric. The keys are the parameter names\n    and the values should be the parameter values (e.g. window=200)\n\n  field:\n    The name of the field for which this metric is being computed\n\n  returnLabel:\n    If True, returns the label of the MetricSpec that was generated"
  },
  {
    "code": "def _generateFileFromTemplates(templateFileNames, outputFilePath,\n                              replacementDict):\n  \"\"\" Generates a file by applying token replacements to the given template\n  file\n\n  templateFileName:\n                  A list of template file names; these files are assumed to be in\n                  the same directory as the running experiment_generator.py script.\n                  ExpGenerator will perform the substitution and concanetate\n                  the files in the order they are specified\n\n  outputFilePath: Absolute path of the output file\n\n  replacementDict:\n                  A dictionary of token/replacement pairs\n  \"\"\"\n\n  # Find out where we're running from so we know where to find templates\n  installPath = os.path.dirname(__file__)\n  outputFile = open(outputFilePath, \"w\")\n  outputLines = []\n  inputLines = []\n\n  firstFile = True\n  for templateFileName in templateFileNames:\n    # Separate lines from each file by two blank lines.\n    if not firstFile:\n      inputLines.extend([os.linesep]*2)\n    firstFile = False\n\n    inputFilePath = os.path.join(installPath, templateFileName)\n    inputFile = open(inputFilePath)\n    inputLines.extend(inputFile.readlines())\n    inputFile.close()\n\n\n  print \"Writing \", len(inputLines), \"lines...\"\n\n  for line in inputLines:\n    tempLine = line\n\n    # Enumerate through each key in replacementDict and replace with value\n    for k, v in replacementDict.iteritems():\n      if v is None:\n        v = \"None\"\n      tempLine = re.sub(k, v, tempLine)\n    outputFile.write(tempLine)\n  outputFile.close()",
    "doc": "Generates a file by applying token replacements to the given template\n  file\n\n  templateFileName:\n                  A list of template file names; these files are assumed to be in\n                  the same directory as the running experiment_generator.py script.\n                  ExpGenerator will perform the substitution and concanetate\n                  the files in the order they are specified\n\n  outputFilePath: Absolute path of the output file\n\n  replacementDict:\n                  A dictionary of token/replacement pairs"
  },
  {
    "code": "def _generateEncoderChoicesV1(fieldInfo):\n  \"\"\" Return a list of possible encoder parameter combinations for the given\n  field and the default aggregation function to use. Each parameter combination\n  is a dict defining the parameters for the encoder. Here is an example\n  return value for the encoderChoicesList:\n\n   [\n     None,\n     {'fieldname':'timestamp',\n      'name': 'timestamp_timeOfDay',\n      'type':'DateEncoder'\n      'dayOfWeek': (7,1)\n      },\n     {'fieldname':'timestamp',\n      'name': 'timestamp_timeOfDay',\n      'type':'DateEncoder'\n      'dayOfWeek': (7,3)\n      },\n  ],\n\n  Parameters:\n  --------------------------------------------------\n  fieldInfo:      item from the 'includedFields' section of the\n                    description JSON object\n\n  retval:  (encoderChoicesList, aggFunction)\n             encoderChoicesList: a list of encoder choice lists for this field.\n               Most fields will generate just 1 encoder choice list.\n               DateTime fields can generate 2 or more encoder choice lists,\n                 one for dayOfWeek, one for timeOfDay, etc.\n             aggFunction: name of aggregation function to use for this\n                           field type\n\n  \"\"\"\n\n  width = 7\n  fieldName = fieldInfo['fieldName']\n  fieldType = fieldInfo['fieldType']\n  encoderChoicesList = []\n\n  # Scalar?\n  if fieldType in ['float', 'int']:\n    aggFunction = 'mean'\n    encoders = [None]\n    for n in (13, 50, 150, 500):\n      encoder = dict(type='ScalarSpaceEncoder', name=fieldName, fieldname=fieldName,\n                     n=n, w=width, clipInput=True,space=\"absolute\")\n      if 'minValue' in fieldInfo:\n        encoder['minval'] = fieldInfo['minValue']\n      if 'maxValue' in fieldInfo:\n        encoder['maxval'] = fieldInfo['maxValue']\n      encoders.append(encoder)\n    encoderChoicesList.append(encoders)\n\n  # String?\n  elif fieldType == 'string':\n    aggFunction = 'first'\n    encoders = [None]\n    encoder = dict(type='SDRCategoryEncoder', name=fieldName,\n                   fieldname=fieldName, n=100, w=width)\n    encoders.append(encoder)\n    encoderChoicesList.append(encoders)\n\n\n  # Datetime?\n  elif fieldType == 'datetime':\n    aggFunction = 'first'\n\n    # First, the time of day representation\n    encoders = [None]\n    for radius in (1, 8):\n      encoder = dict(type='DateEncoder', name='%s_timeOfDay' % (fieldName),\n                     fieldname=fieldName, timeOfDay=(width, radius))\n      encoders.append(encoder)\n    encoderChoicesList.append(encoders)\n\n    # Now, the day of week representation\n    encoders = [None]\n    for radius in (1, 3):\n      encoder = dict(type='DateEncoder', name='%s_dayOfWeek' % (fieldName),\n                     fieldname=fieldName, dayOfWeek=(width, radius))\n      encoders.append(encoder)\n    encoderChoicesList.append(encoders)\n\n  else:\n    raise RuntimeError(\"Unsupported field type '%s'\" % (fieldType))\n\n\n  # Return results\n  return (encoderChoicesList, aggFunction)",
    "doc": "Return a list of possible encoder parameter combinations for the given\n  field and the default aggregation function to use. Each parameter combination\n  is a dict defining the parameters for the encoder. Here is an example\n  return value for the encoderChoicesList:\n\n   [\n     None,\n     {'fieldname':'timestamp',\n      'name': 'timestamp_timeOfDay',\n      'type':'DateEncoder'\n      'dayOfWeek': (7,1)\n      },\n     {'fieldname':'timestamp',\n      'name': 'timestamp_timeOfDay',\n      'type':'DateEncoder'\n      'dayOfWeek': (7,3)\n      },\n  ],\n\n  Parameters:\n  --------------------------------------------------\n  fieldInfo:      item from the 'includedFields' section of the\n                    description JSON object\n\n  retval:  (encoderChoicesList, aggFunction)\n             encoderChoicesList: a list of encoder choice lists for this field.\n               Most fields will generate just 1 encoder choice list.\n               DateTime fields can generate 2 or more encoder choice lists,\n                 one for dayOfWeek, one for timeOfDay, etc.\n             aggFunction: name of aggregation function to use for this\n                           field type"
  },
  {
    "code": "def _generateEncoderStringsV1(includedFields):\n  \"\"\" Generate and return the following encoder related substitution variables:\n\n  encoderSpecsStr:\n    For the base description file, this string defines the default\n    encoding dicts for each encoder. For example:\n         '__gym_encoder' : {   'fieldname': 'gym',\n          'n': 13,\n          'name': 'gym',\n          'type': 'SDRCategoryEncoder',\n          'w': 7},\n        '__address_encoder' : {   'fieldname': 'address',\n          'n': 13,\n          'name': 'address',\n          'type': 'SDRCategoryEncoder',\n          'w': 7}\n\n  encoderSchemaStr:\n    For the base description file, this is a list containing a\n    DeferredDictLookup entry for each encoder. For example:\n        [DeferredDictLookup('__gym_encoder'),\n         DeferredDictLookup('__address_encoder'),\n         DeferredDictLookup('__timestamp_timeOfDay_encoder'),\n         DeferredDictLookup('__timestamp_dayOfWeek_encoder'),\n         DeferredDictLookup('__consumption_encoder')],\n\n  permEncoderChoicesStr:\n    For the permutations file, this defines the possible\n    encoder dicts for each encoder. For example:\n        '__timestamp_dayOfWeek_encoder': [\n                     None,\n                     {'fieldname':'timestamp',\n                      'name': 'timestamp_timeOfDay',\n                      'type':'DateEncoder'\n                      'dayOfWeek': (7,1)\n                      },\n                     {'fieldname':'timestamp',\n                      'name': 'timestamp_timeOfDay',\n                      'type':'DateEncoder'\n                      'dayOfWeek': (7,3)\n                      },\n                  ],\n\n        '__field_consumption_encoder': [\n                    None,\n                    {'fieldname':'consumption',\n                     'name': 'consumption',\n                     'type':'AdaptiveScalarEncoder',\n                     'n': 13,\n                     'w': 7,\n                      }\n                   ]\n\n\n\n  Parameters:\n  --------------------------------------------------\n  includedFields:  item from the 'includedFields' section of the\n                    description JSON object. This is a list of dicts, each\n                    dict defining the field name, type, and optional min\n                    and max values.\n\n  retval:  (encoderSpecsStr, encoderSchemaStr permEncoderChoicesStr)\n\n\n  \"\"\"\n\n  # ------------------------------------------------------------------------\n  # First accumulate the possible choices for each encoder\n  encoderChoicesList = []\n  for fieldInfo in includedFields:\n\n    fieldName = fieldInfo['fieldName']\n\n    # Get the list of encoder choices for this field\n    (choicesList, aggFunction) = _generateEncoderChoicesV1(fieldInfo)\n    encoderChoicesList.extend(choicesList)\n\n\n  # ------------------------------------------------------------------------\n  # Generate the string containing the encoder specs and encoder schema. See\n  #  the function comments for an example of the encoderSpecsStr and\n  #  encoderSchemaStr\n  #\n  encoderSpecsList = []\n  for encoderChoices in encoderChoicesList:\n    # Use the last choice as the default in the base file because the 1st is\n    # often None\n    encoder = encoderChoices[-1]\n\n    # Check for bad characters\n    for c in _ILLEGAL_FIELDNAME_CHARACTERS:\n      if encoder['name'].find(c) >= 0:\n        raise _ExpGeneratorException(\"Illegal character in field: %r (%r)\" % (\n          c, encoder['name']))\n\n    encoderSpecsList.append(\"%s: \\n%s%s\" % (\n        _quoteAndEscape(encoder['name']),\n        2*_ONE_INDENT,\n        pprint.pformat(encoder, indent=2*_INDENT_STEP)))\n\n  encoderSpecsStr = ',\\n  '.join(encoderSpecsList)\n\n\n  # ------------------------------------------------------------------------\n  # Generate the string containing the permutation encoder choices. See the\n  #  function comments above for an example of the permEncoderChoicesStr\n\n  permEncoderChoicesList = []\n  for encoderChoices in encoderChoicesList:\n    permEncoderChoicesList.append(\"%s: %s,\" % (\n        _quoteAndEscape(encoderChoices[-1]['name']),\n        pprint.pformat(encoderChoices, indent=2*_INDENT_STEP)))\n  permEncoderChoicesStr = '\\n'.join(permEncoderChoicesList)\n  permEncoderChoicesStr = _indentLines(permEncoderChoicesStr, 1,\n                                       indentFirstLine=False)\n\n  # Return results\n  return (encoderSpecsStr, permEncoderChoicesStr)",
    "doc": "Generate and return the following encoder related substitution variables:\n\n  encoderSpecsStr:\n    For the base description file, this string defines the default\n    encoding dicts for each encoder. For example:\n         '__gym_encoder' : {   'fieldname': 'gym',\n          'n': 13,\n          'name': 'gym',\n          'type': 'SDRCategoryEncoder',\n          'w': 7},\n        '__address_encoder' : {   'fieldname': 'address',\n          'n': 13,\n          'name': 'address',\n          'type': 'SDRCategoryEncoder',\n          'w': 7}\n\n  encoderSchemaStr:\n    For the base description file, this is a list containing a\n    DeferredDictLookup entry for each encoder. For example:\n        [DeferredDictLookup('__gym_encoder'),\n         DeferredDictLookup('__address_encoder'),\n         DeferredDictLookup('__timestamp_timeOfDay_encoder'),\n         DeferredDictLookup('__timestamp_dayOfWeek_encoder'),\n         DeferredDictLookup('__consumption_encoder')],\n\n  permEncoderChoicesStr:\n    For the permutations file, this defines the possible\n    encoder dicts for each encoder. For example:\n        '__timestamp_dayOfWeek_encoder': [\n                     None,\n                     {'fieldname':'timestamp',\n                      'name': 'timestamp_timeOfDay',\n                      'type':'DateEncoder'\n                      'dayOfWeek': (7,1)\n                      },\n                     {'fieldname':'timestamp',\n                      'name': 'timestamp_timeOfDay',\n                      'type':'DateEncoder'\n                      'dayOfWeek': (7,3)\n                      },\n                  ],\n\n        '__field_consumption_encoder': [\n                    None,\n                    {'fieldname':'consumption',\n                     'name': 'consumption',\n                     'type':'AdaptiveScalarEncoder',\n                     'n': 13,\n                     'w': 7,\n                      }\n                   ]\n\n\n\n  Parameters:\n  --------------------------------------------------\n  includedFields:  item from the 'includedFields' section of the\n                    description JSON object. This is a list of dicts, each\n                    dict defining the field name, type, and optional min\n                    and max values.\n\n  retval:  (encoderSpecsStr, encoderSchemaStr permEncoderChoicesStr)"
  },
  {
    "code": "def _generatePermEncoderStr(options, encoderDict):\n  \"\"\" Generate the string that defines the permutations to apply for a given\n  encoder.\n\n  Parameters:\n  -----------------------------------------------------------------------\n  options: experiment params\n  encoderDict: the encoder dict, which gets placed into the description.py\n\n\n  For example, if the encoderDict contains:\n      'consumption':     {\n                'clipInput': True,\n                'fieldname': u'consumption',\n                'n': 100,\n                'name': u'consumption',\n                'type': 'AdaptiveScalarEncoder',\n                'w': 21},\n\n  The return string will contain:\n    \"PermuteEncoder(fieldName='consumption',\n                    encoderClass='AdaptiveScalarEncoder',\n                    w=21,\n                    n=PermuteInt(28, 521),\n                    clipInput=True)\"\n\n  \"\"\"\n\n  permStr = \"\"\n\n\n  # If it's the encoder for the classifier input, then it's always present so\n  # put it in as a dict in the permutations.py file instead of a\n  # PermuteEncoder().\n  if encoderDict.get('classifierOnly', False):\n    permStr = \"dict(\"\n    for key, value in encoderDict.items():\n      if key == \"name\":\n        continue\n\n      if key == 'n' and encoderDict['type'] != 'SDRCategoryEncoder':\n        permStr += \"n=PermuteInt(%d, %d), \" % (encoderDict[\"w\"] + 7,\n                                               encoderDict[\"w\"] + 500)\n      else:\n        if issubclass(type(value), basestring):\n          permStr += \"%s='%s', \" % (key, value)\n        else:\n          permStr += \"%s=%s, \" % (key, value)\n    permStr += \")\"\n\n\n  else:\n    # Scalar encoders\n    if encoderDict[\"type\"] in [\"ScalarSpaceEncoder\", \"AdaptiveScalarEncoder\",\n                             \"ScalarEncoder\", \"LogEncoder\"]:\n      permStr = \"PermuteEncoder(\"\n      for key, value in encoderDict.items():\n        if key == \"fieldname\":\n          key = \"fieldName\"\n        elif key == \"type\":\n          key = \"encoderClass\"\n        elif key == \"name\":\n          continue\n\n        if key == \"n\":\n          permStr += \"n=PermuteInt(%d, %d), \" % (encoderDict[\"w\"] + 1,\n                                                 encoderDict[\"w\"] + 500)\n        elif key == \"runDelta\":\n          if value and not \"space\" in encoderDict:\n            permStr += \"space=PermuteChoices([%s,%s]), \" \\\n                     % (_quoteAndEscape(\"delta\"), _quoteAndEscape(\"absolute\"))\n          encoderDict.pop(\"runDelta\")\n\n        else:\n          if issubclass(type(value), basestring):\n            permStr += \"%s='%s', \" % (key, value)\n          else:\n            permStr += \"%s=%s, \" % (key, value)\n      permStr += \")\"\n\n    # Category encoder\n    elif encoderDict[\"type\"] in [\"SDRCategoryEncoder\"]:\n      permStr = \"PermuteEncoder(\"\n      for key, value in encoderDict.items():\n        if key == \"fieldname\":\n          key = \"fieldName\"\n        elif key == \"type\":\n          key = \"encoderClass\"\n        elif key == \"name\":\n          continue\n\n        if issubclass(type(value), basestring):\n          permStr += \"%s='%s', \" % (key, value)\n        else:\n          permStr += \"%s=%s, \" % (key, value)\n      permStr += \")\"\n\n\n    # Datetime encoder\n    elif encoderDict[\"type\"] in [\"DateEncoder\"]:\n      permStr = \"PermuteEncoder(\"\n      for key, value in encoderDict.items():\n        if key == \"fieldname\":\n          key = \"fieldName\"\n        elif key == \"type\":\n          continue\n        elif key == \"name\":\n          continue\n\n        if key == \"timeOfDay\":\n          permStr += \"encoderClass='%s.timeOfDay', \" % (encoderDict[\"type\"])\n          permStr += \"radius=PermuteFloat(0.5, 12), \"\n          permStr += \"w=%d, \" % (value[0])\n        elif key == \"dayOfWeek\":\n          permStr += \"encoderClass='%s.dayOfWeek', \" % (encoderDict[\"type\"])\n          permStr += \"radius=PermuteFloat(1, 6), \"\n          permStr += \"w=%d, \" % (value[0])\n        elif key == \"weekend\":\n          permStr += \"encoderClass='%s.weekend', \" % (encoderDict[\"type\"])\n          permStr += \"radius=PermuteChoices([1]),  \"\n          permStr += \"w=%d, \" % (value)\n        else:\n          if issubclass(type(value), basestring):\n            permStr += \"%s='%s', \" % (key, value)\n          else:\n            permStr += \"%s=%s, \" % (key, value)\n      permStr += \")\"\n\n    else:\n      raise RuntimeError(\"Unsupported encoder type '%s'\" % \\\n                          (encoderDict[\"type\"]))\n\n  return permStr",
    "doc": "Generate the string that defines the permutations to apply for a given\n  encoder.\n\n  Parameters:\n  -----------------------------------------------------------------------\n  options: experiment params\n  encoderDict: the encoder dict, which gets placed into the description.py\n\n\n  For example, if the encoderDict contains:\n      'consumption':     {\n                'clipInput': True,\n                'fieldname': u'consumption',\n                'n': 100,\n                'name': u'consumption',\n                'type': 'AdaptiveScalarEncoder',\n                'w': 21},\n\n  The return string will contain:\n    \"PermuteEncoder(fieldName='consumption',\n                    encoderClass='AdaptiveScalarEncoder',\n                    w=21,\n                    n=PermuteInt(28, 521),\n                    clipInput=True)\""
  },
  {
    "code": "def _generateEncoderStringsV2(includedFields, options):\n  \"\"\" Generate and return the following encoder related substitution variables:\n\n  encoderSpecsStr:\n    For the base description file, this string defines the default\n    encoding dicts for each encoder. For example:\n\n         __gym_encoder = {   'fieldname': 'gym',\n          'n': 13,\n          'name': 'gym',\n          'type': 'SDRCategoryEncoder',\n          'w': 7},\n        __address_encoder = {   'fieldname': 'address',\n          'n': 13,\n          'name': 'address',\n          'type': 'SDRCategoryEncoder',\n          'w': 7}\n\n\n  permEncoderChoicesStr:\n    For the permutations file, this defines the possible\n    encoder dicts for each encoder. For example:\n\n        '__gym_encoder' : PermuteEncoder('gym', 'SDRCategoryEncoder', w=7,\n            n=100),\n\n        '__address_encoder' : PermuteEncoder('address', 'SDRCategoryEncoder',\n              w=7, n=100),\n\n        '__timestamp_dayOfWeek_encoder' : PermuteEncoder('timestamp',\n            'DateEncoder.timeOfDay', w=7, radius=PermuteChoices([1, 8])),\n\n        '__consumption_encoder': PermuteEncoder('consumption', 'AdaptiveScalarEncoder',\n            w=7, n=PermuteInt(13, 500, 20), minval=0,\n            maxval=PermuteInt(100, 300, 25)),\n\n\n\n  Parameters:\n  --------------------------------------------------\n  includedFields:  item from the 'includedFields' section of the\n                    description JSON object. This is a list of dicts, each\n                    dict defining the field name, type, and optional min\n                    and max values.\n\n  retval:  (encoderSpecsStr permEncoderChoicesStr)\n\n\n  \"\"\"\n\n  width = 21\n  encoderDictsList = []\n\n\n  # If this is a NontemporalClassification experiment, then the\n  #  the \"predicted\" field (the classification value) should be marked to ONLY\n  #  go to the classifier\n  if options['inferenceType'] in [\"NontemporalClassification\",\n                                  \"NontemporalMultiStep\",\n                                  \"TemporalMultiStep\",\n                                  \"MultiStep\"]:\n    classifierOnlyField = options['inferenceArgs']['predictedField']\n  else:\n    classifierOnlyField = None\n\n\n  # ==========================================================================\n  # For each field, generate the default encoding dict and PermuteEncoder\n  #  constructor arguments\n  for fieldInfo in includedFields:\n\n    fieldName = fieldInfo['fieldName']\n    fieldType = fieldInfo['fieldType']\n\n    # ---------\n    # Scalar?\n    if fieldType in ['float', 'int']:\n      # n=100 is reasonably hardcoded value for n when used by description.py\n      # The swarming will use PermuteEncoder below, where n is variable and\n      # depends on w\n      runDelta = fieldInfo.get(\"runDelta\", False)\n      if runDelta or \"space\" in fieldInfo:\n        encoderDict = dict(type='ScalarSpaceEncoder', name=fieldName,\n                            fieldname=fieldName, n=100, w=width, clipInput=True)\n        if runDelta:\n          encoderDict[\"runDelta\"] = True\n      else:\n        encoderDict = dict(type='AdaptiveScalarEncoder', name=fieldName,\n                            fieldname=fieldName, n=100, w=width, clipInput=True)\n\n      if 'minValue' in fieldInfo:\n        encoderDict['minval'] = fieldInfo['minValue']\n      if 'maxValue' in fieldInfo:\n        encoderDict['maxval'] = fieldInfo['maxValue']\n\n      # If both min and max were specified, use a non-adaptive encoder\n      if ('minValue' in fieldInfo and 'maxValue' in fieldInfo) \\\n            and (encoderDict['type'] == 'AdaptiveScalarEncoder'):\n        encoderDict['type'] = 'ScalarEncoder'\n\n      # Defaults may have been over-ridden by specifying an encoder type\n      if 'encoderType' in fieldInfo:\n        encoderDict['type'] = fieldInfo['encoderType']\n\n      if 'space' in fieldInfo:\n        encoderDict['space'] = fieldInfo['space']\n      encoderDictsList.append(encoderDict)\n\n\n\n    # ---------\n    # String?\n    elif fieldType == 'string':\n      encoderDict = dict(type='SDRCategoryEncoder', name=fieldName,\n                     fieldname=fieldName, n=100+width, w=width)\n      if 'encoderType' in fieldInfo:\n        encoderDict['type'] = fieldInfo['encoderType']\n\n      encoderDictsList.append(encoderDict)\n\n\n\n    # ---------\n    # Datetime?\n    elif fieldType == 'datetime':\n\n      # First, the time of day representation\n      encoderDict = dict(type='DateEncoder', name='%s_timeOfDay' % (fieldName),\n                     fieldname=fieldName, timeOfDay=(width, 1))\n      if 'encoderType' in fieldInfo:\n        encoderDict['type'] = fieldInfo['encoderType']\n      encoderDictsList.append(encoderDict)\n\n\n      # Now, the day of week representation\n      encoderDict = dict(type='DateEncoder', name='%s_dayOfWeek' % (fieldName),\n                     fieldname=fieldName, dayOfWeek=(width, 1))\n      if 'encoderType' in fieldInfo:\n        encoderDict['type'] = fieldInfo['encoderType']\n      encoderDictsList.append(encoderDict)\n\n\n      # Now, the day of week representation\n      encoderDict = dict(type='DateEncoder', name='%s_weekend' % (fieldName),\n                     fieldname=fieldName, weekend=(width))\n      if 'encoderType' in fieldInfo:\n        encoderDict['type'] = fieldInfo['encoderType']\n      encoderDictsList.append(encoderDict)\n\n\n\n\n    else:\n      raise RuntimeError(\"Unsupported field type '%s'\" % (fieldType))\n\n\n    # -----------------------------------------------------------------------\n    # If this was the predicted field, insert another encoder that sends it\n    # to the classifier only\n    if fieldName == classifierOnlyField:\n      clEncoderDict = dict(encoderDict)\n      clEncoderDict['classifierOnly'] = True\n      clEncoderDict['name'] = '_classifierInput'\n      encoderDictsList.append(clEncoderDict)\n\n      # If the predicted field needs to be excluded, take it out of the encoder\n      #  lists\n      if options[\"inferenceArgs\"][\"inputPredictedField\"] == \"no\":\n        encoderDictsList.remove(encoderDict)\n\n  # Remove any encoders not in fixedFields\n  if options.get('fixedFields') is not None:\n    tempList=[]\n    for encoderDict in encoderDictsList:\n      if encoderDict['name'] in options['fixedFields']:\n        tempList.append(encoderDict)\n    encoderDictsList = tempList\n\n  # ==========================================================================\n  # Now generate the encoderSpecsStr and permEncoderChoicesStr strings from\n  #  encoderDictsList and constructorStringList\n\n  encoderSpecsList = []\n  permEncoderChoicesList = []\n  for encoderDict in encoderDictsList:\n\n    if encoderDict['name'].find('\\\\') >= 0:\n      raise _ExpGeneratorException(\"Illegal character in field: '\\\\'\")\n\n    # Check for bad characters\n    for c in _ILLEGAL_FIELDNAME_CHARACTERS:\n      if encoderDict['name'].find(c) >= 0:\n        raise _ExpGeneratorException(\"Illegal character %s in field %r\"  %(c, encoderDict['name']))\n\n    constructorStr = _generatePermEncoderStr(options, encoderDict)\n\n    encoderKey = _quoteAndEscape(encoderDict['name'])\n    encoderSpecsList.append(\"%s: %s%s\" % (\n        encoderKey,\n        2*_ONE_INDENT,\n        pprint.pformat(encoderDict, indent=2*_INDENT_STEP)))\n\n\n    # Each permEncoderChoicesStr is of the form:\n    #  PermuteEncoder('gym', 'SDRCategoryEncoder',\n    #          w=7, n=100),\n    permEncoderChoicesList.append(\"%s: %s,\" % (encoderKey, constructorStr))\n\n\n  # Join into strings\n  encoderSpecsStr = ',\\n  '.join(encoderSpecsList)\n\n  permEncoderChoicesStr = '\\n'.join(permEncoderChoicesList)\n  permEncoderChoicesStr = _indentLines(permEncoderChoicesStr, 1,\n                                       indentFirstLine=True)\n\n  # Return results\n  return (encoderSpecsStr, permEncoderChoicesStr)",
    "doc": "Generate and return the following encoder related substitution variables:\n\n  encoderSpecsStr:\n    For the base description file, this string defines the default\n    encoding dicts for each encoder. For example:\n\n         __gym_encoder = {   'fieldname': 'gym',\n          'n': 13,\n          'name': 'gym',\n          'type': 'SDRCategoryEncoder',\n          'w': 7},\n        __address_encoder = {   'fieldname': 'address',\n          'n': 13,\n          'name': 'address',\n          'type': 'SDRCategoryEncoder',\n          'w': 7}\n\n\n  permEncoderChoicesStr:\n    For the permutations file, this defines the possible\n    encoder dicts for each encoder. For example:\n\n        '__gym_encoder' : PermuteEncoder('gym', 'SDRCategoryEncoder', w=7,\n            n=100),\n\n        '__address_encoder' : PermuteEncoder('address', 'SDRCategoryEncoder',\n              w=7, n=100),\n\n        '__timestamp_dayOfWeek_encoder' : PermuteEncoder('timestamp',\n            'DateEncoder.timeOfDay', w=7, radius=PermuteChoices([1, 8])),\n\n        '__consumption_encoder': PermuteEncoder('consumption', 'AdaptiveScalarEncoder',\n            w=7, n=PermuteInt(13, 500, 20), minval=0,\n            maxval=PermuteInt(100, 300, 25)),\n\n\n\n  Parameters:\n  --------------------------------------------------\n  includedFields:  item from the 'includedFields' section of the\n                    description JSON object. This is a list of dicts, each\n                    dict defining the field name, type, and optional min\n                    and max values.\n\n  retval:  (encoderSpecsStr permEncoderChoicesStr)"
  },
  {
    "code": "def _handleJAVAParameters(options):\n  \"\"\" Handle legacy options (TEMPORARY) \"\"\"\n\n  # Find the correct InferenceType for the Model\n  if 'inferenceType' not in options:\n    prediction = options.get('prediction', {InferenceType.TemporalNextStep:\n                                              {'optimize':True}})\n    inferenceType = None\n    for infType, value in prediction.iteritems():\n      if value['optimize']:\n        inferenceType = infType\n        break\n\n    if inferenceType == 'temporal':\n      inferenceType = InferenceType.TemporalNextStep\n    if inferenceType != InferenceType.TemporalNextStep:\n      raise _ExpGeneratorException(\"Unsupported inference type %s\"  % \\\n                                    (inferenceType))\n    options['inferenceType'] = inferenceType\n\n  # Find the best value for the predicted field\n  if 'predictionField' in options:\n    if 'inferenceArgs' not in options:\n      options['inferenceArgs'] = {'predictedField': options['predictionField']}\n    elif 'predictedField' not in options['inferenceArgs']:\n      options['inferenceArgs']['predictedField'] = options['predictionField']",
    "doc": "Handle legacy options (TEMPORARY)"
  },
  {
    "code": "def _getPropertyValue(schema, propertyName, options):\n  \"\"\"Checks to see if property is specified in 'options'. If not, reads the\n  default value from the schema\"\"\"\n\n  if propertyName not in options:\n    paramsSchema = schema['properties'][propertyName]\n    if 'default' in paramsSchema:\n      options[propertyName] = paramsSchema['default']\n    else:\n      options[propertyName] = None",
    "doc": "Checks to see if property is specified in 'options'. If not, reads the\n  default value from the schema"
  },
  {
    "code": "def _getExperimentDescriptionSchema():\n  \"\"\"\n  Returns the experiment description schema. This implementation loads it in\n  from file experimentDescriptionSchema.json.\n\n  Parameters:\n  --------------------------------------------------------------------------\n  Returns:    returns a dict representing the experiment description schema.\n  \"\"\"\n  installPath = os.path.dirname(os.path.abspath(__file__))\n  schemaFilePath = os.path.join(installPath, \"experimentDescriptionSchema.json\")\n  return json.loads(open(schemaFilePath, 'r').read())",
    "doc": "Returns the experiment description schema. This implementation loads it in\n  from file experimentDescriptionSchema.json.\n\n  Parameters:\n  --------------------------------------------------------------------------\n  Returns:    returns a dict representing the experiment description schema."
  },
  {
    "code": "def _generateExperiment(options, outputDirPath, hsVersion,\n                             claDescriptionTemplateFile):\n  \"\"\" Executes the --description option, which includes:\n\n      1. Perform provider compatibility checks\n      2. Preprocess the training and testing datasets (filter, join providers)\n      3. If test dataset omitted, split the training dataset into training\n         and testing datasets.\n      4. Gather statistics about the training and testing datasets.\n      5. Generate experiment scripts (description.py, permutaions.py)\n\n  Parameters:\n  --------------------------------------------------------------------------\n  options:  dictionary that matches the schema defined by the return value of\n            _getExperimentDescriptionSchema();  NOTE: this arg may be modified\n            by this function.\n\n  outputDirPath:  where to place generated files\n\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n\n\n  Returns:    on success, returns a dictionary per _experimentResultsJSONSchema;\n              raises exception on error\n\n      Assumption1: input train and test files have identical field metadata\n  \"\"\"\n\n  _gExperimentDescriptionSchema = _getExperimentDescriptionSchema()\n\n  # Validate JSON arg using JSON schema validator\n  try:\n    validictory.validate(options, _gExperimentDescriptionSchema)\n  except Exception, e:\n    raise _InvalidCommandArgException(\n      (\"JSON arg validation failed for option --description: \" + \\\n       \"%s\\nOPTION ARG=%s\") % (str(e), pprint.pformat(options)))\n\n  # Validate the streamDef\n  streamSchema = json.load(resource_stream(jsonschema.__name__,\n                                           'stream_def.json'))\n  try:\n    validictory.validate(options['streamDef'], streamSchema)\n  except Exception, e:\n    raise _InvalidCommandArgException(\n      (\"JSON arg validation failed for streamDef \" + \\\n       \"%s\\nOPTION ARG=%s\") % (str(e), json.dumps(options)))\n\n  # -----------------------------------------------------------------------\n  # Handle legacy parameters from JAVA API server\n  # TODO: remove this!\n  _handleJAVAParameters(options)\n\n  # -----------------------------------------------------------------------\n  # Get default values\n  for propertyName in _gExperimentDescriptionSchema['properties']:\n    _getPropertyValue(_gExperimentDescriptionSchema, propertyName, options)\n\n\n  if options['inferenceArgs'] is not None:\n    infArgs = _gExperimentDescriptionSchema['properties']['inferenceArgs']\n    for schema in infArgs['type']:\n      if isinstance(schema, dict):\n        for propertyName in schema['properties']:\n          _getPropertyValue(schema, propertyName, options['inferenceArgs'])\n\n  if options['anomalyParams'] is not None:\n    anomalyArgs = _gExperimentDescriptionSchema['properties']['anomalyParams']\n    for schema in anomalyArgs['type']:\n      if isinstance(schema, dict):\n        for propertyName in schema['properties']:\n          _getPropertyValue(schema, propertyName, options['anomalyParams'])\n\n\n  # If the user specified nonTemporalClassification, make sure prediction\n  # steps is 0\n  predictionSteps = options['inferenceArgs'].get('predictionSteps', None)\n  if options['inferenceType'] == InferenceType.NontemporalClassification:\n    if predictionSteps is not None and predictionSteps != [0]:\n      raise RuntimeError(\"When NontemporalClassification is used, prediction\"\n                         \" steps must be [0]\")\n\n  # -------------------------------------------------------------------------\n  # If the user asked for 0 steps of prediction, then make this a spatial\n  #  classification experiment\n  if predictionSteps == [0] \\\n    and options['inferenceType'] in ['NontemporalMultiStep',\n                                     'TemporalMultiStep',\n                                     'MultiStep']:\n    options['inferenceType'] = InferenceType.NontemporalClassification\n\n\n  # If NontemporalClassification was chosen as the inferenceType, then the\n  #  predicted field can NOT be used as an input\n  if options[\"inferenceType\"] == InferenceType.NontemporalClassification:\n    if options[\"inferenceArgs\"][\"inputPredictedField\"] == \"yes\" \\\n        or options[\"inferenceArgs\"][\"inputPredictedField\"] == \"auto\":\n      raise RuntimeError(\"When the inference type is NontemporalClassification\"\n                         \" inputPredictedField must be set to 'no'\")\n    options[\"inferenceArgs\"][\"inputPredictedField\"] = \"no\"\n\n\n  # -----------------------------------------------------------------------\n  # Process the swarmSize setting, if provided\n  swarmSize = options['swarmSize']\n\n  if swarmSize is None:\n    if options[\"inferenceArgs\"][\"inputPredictedField\"] is None:\n      options[\"inferenceArgs\"][\"inputPredictedField\"] = \"auto\"\n\n  elif swarmSize == 'small':\n    if options['minParticlesPerSwarm'] is None:\n      options['minParticlesPerSwarm'] = 3\n    if options['iterationCount'] is None:\n      options['iterationCount'] = 100\n    if options['maxModels'] is None:\n      options['maxModels'] = 1\n    if options[\"inferenceArgs\"][\"inputPredictedField\"] is None:\n      options[\"inferenceArgs\"][\"inputPredictedField\"] = \"yes\"\n\n  elif swarmSize == 'medium':\n    if options['minParticlesPerSwarm'] is None:\n      options['minParticlesPerSwarm'] = 5\n    if options['iterationCount'] is None:\n      options['iterationCount'] = 4000\n    if options['maxModels'] is None:\n      options['maxModels'] = 200\n    if options[\"inferenceArgs\"][\"inputPredictedField\"] is None:\n      options[\"inferenceArgs\"][\"inputPredictedField\"] = \"auto\"\n\n  elif swarmSize == 'large':\n    if options['minParticlesPerSwarm'] is None:\n      options['minParticlesPerSwarm'] = 15\n    #options['killUselessSwarms'] = False\n    #options['minFieldContribution'] = -1000\n    #options['maxFieldBranching'] = 10\n    #options['tryAll3FieldCombinations'] = True\n    options['tryAll3FieldCombinationsWTimestamps'] = True\n    if options[\"inferenceArgs\"][\"inputPredictedField\"] is None:\n      options[\"inferenceArgs\"][\"inputPredictedField\"] = \"auto\"\n\n  else:\n    raise RuntimeError(\"Unsupported swarm size: %s\" % (swarmSize))\n\n\n\n  # -----------------------------------------------------------------------\n  # Get token replacements\n  tokenReplacements = dict()\n\n  #--------------------------------------------------------------------------\n  # Generate the encoder related substitution strings\n  includedFields = options['includedFields']\n  if hsVersion == 'v1':\n    (encoderSpecsStr, permEncoderChoicesStr) = \\\n        _generateEncoderStringsV1(includedFields)\n  elif hsVersion in ['v2', 'ensemble']:\n    (encoderSpecsStr, permEncoderChoicesStr) = \\\n        _generateEncoderStringsV2(includedFields, options)\n  else:\n    raise RuntimeError(\"Unsupported hsVersion of %s\" % (hsVersion))\n\n\n\n  #--------------------------------------------------------------------------\n  # Generate the string containing the sensor auto-reset dict.\n  if options['resetPeriod'] is not None:\n    sensorAutoResetStr = pprint.pformat(options['resetPeriod'],\n                                         indent=2*_INDENT_STEP)\n  else:\n    sensorAutoResetStr = 'None'\n\n\n  #--------------------------------------------------------------------------\n  # Generate the string containing the aggregation settings.\n  aggregationPeriod = {\n      'days': 0,\n      'hours': 0,\n      'microseconds': 0,\n      'milliseconds': 0,\n      'minutes': 0,\n      'months': 0,\n      'seconds': 0,\n      'weeks': 0,\n      'years': 0,\n  }\n\n  # Honor any overrides provided in the stream definition\n  aggFunctionsDict = {}\n  if 'aggregation' in options['streamDef']:\n    for key in aggregationPeriod.keys():\n      if key in options['streamDef']['aggregation']:\n        aggregationPeriod[key] = options['streamDef']['aggregation'][key]\n    if 'fields' in options['streamDef']['aggregation']:\n      for (fieldName, func) in options['streamDef']['aggregation']['fields']:\n        aggFunctionsDict[fieldName] = str(func)\n\n  # Do we have any aggregation at all?\n  hasAggregation = False\n  for v in aggregationPeriod.values():\n    if v != 0:\n      hasAggregation = True\n      break\n\n\n  # Convert the aggFunctionsDict to a list\n  aggFunctionList = aggFunctionsDict.items()\n  aggregationInfo = dict(aggregationPeriod)\n  aggregationInfo['fields'] = aggFunctionList\n\n  # Form the aggregation strings\n  aggregationInfoStr = \"%s\" % (pprint.pformat(aggregationInfo,\n                                              indent=2*_INDENT_STEP))\n\n\n\n  # -----------------------------------------------------------------------\n  # Generate the string defining the dataset. This is basically the\n  #  streamDef, but referencing the aggregation we already pulled out into the\n  #  config dict (which enables permuting over it)\n  datasetSpec = options['streamDef']\n  if 'aggregation' in datasetSpec:\n    datasetSpec.pop('aggregation')\n  if hasAggregation:\n    datasetSpec['aggregation'] = '$SUBSTITUTE'\n  datasetSpecStr = pprint.pformat(datasetSpec, indent=2*_INDENT_STEP)\n  datasetSpecStr = datasetSpecStr.replace(\n      \"'$SUBSTITUTE'\", \"config['aggregationInfo']\")\n  datasetSpecStr = _indentLines(datasetSpecStr, 2, indentFirstLine=False)\n\n\n  # -----------------------------------------------------------------------\n  # Was computeInterval specified with Multistep prediction? If so, this swarm\n  #  should permute over different aggregations\n  computeInterval = options['computeInterval']\n  if computeInterval is not None \\\n      and options['inferenceType'] in ['NontemporalMultiStep',\n                                       'TemporalMultiStep',\n                                       'MultiStep']:\n\n    # Compute the predictAheadTime based on the minAggregation (specified in\n    #  the stream definition) and the number of prediction steps\n    predictionSteps = options['inferenceArgs'].get('predictionSteps', [1])\n    if len(predictionSteps) > 1:\n      raise _InvalidCommandArgException(\"Invalid predictionSteps: %s. \" \\\n              \"When computeInterval is specified, there can only be one \" \\\n              \"stepSize in predictionSteps.\" % predictionSteps)\n\n    if max(aggregationInfo.values()) == 0:\n      raise _InvalidCommandArgException(\"Missing or nil stream aggregation: \"\n            \"When computeInterval is specified, then the stream aggregation \"\n            \"interval must be non-zero.\")\n\n    # Compute the predictAheadTime\n    numSteps = predictionSteps[0]\n    predictAheadTime = dict(aggregationPeriod)\n    for key in predictAheadTime.iterkeys():\n      predictAheadTime[key] *= numSteps\n    predictAheadTimeStr = pprint.pformat(predictAheadTime,\n                                         indent=2*_INDENT_STEP)\n\n    # This tells us to plug in a wildcard string for the prediction steps that\n    #  we use in other parts of the description file (metrics, inferenceArgs,\n    #  etc.)\n    options['dynamicPredictionSteps'] = True\n\n  else:\n    options['dynamicPredictionSteps'] = False\n    predictAheadTimeStr = \"None\"\n\n\n\n  # -----------------------------------------------------------------------\n  # Save environment-common token substitutions\n\n  tokenReplacements['\\$EXP_GENERATOR_PROGRAM_PATH'] = \\\n                                _quoteAndEscape(os.path.abspath(__file__))\n\n  # If the \"uber\" metric 'MultiStep' was specified, then plug in TemporalMultiStep\n  #  by default\n  inferenceType = options['inferenceType']\n  if inferenceType == 'MultiStep':\n    inferenceType = InferenceType.TemporalMultiStep\n  tokenReplacements['\\$INFERENCE_TYPE'] = \"'%s'\" % inferenceType\n\n  # Nontemporal classificaion uses only encoder and classifier\n  if inferenceType == InferenceType.NontemporalClassification:\n    tokenReplacements['\\$SP_ENABLE'] = \"False\"\n    tokenReplacements['\\$TP_ENABLE'] = \"False\"\n  else:\n    tokenReplacements['\\$SP_ENABLE'] = \"True\"\n    tokenReplacements['\\$TP_ENABLE'] = \"True\"\n    tokenReplacements['\\$CLA_CLASSIFIER_IMPL'] = \"\"\n\n\n  tokenReplacements['\\$ANOMALY_PARAMS'] = pprint.pformat(\n      options['anomalyParams'], indent=2*_INDENT_STEP)\n\n  tokenReplacements['\\$ENCODER_SPECS'] = encoderSpecsStr\n  tokenReplacements['\\$SENSOR_AUTO_RESET'] = sensorAutoResetStr\n\n  tokenReplacements['\\$AGGREGATION_INFO'] = aggregationInfoStr\n\n  tokenReplacements['\\$DATASET_SPEC'] = datasetSpecStr\n  if options['iterationCount'] is None:\n    options['iterationCount'] = -1\n  tokenReplacements['\\$ITERATION_COUNT'] \\\n                                        = str(options['iterationCount'])\n\n  tokenReplacements['\\$SP_POOL_PCT'] \\\n                                        = str(options['spCoincInputPoolPct'])\n\n  tokenReplacements['\\$HS_MIN_PARTICLES'] \\\n                                        = str(options['minParticlesPerSwarm'])\n\n\n  tokenReplacements['\\$SP_PERM_CONNECTED'] \\\n                                        = str(options['spSynPermConnected'])\n\n  tokenReplacements['\\$FIELD_PERMUTATION_LIMIT'] \\\n                                        = str(options['fieldPermutationLimit'])\n\n  tokenReplacements['\\$PERM_ENCODER_CHOICES'] \\\n                                        = permEncoderChoicesStr\n\n  predictionSteps = options['inferenceArgs'].get('predictionSteps', [1])\n  predictionStepsStr = ','.join([str(x) for x in predictionSteps])\n  tokenReplacements['\\$PREDICTION_STEPS'] = \"'%s'\" % (predictionStepsStr)\n\n  tokenReplacements['\\$PREDICT_AHEAD_TIME'] = predictAheadTimeStr\n\n  # Option permuting over SP synapse decrement value\n  tokenReplacements['\\$PERM_SP_CHOICES'] = \"\"\n  if options['spPermuteDecrement'] \\\n        and options['inferenceType'] != 'NontemporalClassification':\n    tokenReplacements['\\$PERM_SP_CHOICES'] = \\\n      _ONE_INDENT +\"'synPermInactiveDec': PermuteFloat(0.0003, 0.1),\\n\"\n\n  # The TM permutation parameters are not required for non-temporal networks\n  if options['inferenceType'] in ['NontemporalMultiStep',\n                                  'NontemporalClassification']:\n    tokenReplacements['\\$PERM_TP_CHOICES'] = \"\"\n  else:\n    tokenReplacements['\\$PERM_TP_CHOICES'] = \\\n        \"  'activationThreshold': PermuteInt(12, 16),\\n\"  \\\n      + \"  'minThreshold': PermuteInt(9, 12),\\n\" \\\n      + \"  'pamLength': PermuteInt(1, 5),\\n\"\n\n\n  # If the inference type is just the generic 'MultiStep', then permute over\n  #  temporal/nonTemporal multistep\n  if options['inferenceType'] == 'MultiStep':\n    tokenReplacements['\\$PERM_INFERENCE_TYPE_CHOICES'] = \\\n      \"  'inferenceType': PermuteChoices(['NontemporalMultiStep', \" \\\n      + \"'TemporalMultiStep']),\"\n  else:\n    tokenReplacements['\\$PERM_INFERENCE_TYPE_CHOICES'] = \"\"\n\n\n  # The Classifier permutation parameters are only required for\n  #  Multi-step inference types\n  if options['inferenceType'] in ['NontemporalMultiStep', 'TemporalMultiStep',\n                                  'MultiStep', 'TemporalAnomaly',\n                                  'NontemporalClassification']:\n    tokenReplacements['\\$PERM_CL_CHOICES'] = \\\n        \"  'alpha': PermuteFloat(0.0001, 0.1),\\n\"\n\n  else:\n    tokenReplacements['\\$PERM_CL_CHOICES'] = \"\"\n\n\n  # The Permutations alwaysIncludePredictedField setting.\n  # * When the experiment description has 'inputPredictedField' set to 'no', we\n  #   simply do not put in an encoder for the predicted field.\n  # * When 'inputPredictedField' is set to 'auto', we include an encoder for the\n  #   predicted field and swarming tries it out just like all the other fields.\n  # * When 'inputPredictedField' is set to 'yes', we include this setting in\n  #   the permutations file which informs swarming to always use the\n  #   predicted field (the first swarm will be the predicted field only)\n  tokenReplacements['\\$PERM_ALWAYS_INCLUDE_PREDICTED_FIELD'] = \\\n      \"inputPredictedField = '%s'\" % \\\n                            (options[\"inferenceArgs\"][\"inputPredictedField\"])\n\n\n  # The Permutations minFieldContribution setting\n  if options.get('minFieldContribution', None) is not None:\n    tokenReplacements['\\$PERM_MIN_FIELD_CONTRIBUTION'] = \\\n        \"minFieldContribution = %d\" % (options['minFieldContribution'])\n  else:\n    tokenReplacements['\\$PERM_MIN_FIELD_CONTRIBUTION'] = \"\"\n\n  # The Permutations killUselessSwarms setting\n  if options.get('killUselessSwarms', None) is not None:\n    tokenReplacements['\\$PERM_KILL_USELESS_SWARMS'] = \\\n        \"killUselessSwarms = %r\" % (options['killUselessSwarms'])\n  else:\n    tokenReplacements['\\$PERM_KILL_USELESS_SWARMS'] = \"\"\n\n  # The Permutations maxFieldBranching setting\n  if options.get('maxFieldBranching', None) is not None:\n    tokenReplacements['\\$PERM_MAX_FIELD_BRANCHING'] = \\\n        \"maxFieldBranching = %r\" % (options['maxFieldBranching'])\n  else:\n    tokenReplacements['\\$PERM_MAX_FIELD_BRANCHING'] = \"\"\n\n  # The Permutations tryAll3FieldCombinations setting\n  if options.get('tryAll3FieldCombinations', None) is not None:\n    tokenReplacements['\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS'] = \\\n        \"tryAll3FieldCombinations = %r\" % (options['tryAll3FieldCombinations'])\n  else:\n    tokenReplacements['\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS'] = \"\"\n\n  # The Permutations tryAll3FieldCombinationsWTimestamps setting\n  if options.get('tryAll3FieldCombinationsWTimestamps', None) is not None:\n    tokenReplacements['\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS_W_TIMESTAMPS'] = \\\n        \"tryAll3FieldCombinationsWTimestamps = %r\" % \\\n                (options['tryAll3FieldCombinationsWTimestamps'])\n  else:\n    tokenReplacements['\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS_W_TIMESTAMPS'] = \"\"\n\n\n  # The Permutations fieldFields setting\n  if options.get('fixedFields', None) is not None:\n    tokenReplacements['\\$PERM_FIXED_FIELDS'] = \\\n        \"fixedFields = %r\" % (options['fixedFields'])\n  else:\n    tokenReplacements['\\$PERM_FIXED_FIELDS'] = \"\"\n\n  # The Permutations fastSwarmModelParams setting\n  if options.get('fastSwarmModelParams', None) is not None:\n    tokenReplacements['\\$PERM_FAST_SWARM_MODEL_PARAMS'] = \\\n        \"fastSwarmModelParams = %r\" % (options['fastSwarmModelParams'])\n  else:\n    tokenReplacements['\\$PERM_FAST_SWARM_MODEL_PARAMS'] = \"\"\n\n\n  # The Permutations maxModels setting\n  if options.get('maxModels', None) is not None:\n    tokenReplacements['\\$PERM_MAX_MODELS'] = \\\n        \"maxModels = %r\" % (options['maxModels'])\n  else:\n    tokenReplacements['\\$PERM_MAX_MODELS'] = \"\"\n\n\n  # --------------------------------------------------------------------------\n  # The Aggregation choices have to be determined when we are permuting over\n  #   aggregations.\n  if options['dynamicPredictionSteps']:\n    debugAgg = True\n\n    # First, we need to error check to insure that computeInterval is an integer\n    #  multiple of minAggregation (aggregationPeriod)\n    quotient = aggregationDivide(computeInterval, aggregationPeriod)\n    (isInt, multiple) = _isInt(quotient)\n    if not isInt or multiple < 1:\n      raise _InvalidCommandArgException(\"Invalid computeInterval: %s. \"\n              \"computeInterval must be an integer multiple of the stream \"\n              \"aggregation (%s).\" % (computeInterval, aggregationPeriod))\n\n\n    # The valid aggregation choices are governed by the following constraint,\n    #   1.) (minAggregation * N) * M = predictAheadTime\n    #       (minAggregation * N) * M = maxPredictionSteps * minAggregation\n    #       N * M = maxPredictionSteps\n    #\n    #   2.) computeInterval = K * aggregation\n    #       computeInterval = K * (minAggregation * N)\n    #\n    # where: aggregation = minAggregation * N\n    #        K, M and N are integers >= 1\n    #          N = aggregation / minAggregation\n    #          M = predictionSteps, for a particular aggregation\n    #          K = number of predictions within each compute interval\n    #\n\n    # Let's build up a a list of the possible N's that satisfy the\n    #  N * M = maxPredictionSteps constraint\n    mTimesN = float(predictionSteps[0])\n    possibleNs = []\n    for n in xrange(1, int(mTimesN)+1):\n      m = mTimesN / n\n      mInt = int(round(m))\n      if mInt < 1:\n        break\n      if abs(m - mInt) > 0.0001 * m:\n        continue\n      possibleNs.append(n)\n\n    if debugAgg:\n      print \"All integer factors of %d are: %s\" % (mTimesN, possibleNs)\n\n    # Now go through and throw out any N's that don't satisfy the constraint:\n    #  computeInterval = K * (minAggregation * N)\n    aggChoices = []\n    for n in possibleNs:\n      # Compute minAggregation * N\n      agg = dict(aggregationPeriod)\n      for key in agg.iterkeys():\n        agg[key] *= n\n\n      # Make sure computeInterval is an integer multiple of the aggregation\n      # period\n      quotient = aggregationDivide(computeInterval, agg)\n      #print computeInterval, agg\n      #print quotient\n      #import sys; sys.exit()\n      (isInt, multiple) = _isInt(quotient)\n      if not isInt or multiple < 1:\n        continue\n      aggChoices.append(agg)\n\n    # Only eveluate up to 5 different aggregations\n    aggChoices = aggChoices[-5:]\n\n    if debugAgg:\n      print \"Aggregation choices that will be evaluted during swarming:\"\n      for agg in aggChoices:\n        print \"  ==>\", agg\n      print\n\n    tokenReplacements['\\$PERM_AGGREGATION_CHOICES'] = (\n        \"PermuteChoices(%s)\" % (\n            pprint.pformat(aggChoices, indent=2*_INDENT_STEP)))\n\n  else:\n    tokenReplacements['\\$PERM_AGGREGATION_CHOICES'] = aggregationInfoStr\n\n\n\n  # Generate the inferenceArgs replacement tokens\n  _generateInferenceArgs(options, tokenReplacements)\n\n  # Generate the metric replacement tokens\n  _generateMetricsSubstitutions(options, tokenReplacements)\n\n\n  # -----------------------------------------------------------------------\n  # Generate Control dictionary\n  environment = options['environment']\n  if environment == OpfEnvironment.Nupic:\n    tokenReplacements['\\$ENVIRONMENT'] = \"'%s'\"%OpfEnvironment.Nupic\n    controlTemplate = \"nupicEnvironmentTemplate.tpl\"\n  elif environment == OpfEnvironment.Experiment:\n    tokenReplacements['\\$ENVIRONMENT'] = \"'%s'\"%OpfEnvironment.Experiment\n    controlTemplate = \"opfExperimentTemplate.tpl\"\n  else:\n    raise _InvalidCommandArgException(\"Invalid environment type %s\"% environment)\n\n  # -----------------------------------------------------------------------\n  if outputDirPath is None:\n    outputDirPath = tempfile.mkdtemp()\n  if not os.path.exists(outputDirPath):\n    os.makedirs(outputDirPath)\n\n  print \"Generating experiment files in directory: %s...\" % (outputDirPath)\n  descriptionPyPath = os.path.join(outputDirPath, \"description.py\")\n  _generateFileFromTemplates([claDescriptionTemplateFile, controlTemplate],\n                              descriptionPyPath,\n                              tokenReplacements)\n\n  permutationsPyPath = os.path.join(outputDirPath, \"permutations.py\")\n\n  if hsVersion == 'v1':\n    _generateFileFromTemplates(['permutationsTemplateV1.tpl'],permutationsPyPath,\n                            tokenReplacements)\n  elif hsVersion == 'ensemble':\n    _generateFileFromTemplates(['permutationsTemplateEnsemble.tpl'],permutationsPyPath,\n                            tokenReplacements)\n  elif hsVersion == 'v2':\n    _generateFileFromTemplates(['permutationsTemplateV2.tpl'],permutationsPyPath,\n                            tokenReplacements)\n  else:\n    raise(ValueError(\"This permutation version is not supported yet: %s\" %\n                        hsVersion))\n\n  print \"done.\"",
    "doc": "Executes the --description option, which includes:\n\n      1. Perform provider compatibility checks\n      2. Preprocess the training and testing datasets (filter, join providers)\n      3. If test dataset omitted, split the training dataset into training\n         and testing datasets.\n      4. Gather statistics about the training and testing datasets.\n      5. Generate experiment scripts (description.py, permutaions.py)\n\n  Parameters:\n  --------------------------------------------------------------------------\n  options:  dictionary that matches the schema defined by the return value of\n            _getExperimentDescriptionSchema();  NOTE: this arg may be modified\n            by this function.\n\n  outputDirPath:  where to place generated files\n\n  hsVersion:  which version of hypersearch permutations file to generate, can\n                be 'v1' or 'v2'\n  claDescriptionTemplateFile: Filename containing the template description\n\n\n  Returns:    on success, returns a dictionary per _experimentResultsJSONSchema;\n              raises exception on error\n\n      Assumption1: input train and test files have identical field metadata"
  },
  {
    "code": "def _generateMetricsSubstitutions(options, tokenReplacements):\n  \"\"\"Generate the token substitution for metrics related fields.\n  This includes:\n    \\$METRICS\n    \\$LOGGED_METRICS\n    \\$PERM_OPTIMIZE_SETTING\n  \"\"\"\n  # -----------------------------------------------------------------------\n  #\n  options['loggedMetrics'] = [\".*\"]\n\n  # -----------------------------------------------------------------------\n  # Generate the required metrics\n  metricList, optimizeMetricLabel = _generateMetricSpecs(options)\n\n  metricListString = \",\\n\".join(metricList)\n  metricListString = _indentLines(metricListString, 2, indentFirstLine=False)\n  permOptimizeSettingStr = 'minimize = \"%s\"' % optimizeMetricLabel\n  # -----------------------------------------------------------------------\n  # Specify which metrics should be logged\n  loggedMetricsListAsStr = \"[%s]\" % (\", \".join([\"'%s'\"% ptrn\n                                              for ptrn in options['loggedMetrics']]))\n\n\n  tokenReplacements['\\$LOGGED_METRICS'] \\\n                                        = loggedMetricsListAsStr\n\n  tokenReplacements['\\$METRICS'] = metricListString\n\n  tokenReplacements['\\$PERM_OPTIMIZE_SETTING'] \\\n                                        = permOptimizeSettingStr",
    "doc": "Generate the token substitution for metrics related fields.\n  This includes:\n    \\$METRICS\n    \\$LOGGED_METRICS\n    \\$PERM_OPTIMIZE_SETTING"
  },
  {
    "code": "def _generateMetricSpecs(options):\n  \"\"\" Generates the Metrics for a given InferenceType\n\n  Parameters:\n  -------------------------------------------------------------------------\n  options: ExpGenerator options\n  retval: (metricsList, optimizeMetricLabel)\n            metricsList: list of metric string names\n            optimizeMetricLabel: Name of the metric which to optimize over\n\n  \"\"\"\n  inferenceType = options['inferenceType']\n  inferenceArgs = options['inferenceArgs']\n  predictionSteps = inferenceArgs['predictionSteps']\n  metricWindow = options['metricWindow']\n  if metricWindow is None:\n    metricWindow = int(Configuration.get(\"nupic.opf.metricWindow\"))\n\n  metricSpecStrings = []\n  optimizeMetricLabel = \"\"\n\n  # -----------------------------------------------------------------------\n  # Generate the metrics specified by the expGenerator paramters\n  metricSpecStrings.extend(_generateExtraMetricSpecs(options))\n\n  # -----------------------------------------------------------------------\n\n  optimizeMetricSpec = None\n  # If using a dynamically computed prediction steps (i.e. when swarming\n  #  over aggregation is requested), then we will plug in the variable\n  #  predictionSteps in place of the statically provided predictionSteps\n  #  from the JSON description.\n  if options['dynamicPredictionSteps']:\n    assert len(predictionSteps) == 1\n    predictionSteps = ['$REPLACE_ME']\n\n  # -----------------------------------------------------------------------\n  # Metrics for temporal prediction\n  if inferenceType in (InferenceType.TemporalNextStep,\n                       InferenceType.TemporalAnomaly,\n                       InferenceType.TemporalMultiStep,\n                       InferenceType.NontemporalMultiStep,\n                       InferenceType.NontemporalClassification,\n                       'MultiStep'):\n\n    predictedFieldName, predictedFieldType = _getPredictedField(options)\n    isCategory = _isCategory(predictedFieldType)\n    metricNames = ('avg_err',) if isCategory else ('aae', 'altMAPE')\n    trivialErrorMetric = 'avg_err' if isCategory else 'altMAPE'\n    oneGramErrorMetric = 'avg_err' if isCategory else 'altMAPE'\n    movingAverageBaselineName = 'moving_mode' if isCategory else 'moving_mean'\n\n    # Multi-step metrics\n    for metricName in metricNames:\n      metricSpec, metricLabel = \\\n        _generateMetricSpecString(field=predictedFieldName,\n                 inferenceElement=InferenceElement.multiStepBestPredictions,\n                 metric='multiStep',\n                 params={'errorMetric': metricName,\n                               'window':metricWindow,\n                               'steps': predictionSteps},\n                 returnLabel=True)\n      metricSpecStrings.append(metricSpec)\n\n    # If the custom error metric was specified, add that\n    if options[\"customErrorMetric\"] is not None :\n      metricParams = dict(options[\"customErrorMetric\"])\n      metricParams['errorMetric'] = 'custom_error_metric'\n      metricParams['steps'] = predictionSteps\n      # If errorWindow is not specified, make it equal to the default window\n      if not \"errorWindow\" in metricParams:\n        metricParams[\"errorWindow\"] = metricWindow\n      metricSpec, metricLabel =_generateMetricSpecString(field=predictedFieldName,\n                   inferenceElement=InferenceElement.multiStepPredictions,\n                   metric=\"multiStep\",\n                   params=metricParams,\n                   returnLabel=True)\n      metricSpecStrings.append(metricSpec)\n\n    # If this is the first specified step size, optimize for it. Be sure to\n    #  escape special characters since this is a regular expression\n    optimizeMetricSpec = metricSpec\n    metricLabel = metricLabel.replace('[', '\\\\[')\n    metricLabel = metricLabel.replace(']', '\\\\]')\n    optimizeMetricLabel = metricLabel\n\n    if options[\"customErrorMetric\"] is not None :\n      optimizeMetricLabel = \".*custom_error_metric.*\"\n\n    # Add in the trivial metrics\n    if options[\"runBaselines\"] \\\n          and inferenceType != InferenceType.NontemporalClassification:\n      for steps in predictionSteps:\n        metricSpecStrings.append(\n          _generateMetricSpecString(field=predictedFieldName,\n                                    inferenceElement=InferenceElement.prediction,\n                                    metric=\"trivial\",\n                                    params={'window':metricWindow,\n                                                  \"errorMetric\":trivialErrorMetric,\n                                                  'steps': steps})\n          )\n\n        ##Add in the One-Gram baseline error metric\n        #metricSpecStrings.append(\n        #  _generateMetricSpecString(field=predictedFieldName,\n        #                            inferenceElement=InferenceElement.encodings,\n        #                            metric=\"two_gram\",\n        #                            params={'window':metricWindow,\n        #                                          \"errorMetric\":oneGramErrorMetric,\n        #                                          'predictionField':predictedFieldName,\n        #                                          'steps': steps})\n        #  )\n        #\n        #Include the baseline moving mean/mode metric\n        if isCategory:\n          metricSpecStrings.append(\n            _generateMetricSpecString(field=predictedFieldName,\n                                      inferenceElement=InferenceElement.prediction,\n                                      metric=movingAverageBaselineName,\n                                      params={'window':metricWindow\n                                                    ,\"errorMetric\":\"avg_err\",\n                                                    \"mode_window\":200,\n                                                    \"steps\": steps})\n            )\n        else :\n          metricSpecStrings.append(\n            _generateMetricSpecString(field=predictedFieldName,\n                                      inferenceElement=InferenceElement.prediction,\n                                      metric=movingAverageBaselineName,\n                                      params={'window':metricWindow\n                                                    ,\"errorMetric\":\"altMAPE\",\n                                                    \"mean_window\":200,\n                                                    \"steps\": steps})\n            )\n\n\n\n\n  # -----------------------------------------------------------------------\n  # Metrics for classification\n  elif inferenceType in (InferenceType.TemporalClassification):\n\n    metricName = 'avg_err'\n    trivialErrorMetric = 'avg_err'\n    oneGramErrorMetric = 'avg_err'\n    movingAverageBaselineName = 'moving_mode'\n\n    optimizeMetricSpec, optimizeMetricLabel = \\\n      _generateMetricSpecString(inferenceElement=InferenceElement.classification,\n                               metric=metricName,\n                               params={'window':metricWindow},\n                               returnLabel=True)\n\n    metricSpecStrings.append(optimizeMetricSpec)\n\n    if options[\"runBaselines\"]:\n      # If temporal, generate the trivial predictor metric\n      if inferenceType == InferenceType.TemporalClassification:\n        metricSpecStrings.append(\n          _generateMetricSpecString(inferenceElement=InferenceElement.classification,\n                                    metric=\"trivial\",\n                                    params={'window':metricWindow,\n                                                  \"errorMetric\":trivialErrorMetric})\n          )\n        metricSpecStrings.append(\n          _generateMetricSpecString(inferenceElement=InferenceElement.classification,\n                                    metric=\"two_gram\",\n                                    params={'window':metricWindow,\n                                                  \"errorMetric\":oneGramErrorMetric})\n          )\n        metricSpecStrings.append(\n          _generateMetricSpecString(inferenceElement=InferenceElement.classification,\n                                    metric=movingAverageBaselineName,\n                                    params={'window':metricWindow\n                                                  ,\"errorMetric\":\"avg_err\",\n                                                  \"mode_window\":200})\n          )\n\n\n    # Custom Error Metric\n    if not options[\"customErrorMetric\"] == None :\n      #If errorWindow is not specified, make it equal to the default window\n      if not \"errorWindow\" in options[\"customErrorMetric\"]:\n        options[\"customErrorMetric\"][\"errorWindow\"] = metricWindow\n      optimizeMetricSpec = _generateMetricSpecString(\n                                inferenceElement=InferenceElement.classification,\n                                metric=\"custom\",\n                                params=options[\"customErrorMetric\"])\n      optimizeMetricLabel = \".*custom_error_metric.*\"\n\n      metricSpecStrings.append(optimizeMetricSpec)\n\n\n  # -----------------------------------------------------------------------\n  # If plug in the predictionSteps variable for any dynamically generated\n  #  prediction steps\n  if options['dynamicPredictionSteps']:\n    for i in range(len(metricSpecStrings)):\n      metricSpecStrings[i] = metricSpecStrings[i].replace(\n          \"'$REPLACE_ME'\", \"predictionSteps\")\n    optimizeMetricLabel = optimizeMetricLabel.replace(\n        \"'$REPLACE_ME'\", \".*\")\n  return metricSpecStrings, optimizeMetricLabel",
    "doc": "Generates the Metrics for a given InferenceType\n\n  Parameters:\n  -------------------------------------------------------------------------\n  options: ExpGenerator options\n  retval: (metricsList, optimizeMetricLabel)\n            metricsList: list of metric string names\n            optimizeMetricLabel: Name of the metric which to optimize over"
  },
  {
    "code": "def _generateExtraMetricSpecs(options):\n  \"\"\"Generates the non-default metrics specified by the expGenerator params \"\"\"\n  _metricSpecSchema = {'properties': {}}\n\n  results = []\n  for metric in options['metrics']:\n\n    for propertyName in _metricSpecSchema['properties'].keys():\n      _getPropertyValue(_metricSpecSchema, propertyName, metric)\n\n\n    specString, label = _generateMetricSpecString(\n                                          field=metric['field'],\n                                          metric=metric['metric'],\n                                          params=metric['params'],\n                                          inferenceElement=\\\n                                                    metric['inferenceElement'],\n                                          returnLabel=True)\n    if metric['logged']:\n      options['loggedMetrics'].append(label)\n\n    results.append(specString)\n\n  return results",
    "doc": "Generates the non-default metrics specified by the expGenerator params"
  },
  {
    "code": "def _getPredictedField(options):\n  \"\"\" Gets the predicted field and it's datatype from the options dictionary\n\n  Returns: (predictedFieldName, predictedFieldType)\n  \"\"\"\n  if not options['inferenceArgs'] or \\\n      not options['inferenceArgs']['predictedField']:\n    return None, None\n\n  predictedField = options['inferenceArgs']['predictedField']\n  predictedFieldInfo = None\n  includedFields = options['includedFields']\n\n  for info in includedFields:\n    if info['fieldName'] == predictedField:\n      predictedFieldInfo = info\n      break\n\n  if predictedFieldInfo is None:\n    raise ValueError(\n      \"Predicted field '%s' does not exist in included fields.\" % predictedField\n    )\n  predictedFieldType = predictedFieldInfo['fieldType']\n\n  return predictedField, predictedFieldType",
    "doc": "Gets the predicted field and it's datatype from the options dictionary\n\n  Returns: (predictedFieldName, predictedFieldType)"
  },
  {
    "code": "def _generateInferenceArgs(options, tokenReplacements):\n  \"\"\" Generates the token substitutions related to the predicted field\n  and the supplemental arguments for prediction\n  \"\"\"\n  inferenceType = options['inferenceType']\n  optionInferenceArgs = options.get('inferenceArgs', None)\n  resultInferenceArgs = {}\n  predictedField = _getPredictedField(options)[0]\n\n  if inferenceType in (InferenceType.TemporalNextStep,\n                       InferenceType.TemporalAnomaly):\n    assert predictedField,  \"Inference Type '%s' needs a predictedField \"\\\n                            \"specified in the inferenceArgs dictionary\"\\\n                            % inferenceType\n\n  if optionInferenceArgs:\n    # If we will be using a dynamically created predictionSteps, plug in that\n    #  variable name in place of the constant scalar value\n    if options['dynamicPredictionSteps']:\n      altOptionInferenceArgs = copy.deepcopy(optionInferenceArgs)\n      altOptionInferenceArgs['predictionSteps'] = '$REPLACE_ME'\n      resultInferenceArgs = pprint.pformat(altOptionInferenceArgs)\n      resultInferenceArgs = resultInferenceArgs.replace(\"'$REPLACE_ME'\",\n                                                        '[predictionSteps]')\n    else:\n      resultInferenceArgs = pprint.pformat(optionInferenceArgs)\n\n  tokenReplacements['\\$INFERENCE_ARGS'] = resultInferenceArgs\n\n  tokenReplacements['\\$PREDICTION_FIELD'] = predictedField",
    "doc": "Generates the token substitutions related to the predicted field\n  and the supplemental arguments for prediction"
  },
  {
    "code": "def expGenerator(args):\n  \"\"\" Parses, validates, and executes command-line options;\n\n  On success: Performs requested operation and exits program normally\n\n  On Error:   Dumps exception/error info in JSON format to stdout and exits the\n              program with non-zero status.\n  \"\"\"\n\n  # -----------------------------------------------------------------\n  # Parse command line options\n  #\n  parser = OptionParser()\n  parser.set_usage(\"%prog [options] --description='{json object with args}'\\n\" + \\\n                   \"%prog [options] --descriptionFromFile='{filename}'\\n\" + \\\n                   \"%prog [options] --showSchema\")\n\n  parser.add_option(\"--description\", dest = \"description\",\n    help = \"Tells ExpGenerator to generate an experiment description.py and \" \\\n           \"permutations.py file using the given JSON formatted experiment \"\\\n           \"description string.\")\n\n  parser.add_option(\"--descriptionFromFile\", dest = 'descriptionFromFile',\n    help = \"Tells ExpGenerator to open the given filename and use it's \" \\\n           \"contents as the JSON formatted experiment description.\")\n\n  parser.add_option(\"--claDescriptionTemplateFile\",\n    dest = 'claDescriptionTemplateFile',\n    default = 'claDescriptionTemplate.tpl',\n    help = \"The file containing the template description file for \" \\\n           \" ExpGenerator [default: %default]\")\n\n  parser.add_option(\"--showSchema\",\n                    action=\"store_true\", dest=\"showSchema\",\n                    help=\"Prints the JSON schemas for the --description arg.\")\n\n  parser.add_option(\"--version\", dest = 'version', default='v2',\n    help = \"Generate the permutations file for this version of hypersearch.\"\n            \" Possible choices are 'v1' and 'v2' [default: %default].\")\n\n  parser.add_option(\"--outDir\",\n                    dest = \"outDir\", default=None,\n                    help = \"Where to generate experiment. If not specified, \" \\\n                           \"then a temp directory will be created\"\n                    )\n  (options, remainingArgs) = parser.parse_args(args)\n\n  #print(\"OPTIONS=%s\" % (str(options)))\n\n  # -----------------------------------------------------------------\n  # Check for unprocessed args\n  #\n  if len(remainingArgs) > 0:\n    raise _InvalidCommandArgException(\n      _makeUsageErrorStr(\"Unexpected command-line args: <%s>\" % \\\n                         (' '.join(remainingArgs),), parser.get_usage()))\n\n  # -----------------------------------------------------------------\n  # Check for use of mutually-exclusive options\n  #\n  activeOptions = filter(lambda x: getattr(options, x) != None,\n                         ('description', 'showSchema'))\n  if len(activeOptions) > 1:\n    raise _InvalidCommandArgException(\n      _makeUsageErrorStr((\"The specified command options are \" + \\\n                          \"mutually-exclusive: %s\") % (activeOptions,),\n                          parser.get_usage()))\n\n\n\n  # -----------------------------------------------------------------\n  # Process requests\n  #\n  if options.showSchema:\n    _handleShowSchemaOption()\n\n  elif options.description:\n    _handleDescriptionOption(options.description, options.outDir,\n           parser.get_usage(), hsVersion=options.version,\n           claDescriptionTemplateFile = options.claDescriptionTemplateFile)\n\n  elif options.descriptionFromFile:\n    _handleDescriptionFromFileOption(options.descriptionFromFile,\n          options.outDir, parser.get_usage(), hsVersion=options.version,\n          claDescriptionTemplateFile = options.claDescriptionTemplateFile)\n\n  else:\n    raise _InvalidCommandArgException(\n      _makeUsageErrorStr(\"Error in validating command options. No option \"\n                         \"provided:\\n\", parser.get_usage()))",
    "doc": "Parses, validates, and executes command-line options;\n\n  On success: Performs requested operation and exits program normally\n\n  On Error:   Dumps exception/error info in JSON format to stdout and exits the\n              program with non-zero status."
  },
  {
    "code": "def parseTimestamp(s):\n  \"\"\"\n  Parses a textual datetime format and return a Python datetime object.\n\n  The supported format is: ``yyyy-mm-dd h:m:s.ms``\n\n  The time component is optional.\n\n  - hours are 00..23 (no AM/PM)\n  - minutes are 00..59\n  - seconds are 00..59\n  - micro-seconds are 000000..999999\n\n  :param s: (string) input time text\n  :return: (datetime.datetime)\n  \"\"\"\n  s = s.strip()\n  for pattern in DATETIME_FORMATS:\n    try:\n      return datetime.datetime.strptime(s, pattern)\n    except ValueError:\n      pass\n  raise ValueError('The provided timestamp %s is malformed. The supported '\n                   'formats are: [%s]' % (s, ', '.join(DATETIME_FORMATS)))",
    "doc": "Parses a textual datetime format and return a Python datetime object.\n\n  The supported format is: ``yyyy-mm-dd h:m:s.ms``\n\n  The time component is optional.\n\n  - hours are 00..23 (no AM/PM)\n  - minutes are 00..59\n  - seconds are 00..59\n  - micro-seconds are 000000..999999\n\n  :param s: (string) input time text\n  :return: (datetime.datetime)"
  },
  {
    "code": "def parseBool(s):\n  \"\"\"\n  String to boolean\n\n  :param s: (string)\n  :return: (bool)\n  \"\"\"\n  l = s.lower()\n  if l in (\"true\", \"t\", \"1\"):\n    return True\n  if l in (\"false\", \"f\", \"0\"):\n    return False\n  raise Exception(\"Unable to convert string '%s' to a boolean value\" % s)",
    "doc": "String to boolean\n\n  :param s: (string)\n  :return: (bool)"
  },
  {
    "code": "def escape(s):\n  \"\"\"\n  Escape commas, tabs, newlines and dashes in a string\n\n  Commas are encoded as tabs.\n\n  :param s: (string) to escape\n  :returns: (string) escaped string\n  \"\"\"\n  if s is None:\n    return ''\n\n  assert isinstance(s, basestring), \\\n        \"expected %s but got %s; value=%s\" % (basestring, type(s), s)\n  s = s.replace('\\\\', '\\\\\\\\')\n  s = s.replace('\\n', '\\\\n')\n  s = s.replace('\\t', '\\\\t')\n  s = s.replace(',', '\\t')\n  return s",
    "doc": "Escape commas, tabs, newlines and dashes in a string\n\n  Commas are encoded as tabs.\n\n  :param s: (string) to escape\n  :returns: (string) escaped string"
  },
  {
    "code": "def unescape(s):\n  \"\"\"\n  Unescapes a string that may contain commas, tabs, newlines and dashes\n\n  Commas are decoded from tabs.\n\n  :param s: (string) to unescape\n  :returns: (string) unescaped string\n  \"\"\"\n  assert isinstance(s, basestring)\n  s = s.replace('\\t', ',')\n  s = s.replace('\\\\,', ',')\n  s = s.replace('\\\\n', '\\n')\n  s = s.replace('\\\\\\\\', '\\\\')\n\n  return s",
    "doc": "Unescapes a string that may contain commas, tabs, newlines and dashes\n\n  Commas are decoded from tabs.\n\n  :param s: (string) to unescape\n  :returns: (string) unescaped string"
  },
  {
    "code": "def parseSdr(s):\n  \"\"\"\n  Parses a string containing only 0's and 1's and return a Python list object.\n\n  :param s: (string) string to parse\n  :returns: (list) SDR out\n  \"\"\"\n  assert isinstance(s, basestring)\n  sdr = [int(c) for c in s if c in (\"0\", \"1\")]\n  if len(sdr) != len(s):\n    raise ValueError(\"The provided string %s is malformed. The string should \"\n                     \"have only 0's and 1's.\")\n\n  return sdr",
    "doc": "Parses a string containing only 0's and 1's and return a Python list object.\n\n  :param s: (string) string to parse\n  :returns: (list) SDR out"
  },
  {
    "code": "def parseStringList(s):\n  \"\"\"\n  Parse a string of space-separated numbers, returning a Python list.\n\n  :param s: (string) to parse\n  :returns: (list) binary SDR\n  \"\"\"\n  assert isinstance(s, basestring)\n  return [int(i) for i in s.split()]",
    "doc": "Parse a string of space-separated numbers, returning a Python list.\n\n  :param s: (string) to parse\n  :returns: (list) binary SDR"
  },
  {
    "code": "def coordinatesFromIndex(index, dimensions):\n  \"\"\"\n  Translate an index into coordinates, using the given coordinate system.\n\n  Similar to ``numpy.unravel_index``.\n\n  :param index: (int) The index of the point. The coordinates are expressed as a \n         single index by using the dimensions as a mixed radix definition. For \n         example, in dimensions 42x10, the point [1, 4] is index \n         1*420 + 4*10 = 460.\n\n  :param dimensions (list of ints) The coordinate system.\n\n  :returns: (list) of coordinates of length ``len(dimensions)``.\n  \"\"\"\n  coordinates = [0] * len(dimensions)\n\n  shifted = index\n  for i in xrange(len(dimensions) - 1, 0, -1):\n    coordinates[i] = shifted % dimensions[i]\n    shifted = shifted / dimensions[i]\n\n  coordinates[0] = shifted\n\n  return coordinates",
    "doc": "Translate an index into coordinates, using the given coordinate system.\n\n  Similar to ``numpy.unravel_index``.\n\n  :param index: (int) The index of the point. The coordinates are expressed as a \n         single index by using the dimensions as a mixed radix definition. For \n         example, in dimensions 42x10, the point [1, 4] is index \n         1*420 + 4*10 = 460.\n\n  :param dimensions (list of ints) The coordinate system.\n\n  :returns: (list) of coordinates of length ``len(dimensions)``."
  },
  {
    "code": "def indexFromCoordinates(coordinates, dimensions):\n  \"\"\"\n  Translate coordinates into an index, using the given coordinate system.\n\n  Similar to ``numpy.ravel_multi_index``.\n\n  :param coordinates: (list of ints) A list of coordinates of length \n         ``dimensions.size()``.\n\n  :param dimensions: (list of ints) The coordinate system.\n\n  :returns: (int) The index of the point. The coordinates are expressed as a \n            single index by using the dimensions as a mixed radix definition. \n            For example, in dimensions 42x10, the point [1, 4] is index \n            1*420 + 4*10 = 460.\n  \"\"\"\n  index = 0\n  for i, dimension in enumerate(dimensions):\n    index *= dimension\n    index += coordinates[i]\n\n  return index",
    "doc": "Translate coordinates into an index, using the given coordinate system.\n\n  Similar to ``numpy.ravel_multi_index``.\n\n  :param coordinates: (list of ints) A list of coordinates of length \n         ``dimensions.size()``.\n\n  :param dimensions: (list of ints) The coordinate system.\n\n  :returns: (int) The index of the point. The coordinates are expressed as a \n            single index by using the dimensions as a mixed radix definition. \n            For example, in dimensions 42x10, the point [1, 4] is index \n            1*420 + 4*10 = 460."
  },
  {
    "code": "def neighborhood(centerIndex, radius, dimensions):\n  \"\"\"\n  Get the points in the neighborhood of a point.\n\n  A point's neighborhood is the n-dimensional hypercube with sides ranging\n  [center - radius, center + radius], inclusive. For example, if there are two\n  dimensions and the radius is 3, the neighborhood is 6x6. Neighborhoods are\n  truncated when they are near an edge.\n\n  This is designed to be fast. In C++ it's fastest to iterate through neighbors\n  one by one, calculating them on-demand rather than creating a list of them.\n  But in Python it's faster to build up the whole list in batch via a few calls\n  to C code rather than calculating them on-demand with lots of calls to Python\n  code.\n\n  :param centerIndex: (int) The index of the point. The coordinates are \n         expressed as a single index by using the dimensions as a mixed radix \n         definition. For example, in dimensions 42x10, the point [1, 4] is index \n         1*420 + 4*10 = 460.\n\n  :param radius: (int) The radius of this neighborhood about the \n         ``centerIndex``.\n\n  :param dimensions: (indexable sequence) The dimensions of the world outside \n         this neighborhood.\n\n  :returns: (numpy array) The points in the neighborhood, including \n            ``centerIndex``.\n  \"\"\"\n  centerPosition = coordinatesFromIndex(centerIndex, dimensions)\n\n  intervals = []\n  for i, dimension in enumerate(dimensions):\n    left = max(0, centerPosition[i] - radius)\n    right = min(dimension - 1, centerPosition[i] + radius)\n    intervals.append(xrange(left, right + 1))\n\n  coords = numpy.array(list(itertools.product(*intervals)))\n  return numpy.ravel_multi_index(coords.T, dimensions)",
    "doc": "Get the points in the neighborhood of a point.\n\n  A point's neighborhood is the n-dimensional hypercube with sides ranging\n  [center - radius, center + radius], inclusive. For example, if there are two\n  dimensions and the radius is 3, the neighborhood is 6x6. Neighborhoods are\n  truncated when they are near an edge.\n\n  This is designed to be fast. In C++ it's fastest to iterate through neighbors\n  one by one, calculating them on-demand rather than creating a list of them.\n  But in Python it's faster to build up the whole list in batch via a few calls\n  to C code rather than calculating them on-demand with lots of calls to Python\n  code.\n\n  :param centerIndex: (int) The index of the point. The coordinates are \n         expressed as a single index by using the dimensions as a mixed radix \n         definition. For example, in dimensions 42x10, the point [1, 4] is index \n         1*420 + 4*10 = 460.\n\n  :param radius: (int) The radius of this neighborhood about the \n         ``centerIndex``.\n\n  :param dimensions: (indexable sequence) The dimensions of the world outside \n         this neighborhood.\n\n  :returns: (numpy array) The points in the neighborhood, including \n            ``centerIndex``."
  },
  {
    "code": "def encodeIntoArray(self, inputData, output):\n    \"\"\"\n    See `nupic.encoders.base.Encoder` for more information.\n\n    @param inputData (tuple) Contains coordinate (numpy.array, N-dimensional\n                             integer coordinate) and radius (int)\n    @param output (numpy.array) Stores encoded SDR in this numpy array\n    \"\"\"\n    (coordinate, radius) = inputData\n\n    assert isinstance(radius, int), (\"Expected integer radius, got: {} ({})\"\n                                     .format(radius, type(radius)))\n\n    neighbors = self._neighbors(coordinate, radius)\n    winners = self._topWCoordinates(neighbors, self.w)\n\n    bitFn = lambda coordinate: self._bitForCoordinate(coordinate, self.n)\n    indices = numpy.array([bitFn(w) for w in winners])\n\n    output[:] = 0\n    output[indices] = 1",
    "doc": "See `nupic.encoders.base.Encoder` for more information.\n\n    @param inputData (tuple) Contains coordinate (numpy.array, N-dimensional\n                             integer coordinate) and radius (int)\n    @param output (numpy.array) Stores encoded SDR in this numpy array"
  },
  {
    "code": "def _neighbors(coordinate, radius):\n    \"\"\"\n    Returns coordinates around given coordinate, within given radius.\n    Includes given coordinate.\n\n    @param coordinate (numpy.array) N-dimensional integer coordinate\n    @param radius (int) Radius around `coordinate`\n\n    @return (numpy.array) List of coordinates\n    \"\"\"\n    ranges = (xrange(n-radius, n+radius+1) for n in coordinate.tolist())\n    return numpy.array(list(itertools.product(*ranges)))",
    "doc": "Returns coordinates around given coordinate, within given radius.\n    Includes given coordinate.\n\n    @param coordinate (numpy.array) N-dimensional integer coordinate\n    @param radius (int) Radius around `coordinate`\n\n    @return (numpy.array) List of coordinates"
  }
]